[{"content":"cowsay is a command line program written in Perl. The original version had a final release in 2016 (that\u0026rsquo;s the version of many installed cowsay programs) and there\u0026rsquo;s a number of forks of that release in Perl. There are also many many versions of cowsay in other programming languages, like the one I maintain written in R, unimaginatively called cowsay.\nI wrote about cowsay here back in 2014. I didn\u0026rsquo;t think this would ever be 300+ stars popular, but here we are. Given that people seem to actually use it - or at least star it - seems worth putting some more time into it.\nReturn to the source I just released v1 of cowsay. At a high level, the major thing in v1 is bringing it closer to the original cowsay. That doesn\u0026rsquo;t mean in how it\u0026rsquo;s used - you still use it within R, and pass arguments to a function rather than flags to a command line program. Instead, the output is as close as I could get to the original cowsay. This goal was spurred on by an issue - cough, sneeze - from 6 years ago.\nThe output of v1 is much closer to the original, for example:\nin R cowsay before v1:\n----- hello world ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || Now in v1:\n______________ \u0026lt; Hello world! \u0026gt; -------------- \\ \\ ^__^ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || in Perl cli cowsay\n______________ \u0026lt; Hello world! \u0026gt; -------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || By much closer I mean:\nInstead of just a top and bottom line there\u0026rsquo;s actually sides now. Fixed bubble top in GitHub main at least (see below note) The bubble expands with the text to contain it all within the bubble, for example: library(cowsay) library(fortunes) say(\u0026#34;fortune\u0026#34;) ________________________________________________________ / The problem, as always, is what the heck does one mean \\ | by \u0026#39;outlier\u0026#39; in these contexts. Seems to be like | | pornography -- \u0026#34;I know it when I see it.\u0026#34; Berton | | Gunter quoting Justice Potter Stewart in a discussion | \\ about tests for outliers R-help April 2005 / -------------------------------------------------------- \\ \\ ^__^ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || A few notes:\nI realized in drafting this post that original cowsay uses underscores for the top of the bubble and hyphens for the bottom of the bubble whereas R cowsay was using hyphens for top and bottom. I just pushed a fix for this, so to get underscores for the bubble top install from GitHub (pak::pak(\u0026quot;sckott/cowsay\u0026quot;)). With the refactoring of bubbles in v1, the \u0026ldquo;tail\u0026rdquo; is now above the animals b/c it was just easier that way. In a future version we\u0026rsquo;ll try to fix that to have the tail coming down farther like original cowsay. The other thing that brings R cowsay closer to og cowsay is having think(), which I hadn\u0026rsquo;t realized was a thing until finding the page in the Wayback Machine for the original cowsay. For example:\nlibrary(cowsay) library(fortunes) think(\u0026#34;fortune\u0026#34;) ________________________________________________________ ( Dear Uwe, thank you very much for your unvaluable time ) ( and effort. Javier Cano thanking Uwe Ligges for ) ( solving a coding problem R-help July 2009 ) -------------------------------------------------------- o o ^__^ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || think() differs from say() in having circles for the tail to the bubble and parens for the bubble sides rather than slashes.\nHand-rolled With v1 you can now hand roll cowsay output, for example:\nlibrary(cowsay) library(fortunes) quote \u0026lt;- as.character(fortune()) chicken \u0026lt;- animals[[\u0026#34;chicken\u0026#34;]] z \u0026lt;- paste( c(bubble_say(quote), bubble_tail(chicken, \u0026#34;\\\\\u0026#34;), chicken), collapse = \u0026#34;\\n\u0026#34; ) cat(z) _______________________________________________________ / This is a bit like asking how should I tweak my \\ | sailboat so I can explore the ocean floor. | | Roger Koenker | | in response to a question about tweaking the quantreg | | package to handle probit and heckit models | | R-help | \\ May 2013 / ------------------------------------------------------- \\ \\ _ _/ } `\u0026gt;\u0026#39; \\ `| \\ | /\u0026#39;-. .-. \\\u0026#39; \u0026#39;;`--\u0026#39; .\u0026#39; \\\u0026#39;. `\u0026#39;-./ \u0026#39;.`-..-;` `;-..\u0026#39; _| _| /` /` [nosig] A note about the refactored bubbles and tails: The tail horizontal position is now calculated based on the animal - so instead of always being in the same horizontal position, we attempt to place the tail close to the head of the animal.\nFin Have fun!\n","permalink":"http://localhost:1313/2024/12/cowsay-v1/","summary":"\u003cp\u003ecowsay is a command line program written in Perl. The original version had a final release in 2016 (that\u0026rsquo;s the version of many installed cowsay programs) and there\u0026rsquo;s a number of forks of that release in Perl. There are also many many versions of cowsay in other programming languages, like \u003ca href=\"https://github.com/sckott/cowsay/\"\u003ethe one I maintain\u003c/a\u003e written in R, unimaginatively called cowsay.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/2014/02/cowsay/\"\u003eI wrote about cowsay here back in 2014\u003c/a\u003e. I didn\u0026rsquo;t think this would ever be 300+ stars popular, but here we are. Given that people seem to actually use it - or at least star it - seems worth putting some more time into it.\u003c/p\u003e","title":"cowsay v1"},{"content":"At work I\u0026rsquo;ve been using Quarto quite a bit for website and books for work projects.\nOne of the projects I\u0026rsquo;ve been working on lately that uses Quarto is the WILDS Contributor Guide (WILDS = Workflows Integrating Large Data and Software). This guide (a Quarto book) is mostly a guide for our own immediate team members, but aims to a) be a guide for any contributors to our open source software work, and b) demonstrate good open source software practices for the greater Fred Hutch community where we work.\nWhenever there\u0026rsquo;s a group of people working on the same software, it can help to have some guidelines - or rules - for how software should be built. With any group there\u0026rsquo;s likely to be some aspects that are left up to the individual, whereas other aspects should be enforced rules.\nThere are many benefits to having everyone follow the same set of rules, including a predictable and consistent software building culture, and trust from the users of the software that the maintainers have reasonable guard rails (assuming those rules are transparent; see note below about Transparency ).\nFor the rules, how do we:\nenforce them with the least effort possible? this is not out of laziness for its own sake, but realizing that if it\u0026rsquo;s not easy it may not happen keep track of any changes in rules (in our case the WILDS Guide) so that what we say we do is what we actually do Searching around I haven\u0026rsquo;t found much out there that formalizes this. There\u0026rsquo;s some great transparent and documented stuff out there, e.g. Thoughtbot\u0026rsquo;s playbook - but they don\u0026rsquo;t describe how they check that their employees do what they say they should do.\nWhat we\u0026rsquo;re doing is the following:\nIn our WILDS Guide we have: Easy to find rules for humans \u0026hellip; that are defined with Machine readable rules for machines For rule compliance repos will have their on GitHub Actions running various things For automatable rule compliance across repos we\u0026rsquo;re using GitHub Actions. I\u0026rsquo;ve just started work on this automated compliance at https://github.com/getwilds/rules. You can see an example of one of the rule checks output For rule compliance that requires human review we\u0026rsquo;ll enfuse into the culture the actions that need to be taken This is very much a work in progress, and could be a fool\u0026rsquo;s errand. Maybe all this rule compliance stuff will make it too hard to get work done. Maybe it will just be too complicated and the work of doing all of this isn\u0026rsquo;t worth it; i.e., rule compliance isn\u0026rsquo;t the goal, but is just a tool to get our real work done and build trust in the community.\nI\u0026rsquo;d love to hear what\u0026rsquo;s working - and what\u0026rsquo;s not - for other folks. Holla at me on Mastodon or Bluesky.\nIf you\u0026rsquo;re interested in the details \u0026hellip;\nMachine readable The machine readable rules are defined using Quarto\u0026rsquo;s _variables.yml file, ours is at https://github.com/getwilds/guide/blob/main/_variables.yml. A snippet of it:\nversion: 1.2 rules: merge-main-release: \u0026gt; Every merge from `dev` into `main` should constitute a release, which should generate a tagged version of the software and an increment to the version number release-tags: Code releases that correspond to specific git tags. The great thing about this approach is that it\u0026rsquo;s a single file and it\u0026rsquo;s easy for a machine to read. So a machine can watch for any changes in this file to trigger any todo\u0026rsquo;s in other places.\nThat\u0026rsquo;s not how I started though. My first thought was child documents, thinking that it would make it clear to have a separate \u0026ldquo;rules\u0026rdquo; dir in the repo with a separate child callout in a qmd file for every rule. This looked like\n::: {.callout-note icon=false} ## {{ iconify carbon rule-draft }} Rule At least two but no more than three designated project leads (specified in the [CODEOWNERS file][codeowners]). ::: Appearance of rules I ended up using just simple Bootstrap badges with icons at the beginning of a rule to indicate that it\u0026rsquo;s a rule. I disregarded this at first as I thought it wasn\u0026rsquo;t grabbing the readers attention enough. But this allowed flexibility to have a rule be embedded within a paragraph or be on its own line in a bullet or not.\nHowever, my first gut feeling was to use alerts, or what Quarto calls \u0026ldquo;callouts\u0026rdquo;.\nThe landing page of our WILDS guide has a section describing the rules.\nTransparency The building trust part of my motivation above I think means that rules need to be transparent to users. If this is done well I think it makes our lives easier as well.\nWhat I mean in practice is the following. So I discussed how we\u0026rsquo;re using badges to indicate a rule in our guide above. One way to approach this is if you click on one of the badges it brings you to another page or github repo that has more details, including more words about how it\u0026rsquo;s implemented, and link to a automated GH Action that does the check or to docs about how a human review is done. This doesn\u0026rsquo;t exist yet.\n","permalink":"http://localhost:1313/2024/09/quarto-rules/","summary":"\u003cp\u003eAt \u003ca href=\"https://www.fredhutch.org/en.html\"\u003ework\u003c/a\u003e I\u0026rsquo;ve been using \u003ca href=\"https://quarto.org/docs/guide/\"\u003eQuarto\u003c/a\u003e quite a bit for website and books for work projects.\u003c/p\u003e\n\u003cp\u003eOne of the projects I\u0026rsquo;ve been working on lately that uses Quarto is the \u003ca href=\"http://getwilds.org/guide/\"\u003eWILDS Contributor Guide\u003c/a\u003e (WILDS = \u003cem\u003eWorkflows Integrating Large Data and Software\u003c/em\u003e). This guide (a Quarto book) is mostly a guide for our own immediate team members, but aims to a) be a guide for any contributors to our open source software work, and b) demonstrate good open source software practices for the greater \u003ca href=\"https://www.fredhutch.org/en.html\"\u003eFred Hutch\u003c/a\u003e community where we work.\u003c/p\u003e","title":"Software rules and Quarto"},{"content":"I worked on a refactor of an R package at work the other day. Here\u0026rsquo;s some notes about that after doing the work. This IS NOT a best practices post - it\u0026rsquo;s just a collection of thoughts.\nFor context, the package is an API client.\nIt made sense to break the work for any given exported function into the following components, as applicable depending on the endpoint being handled (some endpoints needed just a few lines of code, so those funtions were left unchanged):\nquery building http request (including error handling) http response handling Before this separation each exported function did all three of the above items. For example, before the change the single function with all the code is called fetch_items. After the separation we still have the exported function fetch_items, but within fetch_items are up to three functions (as applicable) that have distinct duties:\nfetch_items_query: prepare the http request components fetch_items_http: the http request handling, includes http status code checking/handling fetch_items_process: process the http response So code would be:\nfetch_items \u0026lt;- function(a, b, token) { request \u0026lt;- fetch_items_query(a, b) response \u0026lt;- fetch_items_http(request, token) fetch_items_process(response) } You may still need to do additional refactoring for the functions used inside of fetch_items. In fact, the functions that do processing of the http response (i.e., fetch_items_process) are sometimes pretty massive and need refactoring - BUT! are waiting on examples that will touch all the code paths - womp womp womp\u0026hellip;\nThis separation of concerns and code improves the package because:\nYou can iterate on tests faster for code that\u0026rsquo;s not doing http requests. For example, the response handling function can rapidly run through a lot of tests since it doesn\u0026rsquo;t have to wait on http requests - assuming you have responses cached in the package to run through it, which is easy enough You can still run fast tests on tests that do http requests if you use fixtures so you\u0026rsquo;re not doing real http request other than to record the fixtures, e.g. using package vcr Separating concerns makes the code easier to reason about. That is - assuming you have well named functions whose intent is clear - it\u0026rsquo;s easier to understand code flow, etc. Smaller functions are easier to understand. This is pretty straightforward, and not specific to any particular type of code. If there\u0026rsquo;s less going on in any one function it\u0026rsquo;s easier to make changes to a package. Breaking code down may reveal redundant code blocks that could be reused. For example, after pulling out code from different functions you might notice that you\u0026rsquo;re doing very similar tasks and can make a function that can be used across the exported functions rather than having repeated code. don\u0026rsquo;t forget about failing early I had to go back and make sure fail early code wasn\u0026rsquo;t lost in breaking up code into chunks. For example, if you are checking if a parameter is of an acceptable type, or some other critical piece is not correct/available, those things should be done first thing so the function fails early.\n","permalink":"http://localhost:1313/2024/03/refactoring-notes/","summary":"\u003cp\u003eI worked on a refactor of an R package at work the other day. Here\u0026rsquo;s some notes about that after doing the work. This IS NOT a best practices post - it\u0026rsquo;s just a collection of thoughts.\u003c/p\u003e\n\u003cp\u003eFor context, the package is an API client.\u003c/p\u003e\n\u003cp\u003eIt made sense to break the work for any given exported function into the following components, as applicable depending on the endpoint being handled (some endpoints needed just a few lines of code, so those funtions were left unchanged):\u003c/p\u003e","title":"Refactoring notes"},{"content":"This blog is now using Hugo.\nImportant - if you subscribe to the RSS for this blog you likely have to delete/remove the old one and add the new RSS link. It is:\nhttps://recology.info/index.xml\n","permalink":"http://localhost:1313/2024/03/move-to-hugo/","summary":"\u003cp\u003eThis blog is now using \u003ca href=\"https://gohugo.io\"\u003eHugo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eImportant - if you subscribe to the RSS for this blog you likely have to delete/remove the old one and add the new RSS link. It is:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://recology.info/index.xml\"\u003ehttps://recology.info/index.xml\u003c/a\u003e\u003c/p\u003e","title":"Moved to Hugo"},{"content":"I wrote the other day about overcoming an issue with Shiny.\nAnother issue I ran into concurrently was about file inputs. The issue was that file inputs (i.e., shiny::fileInput) was difficult to clear. That is, after a user uploads a file, it was easy to get some of the various parts cleared/cleaned up, but not others:\n(Not Easy) The UI components of fileInput (the text of the file name, the loading display) (Not Easy) The data behind the fileInput handler (Easy) Displaying some feedback in the UI after handling file input Load libraries\nlibrary(shiny) library(shinyjs) library(bslib) library(DT) library(vroom) A helper function to handle reactive inputs\nreactiveInput \u0026lt;- function(rval, path) { if (is.null(rval)) { return(NULL) } else if (rval == \u0026#39;loaded\u0026#39;) { return(path) } else if (rval == \u0026#39;reset\u0026#39;) { return(NULL) } } A bslib ui component\nui \u0026lt;- page_sidebar( title = \u0026#34;My dashboard\u0026#34;, sidebar = list( actionButton(\u0026#34;submit\u0026#34;, \u0026#34;Submit\u0026#34;), actionButton(\u0026#34;reset\u0026#34;, \u0026#34;Reset\u0026#34;) ), fileInput(inputId = \u0026#34;afile\u0026#34;, \u0026#34;Upload file\u0026#34;, accept = \u0026#34;.csv\u0026#34;), DT::DTOutput(\u0026#34;result\u0026#34;), shinyjs::useShinyjs() ) Here\u0026rsquo;s the server part that was giving me trouble.\nserver \u0026lt;- function(input, output) { observeEvent(input$submit, { output$result \u0026lt;- DT::renderDataTable({ dat \u0026lt;- vroom::vroom( isolate(input$afile$datapath) ) DT::datatable(dat) }) }) observeEvent(input$reset, { shinyjs::reset(\u0026#34;afile\u0026#34;) output$result \u0026lt;- renderText({}) }) } With shinyjs::reset we can reset the UI components of the file input handler, and with renderText with an empty input we can reset the UI feedback. However, the data backing the file input handler is not reset. This led to problems in the UI where you could keep pressing submit after clicking the Reset button because the data for the last uploaded file was still there, whereas the user should get an error that they need to upload a file before clicking Submit.\nTo be able to completey reset data behind the file input handler I found out about a solution using reactive values via stackoverflow. Basically, the change involves handling file input data through a reactive value and keeping track of the state of the file input loader.\nHere\u0026rsquo;s the entire app that doesn\u0026rsquo;t work\nClick to expand library(shiny) library(shinyjs) library(bslib) library(DT) library(vroom) ui \u0026lt;- page_sidebar( title = \u0026#34;My dashboard\u0026#34;, sidebar = list( actionButton(\u0026#34;submit\u0026#34;, \u0026#34;Submit\u0026#34;), actionButton(\u0026#34;reset\u0026#34;, \u0026#34;Reset\u0026#34;) ), fileInput(inputId = \u0026#34;afile\u0026#34;, \u0026#34;Upload file\u0026#34;, accept = \u0026#34;.csv\u0026#34;), DT::DTOutput(\u0026#34;result\u0026#34;), shinyjs::useShinyjs() ) server \u0026lt;- function(input, output) { observeEvent(input$submit, { output$result \u0026lt;- DT::renderDataTable({ dat \u0026lt;- vroom::vroom( isolate(input$afile$datapath) ) DT::datatable(dat) }) }) observeEvent(input$reset, { shinyjs::reset(\u0026#34;afile\u0026#34;) output$result \u0026lt;- renderText({}) }) } shinyApp(ui, server) And here\u0026rsquo;s the entire app that does work\nClick to expand library(shiny) library(shinyjs) library(bslib) library(DT) library(vroom) reactiveInput \u0026lt;- function(rval, path) { if (is.null(rval)) { return(NULL) } else if (rval == \u0026#39;loaded\u0026#39;) { return(path) } else if (rval == \u0026#39;reset\u0026#39;) { return(NULL) } } ui \u0026lt;- page_sidebar( title = \u0026#34;My dashboard\u0026#34;, sidebar = list( actionButton(\u0026#34;submit\u0026#34;, \u0026#34;Submit\u0026#34;), actionButton(\u0026#34;reset\u0026#34;, \u0026#34;Reset\u0026#34;) ), fileInput(inputId = \u0026#34;afile\u0026#34;, \u0026#34;Upload file\u0026#34;, accept = \u0026#34;.csv\u0026#34;), DT::DTOutput(\u0026#34;result\u0026#34;), shinyjs::useShinyjs() ) server \u0026lt;- function(input, output) { rv_file \u0026lt;- reactiveValues(file_state = NULL) thefile \u0026lt;- reactive({ reactiveInput(rv_file$file_state, input$afile$datapath) }) observeEvent(input$afile, { rv_file$file_state \u0026lt;- \u0026#39;loaded\u0026#39; }) observeEvent(input$submit, { output$result \u0026lt;- DT::renderDataTable({ dat \u0026lt;- vroom::vroom( isolate(thefile()) ) DT::datatable(dat) }) }) observeEvent(input$reset, { shinyjs::reset(\u0026#34;afile\u0026#34;) rv_file$file_state \u0026lt;- \u0026#39;reset\u0026#39; output$result \u0026lt;- renderText({}) }) } shinyApp(ui, server) ","permalink":"http://localhost:1313/2024/03/shiny-file-inputs/","summary":"\u003cp\u003eI \u003ca href=\"/2024/03/shiny-events/\"\u003ewrote the other day\u003c/a\u003e about overcoming an issue with \u003ca href=\"https://shiny.posit.co/\"\u003eShiny\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAnother issue I ran into concurrently was about file inputs. The issue was that file inputs (i.e., \u003ccode\u003eshiny::fileInput\u003c/code\u003e) was difficult to clear. That is, after a user uploads a file, it was easy to get some of the various parts cleared/cleaned up, but not others:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(Not Easy) The UI components of \u003ccode\u003efileInput\u003c/code\u003e (the text of the file name, the loading display)\u003c/li\u003e\n\u003cli\u003e(Not Easy) The data behind the \u003ccode\u003efileInput\u003c/code\u003e handler\u003c/li\u003e\n\u003cli\u003e(Easy) Displaying some feedback in the UI after handling file input\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLoad libraries\u003c/p\u003e","title":"Shiny file inputs"},{"content":"I\u0026rsquo;ve been working on Shiny app at work for the past few months. One of the many frustrating things about Shiny lately has been around buttons. Well, it wasn\u0026rsquo;t really about buttons, but that\u0026rsquo;s where it started.\nLoad libraries\nlibrary(shiny) library(bslib) library(crul) Helper function, returned a random UUID from an httpbin server\nhttpbin_uuid \u0026lt;- function(...) { con \u0026lt;- crul::HttpClient$new(\u0026#34;https://hb.opencpu.org\u0026#34;) res \u0026lt;- con$get(\u0026#34;uuid\u0026#34;) jsonlite::fromJSON(res$parse(\u0026#34;UTF-8\u0026#34;))$uuid } A bslib ui component\nui \u0026lt;- page_sidebar( title = \u0026#34;My dashboard\u0026#34;, sidebar = list( actionButton(\u0026#34;submit\u0026#34;, \u0026#34;Submit\u0026#34;), actionButton(\u0026#34;reset\u0026#34;, \u0026#34;Reset\u0026#34;) ), textInput(inputId = \u0026#34;name\u0026#34;, \u0026#34;Your name\u0026#34;), textOutput(\u0026#34;uuid\u0026#34;) ) Here\u0026rsquo;s the server part that was giving me trouble. As I said this was an inherited repo, and the server side handling for many buttons was done with eventReactive as below. Using eventReactive meant that button clicks only sometimes triggered the server side code.\nserver \u0026lt;- function(input, output, session) { tmp \u0026lt;- eventReactive(input$submit, { httpbin_uuid(input$name) }) output$uuid \u0026lt;- renderText({ tmp() }) observeEvent(input$reset, { updateTextInput(session, \u0026#34;name\u0026#34;, \u0026#34;Your name\u0026#34;, \u0026#34;\u0026#34;) output$uuid \u0026lt;- renderText({}) }) } Eventually I landed upon switching from eventReactive to observeEvent for a variety of reasons. And tried something like this:\nobserveEvent(input$submit, { output$uuid \u0026lt;- renderText({ httpbin_uuid(input$name) }) }) However, keen eyes will notice that this still doesn\u0026rsquo;t work. The final missing piece was the function isolate. Without isolate the observeEvent handler was being triggered on changes other than just a button click.\nobserveEvent(input$submit, { output$uuid \u0026lt;- renderText({ isolate( httpbin_uuid(input$name) ) }) }) Here\u0026rsquo;s the entire app with eventReactive that didn\u0026rsquo;t work:\nClick to expand library(shiny) library(bslib) library(crul) httpbin_uuid \u0026lt;- function(...) { con \u0026lt;- crul::HttpClient$new(\u0026#34;https://hb.opencpu.org\u0026#34;) res \u0026lt;- con$get(\u0026#34;uuid\u0026#34;) jsonlite::fromJSON(res$parse(\u0026#34;UTF-8\u0026#34;))$uuid } ui \u0026lt;- page_sidebar( title = \u0026#34;My dashboard\u0026#34;, sidebar = list( actionButton(\u0026#34;submit\u0026#34;, \u0026#34;Submit\u0026#34;), actionButton(\u0026#34;reset\u0026#34;, \u0026#34;Reset\u0026#34;) ), textInput(inputId = \u0026#34;name\u0026#34;, \u0026#34;Your name\u0026#34;), textOutput(\u0026#34;uuid\u0026#34;) ) server \u0026lt;- function(input, output, session) { tmp \u0026lt;- eventReactive(input$submit, { httpbin_uuid(input$name) }) output$uuid \u0026lt;- renderText({ tmp() }) observeEvent(input$reset, { updateTextInput(session, \u0026#34;name\u0026#34;, \u0026#34;Your name\u0026#34;, \u0026#34;\u0026#34;) output$uuid \u0026lt;- renderText({}) }) } shinyApp(ui, server) And here\u0026rsquo;s the entire app with obseveEvent and isolate that worked:\nClick to expand library(shiny) library(bslib) library(crul) httpbin_uuid \u0026lt;- function(...) { con \u0026lt;- crul::HttpClient$new(\u0026#34;https://hb.opencpu.org\u0026#34;) res \u0026lt;- con$get(\u0026#34;uuid\u0026#34;) jsonlite::fromJSON(res$parse(\u0026#34;UTF-8\u0026#34;))$uuid } ui \u0026lt;- page_sidebar( title = \u0026#34;My dashboard\u0026#34;, sidebar = list( actionButton(\u0026#34;submit\u0026#34;, \u0026#34;Submit\u0026#34;), actionButton(\u0026#34;reset\u0026#34;, \u0026#34;Reset\u0026#34;) ), textInput(inputId = \u0026#34;name\u0026#34;, \u0026#34;Your name\u0026#34;), textOutput(\u0026#34;uuid\u0026#34;) ) server \u0026lt;- function(input, output, session) { observeEvent(input$submit, { output$uuid \u0026lt;- renderText({ isolate(httpbin_uuid(input$name)) }) }) observeEvent(input$reset, { updateTextInput(session, \u0026#34;name\u0026#34;, \u0026#34;Your name\u0026#34;, \u0026#34;\u0026#34;) output$uuid \u0026lt;- renderText({}) }) } shinyApp(ui, server) ","permalink":"http://localhost:1313/2024/03/shiny-events/","summary":"\u003cp\u003eI\u0026rsquo;ve been working on \u003ca href=\"https://shiny.posit.co/\"\u003eShiny\u003c/a\u003e app at work for the past few months. One of the many frustrating things about Shiny lately has been around buttons. Well, it wasn\u0026rsquo;t really about buttons, but that\u0026rsquo;s where it started.\u003c/p\u003e\n\u003cp\u003eLoad libraries\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(shiny)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(bslib)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(crul)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHelper function, returned a random UUID from an httpbin server\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehttpbin_uuid \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(\u003cspan style=\"color:#66d9ef\"\u003e...\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  con \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e crul\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eHttpClient\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003enew\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://hb.opencpu.org\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  res \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e con\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eget\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;uuid\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  jsonlite\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003efromJSON\u003c/span\u003e(res\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eparse\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;UTF-8\u0026#34;\u003c/span\u003e))\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003euuid\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eA bslib ui component\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eui \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003epage_sidebar\u003c/span\u003e(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;My dashboard\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  sidebar \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003elist\u003c/span\u003e(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003eactionButton\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;submit\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Submit\u0026#34;\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003eactionButton\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reset\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Reset\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  ),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003etextInput\u003c/span\u003e(inputId \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;name\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Your name\u0026#34;\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003etextOutput\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;uuid\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere\u0026rsquo;s the server part that was giving me trouble. As I said this was an inherited repo, and the server side handling for many buttons was done with \u003ccode\u003eeventReactive\u003c/code\u003e as below. Using \u003ccode\u003eeventReactive\u003c/code\u003e meant that button clicks only sometimes triggered the server side code.\u003c/p\u003e","title":"Shiny button weirdness"},{"content":"I recently opened an issue in a repository for a package I\u0026rsquo;m working on to think about potential footguns and how to avoid them. That word \u0026ldquo;footguns\u0026rdquo; got me thinking; does using phrases/metaphors for a certain topic in a way lend credibility to it? For example, we use a lot of sports metaphors in the US, especially baseball (swing for the fences, anything related to bases, curveballs, heavy hitter, etc.), and that says something about the place of baseball in our culture.\nSo with that I asked on mastodon for ideas:\nAny replacement for the “foot guns” saying for code?\nThe ideas were:\nself-sabotage own-goal power tools can injure hoisted by their own petard fire hazard segfault tripwire landmine stumbling block The person that suggested own-goal also linked to this question on English Language \u0026amp; Usage stack exchange with some additional ideas, none of which I particularly liked so am not including here.\nOf the ideas above I liked stumbling block the most, and was ready to use that moving forward, but then realized/remembered that that phrase is also sort of the translation for the word Stolperstein, physical concrete cubes as memorials for victims of Nazism. I don\u0026rsquo;t think it makes sense to use stumbling blocks then.\nAfter all that, I\u0026rsquo;m not sure what to use moving forward. :shrug:\n","permalink":"http://localhost:1313/2024/03/foot-guns/","summary":"\u003cp\u003eI recently opened an issue in a repository for a package I\u0026rsquo;m working on to think about potential \u003ca href=\"https://en.wiktionary.org/wiki/footgun\"\u003efootguns\u003c/a\u003e and how to avoid them. That word \u0026ldquo;footguns\u0026rdquo; got me thinking; does using phrases/metaphors for a certain topic in a way lend credibility to it? For example, we use a lot of sports metaphors in the US, especially baseball (swing for the fences, anything related to bases, curveballs, heavy hitter, etc.), and that says something about the place of baseball in our culture.\u003c/p\u003e","title":"Avoiding the word footgun(s)"},{"content":"My parents just found this email they had printed out from me from May 19, 2006, when I was 26. I chatted about some family stuff, then had this rambling string of weird thoughts below. I thought others might appreciate a good cringe - or cringy laugh - at my expense. It\u0026rsquo;s especially funny because I\u0026rsquo;m most def an atheist. I don\u0026rsquo;t know, those Tucson sunsets really are transformative.\n… Actually, some deep thoughts:\nSo, what is it about a sunset that makes you feel so good? Is it that the sun is what provides us food ultimately, and we are grateful for its good day\u0026rsquo;s work once again. Or is it that we know that the day is over and we have survived it once again. Perhaps we think that it is god looking down on us and sendins us off to lay our heads down at the pillow again. But, what if a sunset allows us to get in touch with some more spiritual side of ourselves for a small period of time; for the sun setting marks the division between day and night, light and dark, and signifies maybe the division between our physical and spiritual bodies, such that its a moment to connect with that spiritual side for a moment before returning to the physical. Anyway\u0026hellip;blah blah blah\n","permalink":"http://localhost:1313/2023/10/weird-thoughts/","summary":"\u003cp\u003eMy parents just found this email they had printed out from me from May 19, 2006, when I was 26. I chatted about some family stuff, then had this rambling string of weird thoughts below. I thought others might appreciate a good cringe - or cringy laugh - at my expense. It\u0026rsquo;s especially funny because I\u0026rsquo;m most def an atheist. I don\u0026rsquo;t know, those Tucson sunsets really are transformative.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e… Actually, some deep thoughts:\u003c/p\u003e","title":"Weird thoughts"},{"content":"notes to self for next job hunt (some of which may be generally useful):\ndon\u0026rsquo;t apply to general tech companies anymore for many reasons. heard back from very very few but that may be b/c I don\u0026rsquo;t know many people at general tech companies never been able to get through interviews; they\u0026rsquo;re presumably looking for computer science grads (not me) most of their missions are probably not stuff I\u0026rsquo;d be happy about at the end of the day. despite missions of doing xyz, it\u0026rsquo;s probably really about $$ don\u0026rsquo;t apply to pharma companies any more. there\u0026rsquo;s lots of good software jobs in that sector, but i\u0026rsquo;ve struck out 3 times, and so that\u0026rsquo;s a clear pattern my background/whatever isn\u0026rsquo;t something they want next time only apply where I have a connection that can refer me or dig around for a referral. it\u0026rsquo;s super easy to apply for jobs, especially if you don\u0026rsquo;t write a cover letter; however, the less time I spend surely the less likely I am to hear back make sure (and I\u0026rsquo;ll probably fail to do it again this time, ugh) to write down what questions I was asked, how I answered, and how to improve on that answer. then study and reference those questions and answers for the next interview its good to have multiple offers at the same time, but then deciding is harder - \u0026amp; I don\u0026rsquo;t love to negotiating - so maybe don\u0026rsquo;t worry about multiple offers at the same time next time around I have relatively low expectations in any interview b/c I don\u0026rsquo;t do technical interviews well - I also try to seek out orgs that do not have crazy technical interview processes - eg., Roche had a whiteboard technical interview that I totally bombed, but was unsurprising in hindsight since the interviewer was an ex-Googler. I\u0026rsquo;m more of a thinker than a quick responder, making it hard to do well in very fast paced (for me) tech interviews. Though I know i have been a good software engineer where I\u0026rsquo;ve worked, so these fast paced tech interviews are probably selecting for a certain kind of brain function I guess? seek out orgs with interview processes that have take home assignments - or at least timed coding tests on something like hackerrank - instead of live whiteboard/zoom tech inteviews my last job Deck had a take home test Axiom DS has a take home test approach AdHoc uses a take home test approach Invitae had a hackerrank test, not a take home but better than live coding test cover letters? I still don\u0026rsquo;t know whether these are worth doing or not. the advice seems to be mixed. they sure take a lot of time, so I hope they\u0026rsquo;re not necessary for most hiring managers; given my bullet above about spending more time on fewer applications, I could find time for a cover letter on every application if ther\u0026rsquo;s not that many some data about this last job search:\n48: orgs that took time to say No 51: orgs that didn\u0026rsquo;t respond 10: orgs that interviewed me (including recruiter only) 1: thanks hutch! ","permalink":"http://localhost:1313/2023/10/job-searching/","summary":"\u003cp\u003enotes to self for next job hunt (some of which may be generally useful):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edon\u0026rsquo;t apply to general tech companies anymore for many reasons.\n\u003col\u003e\n\u003cli\u003eheard back from very very few but that may be b/c I don\u0026rsquo;t know many people at general tech companies\u003c/li\u003e\n\u003cli\u003enever been able to get through interviews; they\u0026rsquo;re presumably looking for computer science grads (not me)\u003c/li\u003e\n\u003cli\u003emost of their missions are probably not stuff I\u0026rsquo;d be happy about at the end of the day. despite missions of doing xyz, it\u0026rsquo;s probably really about $$\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003edon\u0026rsquo;t apply to pharma companies any more. there\u0026rsquo;s lots of good software jobs in that sector, but i\u0026rsquo;ve struck out 3 times, and so that\u0026rsquo;s a clear pattern my background/whatever isn\u0026rsquo;t something they want\u003c/li\u003e\n\u003cli\u003enext time only apply where I have a connection that can refer me or dig around for a referral. it\u0026rsquo;s super easy to apply for jobs, especially if you don\u0026rsquo;t write a cover letter; however, the less time I spend surely the less likely I am to hear back\u003c/li\u003e\n\u003cli\u003emake sure (and I\u0026rsquo;ll probably fail to do it again this time, ugh) to write down what questions I was asked, how I answered, and how to improve on that answer. then study and reference those questions and answers for the next interview\u003c/li\u003e\n\u003cli\u003eits good to have multiple offers at the same time, but then deciding is harder - \u0026amp; I don\u0026rsquo;t love to negotiating - so maybe don\u0026rsquo;t worry about multiple offers at the same time next time around\u003c/li\u003e\n\u003cli\u003eI have relatively low expectations in any interview b/c I don\u0026rsquo;t do technical interviews well - I also try to seek out orgs that do not have crazy technical interview processes - eg., Roche had a whiteboard technical interview that I totally bombed, but was unsurprising in hindsight since the interviewer was an ex-Googler. I\u0026rsquo;m more of a thinker than a quick responder, making it hard to do well in very fast paced (for me) tech interviews. Though I know i have been a good software engineer where I\u0026rsquo;ve worked, so these fast paced tech interviews are probably selecting for a certain kind of brain function I guess?\u003c/li\u003e\n\u003cli\u003eseek out orgs with interview processes that have take home assignments - or at least timed coding tests on something like hackerrank - instead of live whiteboard/zoom tech inteviews\n\u003cul\u003e\n\u003cli\u003emy last job Deck had a take home test\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.axiomdatascience.com/\"\u003eAxiom DS\u003c/a\u003e has a take home test approach\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://adhoc.team/\"\u003eAdHoc\u003c/a\u003e uses a take home test approach\u003c/li\u003e\n\u003cli\u003eInvitae had a hackerrank test, not a take home but better than live coding test\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ecover letters? I still don\u0026rsquo;t know whether these are worth doing or not. the advice seems to be mixed. they sure take a lot of time, so I hope they\u0026rsquo;re not necessary for most hiring managers; given my bullet above about spending more time on fewer applications, I could find time for a cover letter on every application if ther\u0026rsquo;s not that many\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003esome data about this last job search:\u003c/p\u003e","title":"Job searching notes"},{"content":"Soooo, my last job at Deck was amazing. I loved it. I was doing data engineer stuff there, mostly maintaining infrastructure for data pipelines. Everyone was great and the mission was amazing: helping Democrats win. Yet the company was shut down about a month ago, sending me on another job search, the 3rd since early/mid 2021.\nI\u0026rsquo;m super thrilled to have landed a job (Software and Reproducibility Software Developer) at the Fred Hutch Data Science Lab (DASL), headed up by Jeff Leek, working with Sean Kross, Amy Paguirigan, and Monica Gerber, among many other amazing folks.\nI\u0026rsquo;m excited to help support the DASL mission to help cancer patients, care providers and researchers.\nIn addition, I get to do a bunch (maybe all?) of software work as opensource, mostly in R and Python. After two jobs where opensource wasn\u0026rsquo;t really part of the job, it\u0026rsquo;s such a treat to get back to doing opensource work. Perhaps it will mostly be conversing with Fred Hutch people, but the goal is to make tools that will help far beyond Fred Hutch.\n","permalink":"http://localhost:1313/2023/10/fred-hutch/","summary":"\u003cp\u003eSoooo, my last job at \u003ca href=\"https://welcome.deck.tools/\"\u003eDeck\u003c/a\u003e was amazing. I loved it. I was doing data engineer stuff there, mostly maintaining infrastructure for data pipelines. Everyone was great and the mission was amazing: helping Democrats win. Yet the company was shut down about a month ago, sending me on another job search, the 3rd since early/mid 2021.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m super thrilled to have landed a job (Software and Reproducibility Software Developer) at the \u003ca href=\"https://hutchdatascience.org/\"\u003eFred Hutch Data Science Lab (DASL)\u003c/a\u003e, headed up by \u003ca href=\"https://jtleek.com/\"\u003eJeff Leek\u003c/a\u003e, working with \u003ca href=\"https://seankross.com/\"\u003eSean Kross\u003c/a\u003e, \u003ca href=\"https://amypag.com/\"\u003eAmy Paguirigan\u003c/a\u003e, and \u003ca href=\"https://www.monicagerber.com/\"\u003eMonica Gerber\u003c/a\u003e, among many other amazing folks.\u003c/p\u003e","title":"Working at Fred Hutchinson Cancer Center"},{"content":"I recently had a use case at work where I wanted to check that file paths given in a Python script actually existed. These paths were in various GitHub repositories, so all I had to do was pull out the paths and check if they exist on GitHub.\nThere were a few catches though.\nFirst, I couldn\u0026rsquo;t simply get any string out of each Python script - they needed to be strings specficied by a specific function parameter, and match a regex (e.g., start with \u0026lsquo;abc\u0026rsquo;).\nSecond, the script paths lack the GitHub repository root name. This name was part of the function name - so I needed to get access to the function that the path was specified within, and then parse the function name to get the repository name.\nThe obvious solution I thought was the ast library.\nast library I started by using ast. The ast.NodeVisitor class seemed like it would do the trick.\nAn example script (\u0026ldquo;my_script.py\u0026rdquo;):\ndef hello(path, stuff=None): return path if __name__ == \u0026#34;__main__\u0026#34;: print(hello(path=\u0026#34;hello/world.py\u0026#34;, stuff=\u0026#34;hello mars\u0026#34;)) import ast class CollectStrings(ast.NodeVisitor): def visit_Module(self, node): self.out = set() self.generic_visit(node) return list(filter(lambda w: w.startswith(\u0026#34;hello\u0026#34;) and w.endswith(\u0026#34;.py\u0026#34;), self.out)) def visit_Str(self, node): self.out.add(node.s) file = \u0026#34;my_script.py\u0026#34; with open(file, \u0026#34;r\u0026#34;) as f: body = ast.parse(f.read()) coll = CollectStrings() coll.visit(body) ## [\u0026#39;hello/world.py\u0026#39;] That worked great at fetching paths - only because all the paths I was looking for started with the same text and all have the same file extension.\nHOWEVER - I also needed the function name that the path argument was called from. I tried to make this work with ast.NodeVisitor but couldn\u0026rsquo;t get it to work.\nI eventually got frustrated enough and figured there must be some libraries that build on top of ast that make it easier to work with ast\u0026rsquo;s in Python.\nredbaron Enter redbaron. I found this library pretty quickly upon searching for a library building on top of ast.\nAnother example script (\u0026ldquo;their_script.py\u0026rdquo;):\ndef hello(path, stuff=None): return path def goodbye(path, stuff=None): return path def world(): path_str = hello(path=\u0026#34;src/world.py\u0026#34;, stuff=\u0026#34;hello mars\u0026#34;) other_path_str = goodbye(path=\u0026#34;src/world.py\u0026#34;, stuff=\u0026#34;hello saturn\u0026#34;) return path_str, other_path_str if __name__ == \u0026#34;__main__\u0026#34;: print(world()) import re from redbaron import RedBaron file = \u0026#34;their_script.py\u0026#34; with open(file, \u0026#34;r\u0026#34;) as src: red = RedBaron(src.read()) red ## 0 def hello(path, stuff=None): ## return path ## ## ## ## 1 def goodbye(path, stuff=None): ## return path ## ## ## ## 2 def world(): ## path_str = hello(path=\u0026#34;src/world.py\u0026#34;, stuff=\u0026#34;hello mars\u0026#34;) ## other_path_str = goodbye(path=\u0026#34;src/world.py\u0026#34;, stuff=\u0026#34;hello saturn\u0026#34;) ## ## return path_str, other_path_str ## ## ## ## 3 if __name__ == \u0026#34;__main__\u0026#34;: ## print(world()) ## Even just the resulting object you get from parsing something is useful:\nAnd with .help() you get a very detailed map of the structure of the thing you\u0026rsquo;re trying to navigate (only printing first 20 lines):\nred.help() ## 0 ----------------------------------------------------- ## DefNode() ## # identifiers: def, def_, defnode, funcdef, funcdef_ ## # default test value: name ## async=False ## name=\u0026#39;hello\u0026#39; ## return_annotation -\u0026gt; ## None ## decorators -\u0026gt; ## arguments -\u0026gt; ## * DefArgumentNode() ## # identifiers: def_argument, def_argument_, defargument, defargumentnode ## target -\u0026gt; ## NameNode() ... ## annotation -\u0026gt; ## None ## value -\u0026gt; ## None ## * DefArgumentNode() ## # identifiers: def_argument, def_argument_, defargument, defargumentnode ... Looking at the result from red.help() I can then use .find_all() to find certain nodes in the ast.\nnodes = red.find_all(\u0026#34;AtomtrailersNode\u0026#34;) nodes = list(filter(lambda w: \u0026#34;hello\u0026#34; in w.dumps(), nodes)) nodes ## [hello(path=\u0026#34;src/world.py\u0026#34;, stuff=\u0026#34;hello mars\u0026#34;), goodbye(path=\u0026#34;src/world.py\u0026#34;, stuff=\u0026#34;hello saturn\u0026#34;)] Then I can write some okay code to extract out the function name, and ugly code to get the string supplied to the path parameter. Then f-string those together to get the path I\u0026rsquo;m after.\npaths = [] for node in nodes: fxn_name = node.name.value command = re.search(\u0026#34;src/.*\\\\.py\u0026#34;, node.dumps()).group() paths.append(f\u0026#34;{fxn_name}/{command}\u0026#34;) for path in paths: print(path) ## hello/src/world.py ## goodbye/src/world.py Not super proud of this but gets the job done for my use case - and when you\u0026rsquo;re not making open source for others, you don\u0026rsquo;t need to worry about other use cases :)\nI\u0026rsquo;ll definitely try to learn how to properly extract stuff using redbaron - but it got me to answer much faster than the ast library.\n","permalink":"http://localhost:1313/2023/04/python-ast/","summary":"\u003cp\u003eI recently had a use case at work where I wanted to check that file paths given in a Python script actually existed. These paths were in various GitHub repositories, so all I had to do was pull out the paths and check if they exist on GitHub.\u003c/p\u003e\n\u003cp\u003eThere were a few catches though.\u003c/p\u003e\n\u003cp\u003eFirst, I couldn\u0026rsquo;t simply get any string out of each Python script - they needed to be strings specficied by a specific function parameter, and match a regex (e.g., start with \u0026lsquo;abc\u0026rsquo;).\u003c/p\u003e","title":"Python, ast, and redbaron"},{"content":"TL;DR In 6 months (end of November 2022) the CRAN Checks API https://cranchecks.info/ will be gone You can still get badges at https://badges.cranchecks.info You can use the new badges like: [![cran checks](https://badges.cranchecks.info/worst/dplyr.svg)](https://cran.r-project.org/web/checks/check_results_dplyr.html) Find more details at https://github.com/sckott/cchecksbadges\nSunsetting the CRAN Checks API If you contribute an R package to CRAN, you may use badges from the CRAN checks API at https://cranchecks.info/. The CRAN Checks API has been operating since about September 2017 (I think).\nThe API has a number of routes, but really people only use the badges.\nGiven this usage pattern, and not wanting to pay for a server anymore, I\u0026rsquo;ve decided to make the badges available on a static endpoint that doesn\u0026rsquo;t cost me anything. There are costs of course - but they\u0026rsquo;re on Github and Netlify (thanks y\u0026rsquo;all!).\nThe new static site version The static site is created using GitHub Actions.\nFor a static site you need to create files for any route you want to support - so the code for the static site creates 19 routes x No. of CRAN packages = approx. 360,000 svg files.\nThe badges will be updated once a day - the same schedule as the API.\nI had to use Netlify because Github pages (as far as I know) doesn\u0026rsquo;t provide ssl certs for custom domains and my domain host doesn\u0026rsquo;t provide free Lets Encrypt certs - whereas Netlify does.\nSome example routes you can look at\nhttps://badges.cranchecks.info/summary/taxize.svg https://badges.cranchecks.info/worst/dplyr.svg https://badges.cranchecks.info/flavor/r-devel-linux-x86_64-fedora-clang/DT.svg\nIf you find any issues with the badges at https://badges.cranchecks.info open an issue.\n","permalink":"http://localhost:1313/2022/06/cran-checks-badges/","summary":"\u003ch2 id=\"tldr\"\u003eTL;DR\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eIn 6 months (end of November 2022) the CRAN Checks API \u003ca href=\"https://cranchecks.info/\"\u003ehttps://cranchecks.info/\u003c/a\u003e will be gone\u003c/li\u003e\n\u003cli\u003eYou can still get badges at \u003ca href=\"https://badges.cranchecks.info\"\u003ehttps://badges.cranchecks.info\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eYou can use the new badges like:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-markdown\" data-lang=\"markdown\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[\u003cspan style=\"color:#f92672\"\u003e![cran checks\u003c/span\u003e](\u003cspan style=\"color:#a6e22e\"\u003ehttps://badges.cranchecks.info/worst/dplyr.svg\u003c/span\u003e)](https://cran.r-project.org/web/checks/check_results_dplyr.html)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFind more details at \u003ca href=\"https://github.com/sckott/cchecksbadges\"\u003ehttps://github.com/sckott/cchecksbadges\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"sunsetting-the-cran-checks-api\"\u003eSunsetting the CRAN Checks API\u003c/h2\u003e\n\u003cp\u003eIf you contribute an R package to \u003ca href=\"https://cloud.r-project.org/\"\u003eCRAN\u003c/a\u003e, you may use badges from the CRAN checks API at \u003ca href=\"https://cranchecks.info/\"\u003ehttps://cranchecks.info/\u003c/a\u003e. The CRAN Checks API has been operating \u003ca href=\"https://recology.info/2017/09/cranchecks-api/\"\u003esince about September 2017\u003c/a\u003e (I think).\u003c/p\u003e\n\u003cp\u003eThe API has a number of routes, but really people only use the badges.\u003c/p\u003e","title":"CRAN Checks API and Badges"},{"content":"I was working on a work task last week, and needed to filter out one instance of a class from a list of class instances. No matter how you do this speed doesn\u0026rsquo;t matter too much if you\u0026rsquo;re doing this operation once or a few times.\nHowever, I this operation needs to be done about 100K times each time the script runs - so speed definitely does matter in this case.\nFirst, I naively started off with using filter(). When that lead to waiting more than ten minutes, I read that list comprehensions are faster.\nSecond, I tried list comprehensions. Also waited more then ten minutes and gave up again.\nLast, I thought perhaps it would work to make a dictionary where the keys are the things I need to filter on and the values the class instances. This was the answer! Super fast.\nNow some examples of what I\u0026rsquo;m talking about.\nMake a class called Stuff.\nclass Stuff: def __init__(self, letter): self.letter = letter super(Stuff, self).__init__() def __repr__(self): return \u0026#34;\u0026lt;{} ({})\u0026gt;\u0026#34;.format(self.__class__.__name__, self.letter) x = Stuff(\u0026#39;s\u0026#39;) x #\u0026gt; \u0026lt;Stuff (s)\u0026gt; x.letter #\u0026gt; \u0026#39;s\u0026#39; Make a list of instances of the class Stuff\nimport string lst = [] for x in string.ascii_lowercase: lst.append(Stuff(x)) lst #\u0026gt; [\u0026lt;Stuff (a)\u0026gt;, \u0026lt;Stuff (b)\u0026gt;, \u0026lt;Stuff (c)\u0026gt;, \u0026lt;Stuff (d)\u0026gt;, \u0026lt;Stuff (e)\u0026gt;, \u0026lt;Stuff (f)\u0026gt;, \u0026lt;Stuff (g)\u0026gt;, \u0026lt;Stuff (h)\u0026gt;, \u0026lt;Stuff (i)\u0026gt;, \u0026lt;Stuff (j)\u0026gt;, \u0026lt;Stuff (k)\u0026gt;, \u0026lt;Stuff (l)\u0026gt;, \u0026lt;Stuff (m)\u0026gt;, \u0026lt;Stuff (n)\u0026gt;, \u0026lt;Stuff (o)\u0026gt;, \u0026lt;Stuff (p)\u0026gt;, \u0026lt;Stuff (q)\u0026gt;, \u0026lt;Stuff (r)\u0026gt;, \u0026lt;Stuff (s)\u0026gt;, \u0026lt;Stuff (t)\u0026gt;, \u0026lt;Stuff (u)\u0026gt;, \u0026lt;Stuff (v)\u0026gt;, \u0026lt;Stuff (w)\u0026gt;, \u0026lt;Stuff (x)\u0026gt;, \u0026lt;Stuff (y)\u0026gt;, \u0026lt;Stuff (z)\u0026gt;] len(lst) #\u0026gt; 26 List comprehension: This is how I did the list comprehension method. Filter the list lst where some attibute matched some value.\n[x for x in lst if x.letter == \u0026#39;f\u0026#39;] #\u0026gt; [\u0026lt;Stuff (f)\u0026gt;] Filter: This is how I did the filter method. Filter the list lst where some attibute matched some value.\nlist(filter(lambda x: x.letter == \u0026#39;f\u0026#39;, lst)) #\u0026gt; [\u0026lt;Stuff (f)\u0026gt;] And here\u0026rsquo;s the dictionary approach. Here, I first make a dictionary via dict(zip()) where the keys are some attribute of each instance. You can lookup by key.\nA major difference/drawback of this approach is that it only works if there\u0026rsquo;s only one match per key because dictionaries don\u0026rsquo;t allow duplicate keys.\nlst_map = dict(zip([w.letter for w in lst], lst)) lst_map[\u0026#39;f\u0026#39;] #\u0026gt; \u0026lt;Stuff (f)\u0026gt; And better yet, use .get() so you don\u0026rsquo;t run into a KeyError when the key doesn\u0026rsquo;t exist\nlst_map.get(\u0026#39;f\u0026#39;) #\u0026gt; \u0026lt;Stuff (f)\u0026gt; lst_map.get(\u0026#39;5\u0026#39;) #\u0026gt; (returns None) What about the timings:\nfrom timeit import timeit n = 100000 time_list_comp = timeit(\u0026#34;[x for x in lst if x.letter == \u0026#39;f\u0026#39;]\u0026#34;, number=n, globals=globals()) time_filter = timeit(\u0026#34;list(filter(lambda x: x.letter == \u0026#39;f\u0026#39;, lst))\u0026#34;, number=n, globals=globals()) time_dict = timeit(\u0026#34;lst_map[\u0026#39;f\u0026#39;]\u0026#34;, number=n, globals=globals()) time_dict_get = timeit(\u0026#34;lst_map.get(\u0026#39;f\u0026#39;)\u0026#34;, number=n, globals=globals()) round(time_list_comp, 3) #\u0026gt; 0.088 round(time_filter, 3) #\u0026gt; 0.134 round(time_dict, 3) #\u0026gt; 0.002 round(time_dict_get, 3) #\u0026gt; 0.003 For bracketed lookup, the list comprehension is 39 times slower, and the filter is 59 times slower.\nFor the get() lookup, the list comprehension is 26 times slower, and the filter is 39 times slower.\n","permalink":"http://localhost:1313/2022/04/lookup-vs-filter/","summary":"\u003cp\u003eI was working on a work task last week, and needed to filter out one instance of a class from a list of class instances. No matter how you do this speed doesn\u0026rsquo;t matter too much if you\u0026rsquo;re doing this operation once or a few times.\u003c/p\u003e\n\u003cp\u003eHowever, I this operation needs to be done about 100K times each time the script runs - so speed definitely does matter in this case.\u003c/p\u003e","title":"List comprehension vs. filter vs. key lookup"},{"content":"It\u0026rsquo;s been interesting switching jobs with respect to programming languages. I used to write 95% R - now I write 95% Python.\nI have been using Python for many years, but not seriously or getting paid either. I\u0026rsquo;ve learned alot in the first 6 months.\nSome Python things learned:\nFunctions and methods I used to think functions and methods were the same thing. But during the last 6 months I learned that functions and methods are not the same. Well, they\u0026rsquo;re not that different. A function outside a class is just called a function while a function inside a class is called a method. They could be exactly the same and do the same thing, but one is outside a class and the other inside a class.\nclass Stuff(object): def things(): return 5 def things(): return 5 Stuff.things() # 5 things() # 5 .sort and sorted .sort called on an object changes the object in place and sorted() creates a new object.\nx = [4,1,7,2,6,5,3] z = x.sort() # nothing returned, z = None sorted(x) # [1, 2, 3, 4, 5, 6, 7] context managers I\u0026rsquo;d surely used a context manager in Python before but didn\u0026rsquo;t realize what was happening. The code base I work in uses many with statements and these are used with context managers like:\nwith EXPRESSION as TARGET: SUITE In the above case I often create a connection to a database using the with statement, then once the block is exited, the connection is cleaned up.\nimports Coming from R, it\u0026rsquo;s so nice in Python to be able to import specific functions, classes, etc. rather than having to load an entire file or package in R. In addition, as in Python imports is really nice to have.\nPython\nfrom x import y import pandas as pd R\nrequire(x) library(x) Sometimes I run into circular import issues in Python (as I did in R), which I\u0026rsquo;ve yet to find a neat solution to sorting out.\n","permalink":"http://localhost:1313/2022/02/python-notes/","summary":"\u003cp\u003eIt\u0026rsquo;s been interesting switching jobs with respect to programming languages. I used to write 95% R - now I write 95% Python.\u003c/p\u003e\n\u003cp\u003eI have been using Python for many years, but not seriously or getting paid either. I\u0026rsquo;ve learned alot in the first 6 months.\u003c/p\u003e\n\u003cp\u003eSome Python things learned:\u003c/p\u003e\n\u003ch2 id=\"functions-and-methods\"\u003eFunctions and methods\u003c/h2\u003e\n\u003cp\u003eI used to think functions and methods were the same thing. But during the last 6 months I learned that functions and methods are not the same. Well, they\u0026rsquo;re not that different. A function outside a class is just called a \u003ca href=\"https://docs.python.org/3/glossary.html#term-function\"\u003efunction\u003c/a\u003e while a function inside a class is called a \u003ca href=\"https://docs.python.org/3/glossary.html#term-method\"\u003emethod\u003c/a\u003e.  They could be exactly the same and do the same thing, but one is outside a class and the other inside a class.\u003c/p\u003e","title":"Notes on Python"},{"content":"You\u0026rsquo;ve experienced an HTTP redirect (or URL redirect, or URL forwarding) even if you haven\u0026rsquo;t noticed. We all use browsers (I assume, since you are reading this), either on a phone or laptop/desktop computer. Browsers don\u0026rsquo;t show all the HTTP requests going on in the background, some of which are redirects. Redirection is used for various reasons, including to prevent broken links when web pages are moved, for privacy protection, to allow multiple domains to refer to a single web page, and more.\nThe easiest way to know if you hit a redirect is to look at the HTTP status code. Status codes in the 3xx series are mostly about URL redirection. The most common you will see are 301 (moved permanently), 302 (moved temporarily), and 303 (see other URI; usually in a \u0026ldquo;Location\u0026rdquo; response header).\nWhen making HTTP requests in R, redirects are generally handled automatically by the three HTTP clients (curl, crul, httr). That is, if a 300 series code is detected, all three clients will go to the next URI and so on until there are no more redirects. Automatically following redirects may not be default behavior elsewhere (e.g., crul command line tool doesn\u0026rsquo;t follow redirects by default), so beware.\nHTTP redirects become more tricky when we have to mock them in unit tests or other similar situations. I\u0026rsquo;ll cover the various tools for doing this in R.\nRedirects First, I\u0026rsquo;ll show how redirects work with three major HTTP clients:\ncurl\nlibrary(curl) h \u0026lt;- curl::new_handle() handle_setopt(h, followlocation=0L) out \u0026lt;- curl_fetch_memory(\u0026#34;https://hb.opencpu.org/redirect/3\u0026#34;, handle = h) curl::parse_headers(out$headers, multiple = TRUE) crul\nlibrary(crul) con \u0026lt;- HttpClient$new(\u0026#34;https://hb.opencpu.org/redirect/3\u0026#34;) res \u0026lt;- con$get() length(res$response_headers_all) #\u0026gt; [1] 4 httr\nlibrary(httr) z \u0026lt;- GET(\u0026#34;https://hb.opencpu.org/redirect/3\u0026#34;) length(z$all_headers) #\u0026gt; [1] 4 Mocking redirects If you want to mock HTTP redirects, you can do so with the webmockr package. Why would you want to mock redirects?\nHere\u0026rsquo;s one use case: Say you have a library/package interacting with a web resource that you interact with. You want to add some unit tests for a route that responds with one or more redirects. You\u0026rsquo;d prefer not to perform real HTTP requests against the remote service for one reason or another (e.g., extreme rate limiting); and some would say it\u0026rsquo;s best not to test with real HTTP requests b/c you want to test the functionality of the package, NOT the remote server with which it interacts.\nIn the following, we re-create what happens in real HTTP requests - but just status codes and the location response header.\nlibrary(webmockr) library(crul) webmockr::enable() Make a single stub with each redirect response with to_return()\nstub_request(\u0026#34;get\u0026#34;, \u0026#34;https://hb.opencpu.org/redirect/3\u0026#34;) %\u0026gt;% to_return(status = 302, headers = list(location = \u0026#34;/relative-redirect/2\u0026#34;)) %\u0026gt;% to_return(status = 302, headers = list(location = \u0026#34;/relative-redirect/1\u0026#34;)) %\u0026gt;% to_return(status = 302, headers = list(location = \u0026#34;/get\u0026#34;)) %\u0026gt;% to_return(status = 200, headers = list(location = \u0026#34;hooray, all done!\u0026#34;)) Then make four different requests to https://hb.opencpu.org/redirect/3:\ncon \u0026lt;- crul::HttpClient$new(url = \u0026#34;https://hb.opencpu.org\u0026#34;) con$get(\u0026#34;redirect/3\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;/relative-redirect/2\u0026#34; con$get(\u0026#34;redirect/3\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;/relative-redirect/1\u0026#34; con$get(\u0026#34;redirect/3\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;/get\u0026#34; con$get(\u0026#34;redirect/3\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;hooray, all done!\u0026#34; This isn\u0026rsquo;t ideal because it doesn\u0026rsquo;t reflect how the real HTTP request equivalent happens.\nAlernatively, you could set it up like this, with four separate stubs:\nstub_request(\u0026#34;get\u0026#34;, \u0026#34;https://hb.opencpu.org/redirect/3\u0026#34;) %\u0026gt;% to_return(status = 302, headers = list(location = \u0026#34;/relative-redirect/2\u0026#34;)) stub_request(\u0026#34;get\u0026#34;, \u0026#34;https://hb.opencpu.org/relative-redirect/2\u0026#34;) %\u0026gt;% to_return(status = 302, headers = list(location = \u0026#34;/relative-redirect/1\u0026#34;)) stub_request(\u0026#34;get\u0026#34;, \u0026#34;https://hb.opencpu.org/relative-redirect/1\u0026#34;) %\u0026gt;% to_return(status = 302, headers = list(location = \u0026#34;/get\u0026#34;)) stub_request(\u0026#34;get\u0026#34;, \u0026#34;https://hb.opencpu.org/get\u0026#34;) %\u0026gt;% to_return(status = 200, headers = list(location = \u0026#34;hooray, all done!\u0026#34;)) Then make each request in turn to each successive URL:\ncon \u0026lt;- crul::HttpClient$new(url = \u0026#34;https://hb.opencpu.org\u0026#34;) con$get(\u0026#34;redirect/3\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;/relative-redirect/2\u0026#34; con$get(\u0026#34;relative-redirect/2\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;/relative-redirect/1\u0026#34; con$get(\u0026#34;relative-redirect/1\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;/get\u0026#34; con$get(\u0026#34;get\u0026#34;)$response_headers #\u0026gt; $location #\u0026gt; [1] \u0026#34;hooray, all done!\u0026#34; Faking real redirects vcr is built on top of webmockr, but instead of returning stubbed responses and not allowing real HTTP requests, vcr stores real HTTP request/response and uses them on all subsequent matching HTTP requests.\nI wrote this back in March 2021 - and was waiting to figure out how to deal with redirects in vcr before finishing this post - see vcr issue #220. I still have and may never get to that issue. If you are interested, please do stop by vcr and make a pull request to get it fixed. The major issue is that vcr stores only the first HTTP response in a redirect chain, rather than the last HTTP response - as I would expect.\n","permalink":"http://localhost:1313/2021/11/mocking-redirects/","summary":"\u003cp\u003eYou\u0026rsquo;ve experienced an \u003ca href=\"https://en.wikipedia.org/wiki/URL_redirection\"\u003eHTTP redirect\u003c/a\u003e (or URL redirect, or URL forwarding) even if you haven\u0026rsquo;t noticed. We all use browsers (I assume, since you are reading this), either on a phone or laptop/desktop computer. Browsers don\u0026rsquo;t show all the HTTP requests going on in the background, some of which are redirects. Redirection is used for various reasons, including to prevent broken links when web pages are moved, for privacy protection, to allow multiple domains to refer to a single web page, and more.\u003c/p\u003e","title":"Mocking HTTP redirects"},{"content":"In February this year I wroute about how many parameters functions should have, looking at some other languages, with a detailed look at R. On a related topic \u0026hellip;\nAs I work on many R packages that are API clients for various web services, I began wondering: What is the best way to deal with API routes that have a lot of parameters?\nThe general programming wisdom I\u0026rsquo;ve seen is that a function should have no more than 3-4 parameters (e.g., this long SO thread, or this one). So should one do anything different from a normal function when that function is connecting to a web API route with a lot of parameters? I\u0026rsquo;ve not found very much spilled ink on this exact topic, but I\u0026rsquo;ll discuss what I have found.\nUse cases? A Software Engineering StackExchange thread How to handle many arguments in an API wrapper? had a couple ideas. One idea is to consider use cases, and then make separate functions covering those use cases. This might work, but I haven\u0026rsquo;t explored it thoroughly for a real API route yet. Pondering on it though I doubt this would work since you\u0026rsquo;d have to pre-emptively think about all the different scenarios users might dream up, which seems like a fools errand.\nBuilder pattern Another concept brought up in the thread mentioned above was the Builder pattern. It\u0026rsquo;s hard for me to understand the idea in abstract - here\u0026rsquo;s a nicer discussion of this in Ruby.\nThis is a good concept to know about, but I don\u0026rsquo;t think is appropriate for the issue at hand, of how to handle many API parameters.\nNamed parameter map Another idea in that thread was to use a named parameter map. In R this would look something like this (imagine a lot more parameters though):\nfoo \u0026lt;- function(args) { get(\u0026#34;/some-api-route\u0026#34;, args) } api_args \u0026lt;- list(query = \u0026#34;*:*\u0026#34;, limit = 10) my_args \u0026lt;- modifyList(api_args, list(query = \u0026#34;bears\u0026#34;, limit = 300)) foo(my_args) That is, the above would replace this:\nfoo \u0026lt;- function(query = \u0026#34;*:*\u0026#34;, limit = 10) { get(\u0026#34;/some-api-route\u0026#34;, list(query = query, limit = limit)) } foo(query = \u0026#34;*:*\u0026#34;, limit = 10) So in the first code block the function no longer has a lot of parameters in it. The drawback of this in R (and I\u0026rsquo;m sure is similar in other languages) is that users lose the autocomplete helpers that most modern IDE\u0026rsquo;s and text editors have - helping users type less and quickly get a tip on what each parameter is intended to do and importantly (if the developer has documented the function well) what types the parameters expects and what (if any) options there are to pass to the parameter.\nOf course a user can \u0026ldquo;just\u0026rdquo; read the docs to figure out what each parameter expects, but it sure can save a lot of time if the help is right there in the tooltips of the IDE/text editor. In addition, in R there\u0026rsquo;s automated checking that parameters in functions are also documented, which is nice for making sure parameters and docs don\u0026rsquo;t get out of sync. You\u0026rsquo;d lose this by using a parameter map - though you could document the parameter map - and perhaps wire together some custom code to make sure the parameters in the parameter map are all handled by the function. This does seem like a lot of fuss though compared to simply having the parameters in the function itself.\nThis approach probably becomes more attractive if a client has many functions that take the same parameters - in which a named parameter map could handle the parameters and any logic behind checking those parameters.\nInclude no parameters in the function In other words: Pass all parameters on to the API w/o including any of them in the function - i.e., let the API handle any problems in parameters.\nAnother approach I\u0026rsquo;ve not seen written about but that I\u0026rsquo;ve seen in code is having a rather lite wrapper around an API and letting the API itself sort out any problems due to user inputs.\nAn example is the gh R package, a client for the GitHub API. For query parameters you can pass in named parameters through the ellipsis ..., all of which are passed as query parameters. The gh package does no checking of these parameters (that I know of); simply passes them to the GitHub API. The GitHub API happens to apparently ignore invalid (silently drop) parameters and invalid valuses of parameters (here, \u0026ldquo;stuff\u0026rdquo; is an invalid value for the page parameter).\nx \u0026lt;- gh(\u0026#34;GET /users/{username}/repos\u0026#34;, username = \u0026#34;gaborcsardi\u0026#34;, page = \u0026#34;stuff\u0026#34;) length(x) #\u0026gt; 30 I don\u0026rsquo;t hate this solution, but I don\u0026rsquo;t love it either. This approach is highly dependent on a well designed API that fails gracefully, with informative error messages and with correct status codes, etc. I would say most APIs are not as nice as GitHub\u0026rsquo;s, at least in the scientific API space in which I work.\nOne plus side of this approach is the R package gh only has one parameter (...) to handle all query parameters, so you do solve the too many query parameters problem.\nAnother upside to this approach is you do not have to keep up with any changes in parameters on each API route - for example, an API route could drop one parameter, and add another, and the R client wouldn\u0026rsquo;t have to change anything (assuming the change in parameters wasn\u0026rsquo;t associated with a change that breaks code in the client).\nA major downside of this approach is that the user often has to mount a time-consuming expedition to figure out what parameters are accepted. Some API clients may document them, and some will simply direct users to the web APIs docs. I think this part alone makes this solution (include no parameters in the function) not a good one since the user experience can be so bad if the documentation is not good. And all developers know its much easier for their docs to get out of date than their code.\nInclude some parameters in the function Another approach is to define some query parameters in the function, and handle all others via R\u0026rsquo;s ellipsis (...) - or similar in other languages. I\u0026rsquo;ve seen this relatively often and have used it myself. It\u0026rsquo;s often used when there\u0026rsquo;s a clear smaller set of important parameters - those can be put in the function as named parameters. And then there\u0026rsquo;s a long tail of other parameters that the maintainer thinks are not likely to be used very often. Those can be looked up by the user in the API docs for whatever API the client interacts with.\nAn example of this is the rOpenSci package rtweet - a client for the Twitter API. In the search_tweets() function there are a half dozen or so named parameters in the function, but then the ellipsis handles all other parameters.\nThe drawback to this approach is that no two APIs behave the same way. In the case of Twitter they silently ignore/drop parameters they do not support (same as the GitHub API, see above). For example:\nlibrary(rtweet) search_tweets(\u0026#34;hillaryclinton\u0026#34;, n = 3, foo = \u0026#34;bar\u0026#34;) Works just fine even though foo is absolutely not a parameter supported by the Twitter API. They must ignore parameters they don\u0026rsquo;t support. This is same behavior as the GitHub API we saw above.\nIn the case of Twitter and GitHub one might want to raise errors on unsupported parameters client side in rtweet to avoid any use confusion of parameters being silently dropped.\nGrouping similar parameters together Many threads (e.g., this one) suggest that similar parameters could be grouped together to reduce the number of parameters passed to a function. For example, if a function has the parameters latitude and longitude you could group those into a single parameter called e.g., coordinates.\n# Original function, each parameter separate foo \u0026lt;- function(latitude, longitude) { # do something with latitude/longitude latitude longitude } # Modified function, grouping the two parameters into one foo \u0026lt;- function(coordinates) { # do something with latitude/longitude coordinates$latitude coordinates$longitude } Though I\u0026rsquo;ve not tried this approach myself, it might be a good compromise between a function not handling any parameters (i.e., just passing all to the web API unhandled) and handling every parameter individually.\nClosing thoughts The benefit of documenting API query parameters in a client package is that you can tell the user what each parameters expects in language they can understand. That is, if you simply direct users to the docs for the web API with which the client interacts, the API docs could be not very good and/or specify types expected that the user may not understand. In addition, there may be edge cases or similar with some parameters that are not documented in the API docs but that you can document in the client docs for each parameter.\nI would say the vast majority of web API clients I use that do succeed in having very few parameters also have docs in which it\u0026rsquo;s a nightmare trying to figure out what parameters each method accepts. That is, the pursuit of very few parameters at least is correlated with a very poor user experience - in my experience.\n","permalink":"http://localhost:1313/2020/12/api-client-params/","summary":"\u003cp\u003eIn February this year I wroute about \u003ca href=\"https://recology.info/2020/02/how-many-parameters/\"\u003ehow many parameters functions should have\u003c/a\u003e, looking at some other languages, with a detailed look at R. On a related topic \u0026hellip;\u003c/p\u003e\n\u003cp\u003eAs I work on many R packages that are API clients for various web services, I began wondering: What is the best way to deal with API routes that have a lot of parameters?\u003c/p\u003e\n\u003cp\u003eThe general programming wisdom I\u0026rsquo;ve seen is that a function should have no more than 3-4 parameters (e.g., \u003ca href=\"https://stackoverflow.com/questions/174968/how-many-parameters-are-too-many\"\u003ethis long SO thread\u003c/a\u003e, or \u003ca href=\"https://softwareengineering.stackexchange.com/questions/331803/techniques-for-minimising-number-of-function-arguments\"\u003ethis one\u003c/a\u003e). So should one do anything different from a normal function when that function is connecting to a web API route with a lot of parameters? I\u0026rsquo;ve not found very much spilled ink on this exact topic, but I\u0026rsquo;ll discuss what I have found.\u003c/p\u003e","title":"API client design: how to deal with lots of parameters?"},{"content":" Update on 2021-02-09: I\u0026rsquo;ve archived 8 more packages. Post below updated\nCode is often arranged in packages for any given language. Packages are often cataloged in a package registry of some kind: NPM for node, crates.io for Rust, etc. For R, that registry is either CRAN or Bioconductor (for the most part).\nCRAN has the concept of an archived package. That is, the namespace for a package (foo) is still in the registry (and can not be used again), but the package is archived - no longer gets updated and checks I think are no longer performed.\nWe rarely hear the stories behind how software gets laid to rest. What are the most common reasons for software to be abandoned?\nMy CRAN archived packages First, my archived CRAN packages:\nlibrary(pkgsearch) library(dplyr) library(data.table) library(tibble) x = cran_events(releases = FALSE, archivals = TRUE, limit = 4000L) res = lapply(x, function(w) tibble(pkg=w$name, maintainer=w$package$Maintainer)) df = rbindlist(res, use.names = TRUE, fill = TRUE) df = as_tibble(df) scott \u0026lt;- filter(df, grepl(\u0026#34;chamberlain\u0026#34;, maintainer, ignore.case = TRUE)) %\u0026gt;% select(pkg) %\u0026gt;% data.frame() scott #\u0026gt; pkg #\u0026gt; 1 originr #\u0026gt; 2 geoaxe #\u0026gt; 3 lawn #\u0026gt; 4 pleiades #\u0026gt; 5 geoops #\u0026gt; 6 rif #\u0026gt; 7 rbraries #\u0026gt; 8 ccafs #\u0026gt; 9 rjsonapi #\u0026gt; 10 rdpla #\u0026gt; 11 seaaroundus #\u0026gt; 12 crevents #\u0026gt; 13 etseed #\u0026gt; 14 rtimes #\u0026gt; 15 rsunlight #\u0026gt; 16 nneo #\u0026gt; 17 binomen #\u0026gt; 18 solr #\u0026gt; 19 enigma #\u0026gt; 20 alm #\u0026gt; 21 ropensnp #\u0026gt; 22 govdat #\u0026gt; 23 spoccutils #\u0026gt; 24 rgauges I have 24 archived packages on CRAN.\nStories The following are brief stories of why each package was archived on CRAN.\nspoccutils: was a package of utility functions that didn\u0026rsquo;t quite fit within the scope of another package spocc. It was renamed to mapr. I suppose I could have asked CRAN to change the name, hmmm rgauges: was a client for the Gaug.es website analytics API - we started the package to gather data on visitors to the rOpenSci website. Eventually we stopped using Gaug.es and then it didn\u0026rsquo;t make sense to maintain the package, so it was archived. alm: was a client for a generic article-level metrics web service framework called Lagotto. In the early days of rOpenSci we were quite engaged with the community of folks working on article-level metrics. If I remember correctly, Lagotto usage slowed down and wasn\u0026rsquo;t used much so I gave up on maintaining alm ropensnp: was a client for the service OpenSNP. There were other sources of SNP data that I thought would be nice to access all from one package; so a new package (rsnps) was created and incorporated functions for OpenSNP and ropensnp was archived. solr: was a package client for the Apache Solr database. At some point it got a major overhual and I decided to change the package name to solrium. binomen: aim of the package was functions for creating taxonomic classes/objects and functions for manipulating taxonomic data, sort of like dplyr. Evolution of ideas in binomen gave way to a new package taxa, now maintained by Zach Foster. nneo: was a client for APIs for NEON data. At some point I stumbled upon someone else from NEON making essentially the same package, so I archived mine. etseed: was a package for interacting with the distributed key-value store etcd. I was on a kick at the time of making R packages for databases, and saw a missing package I thought. After getting familiar with etcd, I realized I would probably never use it myself, and further, it probably didn\u0026rsquo;t make sense to interact with etcd from R anyway. crevents: Crossref mints DOIs for scholarly articles (among other works). A neat service they started was collecting and making searchable the \u0026ldquo;events\u0026rdquo; on DOIs - that is, the links pointing to DOIs, e.g., from Twitter, etc. The service at some point became very unreliable (was often down), so the package was archived. seaaroundus: Seaaroundus maintains fisheries and fisheries-related data. I was helping maintain an R package for the API, but it was a difficult one to maintain, and most users simply sent emails requesting dumps of data anyway - so the package was archived. rdpla: The Digital Public Library of America is a very cool organization that is similar to Crossref in a way, in that they centralize metadata about \u0026ldquo;things\u0026rdquo;; metadata on museum collections for DPLA and scholarly works for Crossref. I figured many researchers would enjoy being able to easily get metadata from DPLA for research on museum collections. In the end not many people used the package. rjsonapi: JSON:API is a cool idea - a sort of specification for building APIs in JSON. REST APIs are incredibly variable - this is an attempto standardize it a bit. I thought perhaps JSON:API would be adopted widely and that an R client would be useful for consuming JSON:API services - however, I\u0026rsquo;ve seen only very few APIs using JSON:API. originr: The idea with originr originally was to centralize in one package tools for biologists to get data on \u0026ldquo;nativity\u0026rdquo; of species in their studies: are species X and Y native to A, B, and C countries. I was lucky to get a collaborator (Ignasi Bartomeus) to help on that package. The package simply was not used much at all, and the data sources used were very flaky, making for a buggy user experience. geoaxe: This package always had a very narrow scope. It was created to solve a problem in two widely used packages: rgbif and spocc. In both of those packages we needed to take user input of a Well-known text (WKT) string representing a polygon in which the user wanted to search for something in a remote data source. And with that WKT we would chop up the polygon into smaller polygons to be submitted in multiple (spatial) requests instead of one very large (spatial) request. I didn\u0026rsquo;t want to depend on the heavy rgdal depenency, so I cooked up geoaxe that only used sp and rgeos. geoaxe lasted for many years, but there\u0026rsquo;s better tools out there now, so it was archived. lawn: This package started out when I was trying to see what we could do in spatial R packages, specifically around GeoJSON. Jeroen had recently created the V8 package so you could leverage bundled Javascript libraries in R. turf.js was a neat project for Javascript for spatial analysis and built in a modular, approachable manner - I thought. So collaborator Jeff Hollister and I wrapped turf.js in R. It was not used that much - and especially not used much after the rise of sf and related \u0026ldquo;tidy\u0026rdquo; spatial packages, leading to archival. geoops: This package was a follow on from lawn, in exploring more GeoJSON focused work. I wanted to learn how to make an R package mostly out of C++, with just a thin layer of R on top. Like lawn, geoops was not used much at all as far as I could tell, maybe partly because it focused only on GeoJSON - but this was also during the rise of sf and friends. I moved it to my own personal account just as a little C++ in R playground in case I want to brush up on that. pleiades: I don\u0026rsquo;t remember how or why this package started. It was a client for Pleiades (https://pleiades.stoa.org/home), a database of historical geographic information about the ancient world. It was used very, very little as far as I could tell - so was archived. rif: This package was started after a some exploration of what R work we (rOpenSci) could do in the neuroscience field. It was a client for the Neuroscience Information Framework (https://neuinfo.org/), a database of neuroscience information. Another package not used - so was archived. rbraries: This package started because I thought rOpenSci would make use of the metadata around R package downloads/etc. that https://libraries.io/ collected. We never ended up using the data - and no one seemed to use the package - so was archived. ccafs: A client for Climate Change, Agriculture, and Food Security (CCAFS) General Circulation Models (GCM) data (https://www.ccafs-climate.org/). All I could find for motivation for this package was an email with someone where they shared a few links to different sources of climate data - then apparently this package was born shortly thereafter. It wasn\u0026rsquo;t used much at all - so was archived. The following four packages were all R clients for sources of government open data - see the organization rOpenGov for R packages on government data.\ngovdat: was split into two packages (rsunlight and rtimes) and govdat was archived rsunlight: was a client for many APIs of the organization Sunlight Labs - part of the reason for archiving this package was the disintegration of Sunlight Labs, which made the previously sensible organization of many APIs into one R package not sensible anymore. Also, government data was considered out of scope for our work at rOpenSci. rtimes: was a client for a number of the government data APIs from the New York Times. One reason for abandoning this package was that NYT almost never responded to questions/feedback on their APIs. Another reason was the aforementioned focus of rOpenSci. enigma: was a client for the Enigma API - the company I think was first focused on making open government data easier to access - as well as data on companies. I didn\u0026rsquo;t really use the package at all though, and there wasn\u0026rsquo;t much usage of the package, so archived it. As a summary of the lists above, a list of the major reasons each package was archived:\nNot used rjsonapi rdpla etseed enigma rgauges rtimes originr geaoxe pleiades rif rbraries ccafs The fall of the GeoJSON-verse 1 lawn geoops Bad/retired service rtimes seaaroundus crevents alm Name change solr ropensnp spoccutils Evolution to new package binomen govdat Duplicated work nneo Out of scope rsunlight Footnotes:\nalthough, geojsonsf is very successful so I think my projects were just crap I guess :)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/2020/09/archived-pkgs/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUpdate on 2021-02-09: I\u0026rsquo;ve archived 8 more packages. Post below updated\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003cp\u003eCode is often arranged in packages for any given language. Packages\nare often cataloged in a package registry of some kind: NPM for\nnode, crates.io for Rust, etc. For R, that registry is either\n\u003ca href=\"https://cran.r-project.org/\"\u003eCRAN\u003c/a\u003e or \u003ca href=\"https://bioconductor.org/\"\u003eBioconductor\u003c/a\u003e\n(for the most part).\u003c/p\u003e\n\u003cp\u003eCRAN has the concept of an archived package. That is, the namespace\nfor a package (\u003ccode\u003efoo\u003c/code\u003e) is still in the registry (and can not be used again),\nbut the package is archived - no longer gets updated and checks\nI think are no longer performed.\u003c/p\u003e","title":"stories behind archived packages"},{"content":"taxizedb arose from pain in using taxize when dealing with large amounts of data in a single request or doing a lot of requests of any data size. taxize works with remote data sources on the web, so there\u0026rsquo;s a number of issues that can slow the response down: internet speed, server response speed (was a response already cached or not; or do they even use caching), etc.\nThe idea with taxizedb was to allow users to do the same things as taxize allows, but much faster by accessing the entire database for a data source on their own computer. The previous versions of taxizedb used a variety of different databases (MySQL/MariaDB, PostgreSQL, SQLite), so the technical barrier to entry was pretty high. In the newest version just released, we\u0026rsquo;ve drastically simplified the database situation, among other things.\ntaxadb was developed as an alternative approach to taxizedb and should also be considered when dealing with taxonomic names. It takes a different approach for the data, with tabular files hosted on GitHub releases, but is similar in that after downloading the data is put into a SQL database, SQLite by default (with other options for databases). taxadb user facing functions are different from those in taxizedb, and largely don\u0026rsquo;t overlap.\ntaxizedb quick links:\ntaxizedb repo taxizedb on cran taxizedb docs Install the latest version, if you don\u0026rsquo;t get v0.2.0 with\ninstall.packages(\u0026#34;taxizedb\u0026#34;) then use\ninstall.packages(\u0026#34;taxizedb\u0026#34;, repos = \u0026#34;https://dev.ropensci.org\u0026#34;) Load the package\nlibrary(taxizedb) All SQLite! SQLite is shipped in nearly every device these days, so taxizedb now uses only SQLite for the database backend for each data source. Every person that installs taxizedb should have SQLite already installed. In addition, there\u0026rsquo;s no usernames/passwords/ports to setup with. How we\u0026rsquo;ve accomplished this is partly through automation:\nNCBI: SQLite built within R from tabular files ITIS: they provide a SQLite dump Plantlist: is no longer updated; we build a SQLite manually from csv files COL: a SQLite is built once a day via GitHub Actions GBIF: a SQLite is built once a day via GitHub Actions Wikidata: SQLite built within R from a tabular file World Flora Online: SQLite built within R from a tabular file Some of the databases have indices to speed up queries, making them a bit larger relative to no indices, but these days most people likely are willing to use up a little more disk space on their computer to have faster queries.\nNew data sources Three new data sources were added:\nNCBI taxonomy - all of this work was done by Zebulun Arendsee World Flora Online - the replacement for The Plant List Wikidata - the table wikidata-taxon-info, extracted taxon objects from Wikidata, last updated April 2018, on Zenodo New functions: taxize equivalents Three new high level functions matching those in taxize were added: children, classification, downstream. The taxize version of those functions are still good for smaller requests, but with larger requests, its probably best to use taxizedb. The most common problem where taxize becomes frustrating is with downstream where a user wants all species within a high taxonomic rank like phylum. The original work for these functions was done by Zebulun Arendsee.\nHere\u0026rsquo;s a comparison of taxize vs. taxizedb with downstream - getting all species within the genus Bombus (bumble bees)\nid_tx \u0026lt;- taxize::get_tsn(\u0026#34;Bombus\u0026#34;) system.time(taxize::downstream(id_tx, db = \u0026#34;itis\u0026#34;, downto = \u0026#34;species\u0026#34;)) #\u0026gt; user system elapsed #\u0026gt; 2.144 0.130 20.533 id_txdb \u0026lt;- taxizedb::name2taxid(\u0026#39;Bombus\u0026#39;, db = \u0026#34;itis\u0026#34;) system.time(taxizedb::downstream(id_txdb, db = \u0026#34;itis\u0026#34;, downto = \u0026#34;species\u0026#34;)) #\u0026gt; user system elapsed #\u0026gt; 0.132 0.051 0.186 In addition, three new \u0026ldquo;mapping\u0026rdquo; functions were added that are similar to those in taxize, but with different names: name2taxid (scientific or common name to taxonomy ID); taxid2name (taxonomy ID to scientific name); taxid2rank (taxonomy ID to rank).\nWe saw name2taxid above. Below we get the taxonomic ID for COL, ITIS and GBIF for Bombus\nname2taxid(\u0026#39;Bombus\u0026#39;, db = \u0026#34;col\u0026#34;) #\u0026gt; [1] \u0026#34;3993765\u0026#34; name2taxid(\u0026#39;Bombus\u0026#39;, db = \u0026#34;itis\u0026#34;) #\u0026gt; [1] \u0026#34;154397\u0026#34; name2taxid(\u0026#39;Bombus\u0026#39;, db = \u0026#34;gbif\u0026#34;) #\u0026gt; [1] \u0026#34;1340278\u0026#34; Get the scientific name from a taxonomic ID\ntaxid2name(3993765, db = \u0026#34;col\u0026#34;) #\u0026gt; [1] \u0026#34;Bombus\u0026#34; taxid2rank(3993765, db = \u0026#34;col\u0026#34;) #\u0026gt; [1] \u0026#34;genus\u0026#34; These functions are quite fast too:\nx \u0026lt;- taxize::names_list(rank = \u0026#34;species\u0026#34;, size = 10000L) system.time(name2taxid(x, db = \u0026#34;gbif\u0026#34;, out_type = \u0026#34;summary\u0026#34;)) #\u0026gt; user system elapsed #\u0026gt; 0.096 0.206 1.053 Thoughts? Get in touch if you have any feedback at https://github.com/ropensci/taxizedb/issues\n","permalink":"http://localhost:1313/2020/08/taxizedb-update/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/ropensci/taxizedb\"\u003etaxizedb\u003c/a\u003e arose from pain in using \u003ca href=\"https://github.com/ropensci/taxize\"\u003etaxize\u003c/a\u003e when dealing with large amounts of data in a single request or doing a lot of requests of any data size. \u003ca href=\"https://github.com/ropensci/taxize\"\u003etaxize\u003c/a\u003e works with remote data sources on the web, so there\u0026rsquo;s a number of issues that can slow the response down: internet speed, server response speed (was a response already cached or not; or do they even use caching), etc.\u003c/p\u003e\n\u003cp\u003eThe idea with \u003ca href=\"https://github.com/ropensci/taxizedb\"\u003etaxizedb\u003c/a\u003e was to allow users to do the same things as taxize allows, but much faster by accessing the entire database for a data source on their own computer. The previous versions of taxizedb used a variety of different databases (MySQL/MariaDB, PostgreSQL, SQLite), so the technical barrier to entry was pretty high. In the newest version just released, we\u0026rsquo;ve drastically simplified the database situation, among other things.\u003c/p\u003e","title":"taxizedb: an update"},{"content":"Functions can have no parameters, or have a lot of parameters, or somewhere in between. How many parameters is too many? Does it even matter how many parameters there are in a function?\nThere\u0026rsquo;s AFAIK no \u0026ldquo;correct\u0026rdquo; answer to this question. And surely the \u0026ldquo;best practice\u0026rdquo; varies among programming languages. What do folks say about this and what should we be doing in R?\nFrom other languages Many of the blog posts and SO posts on this topic cite the book Clean Code by \u0026ldquo;Uncle Bob\u0026rdquo;. I\u0026rsquo;ve not read the book, but it sounds worth a read.\nSome of the arguments go like: too many arguments can \u0026hellip;\nmakes it easier to pass arguments in the wrong order reduce code readability make it harder to test a function; it’s difficult/time consuming to test all various combinations of arguments work together An analysis was done in 2018 of php open source projects, and they found that the most common number of parameters was 5; functions with 10 parameters or more were found in \u0026lt;20% of projects.\nOn the other side, some argue that you shouldn\u0026rsquo;t worry so much about the correct number of parameters, but rather make sure that all the parameters make sense, and are documented and tested.\nTo the extreme, a number of people quote the Clean Code book:\nThe ideal number of arguments for a function is zero\nSome general threads on this topic:\nSoftware engineering Stackexchange Stackoverflow Data Data for this post, created below, is in the github repo sckott/howmanyparams.\nWhat about R? What do the data show in the R language? Just like the blog post on php above, let\u0026rsquo;s have a look at a lot of R packages to get a general data informed consensus on how many parameters are used per function.\nIt\u0026rsquo;s incredibly likely that there is a better way to do what I\u0026rsquo;ve done below; but this is my hacky way of getting to the data.\nWhat I\u0026rsquo;ve done in words:\nGet a list of all available package names on CRAN Install about half of them (didn\u0026rsquo;t do all cause it takes time and I don\u0026rsquo;t think I need all 15K packages to get a good answer) List the exported functions in each package Count the arguments (parameters) per function in each package Visualize the results I ended up using 82489 functions across 4777 packages\nLoad packages\nlibrary(plyr) library(dplyr) library(tibble) library(ggplot2) Use a different path from my actual R library location to not pollute my current setup, and put binaries into a temporary directory so they are cleaned up on exiting R.\npath \u0026lt;- \u0026#34;/some/path\u0026#34; binaries \u0026lt;- file.path(tempdir(), \u0026#34;binaries\u0026#34;) dir.create(path) dir.create(binaries) .libPaths(path) .libPaths() # check that the path was set Function do_one() to run on each package:\ntry to load the package if not found install it get a vector of the exported functions in the package count how many arguments each function has, make a data.frame unload the package namespace do_one \u0026lt;- function(pkg) { if (!requireNamespace(pkg)) install.packages(pkg, quiet=TRUE, verbose=FALSE, destdir = binaries) on.exit(unloadNamespace(pkg)) funs \u0026lt;- paste0(pkg, \u0026#34;::\u0026#34;, getNamespaceExports(pkg)) enframe(vapply(funs, function(w) { tt \u0026lt;- tryCatch(parse(text = w), error = function(e) e) if (!inherits(tt, \u0026#34;error\u0026#34;)) length(suppressWarnings(formals(eval(tt)))) else 0 }, numeric(1))) } do_one_safe \u0026lt;- failwith(tibble(), do_one) Get a list of packages. First time running through I used available.packages() which gets you all available packages. After installing packages though, I used installed.packages() to get the list of packages I already installed.\n# pkg_names \u0026lt;- unname(available.packages()[,\u0026#34;Package\u0026#34;]) pkg_names \u0026lt;- unname(installed.packages()[,\u0026#34;Package\u0026#34;]) Run each package through the do_one() function. This had to be stopped and re-started a few times. This failed for quite a few packages - I wasn\u0026rsquo;t trying to get every single package, just a large set of packages to get an idea of what packages do on average.\ntbls \u0026lt;- stats::setNames(lapply(pkg_names, do_one_safe), pkg_names) Combine list of data.frame\u0026rsquo;s into one data.frame\ndf \u0026lt;- dplyr::bind_rows(tbls, .id = \u0026#34;pkg\u0026#34;) readr::write_csv(df, \u0026#34;params_per_fxn.csv\u0026#34;) note: you can get this data at sckott/howmanyparams\ndf \u0026lt;- readr::read_csv(\u0026#34;~/params_per_fxn.csv\u0026#34;) Visualize\nAll functions across all packages\nggplot(df, aes(x = value)) + geom_histogram(bins = 30) + scale_x_continuous(limits = c(0, 30)) + theme_grey(base_size = 15) The mean number of arguments per function across all packages was 4.4, and the most common value was 3. The maximum number of arguments was 209, and there were 5306 functions (or 6.4%) with zero parameters.\nMean params across functions for each pkg\ndf_means \u0026lt;- group_by(df, pkg) %\u0026gt;% summarise(mean_params = mean(value, na.rm=TRUE)) %\u0026gt;% ungroup() # arrange(df_means, desc(mean_params)) ggplot(df_means, aes(x = mean_params)) + geom_histogram(bins = 50) + scale_x_continuous(limits = c(0, 30)) + theme_grey(base_size = 15) Taking the mean within each package first pulls the number of arguments to the right some, with a mean of 5 arguments, and the most common value at 4.\nThoughts In terms of getting around the too many arguments thing, there\u0026rsquo;s talk of using global variables, which seems like generally a bad idea; unless perhaps they are environment variables that are meant to be set by the user in non-interactive environments, etc.\nOther solutions are to use ... in R, or similarly **kwargs or *args in Python (ref.), or the newly added ... in Ruby (ref). With this approach you could have very few parameters defined in the function, and then internally within the function handle any parameter filtering, etc. The downside of this in R is that you don\u0026rsquo;t get the automated checks making sure all function arguments are documented, and there\u0026rsquo;s no documented arguments that don\u0026rsquo;t exist in the function.\nI\u0026rsquo;m not suggesting a solution is needed though; there\u0026rsquo;s probably no right answer, but rather lots of opinions.\nHaving said that, the average R function does use about 4 arguments, so if you keep your functions to around 4 arguments you\u0026rsquo;ll be approaching the sort of consensus of a large number of R developers.\nLast, I should admit that some of the functions in my packages have quite a lot of parameters - which was sort of the motivation for this post - that is, to explore what most functions do. For example, brranching::phylomatic has 13 parameters, three functions in the crevents package have 24 parameters \u0026hellip; and I wonder about these types of functions. Should I refactor? Or is it good enough to make sure these functions are thoroughly documented and tested?\n","permalink":"http://localhost:1313/2020/02/how-many-parameters/","summary":"\u003cp\u003eFunctions can have no parameters, or have a lot of parameters, or somewhere\nin between. How many parameters is too many? Does it even matter how many\nparameters there are in a function?\u003c/p\u003e\n\u003cp\u003eThere\u0026rsquo;s AFAIK no \u0026ldquo;correct\u0026rdquo; answer to this question. And surely the \u0026ldquo;best\npractice\u0026rdquo; varies among programming languages. What do folks say about\nthis and what should we be doing in R?\u003c/p\u003e\n\u003ch2 id=\"from-other-languages\"\u003eFrom other languages\u003c/h2\u003e\n\u003cp\u003eMany of the blog posts and SO posts on this topic cite the book\n\u003ca href=\"https://www.goodreads.com/book/show/3735293-clean-code\"\u003eClean Code\u003c/a\u003e by \u0026ldquo;Uncle Bob\u0026rdquo;. I\u0026rsquo;ve not read the book, but\nit sounds worth a read.\u003c/p\u003e","title":"how many parameters?"},{"content":"The bad thing about making software is that you can sometimes make it easier for someone to shoot themselves in the foot. The good thing about software is that you can make more software to help them not shoot a foot off.\nThe R package vcr, an R port of the Ruby library of the same name, records and plays back HTTP requests. Some HTTP requests can have secrets (e.g., passwords, API keys, etc.) in their requests and/or responses. These secrets can then accidentally end up on the Internet, where bad people may find them. These secrets are sometimes called \u0026ldquo;truffles\u0026rdquo;.\nThere\u0026rsquo;s a suite of tools out there for finding these truffles (e.g., truffleHog, gitsecrets) that use tools like regex and entropy.\nDespite there being existing tools, users tend to use things that are built in the language(s) they know; that are easy to incorporate into their existing workflows. Towards this end, I\u0026rsquo;ve been working on a new R package trufflesniffer.\ntrufflesniffer doesn\u0026rsquo;t do any fancy entropy stuff, and doesn\u0026rsquo;t try to find secrets without any informed knowledge. Rather, the user supplies the secrets that they want to look for and trufflesniffer looks for them. In the future I\u0026rsquo;d look to see if it can be used without any user inputs.\nterminology:\nsniff: search for a secret links:\nsrc: https://github.com/ropenscilabs/trufflesniffer docs: https://docs.ropensci.org/trufflesniffer Install remotes::install_github(\u0026#34;ropenscilabs/trufflesniffer\u0026#34;) library(trufflesniffer) directory You can \u0026ldquo;sniff\u0026rdquo; a file directory or a package: sniff_one()\n# crete a directory Sys.setenv(A_KEY = \u0026#34;a8d#d%d7g7g4012a4s2\u0026#34;) path \u0026lt;- file.path(tempdir(), \u0026#34;foobar\u0026#34;) dir.create(path) # no matches sniff_one(path, Sys.getenv(\u0026#34;A_KEY\u0026#34;)) #\u0026gt; named list() # add files with the secret cat(paste0(\u0026#34;foo\\nbar\\nhello\\nworld\\n\u0026#34;, Sys.getenv(\u0026#34;A_KEY\u0026#34;), \u0026#34;\\n\u0026#34;), file = file.path(path, \u0026#34;stuff.R\u0026#34;)) # matches! prints the line number where the key was found sniff_one(path, Sys.getenv(\u0026#34;A_KEY\u0026#34;)) #\u0026gt; $stuff.R #\u0026gt; [1] 5 package sniff through a whole package\nfoo \u0026lt;- function(key = NULL) { if (is.null(key)) key \u0026lt;- \u0026#34;mysecretkey\u0026#34; } package.skeleton(name = \u0026#34;mypkg\u0026#34;, list = \u0026#34;foo\u0026#34;, path = tempdir()) pkgpath \u0026lt;- file.path(tempdir(), \u0026#34;mypkg\u0026#34;) list.files(pkgpath, recursive=TRUE) #\u0026gt; [1] \u0026#34;DESCRIPTION\u0026#34; \u0026#34;man/foo.Rd\u0026#34; \u0026#34;man/mypkg-package.Rd\u0026#34; #\u0026gt; [4] \u0026#34;NAMESPACE\u0026#34; \u0026#34;R/foo.R\u0026#34; \u0026#34;Read-and-delete-me\u0026#34; # check the package sniff_secrets_pkg(dir = pkgpath, secrets = c(\u0026#34;mysecretkey\u0026#34;)) #\u0026gt; $mysecretkey #\u0026gt; $mysecretkey$foo.R #\u0026gt; [1] 3 fixtures sniff specifically in a package\u0026rsquo;s test fixtures.\nCreate a package\nfoo \u0026lt;- function(key = NULL) { if (is.null(key)) key \u0026lt;- \u0026#34;a2s323223asd423adsf4\u0026#34; } package.skeleton(\u0026#34;herpkg\u0026#34;, list = \u0026#34;foo\u0026#34;, path = tempdir()) pkgpath \u0026lt;- file.path(tempdir(), \u0026#34;herpkg\u0026#34;) dir.create(file.path(pkgpath, \u0026#34;tests/testthat\u0026#34;), recursive = TRUE) dir.create(file.path(pkgpath, \u0026#34;tests/fixtures\u0026#34;), recursive = TRUE) cat(\u0026#34;library(vcr) vcr::vcr_configure(\u0026#39;../fixtures\u0026#39;, filter_sensitive_data = list(\u0026#39;\u0026lt;\u0026lt;mytoken\u0026gt;\u0026gt;\u0026#39; = Sys.getenv(\u0026#39;MY_KEY\u0026#39;)) )\\n\u0026#34;, file = file.path(pkgpath, \u0026#34;tests/testthat/helper-herpkg.R\u0026#34;)) cat(\u0026#34;a2s323223asd423adsf4\\n\u0026#34;, file = file.path(pkgpath, \u0026#34;tests/fixtures/foo.yml\u0026#34;)) # check that you have a pkg at herpkg list.files(pkgpath) #\u0026gt; [1] \u0026#34;DESCRIPTION\u0026#34; \u0026#34;man\u0026#34; \u0026#34;NAMESPACE\u0026#34; #\u0026gt; [4] \u0026#34;R\u0026#34; \u0026#34;Read-and-delete-me\u0026#34; \u0026#34;tests\u0026#34; list.files(file.path(pkgpath, \u0026#34;tests/testthat\u0026#34;)) #\u0026gt; [1] \u0026#34;helper-herpkg.R\u0026#34; cat(readLines(file.path(pkgpath, \u0026#34;tests/testthat/helper-herpkg.R\u0026#34;)), sep = \u0026#34;\\n\u0026#34;) #\u0026gt; library(vcr) #\u0026gt; vcr::vcr_configure(\u0026#39;../fixtures\u0026#39;, #\u0026gt; filter_sensitive_data = list(\u0026#39;\u0026lt;\u0026lt;mytoken\u0026gt;\u0026gt;\u0026#39; = Sys.getenv(\u0026#39;MY_KEY\u0026#39;)) #\u0026gt; ) list.files(file.path(pkgpath, \u0026#34;tests/fixtures\u0026#34;)) #\u0026gt; [1] \u0026#34;foo.yml\u0026#34; readLines(file.path(pkgpath, \u0026#34;tests/fixtures/foo.yml\u0026#34;)) #\u0026gt; [1] \u0026#34;a2s323223asd423adsf4\u0026#34; Check the package\nSys.setenv(\u0026#39;MY_KEY\u0026#39; = \u0026#39;a2s323223asd423adsf4\u0026#39;) sniff_secrets_fixtures(pkgpath) #\u0026gt; $MY_KEY #\u0026gt; $MY_KEY$foo.yml #\u0026gt; [1] 1 sniffer The function sniffer() wraps the function sniff_secrets_fixtures() and pretty prints to optimize non-interactive use. Run from within R or from the command line non-interactively.\nExample where a secret is found:\nsniffer(pkgpath) Example where a secret is not found:\nSys.unsetenv(\u0026#39;MY_KEY\u0026#39;) sniffer(pkgpath) To do There\u0026rsquo;s more to do. trufflesniffer hasn\u0026rsquo;t been tested thoroughly yet; I\u0026rsquo;ll do more testing to make the experience better. In addition, it\u0026rsquo;d probably be best to integrate this into the R vcr package so that the user doesn\u0026rsquo;t have to take an extra step to make sure they aren\u0026rsquo;t going to put any secrets on the web.\nack: trufflesniffer uses R packages cli and crayon\n","permalink":"http://localhost:1313/2020/01/test-truffles/","summary":"\u003cp\u003eThe bad thing about making software is that you can sometimes make it easier\nfor someone to shoot themselves in the foot. The good thing about software\nis that you can make more software to help them not shoot a foot off.\u003c/p\u003e\n\u003cp\u003eThe R package \u003ca href=\"https://github.com/ropensci/vcr\"\u003evcr\u003c/a\u003e, an R port of the \u003ca href=\"https://github.com/vcr/vcr\"\u003eRuby library\u003c/a\u003e of the same name,\nrecords and plays back HTTP requests. Some HTTP requests can have secrets (e.g.,\npasswords, API keys, etc.) in their requests and/or responses. These secrets\ncan then accidentally end up on the Internet, where bad people may find them.\nThese secrets are sometimes called \u0026ldquo;truffles\u0026rdquo;.\u003c/p\u003e","title":"finding truffles"},{"content":"Acquiring full text articles fulltext is an R package I maintain to obtain full text versions of research articles for text mining.\nIt\u0026rsquo;s a hard problem, with a spaghetti web of code. One of the hard problems is figuring out what the URL is for the full text version of an article. Publishers do not have consistent URL patterns through time, and so you can not set rules once and never revisit them.\nThe Crossref API has links available to full text versions for publishers that choose to share them. However, even if publishers choose to share their full text links, they may be out of date or completely wrong (not actually lead to the full text).\nThere\u0026rsquo;s a variety of other APIs out there for getting links to articles, but none really hit the spot, which lead to the creation of the ftdoi API.\nthe ftdoi API The ftdoi API is a web API, with it\u0026rsquo;s main goal for getting a best guess at the URL for the full text version of an article from its DOI (this is done via the /api/doi/{doi}/ route). The API gives back URLs for all those possible, including pdf, xml, and html. Most publishers only give full text as PDF, but when XML is available we give those URLs as well.\nThe API uses the rules maintained in the pubpatterns repo. The rules are only rough guidelines though and often require at least one step of making a web request to the publishers site or another site, that\u0026rsquo;s NOT specified in the pubpatterns rules. For example, the Biorxiv file has notes about how to get the parts necessary for the full URL, but the actual logic to do so in in the API code base here.\nThe ftdoi API caches responses for requests for 24 hrs, so even if a request takes 5 seconds or so to process, at least for the next 24 hrs it will be nearly instantaneous. We don\u0026rsquo;t want to cache indefinitely because URLs may be changed at any time by the publishers.\nThe fulltext package uses the ftdoi API internally, mostly hidden from users, to get a full text URL.\nBut why an API? Why not just have a set of rules in the fulltext R package, and go from there? An API was relatively easy for me to stand up, and i think it has many benefits: can be used by anything/anyone, not just R users; updates to publisher specific rules for generating URLs can evolve independently of fulltext; the logs can be used as a tool to improve the API.\nwhat do people actually want? The ftdoi API has been running for a while now, maybe a year or so, and I\u0026rsquo;ve been collecting logs. Seems smart to look at the logs to determine what publishers users are requesting articles from that the ftdoi API does not currently support, so that the API can hopefully add those publishers. For obvious reasons, I can\u0026rsquo;t share the log data.\nLoad packages and define file path.\nlibrary(rcrossref) library(dplyr) library(rex) logs \u0026lt;- \u0026#34;~/pubpatterns_api_calls.log\u0026#34; The logs look like (IP addresses removed, some user agents shortened):\n[28/Nov/2018:20:09:49 +0000] GET /api/members/ HTTP/2.0 200 4844 Mozilla/5.0 ... [28/Nov/2018:20:23:15 +0000] GET /api/members/317/ HTTP/2.0 200 228 Mozilla/5.0 ... [29/Nov/2018:01:52:58 +0000] GET /api/members/19/ HTTP/1.1 400 65 fulltext/1.1.0 [29/Nov/2018:01:52:58 +0000] GET /api/members/2308/ HTTP/1.1 400 65 fulltext/1.1.0 [29/Nov/2018:01:52:59 +0000] GET /api/members/239/ HTTP/1.1 400 65 fulltext/1.1.0 [29/Nov/2018:01:53:00 +0000] GET /api/members/2581/ HTTP/1.1 400 65 fulltext/1.1.0 [29/Nov/2018:01:53:00 +0000] GET /api/members/27/ HTTP/1.1 400 65 fulltext/1.1.0 [29/Nov/2018:01:53:01 +0000] GET /api/members/297/ HTTP/1.1 200 336 fulltext/1.1.0 [29/Nov/2018:01:53:01 +0000] GET /api/members/7995/ HTTP/1.1 400 65 fulltext/1.1.0 [29/Nov/2018:10:46:53 +0000] GET /api/members/unknown/ HTTP/1.1 400 65 fulltext/1.1.0.9130 Use the awesome rex package from Kevin Ushey et al. to parse the logs, pulling out just the Crossref member ID in the API request, as well as the HTTP status code. There are of course other API requests in the logs, but we\u0026rsquo;re only interested here in the ones to the /api/doi/{doi}/ route.\ndf \u0026lt;- tbl_df(scan(logs, what = \u0026#34;character\u0026#34;, sep = \u0026#34;\\n\u0026#34;) %\u0026gt;% re_matches( rex( \u0026#34;/api/members/\u0026#34;, capture(name = \u0026#34;route\u0026#34;, one_or_more(numbers) ), \u0026#34;/\u0026#34;, space, space, \u0026#34;HTTP/\u0026#34;, numbers, \u0026#34;.\u0026#34;, numbers, space, capture(name = \u0026#34;status_code\u0026#34;, one_or_more(numbers) ) ) )) df$route \u0026lt;- as.numeric(df$route) df #\u0026gt; # A tibble: 896,035 x 2 #\u0026gt; route status_code #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 NA \u0026lt;NA\u0026gt; #\u0026gt; 2 317 200 #\u0026gt; 3 19 400 #\u0026gt; 4 2308 400 #\u0026gt; 5 239 400 #\u0026gt; 6 2581 400 #\u0026gt; 7 27 400 #\u0026gt; 8 297 200 #\u0026gt; 9 7995 400 #\u0026gt; 10 NA \u0026lt;NA\u0026gt; #\u0026gt; # … with 896,025 more rows Filter to those requests that resulted in a 400 HTTP status code, that is, they resulted in no returned data, indicating that we likely do not have a mapping for that Crossref member.\nres \u0026lt;- df %\u0026gt;% filter(status_code == 400) %\u0026gt;% select(route) %\u0026gt;% group_by(route) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) res #\u0026gt; # A tibble: 530 x 2 #\u0026gt; route count #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 10 345045 #\u0026gt; 2 530 7165 #\u0026gt; 3 286 3062 #\u0026gt; 4 276 2975 #\u0026gt; 5 239 2493 #\u0026gt; 6 8611 1085 #\u0026gt; 7 56 853 #\u0026gt; 8 235 722 #\u0026gt; 9 382 706 #\u0026gt; 10 175 590 #\u0026gt; # … with 520 more rows Add crossref metadata\n(members \u0026lt;- cr_members(res$route)) #\u0026gt; $meta #\u0026gt; NULL #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 530 x 56 #\u0026gt; id primary_name location last_status_che… total.dois current.dois #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 10 American Me… 330 N. … 2019-03-20 600092 14714 #\u0026gt; 2 530 FapUNIFESP … FAP-UNI… 2019-03-20 353338 38339 #\u0026gt; 3 286 Oxford Univ… Academi… 2019-03-20 3696643 289338 #\u0026gt; 4 276 Ovid Techno… 100 Riv… 2019-03-20 2059352 167272 #\u0026gt; 5 239 BMJ BMA Hou… 2019-03-20 891239 61267 #\u0026gt; 6 8611 AME Publish… c/o NAN… 2019-03-20 20067 15666 #\u0026gt; 7 56 Cambridge U… The Edi… 2019-03-20 1529029 84018 #\u0026gt; 8 235 American So… 1752 N … 2019-03-20 178890 13984 #\u0026gt; 9 382 Joule Inc. 1031 Ba… 2019-03-20 12666 1868 #\u0026gt; 10 175 The Royal S… 6 Carlt… 2019-03-20 89219 7262 #\u0026gt; # … with 520 more rows, and 50 more variables: backfile.dois \u0026lt;chr\u0026gt;, #\u0026gt; # prefixes \u0026lt;chr\u0026gt;, coverge.affiliations.current \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.similarity.checking.current \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.funders.backfile \u0026lt;chr\u0026gt;, coverge.licenses.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.funders.current \u0026lt;chr\u0026gt;, coverge.affiliations.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.resource.links.backfile \u0026lt;chr\u0026gt;, coverge.orcids.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.update.policies.current \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.open.references.backfile \u0026lt;chr\u0026gt;, coverge.orcids.current \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.similarity.checking.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.references.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.award.numbers.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.update.policies.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.licenses.current \u0026lt;chr\u0026gt;, coverge.award.numbers.current \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.abstracts.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.resource.links.current \u0026lt;chr\u0026gt;, coverge.abstracts.current \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.open.references.current \u0026lt;chr\u0026gt;, #\u0026gt; # coverge.references.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.abstracts.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.orcids.current \u0026lt;chr\u0026gt;, flags.deposits \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.affiliations.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.update.policies.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.similarity.checking.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.award.numbers.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.resource.links.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.articles \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.affiliations.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.funders.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.references.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.abstracts.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.licenses.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.award.numbers.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.open.references.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.open.references.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.references.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.resource.links.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.orcids.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.funders.backfile \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.update.policies.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.similarity.checking.current \u0026lt;chr\u0026gt;, #\u0026gt; # flags.deposits.licenses.current \u0026lt;chr\u0026gt;, names \u0026lt;chr\u0026gt;, tokens \u0026lt;chr\u0026gt; #\u0026gt; #\u0026gt; $facets #\u0026gt; NULL Add Crossref member names to the log data.\nalldat \u0026lt;- left_join(res, select(members$data, id, primary_name), by = c(\u0026#34;route\u0026#34; = \u0026#34;id\u0026#34;)) alldat #\u0026gt; # A tibble: 530 x 3 #\u0026gt; route count primary_name #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 10 345045 American Medical Association (AMA) #\u0026gt; 2 530 7165 FapUNIFESP (SciELO) #\u0026gt; 3 286 3062 Oxford University Press (OUP) #\u0026gt; 4 276 2975 Ovid Technologies (Wolters Kluwer Health) #\u0026gt; 5 239 2493 BMJ #\u0026gt; 6 8611 1085 AME Publishing Company #\u0026gt; 7 56 853 Cambridge University Press (CUP) #\u0026gt; 8 235 722 American Society for Microbiology #\u0026gt; 9 382 706 Joule Inc. #\u0026gt; 10 175 590 The Royal Society #\u0026gt; # … with 520 more rows Theres A LOT of requests to the American Medical Association. Coming in a distant second is FapUNIFESP (SciELO), then the Oxford University Press, Ovid Technologies (Wolters Kluwer Health), BMJ, and AME Publishing Company, all with greater than 1000 requests.\nThese are some clear leads for publishers to work into the ftdoi API, working my way down the data.frame to less frequently requested publishers.\nmore work to do I\u0026rsquo;ve got a good list of publishers which I know users want URLs for, so I can get started implementing rules/etc. for those publishers. And I can repeat this process from time to time to add more publishers in high demand.\n","permalink":"http://localhost:1313/2019/03/apis-text-mining-logs/","summary":"\u003ch2 id=\"acquiring-full-text-articles\"\u003eAcquiring full text articles\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ropensci/fulltext/\"\u003efulltext\u003c/a\u003e is an R package I maintain to obtain full text versions of research articles\nfor text mining.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s a hard problem, with a spaghetti web of code. One of the hard problems is\nfiguring out what the URL is for the full text version of an article. Publishers\ndo not have consistent URL patterns through time, and so you can not set rules once\nand never revisit them.\u003c/p\u003e","title":"text mining, apis, and parsing api logs"},{"content":"I was listening to a Bike Shed podcast episode 189, \u0026ldquo;It\u0026rsquo;s Gonna Work, Definitely, No Problems Whatsoever\u0026rdquo;, and starting at 27:44 there was a conversation about exception handling. Specifically it was about exception handling in control flow when doing web API requests. This topic piqued my interest straight away as I do a lot of API stuff (making and wrapping).\nThe part of the conversation that I want to address is their conclusion that exceptions in control flow are an anti-pattern. Seems this is a general pattern in programming languages, e.g., this SO thread. But on the contrary there are some languages in which exceptions in control flow are considered normal behavior; e.g., Python (this, this).\nMy first reaction to this was one of vehement disagreement because in my experience wrapping web APIs raising exceptions on HTTP status codes of 400 and 500 series is the norm, in at least R and Ruby. They argued that there are better ways of handling these cases. After a whg with my gut reaction.\nLet\u0026rsquo;s take a step back first and look at some concepts before diving further into this. ile I thought maybe the topic is worth thinking harder about rather than goin\ncontrol flow Control flow in programming is\nthe order in which individual statements \u0026hellip; are executed or evaluated \u0026hellip; a control flow statement is a statement, the execution of which results in a choice being made as to which of two or more paths to follow.\nControl flow in R has an overview of control flow in R. Control flow constructs in R include if/else, for, while, repeat, break, next.\nexceptions Exceptions are\nanomalous or exceptional conditions requiring special processing - often changing the normal flow of program execution (source: wikiex)\nIn R, exception handling can be done with try, tryCatch, withCallingHandlers and others. Often warning() is used to signal to the user what happened, but does not stop execution, and can be suppressed with suppressWarnings(). To stop execution, stop() is used.\nback to web API exceptions and control flow in R Note the word exceptional above in our definition of exceptions. The BikeShed pod hosts were surprised to see exceptions raised with bad API requests because they didn\u0026rsquo;t think a bad API request was exceptional, but rather an expected result given certain conditions (e.g., an HTTP 400 series client error means the client did something wrong and perhaps the server gave back a useful error message to help fix the request).\nThey observed that most Ruby API wrappers did have the behavior of raising an exception on a 400/500 series API status, but they disagreed with this approach.\nIn R world, most API wrappers in my experience also follow the pattern of raising an exception stopping the code flow on a 400/500 series HTTP error.\nWhat would it look like to not stop code execution flow when 400/500 series errors are returned from web API requests? What would need to change from the current setup? How would users be affected?\nA typical R function that makes a web API request looks like the following:\nfoo = function(path, query = list()) { conn = crul::HttpClient$new(\u0026#34;https://httpbin.org\u0026#34;) res = conn$get(path = path, query = query) res$raise_for_status() res$parse(\u0026#34;UTF-8\u0026#34;) } On a successful request all is good and we get back the JSON payload\nfoo(path = \u0026#34;get\u0026#34;, query = list(apple = \u0026#34;pink lady\u0026#34;)) #\u0026gt; [1] \u0026#34;{\\n \\\u0026#34;args\\\u0026#34;: {\\n \\\u0026#34;\\\u0026#34;: \\\u0026#34;pink lady\\\u0026#34;\\n }, ... When there is a 400/500 series code the line res$raise_for_status() throws an error, stopping execution\nfoo(path = \u0026#34;status/400\u0026#34;) #\u0026gt; Error: Bad Request (HTTP 400) Instead of raising an error we could throw a warning and proceed to the next step\nbar = function(path, query = list()) { conn = crul::HttpClient$new(\u0026#34;https://httpbin.org\u0026#34;) res = conn$get(path = path, query = query) if (res$status_code \u0026gt;= 400) { warning(sprintf(\u0026#34;HTTP %s %s\u0026#34;, res$status_code, res$status_http()$explanation)) } res$parse(\u0026#34;UTF-8\u0026#34;) } bar(path = \u0026#34;status/400\u0026#34;) #\u0026gt; [1] \u0026#34;\u0026#34; #\u0026gt; Warning message: #\u0026gt; In bar(path = \u0026#34;status/400\u0026#34;) : #\u0026gt; HTTP 400 Bad request syntax or unsupported method This is fine, but there\u0026rsquo;s a few scenarios in which this will be problematic:\nMany APIs DO NOT return the same content-type on a 400 series error, and even more common on 500 series errors. In fact, often JSON APIs return an HTML error page, which may or may not contain a meaningul message, instead of the same content type as a successful response (e.g., JSON). Rather then simply parsing the response res$parse(\u0026quot;UTF-8\u0026quot;), the downstream code may be more complex (e.g., selecting particular fields/keys), and may fail out (and in R, this often means useless error messages for the user). If we take their advice and don\u0026rsquo;t fail out on 400/500 series codes, what would that look like? One could do something like:\nhello_world \u0026lt;- function(path, query = list()) { conn = crul::HttpClient$new(\u0026#34;https://httpbin.org\u0026#34;) res = conn$get(path = path, query = query) if (res$status_code \u0026gt;= 400) { warning(sprintf(\u0026#34;HTTP %s %s\u0026#34;, res$status_code, res$status_http()$explanation)) } res } We still get the warning on an error\nhello_world(path = \u0026#34;status/400\u0026#34;) #\u0026gt; Warning message: #\u0026gt; In hello_world(path = \u0026#34;status/400\u0026#34;) : #\u0026gt; HTTP 400 Bad request syntax or unsupported method But also we return the response object (HttpResponse from the crul package in this case):\n#\u0026gt; \u0026lt;crul response\u0026gt; #\u0026gt; url: https://httpbin.org/status/400 #\u0026gt; request_headers: #\u0026gt; User-Agent: libcurl/7.54.0 r-curl/3.3 crul/0.7.0 #\u0026gt; Accept-Encoding: gzip, deflate #\u0026gt; Accept: application/json, text/xml, application/xml, */* #\u0026gt; response_headers: #\u0026gt; status: HTTP/1.1 400 BAD REQUEST #\u0026gt; access-control-allow-credentials: true #\u0026gt; access-control-allow-origin: * #\u0026gt; content-type: text/html; charset=utf-8 #\u0026gt; date: Mon, 04 Mar 2019 17:49:39 GMT #\u0026gt; server: nginx #\u0026gt; content-length: 0 #\u0026gt; connection: keep-alive #\u0026gt; status: 400 Now the user can explore the response body, response headers, etc. and decide on their own what to do instead of the function failing out and returning nothing.\nThis approach is fine if your users are more advanced, but most packages/libraries are probably trying to give back a data object that users are familiar with. In R, that is clearly the data.frame. When there is a 400/500 series error, one option is to return an empty data.frame and throw a warning about the error, hopefully with enough information for the user to fix the request. This is probably best for naive users, but any package has some more advanced users that would benefit from more information; and more information will help a naive user + the maintainer debug a problem easier.\nThe next more complicated option would be a list that can have the same format regardless of errors or not:\nfunc \u0026lt;- function() { res \u0026lt;- hello_world(path = \u0026#34;status/400\u0026#34;) mssg \u0026lt;- sprintf(\u0026#34;HTTP %s %s\u0026#34;, res$status_code, res$status_http()$explanation) list(data = res$parse(\u0026#34;UTF-8\u0026#34;), error = mssg) } gives\nfunc() #\u0026gt; $data #\u0026gt; [1] \u0026#34;\u0026#34; #\u0026gt; #\u0026gt; $error #\u0026gt; [1] \u0026#34;HTTP 400 Bad request syntax or unsupported method\u0026#34; Or possibly something more complex where you can build in accessors to make it easy to get data the user expects, but also dig into the HTTP response object itself if needed:\nResponse \u0026lt;- R6::R6Class(\u0026#34;Response\u0026#34;, public = list( x = NULL, initialize = function(resp) self$x \u0026lt;- resp, data = function() self$x$parse(\u0026#34;UTF-8\u0026#34;), error = function() { sprintf(\u0026#34;HTTP %s %s\u0026#34;, self$x$status_code, self$x$status_http()$explanation) } ) ) myfunc \u0026lt;- function() { res \u0026lt;- hello_world(path = \u0026#34;status/400\u0026#34;) Response$new(res) } Which gives:\nout \u0026lt;- myfunc() # the HTTP message out$error() #\u0026gt; [1] \u0026#34;HTTP 400 Bad request syntax or unsupported method\u0026#34; # the response body, parsed out$data() #\u0026gt; [1] \u0026#34;\u0026#34; # the full HTTP response object out$x what about users handling exceptions on their side? If one sticks swith erroring out of excecution flow with 400/500 series errors, the user can still handle it on their end. For example, if they are using a function in a loop/appply type call, they can use tryCatch or similar and check for an error and proceed one of two or more ways depending on the error or successful request. Of course this assumes that the user knows how to do this.\nAdditionally, this means that each user will handle errors in different ways, possibly making mistakes in the process - arguing for the developer of the package to handle exceptions instead.\nit\u0026rsquo;s too complex, just fail out One reason I like to fail out on 400/500 series errors in my packages is that there is often significant data munging of the response. Failing out makes my life easier as I don\u0026rsquo;t have to worry about what to do with HTTP responses that fail. In the world I run in of smallish APIs for science/research, API failure behavior often is not very good; it\u0026rsquo;s typically unpredictable, changes from time to time, and failure response bodies are often just their HTML failure page, leading to brittle code for parsing that HTML as that HTML can change often. It\u0026rsquo;d be great if every API was as good as Github\u0026rsquo;s for example, but we\u0026rsquo;ll never be in that place.\nperformance considerations In reading about exceptions in control flow, there\u0026rsquo;s a common thread about performance (e.g., c++, Ruby 1, Ruby 2). That is, if throwing exceptions is a slow procedure, that\u0026rsquo;s one reason to avoid them. But if exceptions aren\u0026rsquo;t slow then that\u0026rsquo;s not a great argument for avoiding them.\nI haven\u0026rsquo;t seen anything on performance an exceptions in R, though I\u0026rsquo;m sure there\u0026rsquo;s something out there.\nEven if exceptions are a slowish procedure, there is an argument to be made that failing early also saves time; that is, if you get a 400/500 series error you aren\u0026rsquo;t then spending time with downstream processing of the response. However, then the user has less information. Trade-offs all the way down.\nconclusion I\u0026rsquo;m not sure if I\u0026rsquo;ll change anything in packages I maintain or not. I\u0026rsquo;ll keep thinking about this and ask around to gauge others opinions on this. Part of me wants to follow the avoid exceptions path, but I worry about two things. First, the complexity increases for me as the developer. If I don\u0026rsquo;t fail out, then I have to deal with parsing somehow every response. It\u0026rsquo;s not as simple as giving back the HTTP response; I ideally want to give users a data structure they are familiar with, i.e., a data.frame. Second, for the user, if I give back a list or an R6 object, that increases complexity on their side. Is the benefit of more information worth the cost of more complexity for the user? I\u0026rsquo;ve no idea.\n","permalink":"http://localhost:1313/2019/03/control-flow-exceptions/","summary":"\u003cp\u003eI was listening to a Bike Shed podcast \u003ca href=\"http://bikeshed.fm/189\"\u003eepisode 189, \u0026ldquo;It\u0026rsquo;s Gonna Work, Definitely, No Problems Whatsoever\u0026rdquo;\u003c/a\u003e, and starting at 27:44 there was a conversation about exception handling. Specifically it was about exception handling in control flow when doing web API requests. This topic piqued my interest straight away as I do a lot of API stuff (making and wrapping).\u003c/p\u003e\n\u003cp\u003eThe part of the conversation that I want to address is their conclusion that exceptions in control flow are an anti-pattern. Seems this is a general pattern in programming languages, e.g., this \u003ca href=\"https://softwareengineering.stackexchange.com/a/189225/329940\"\u003eSO thread\u003c/a\u003e. But on the contrary there are some languages in which exceptions in control flow are considered normal behavior; e.g., Python (\u003ca href=\"https://softwareengineering.stackexchange.com/a/318542/329940\"\u003ethis\u003c/a\u003e, \u003ca href=\"https://softwareengineering.stackexchange.com/a/351121/329940\"\u003ethis\u003c/a\u003e).\u003c/p\u003e","title":"Exceptions in control flow in R"},{"content":"In doing a number of ports of Ruby gems to R (vcr, webmockr), I\u0026rsquo;ve noticed a few differences between the languages that are fun to dive into, at least for me.\nmonkey patching Ruby has a nice thing where you can \u0026ldquo;monkey patch\u0026rdquo; classes/methods/etc. in other Ruby libraries. For example, lets say you have Ruby gems foo and bar. If foo has a method hello, you can override the hello method in foo with one from bar. AFAICT this is acceptable in gems on Rubygems.org and in general in the community.\nMonkey patching is technically possible in R, but is not allowed in packages on CRAN (see ?assignInNamespace help for the warnings), even though there is some usage in CRAN packages. We can do this using utils::assignInNamespace. Let\u0026rsquo;s say you have an R package foo and another R package bar. Here, we can assign a new hello method to the one already defined in foo:\n# the foo::hello method looks like hello \u0026lt;- function() return(\u0026#34;world!\u0026#34;) # make a new hello method hello2 \u0026lt;- function() return(\u0026#34;mars!\u0026#34;) # override the hello method in foo utils::assignInNamespace(\u0026#34;hello\u0026#34;, hello2, \u0026#34;foo\u0026#34;) Try it with any package. It\u0026rsquo;s fun.\nYou can do this in a package, by using a .onAttach directive.\n.onAttach \u0026lt;- function(libname, pkgname) { utils::assignInNamespace(\u0026#34;bar\u0026#34;, bar, \u0026#34;foo\u0026#34;) } Anyway, monkey patching isn\u0026rsquo;t really a thing in R, so that makes it more difficult to port Ruby things to R. The inability to do this in R makes many things much harder. For example, in vcr and webmockr I couldn\u0026rsquo;t simply override methods in http libraries they hook into, but have to make changes in the http libraries themselves to support the HTTP mocking - we get there in the end, but it takes much longer, though possibly safer?\n0 (Ruby) vs. 1 (R) based indexing Never hurts to keep repeating this.\nsequences Ruby has the ability to construct numeric sequences with .. and ..., e.g.,\n# inclusive of second number x = 1..3 x.to_a =\u0026gt; [1, 2, 3] # exclusive of second number x = 1...3 x.to_a =\u0026gt; [1, 2] AFAIK, in R we can only do inclusive sequences\n1:3 #\u0026gt; [1] 1 2 3 explicit imports In at least Ruby and Python you have to be explicit about saying where to import methods from other files.\nWhereas in R you can just use a function/etc. from any other file in the package without stating that you need it. This makes it harder to reason about the dependent functions/etc. needed in any one file. One tool that helps with this is functionMap (though last commit in 2016, not sure if maintained anymore, is it Gábor?).\nOn a related note, in Ruby we can use global variables like:\n$foo = 5 From what I understand the above is bad pratice, but I do use them sometimes in my own Ruby stuff.\nIn R all variables/methods/classes are \u0026ldquo;global\u0026rdquo; within the namespace of the package.\nadding strings ugh, I wish R had the ability to add strings together with +.\n? as a valid character um, yes please. I love methods in Ruby like nil?, empty?, etc. Such a straight-forward way to indicate intent. Wish we had these in R, but ? isn\u0026rsquo;t even a valid character on its own, so not (ever?) gonna happen.\nClasses R\u0026rsquo;s closest class system to Ruby (that I\u0026rsquo;m willing to use) is R6 from Winston Chang. Using R6 makes it a bit easier to port from Ruby or a similar language as you can directly translate classes that have public vs. private methods, an initializer, print method, etc. Plus, with any sufficiently complex R package, using R6 makes it much easier to manage the complexity.\nRuby\u0026rsquo;s ||= In ruby this operator means essentially \u0026ldquo;if a is undefined or falsey, evaluate b and set a to the result\u0026rdquo;. In R there\u0026rsquo;s AFAIK nothing like this. ||= was used extensively in the Ruby gems I was porting, making the ported version in R more verbose. I could do in R a %||% b (where %||% = function(x, y) if (is.null(x) || !x) y else x) essentially doing \u0026ldquo;if a is null, undefined or falsey, evaluate b\u0026rdquo;; but then I have to still assign the result, giving a = a %||% b.\nsplat args The splat operator is used heavily in Ruby. It looks like:\ndef foo(*args) p args end foo(1, 2, 3) # =\u0026gt; [1, 2, 3] In R the most similar thing we have is the ellipsis, so\nfoo \u0026lt;- function(...) c(...) foo(1, 2, 3) #\u0026gt; [1] 1 2 3 Ruby splat args won\u0026rsquo;t trip you up if you know how to do this conversion. Of course there\u0026rsquo;s rlang and such in R as well.\n","permalink":"http://localhost:1313/2019/02/ruby-ports-to-r/","summary":"\u003cp\u003eIn doing a number of ports of Ruby gems to R (\u003ca href=\"https://github.com/ropensci/vcr\"\u003evcr\u003c/a\u003e, \u003ca href=\"https://github.com/ropensci/webmockr\"\u003ewebmockr\u003c/a\u003e), I\u0026rsquo;ve noticed a few differences between the languages that are fun to dive into, at least for me.\u003c/p\u003e\n\u003ch2 id=\"monkey-patching\"\u003emonkey patching\u003c/h2\u003e\n\u003cp\u003eRuby has a nice thing where you can \u003ca href=\"https://en.wikipedia.org/wiki/Monkey_patch\"\u003e\u0026ldquo;monkey patch\u0026rdquo;\u003c/a\u003e classes/methods/etc. in other Ruby libraries. For example, lets say you have Ruby gems \u003ccode\u003efoo\u003c/code\u003e and \u003ccode\u003ebar\u003c/code\u003e. If \u003ccode\u003efoo\u003c/code\u003e has a method \u003ccode\u003ehello\u003c/code\u003e, you can override the \u003ccode\u003ehello\u003c/code\u003e method in \u003ccode\u003efoo\u003c/code\u003e with one from \u003ccode\u003ebar\u003c/code\u003e. AFAICT this is acceptable in gems on Rubygems.org and in general in the community.\u003c/p\u003e","title":"Notes on porting Ruby to R"},{"content":"Let\u0026rsquo;s talk about trailing commas (aka: \u0026ldquo;final commas\u0026rdquo;, \u0026ldquo;dangling commas\u0026rdquo;). Trailing commas refers to a comma at the end of a series of values in an array or array like object, leaving an essentially empty slot. e.g., [1, 2, 3, ]\nI kind of like them when I work on Ruby and Python projects. A number of advantages of trailing commas have been pointed out, the most common of which is diffs:\ndiff --git a/hello.json b/hello.json index e36ffac..d387a2f 100644 --- a/hello.json +++ b/hello.json @@ -1,4 +1,5 @@ [ \u0026#34;foo\u0026#34;: 5, \u0026#34;bar\u0026#34;: 6, + \u0026#34;apple\u0026#34;: 7, ] diff --git a/world.json b/world.json index 14a2818..41f8a01 100644 --- a/world.json +++ b/world.json @@ -1,4 +1,5 @@ [ \u0026#34;foo\u0026#34;: 5, - \u0026#34;bar\u0026#34;: 6 + \u0026#34;bar\u0026#34;: 6, + \u0026#34;apple\u0026#34;: 7 ] Example blog posts on the topic: https://dontkry.com/posts/code/trailing-commas.html, https://medium.com/@nikgraf/why-you-should-enforce-dangling-commas-for-multiline-statements-d034c98e36f8\nMany languages support trailing commas, and some even consider it best practice to use trailing commas.\nRuby [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34; ] # =\u0026gt; [\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;] [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, ] # =\u0026gt; [\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;] Works the same for hashes.\nPython [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34; ] # Out[1]: [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;] [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, ] # Out[2]: [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;] Works the same for sets and dictionaries.\nJavascript Mozilla gives a thorough overview of trailing commas in Javascript.\n[ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34; ] // [ \u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39; ] [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, ] // [ \u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39; ] Probably works for other data types\u0026hellip;?\nRust https://users.rust-lang.org/t/trailing-commas/13993\n[ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34; ] // vs [ \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, ] Probably works for other data types\u0026hellip;?\nJulia https://users.rust-lang.org/t/trailing-commas/13993\n( 1, 2 ) # (1, 2) ( 1, 2, ) # (1, 2) works the same with arrays in Julia.\nothers Apparently others do as well: Perl, C#, Swift, etc \u0026hellip;\nDisagree Some do not like trailing commas:\nTrailing commas: good or bad practice? (TL;DR: it’s bad) R However, the main dev work I do is in R, which does not support trailing commas.\nc(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) #\u0026gt; [1] \u0026#34;hello\u0026#34; \u0026#34;world c(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, ) #\u0026gt; Error in c(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, ) : argument 3 is empty The one caveat is that you will see trailing commas in subsetting procedures of lists, vectors, data.frames, matrices, e.g.,\nmtcars[1:3, ] One blogger provides an override to allow trailing commas though I\u0026rsquo;d imagine it\u0026rsquo;s not a good idea to use as you probably don\u0026rsquo;t want such fundamentally different behavior in your own R console compared to others.\nI\u0026rsquo;ve not seen any discussion of trailing commas in R as a language feature, whether good, bad or otherwise. Doesn\u0026rsquo;t mean it doesn\u0026rsquo;t exist though :)\nHaskell Like R, doesn\u0026rsquo;t allow trailing commas!\nAnd in fact, allegedly (I don\u0026rsquo;t use Haskell):\nBecause it is much more common to append to lists rather than to prepend, Haskellers have developed the idiom of leading comma:\n( foo , bar , baz , quux ) JSON Unfortunately for many people JSON does not allow trailing commas\nsee also: leading with commas https://hackernoon.com/winning-arguments-with-data-leading-with-commas-in-sql-672b3b81eac9 https://gist.github.com/isaacs/357981 https://community.rstudio.com/t/leading-vs-trailing-commas-on-new-lines/6744/5 ","permalink":"http://localhost:1313/2019/02/trailing-commas/","summary":"\u003cp\u003eLet\u0026rsquo;s talk about trailing commas (aka: \u0026ldquo;final commas\u0026rdquo;, \u0026ldquo;dangling commas\u0026rdquo;). Trailing commas refers to a comma at the end of a series of values in an array or array like object, leaving an essentially empty slot. e.g., \u003ccode\u003e[1, 2, 3, ]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eI kind of like them when I work on Ruby and Python projects. A number of advantages of trailing commas have been pointed out, the most common of which is diffs:\u003c/p\u003e","title":"trailing commas"},{"content":"I\u0026rsquo;m sure there\u0026rsquo;s already a way to do this, but here goes. OR maybe this is an anti-pattern. Either way, this is me, asking the stupid question.\nI ran into this a few hours ago:\nSys.unsetenv(\u0026#34;ENTREZ_KEY\u0026#34;) library(brranching) mynames \u0026lt;- c(\u0026#34;Poa annua\u0026#34;, \u0026#34;Salix goodingii\u0026#34;, \u0026#34;Helianthus annuus\u0026#34;) phylomatic_names(taxa = mynames, format=\u0026#39;rsubmit\u0026#39;) No ENTREZ API key provided Get one via taxize::use_entrez() See https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ No ENTREZ API key provided Get one via taxize::use_entrez() See https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ No ENTREZ API key provided Get one via taxize::use_entrez() See https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ [1] \u0026#34;poaceae%2Fpoa%2Fpoa_annua\u0026#34; \u0026#34;salicaceae%2Fsalix%2Fsalix_goodingii\u0026#34; \u0026#34;asteraceae%2Fhelianthus%2Fhelianthus_annuus\u0026#34; The brranching package uses the taxize package internally, calling it\u0026rsquo;s function taxize::tax_name(). The taxize::tax_name() function throws useful messages to the user if their NCBI Entrez API key is not found, and gives them instructions on how to find it.\nHowever, the user does not have to get an API key. If they don\u0026rsquo;t they then get subjected to lots of repeats of the same message.\nI wondered if there\u0026rsquo;s anything that could be done about this. That is, if the same message is going to be thrown that was already thrown within a function call, just skip additional messages that are the same.\nThere is of course suppressMessages() for messages, but in package development if you do want a user to see a message, you don\u0026rsquo;t want to suppress messages. suppressMessages is too blunt of an instrument for this use case.\nthe code with_mssgs() captures values and messages, suppressing the message\nwith_mssgs \u0026lt;- function(expr) { my_mssgs \u0026lt;- NULL w_handler \u0026lt;- function(w) { my_mssgs \u0026lt;\u0026lt;- c(my_mssgs, list(w)) invokeRestart(\u0026#34;muffleMessage\u0026#34;) } val \u0026lt;- withCallingHandlers(expr, message = w_handler) list(value = val, messages = my_mssgs) } MessageKeeper is a little R6 class to handle messages, matching, and simple checks to see if messages have been used or not.\nlibrary(R6) MessageKeeper \u0026lt;- R6::R6Class(\u0026#34;MessageKeeper\u0026#34;, public = list( bucket = NULL, print = function(x, ...) { cat(\u0026#39;MessageKeeper\u0026#39;, sep = \u0026#34;\\n\u0026#34;) cat(paste0(\u0026#39; messages: \u0026#39;, length(self$bucket))) if (length(self$bucket) \u0026gt; 0) { cat(\u0026#34;\\n\u0026#34;) for (i in self$bucket) { cat(paste0(\u0026#34; \u0026#34;, substring(i, 1, 50))) } } }, add = function(x) { self$bucket \u0026lt;- c(self$bucket, list(x)) invisible(self) }, remove = function() { if (self$length() == 0) return(NULL) head \u0026lt;- self$bucket[[1]] self$bucket \u0026lt;- self$bucket[-1] head }, purge = function() { self$bucket \u0026lt;- NULL }, thrown_already = function(x) { x %in% self$bucket }, not_thrown_yet = function(x) { !self$thrown_already(x) } ) ) MessageKeeper examples\nmssger \u0026lt;- MessageKeeper$new() mssger #\u0026gt; MessageKeeper #\u0026gt; messages: 0 mssger$add(\u0026#34;one\u0026#34;) mssger$add(\u0026#34;two\u0026#34;) mssger #\u0026gt; MessageKeeper #\u0026gt; messages: 2 #\u0026gt; one two mssger$thrown_already(\u0026#34;one\u0026#34;) #\u0026gt; [1] TRUE mssger$thrown_already(\u0026#34;bears\u0026#34;) #\u0026gt; [1] FALSE mssger$not_thrown_yet(\u0026#34;bears\u0026#34;) #\u0026gt; [1] TRUE mssger$purge() handle_mssgs() is a function you wrap your target function in to handle the messages\nhandle_mssgs \u0026lt;- function(expr) { res \u0026lt;- with_mssgs(expr) if (!is.null(res$messages)) { # if not thrown yet, add to bucket and throw it if (my_mssger$not_thrown_yet(res$messages[[1]]$message)) { my_mssger$add(res$messages[[1]]$message) message(res$messages[[1]]$message) } } return(res$value) } Set up the message keeper\nmy_mssger \u0026lt;- MessageKeeper$new() squared() squares a numeric value and returns it, throwing a message if it\u0026rsquo;s greater than 20\nsquared \u0026lt;- function(x) { stopifnot(is.numeric(x)) y \u0026lt;- x^2 if (y \u0026gt; 20) message(\u0026#34;woops, \u0026gt; than 20! check your numbers\u0026#34;) return(y) } foo() runs any vector of numbers through squared() using vapply()\nfoo \u0026lt;- function(x) { vapply(x, function(z) squared(z), numeric(1)) } bar() does the same, but uses our MessageKeeper thingy\nbar \u0026lt;- function(x) { # tear down on exit on.exit(my_mssger$purge()) vapply(x, function(z) handle_mssgs(squared(z)), numeric(1)) } foo() annoyingly throws a message for every instance possible\nfoo(1:20) #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; [1] 1 4 9 16 25 36 49 64 81 100 121 144 169 196 225 256 289 #\u0026gt; [18] 324 361 400 while bar() only throws the message once\nbar(1:20) #\u0026gt; woops, \u0026gt; than 20! check your numbers #\u0026gt; [1] 1 4 9 16 25 36 49 64 81 100 121 144 169 196 225 256 289 #\u0026gt; [18] 324 361 400 ","permalink":"http://localhost:1313/2018/12/condition-control/","summary":"\u003cp\u003eI\u0026rsquo;m sure there\u0026rsquo;s already a way to do this, but here goes. OR maybe this is an\nanti-pattern. Either way, this is me, asking the stupid question.\u003c/p\u003e\n\u003cp\u003eI ran into this a few hours ago:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eSys.unsetenv\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ENTREZ_KEY\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(brranching)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emynames \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Poa annua\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Salix goodingii\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Helianthus annuus\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ephylomatic_names\u003c/span\u003e(taxa \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mynames, format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;rsubmit\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eNo ENTREZ API key provided\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e Get one via taxize\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003euse_entrez\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e See https\u003cspan style=\"color:#f92672\"\u003e://\u003c/span\u003encbiinsights.ncbi.nlm.nih.gov\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2017\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003enew\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eapi\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ekeys\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ethe\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ee\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eutilities\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eNo ENTREZ API key provided\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e Get one via taxize\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003euse_entrez\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e See https\u003cspan style=\"color:#f92672\"\u003e://\u003c/span\u003encbiinsights.ncbi.nlm.nih.gov\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2017\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003enew\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eapi\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ekeys\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ethe\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ee\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eutilities\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eNo ENTREZ API key provided\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e Get one via taxize\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003euse_entrez\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e See https\u003cspan style=\"color:#f92672\"\u003e://\u003c/span\u003encbiinsights.ncbi.nlm.nih.gov\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2017\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003enew\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eapi\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ekeys\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ethe\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ee\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eutilities\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[1] \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;poaceae%2Fpoa%2Fpoa_annua\u0026#34;\u003c/span\u003e                   \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;salicaceae%2Fsalix%2Fsalix_goodingii\u0026#34;\u003c/span\u003e        \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;asteraceae%2Fhelianthus%2Fhelianthus_annuus\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe \u003ca href=\"https://github.com/ropensci/brranching/\"\u003ebrranching\u003c/a\u003e package uses the \u003ca href=\"https://github.com/ropensci/taxize/\"\u003etaxize\u003c/a\u003e package internally, calling it\u0026rsquo;s function\n\u003ccode\u003etaxize::tax_name()\u003c/code\u003e. The \u003ccode\u003etaxize::tax_name()\u003c/code\u003e function throws useful messages to the user\nif their NCBI Entrez API key is not found, and gives them instructions on how to find it.\u003c/p\u003e","title":"condition control: I just want that message once"},{"content":"The longer you do anything, the more preferences you may develop for that thing. One of these things is making R packages.\nOne preference I\u0026rsquo;ve developed is in limiting package dependencies - or at least limiting to the least painful dependencies - in the packages I maintain. Ideally, if a base R solution can be done then do it that way. Everybody has base R packages if they are using R, so you can\u0026rsquo;t fail there, at least on package installation.\nThis is largely not about trusting individual packages (cf. Jeff Leek\u0026rsquo;s post), but trust does play a role in deciding which packages to use (see choosing among packages that do the same thing below).\nwhy? There\u0026rsquo;s sure to be different opinions on this, but this is why I defend this hill:\nWhy introduce more complexity if it can be avoided? It\u0026rsquo;s one more thing out of your control. sure, in a perfect world package API\u0026rsquo;s never break, at least after a certain version (v1 or so), but we can\u0026rsquo;t depend on that. Rolling your own solution for a problem (aka function/class/etc.) means its completely under your control There\u0026rsquo;s a lot of great packages out there. However, in my opinion, many packages, including many of my own, are targeted at interactive users, not necessarily optimizing for other packages to use. So even though a package may do X really well, you can also do X on your own if it\u0026rsquo;s simple enough. base R solutions Some examples of base R solutions I like to use rather than using an off the shelf package:\nRemove NULL elements from a list. The function function(l) Filter(Negate(is.null), l) is stolen from plyr::compact originally. I include it as a utility function in many of my packages. It\u0026rsquo;s simple base R stuff. Easy peasy. Extract string form another string based on a pattern. The function function(x, y) regmatches(x, regexpr(y, x)) is what stringr::str_extract used to do before it moved to wrapping stringi functions. I like the pattern of the input first, and your pattern second, but don\u0026rsquo;t want to import a huge dependency (stringi) if I know i just need a simple string extraction. Infix function %||%. originally saw this in dplyr, but now has left that package. the function: function(x, y) if (is.null(x) || length(x) == 0) y else x. It provides an elegant solution of a in place defined default value for when you can\u0026rsquo;t be certain of the result. It\u0026rsquo;s a very brief function, so no need to import a package just for this function. Check that a parameter input is of the right type. R doesn\u0026rsquo;t have type checking like some other languages. we can do it internally within the package though. There are packages to do this (check out assertr), but instead of importing a package, I do something like the below: assert \u0026lt;- function (x, y) { if (!is.null(x)) { if (!inherits(x, y)) { stop(deparse(substitute(x)), \u0026#34; must be of class \u0026#34;, paste0(y, collapse = \u0026#34;, \u0026#34;), call. = FALSE) } } } It seems simple enough that I don\u0026rsquo;t think importing a package is warranted.\nchoosing among packages that do the same thing I often need to combine many rows/lists into a data.frame in my packages. dplyr::bind_rows and data.table::rbindlist do this (there\u0026rsquo;s probably others too). I\u0026rsquo;ve found that data.table is a slightly/somewhat easier dependency WRT installation, so I commonly use the below function for binding named lists into rows of a data.frame, with the optional conversion to a tbl_df. function(x) { tibble::as_tibble((x \u0026lt;- data.table::setDF( data.table::rbindlist(x, use.names = TRUE, fill = TRUE, idcol = \u0026#34;id\u0026#34;)) )) } other bits Jim Hester did a presentation on the glue package: Glue Strings to Data with Glue - and emphasized on one slide that glue is for packages because it has zero dependencies, is customizable, and fast - all things you want in a dependency in your own package. As I was wrapping up this post I found a few papers: Claes et al. 1 found that \u0026ldquo;occurrence of errors in CRAN packages over a period of three months \u0026hellip; resulted mostly from updates in the packages’ dependencies \u0026hellip;\u0026rdquo; In another paper Plakidas et al. 2 extend the previous finding and say \u0026ldquo;\u0026hellip; this potentially implies a heavy workload for package maintainers when they depend on a package that is frequently updated\u0026rdquo; These statements mirror my hesitation to depend on other packages if in fact X task can be done internally Contributors: if you do write your own internal functions, or borrow from elsewhere, new contributors to your package may need to understand your internal function instead of an imported function from another package - but the plus side is if the function resides in your own package you can change it easily. Rapid development phase: often package development involves a rapid change phase where you want to get to a working prototype first, then refine later. During this development phase it makes sense to use off the shelf packages to get things working. Later, you may want to swap out packages or write your own R or compiled code to speed things up, or introduce different behavior, etc. but There are of course good reasons to just import the package that\u0026rsquo;s best at doing X or Y and leave it at that. Sometimes I do that too. I don\u0026rsquo;t always stay on my hill.\nAnd: Maybe I\u0026rsquo;m totally wrong here? Maybe I should be at all times using other packages to do X, Y, and Z? Despite the dozens of packages I maintain, I am fully aware I could be completely wrong here.\nthanks to Maëlle Salmon for helpful advice on this post!\nreferences Claes, M., Mens, T., \u0026amp; Grosjean, P. (2014). On the maintainability of CRAN packages. 2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE). https://doi.org/10.1109/csmr-wcre.2014.6747183\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPlakidas, K., Schall, D., \u0026amp; Zdun, U. (2017). Evolution of the R software ecosystem: Metrics, relationships, and their impact on qualities. Journal of Systems and Software, 132, 119–146. https://doi.org/10.1016/j.jss.2017.06.095\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/2018/10/limiting-dependencies/","summary":"\u003cp\u003eThe longer you do anything, the more preferences you may develop for that thing. One of these things is making R packages.\u003c/p\u003e\n\u003cp\u003eOne preference I\u0026rsquo;ve developed is in limiting package dependencies - or at least limiting to the least painful dependencies - in the packages I maintain. Ideally, if a base R solution can be done then do it that way. Everybody has base R packages if they are using R, so you can\u0026rsquo;t fail there, at least on package installation.\u003c/p\u003e","title":"limiting dependencies in R package development"},{"content":"I occasionally think about these various topics and ping back and forth between them, thinking I\u0026rsquo;ve got to make a package more user friendly, then back to thinking oh, I really should make this package easier to maintain, but what if that makes it less user friendly?\nI\u0026rsquo;ve wanted to get these thoughts written down for a while now, so here goes.\nUser friendliness and code fragility It\u0026rsquo;s an unassailable good to make your code more user friendly. There\u0026rsquo;s no point of making your package harder to use unless you really don\u0026rsquo;t want people using it.\nHaving said that, can a user friendly API come at the cost of code simplicity/maintainability?\nAn example of user friendly code vs. not user friendly code is: Let\u0026rsquo;s say you have a function foo(). There\u0026rsquo;s a lot of things you can do to make the function user friendly, e.g., the function:\nerrors/returns as early as possible has good documentation has well named parameters returns easy to understand output (see also good docs) handles complexity sufficiently so the user doesn\u0026rsquo;t have to This is all well and good, and most of the points above don\u0026rsquo;t have to trade off with making code more complex/harder to maintain. However, the last point does I think.\nThat is, handling complexity for the user is a good thing, BUT it makes for more code and probably more complex code. I\u0026rsquo;ll highlight one particular example of this that I often deal with.\nPagination I make many packages that interact with web APIs, many of which have pagination. Pagination is just as it sounds: you don\u0026rsquo;t get back all possible results for your query but instead you get back a certain number of results, then you have to request the next set, and so on. This helps lighten the load on the server delivering the data. And pagination is useful for users so you can get a sense of what the data looks like without have to wait for all the data, which in some cases can be quite large.\nHere\u0026rsquo;s the question: Do you let the user handle pagination themselves with parameters to a function foo()? Or do you handle pagination internally within the function foo() with the user just stating how many results they want? The former scenario means that if the user wants 30 results they do:\nfoo(limit = 10, page = 1) foo(limit = 10, page = 2) foo(limit = 10, page = 3) # ... and so on While the latter means:\nfoo(limit = 30) The second example is definitely easier for the user. There are still three HTTP requests being made, so probably the code runs no faster, but it\u0026rsquo;s easier from a user standpoint.\nHere\u0026rsquo;s how foo() might handle the paging internally:\nmyGET \u0026lt;- function(x) { conn \u0026lt;- crul::HttpClient$new(\u0026#34;https://someurl.com\u0026#34;) res \u0026lt;- conn$get() txt \u0026lt;- res$parse(encoding = \u0026#34;UTF-8\u0026#34;) jsonlite::fromJSON(txt) } foo \u0026lt;- function(limit = 10) { limit \u0026lt;- plyr::round_any(limit, 10) out \u0026lt;- list() for (i in seq(limit / 10)) out[[i]] \u0026lt;- myGET(limit, page = i) df \u0026lt;- dplyr::bind_rows(out) return(df) } This is psuedocode, so you can\u0026rsquo;t run this.\nIn general I like to return data.frame\u0026rsquo;s to users whenever possible as I think most users are most familiar with data.frame\u0026rsquo;s.\nIn the above example we need to do a few things when dealing with pagination:\nsort out how many requests to make. the above doesn\u0026rsquo;t yet check that the limit value is a numeric or integer, and there\u0026rsquo;s all kinds of edge cases depending on what number is given by the user with respect to pagination make each http request. I used a for loop, but anything similar can be used. one needs to decide how to handle errors if you\u0026rsquo;re doing multiple requests. do you stop with an error if there\u0026rsquo;s an error in one of the requests, or do you catch that and proceed? If you do catch it how do you let the user know, or do you just remove that error from results? combine results into a single output (data.frame most likely/ideally). we want the user to get the same results back whether they request one page of results or many, so we need to do the work to make sure the output looks the same. This step also introduces possible pain points in that any result record can have novel things in it that cause your result combining code to error. Do you do a minimal combination approach (e.g. let jsonlite::fromJSON convert to list/data.frame\u0026rsquo;s where possible; but this means that there can be nested lists in data.frame\u0026rsquo;s, which many users do not like); or do you roll your own bespoke data munging/combination code to make sure the output data.frame is really easy to use with no nested lists, etc.? If you do the latter that will most likely be slower, but will be better output for the user. However, maybe most users want to combine the data on their own, so perhaps you should take up as little time as possible parsing/munging data so the user has to wait less time. The overall message here is that there\u0026rsquo;s many points throughout this process that require decisions to be made with respect to how much complexity you\u0026rsquo;d like to take care of yourself as the developer vs. how much you\u0026rsquo;d like to leave up to the user.\nWith complexity inside the function, there\u0026rsquo;s more to maintain and more possible bugs, but it\u0026rsquo;s easier for the user.\nWith complexity exposed to the user, and simpler code inside the function, each user has to sort out how to work with the output and/or do the pagination (or whatever it is) themselves. But with less complexity inside the function, there will likely be fewer bugs.\nNote: i\u0026rsquo;ve been trying to make pagination with web APIs easier, check out the Paginator helper in the crul package https://github.com/ropensci/crul/blob/master/R/paginator.R\nSo what? Perhaps others have figured this out and I\u0026rsquo;m the only one struggling with this? I\u0026rsquo;m sure I\u0026rsquo;ll continue to go back and forth on this pendulum. Would love to know how others think about this.\n","permalink":"http://localhost:1313/2018/07/friendliness-fragility/","summary":"\u003cp\u003eI occasionally think about these various topics and ping back and forth between them, thinking I\u0026rsquo;ve got to make a package more user friendly, then back to thinking oh, I really should make this package easier to maintain, but what if that makes it less user friendly?\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve wanted to get these thoughts written down for a while now, so here goes.\u003c/p\u003e\n\u003ch2 id=\"user-friendliness-and-code-fragility\"\u003eUser friendliness and code fragility\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s an unassailable good to make your code more user friendly. There\u0026rsquo;s no point of making your package harder to use unless you really don\u0026rsquo;t want people using it.\u003c/p\u003e","title":"Balancing user friendliness and code fragility"},{"content":"Why Butte County? I went to college at California State University, Chico - in Butte County, CA. I did a BA degree in Biology there. It was a great program as it was heavily focused on natural history - with classes on herps, birds, insects, fish, etc.\nSpecimen collections data Specimen collections data are increasingly being digitized, and often accessed via largeish platforms like GBIF and iDigBio. Here I\u0026rsquo;ll explore Butte County data found with iDigBio with the spocc R package. You could also use the ridigbio package to go directly to iDigBio data.\nGet some data Get county GeoJSON data using jqr\nlibrary(jqr) butte \u0026lt;- jq(url(\u0026#34;https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_5m.json\u0026#34;), \u0026#39;.features[] | select(.properties.NAME == \u0026#34;Butte\u0026#34; and .properties.STATE == \u0026#34;06\u0026#34;)\u0026#39;) mean_lon \u0026lt;- mean(as.numeric(jq(butte, \u0026#34;.geometry.coordinates[][] | first\u0026#34;))) mean_lat \u0026lt;- mean(as.numeric(jq(butte, \u0026#34;.geometry.coordinates[][] | last\u0026#34;))) Install spocc\nif (!requireNamespace(\u0026#34;spocc\u0026#34;)) install.packages(\u0026#34;spocc\u0026#34;) library(spocc) Search for data in Butte County. First lets get a look at how many records there are:\nopts \u0026lt;- list(rq = list( stateprovince = \u0026#34;California\u0026#34;, county = \u0026#34;Butte\u0026#34;, geopoint = list(type = \u0026#34;exists\u0026#34;) )) res \u0026lt;- occ(from = \u0026#34;idigbio\u0026#34;, idigbioopts = opts, limit = 1) res$idigbio$meta$found #\u0026gt; [1] 45075 Looks like 45075 records found. Now let\u0026rsquo;s get all those records\nres \u0026lt;- occ(from = \u0026#34;idigbio\u0026#34;, idigbioopts = opts, limit = 46000L) Make a map!\nres$idigbio$data$custom_query$name \u0026lt;- res$idigbio$data$custom_query$name[1] library(mapr) mapr::map_leaflet(res) %\u0026gt;% leaflet::addGeoJSON(butte) %\u0026gt;% leaflet::setView(mean_lon, mean_lat, 10) note: there\u0026rsquo;s definitely points that fall outside of Butte County, but we\u0026rsquo;ll ignore those for simplicity sake\nWhat\u0026rsquo;s the taxonomic breakdown like?\nlibrary(dplyr) df \u0026lt;- res$idigbio$data$custom_query (x \u0026lt;- df %\u0026gt;% group_by(class) %\u0026gt;% summarise(n = n()) %\u0026gt;% arrange(desc(n))) #\u0026gt; # A tibble: 42 x 2 #\u0026gt; class n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 magnoliopsida 25449 #\u0026gt; 2 liliopsida 9174 #\u0026gt; 3 \u0026lt;NA\u0026gt; 6355 #\u0026gt; 4 insecta 1490 #\u0026gt; 5 polypodiopsida 891 #\u0026gt; 6 pinopsida 351 #\u0026gt; 7 aves 283 #\u0026gt; 8 bryopsida 255 #\u0026gt; 9 lycopodiopsida 161 #\u0026gt; 10 equisetopsida 99 #\u0026gt; # ... with 32 more rows Looks like the vast, vast majority are plants, and more specifically Magnoliopsida (56%). About 3% are insects; about 0.6% birds; 0.1% reptiles; and 0.11% mammals.\nFirst, get Butte County data in a sp class\nlibrary(sp) library(ggplot2) county \u0026lt;- map_data(\u0026#34;county\u0026#34;) butte_co \u0026lt;- filter(county, region == \u0026#34;california\u0026#34;, subregion == \u0026#34;butte\u0026#34;) butte_poly \u0026lt;- SpatialPolygons(list(Polygons(list(Polygon(butte_co[, c(1,2)])), ID=1))) Insects:\nlibrary(tmap) insects \u0026lt;- df %\u0026gt;% dplyr::filter(class == \u0026#34;insecta\u0026#34;) coordinates(insects) \u0026lt;- c(\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;) tm_shape(butte_poly) + tm_borders() + tm_shape(insects) + tm_symbols(col = \u0026#34;black\u0026#34;, border.col = \u0026#34;white\u0026#34;, size = 0.5, alpha = 0.5) Birds:\nbirds \u0026lt;- df %\u0026gt;% dplyr::filter(class == \u0026#34;aves\u0026#34;) coordinates(birds) \u0026lt;- c(\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;) tm_shape(butte_poly) + tm_borders() + tm_shape(birds) + tm_symbols(col = \u0026#34;black\u0026#34;, border.col = \u0026#34;white\u0026#34;, size = 0.5, alpha = 0.5) Facet by taxonomic group\nlibrary(sp) library(rgeos) library(ggplot2) library(tmap) # get subset of data for particular classes # this is a very large portion of the data df_class_subset \u0026lt;- df %\u0026gt;% filter(class %in% c(\u0026#34;magnoliopsida\u0026#34;, \u0026#34;liliopsida\u0026#34;, NA, \u0026#34;insecta\u0026#34;, \u0026#34;pinopsida\u0026#34;, \u0026#34;aves\u0026#34;, \u0026#34;amphibia\u0026#34;, \u0026#34;mammalia\u0026#34;, \u0026#34;reptilia\u0026#34;)) coordinates(df_class_subset) \u0026lt;- c(\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;) # get butte county data into a polygon county \u0026lt;- map_data(\u0026#34;county\u0026#34;) butte_co \u0026lt;- filter(county, region == \u0026#34;california\u0026#34;, subregion == \u0026#34;butte\u0026#34;) butte_poly \u0026lt;- SpatialPolygons(list(Polygons(list(Polygon(butte_co[, c(1,2)])), ID=1))) # clip data to butte county df_class_subset_clipped \u0026lt;- raster::intersect(df_class_subset, butte_poly) tm_shape(butte_poly) + tm_borders() + tm_shape(df_class_subset_clipped) + tm_symbols(col = \u0026#34;black\u0026#34;, border.col = \u0026#34;white\u0026#34;, size = 0.5, alpha = 0.5) + tm_facets(by = \u0026#34;class\u0026#34;, nrow = 3, free.coords = FALSE) Collectors. Lowell Ahart is kind of a legend in Butte County, collecting plants like crazy. Let\u0026rsquo;s see how many records he has\ndf %\u0026gt;% filter(grepl(\u0026#34;ahart\u0026#34;, collector, ignore.case = TRUE)) #\u0026gt; # A tibble: 15,864 x 83 #\u0026gt; associatedsequenc… barcodevalue basisofrecord bed canonicalname #\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 NA NA preservedspeci… NA isolepis setacea #\u0026gt; 2 NA NA preservedspeci… NA fallopia convolv… #\u0026gt; 3 NA NA preservedspeci… NA carex densa #\u0026gt; 4 NA NA preservedspeci… NA datura wrightii #\u0026gt; 5 NA NA preservedspeci… NA hieracium argutum #\u0026gt; 6 NA NA preservedspeci… NA centunculus mini… #\u0026gt; 7 NA NA preservedspeci… NA dryopteris arguta #\u0026gt; 8 NA NA preservedspeci… NA epilobium cleist… #\u0026gt; 9 NA NA preservedspeci… NA psilocarphus ten… #\u0026gt; 10 NA NA preservedspeci… NA lycium barbarum #\u0026gt; # ... with 15,854 more rows, and 78 more variables: catalognumber \u0026lt;chr\u0026gt;, #\u0026gt; # class \u0026lt;chr\u0026gt;, collectioncode \u0026lt;chr\u0026gt;, collectionid \u0026lt;chr\u0026gt;, #\u0026gt; # collectionname \u0026lt;lgl\u0026gt;, collector \u0026lt;chr\u0026gt;, commonname \u0026lt;chr\u0026gt;, #\u0026gt; # commonnames \u0026lt;list\u0026gt;, continent \u0026lt;chr\u0026gt;, coordinateuncertainty \u0026lt;dbl\u0026gt;, #\u0026gt; # country \u0026lt;chr\u0026gt;, countrycode \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, datasetid \u0026lt;chr\u0026gt;, #\u0026gt; # datecollected \u0026lt;date\u0026gt;, datemodified \u0026lt;chr\u0026gt;, dqs \u0026lt;dbl\u0026gt;, #\u0026gt; # earliestageorloweststage \u0026lt;lgl\u0026gt;, earliesteonorlowesteonothem \u0026lt;lgl\u0026gt;, #\u0026gt; # earliestepochorlowestseries \u0026lt;chr\u0026gt;, earliesteraorlowesterathem \u0026lt;lgl\u0026gt;, #\u0026gt; # earliestperiodorlowestsystem \u0026lt;chr\u0026gt;, etag \u0026lt;chr\u0026gt;, eventdate \u0026lt;chr\u0026gt;, #\u0026gt; # family \u0026lt;chr\u0026gt;, fieldnumber \u0026lt;chr\u0026gt;, flags \u0026lt;list\u0026gt;, formation \u0026lt;chr\u0026gt;, #\u0026gt; # genus \u0026lt;chr\u0026gt;, geologicalcontextid \u0026lt;lgl\u0026gt;, longitude \u0026lt;dbl\u0026gt;, #\u0026gt; # latitude \u0026lt;dbl\u0026gt;, group \u0026lt;lgl\u0026gt;, hasImage \u0026lt;lgl\u0026gt;, hasMedia \u0026lt;lgl\u0026gt;, #\u0026gt; # highertaxon \u0026lt;chr\u0026gt;, highestbiostratigraphiczone \u0026lt;lgl\u0026gt;, #\u0026gt; # individualcount \u0026lt;int\u0026gt;, infraspecificepithet \u0026lt;chr\u0026gt;, #\u0026gt; # institutioncode \u0026lt;chr\u0026gt;, institutionid \u0026lt;chr\u0026gt;, institutionname \u0026lt;lgl\u0026gt;, #\u0026gt; # kingdom \u0026lt;chr\u0026gt;, latestageorhigheststage \u0026lt;lgl\u0026gt;, #\u0026gt; # latesteonorhighesteonothem \u0026lt;lgl\u0026gt;, latestepochorhighestseries \u0026lt;lgl\u0026gt;, #\u0026gt; # latesteraorhighesterathem \u0026lt;lgl\u0026gt;, latestperiodorhighestsystem \u0026lt;lgl\u0026gt;, #\u0026gt; # lithostratigraphicterms \u0026lt;lgl\u0026gt;, locality \u0026lt;chr\u0026gt;, #\u0026gt; # lowestbiostratigraphiczone \u0026lt;lgl\u0026gt;, maxdepth \u0026lt;lgl\u0026gt;, maxelevation \u0026lt;dbl\u0026gt;, #\u0026gt; # mediarecords \u0026lt;list\u0026gt;, member \u0026lt;lgl\u0026gt;, mindepth \u0026lt;lgl\u0026gt;, minelevation \u0026lt;dbl\u0026gt;, #\u0026gt; # municipality \u0026lt;chr\u0026gt;, occurrenceid \u0026lt;chr\u0026gt;, order \u0026lt;chr\u0026gt;, phylum \u0026lt;chr\u0026gt;, #\u0026gt; # recordids \u0026lt;list\u0026gt;, recordnumber \u0026lt;chr\u0026gt;, recordset \u0026lt;chr\u0026gt;, name \u0026lt;chr\u0026gt;, #\u0026gt; # specificepithet \u0026lt;chr\u0026gt;, startdayofyear \u0026lt;int\u0026gt;, stateprovince \u0026lt;chr\u0026gt;, #\u0026gt; # taxonid \u0026lt;chr\u0026gt;, taxonomicstatus \u0026lt;chr\u0026gt;, taxonrank \u0026lt;chr\u0026gt;, #\u0026gt; # typestatus \u0026lt;chr\u0026gt;, uuid \u0026lt;chr\u0026gt;, verbatimeventdate \u0026lt;chr\u0026gt;, #\u0026gt; # verbatimlocality \u0026lt;chr\u0026gt;, version \u0026lt;lgl\u0026gt;, waterbody \u0026lt;chr\u0026gt;, prov \u0026lt;chr\u0026gt; Wow. That\u0026rsquo;s a big portion of the total records in the county.\n","permalink":"http://localhost:1313/2018/06/butte-county/","summary":"\u003ch1 id=\"why-butte-county\"\u003eWhy Butte County?\u003c/h1\u003e\n\u003cp\u003eI went to college at California State University, Chico - in Butte County, CA. I did a BA degree in Biology there. It was a great program as it was heavily focused on natural history - with classes on herps, birds, insects, fish, etc.\u003c/p\u003e\n\u003ch1 id=\"specimen-collections-data\"\u003eSpecimen collections data\u003c/h1\u003e\n\u003cp\u003eSpecimen collections data are increasingly being digitized, and often accessed via largeish platforms like GBIF and iDigBio. Here I\u0026rsquo;ll explore Butte County data found with iDigBio with the \u003ca href=\"https://github.org/ropensci/spocc\"\u003espocc\u003c/a\u003e R package. You could also use the \u003ca href=\"https://cran.r-project.org/web/packages/ridigbio/index.html\"\u003eridigbio\u003c/a\u003e package to go directly to iDigBio data.\u003c/p\u003e","title":"Exploring specimen collections data in Butte County, California"},{"content":"In rOpenSci - as in presumably most open source projects - we want the entire project to be sustainable, but also each individual software project to be sustainable.\nA big part of each software project (aka R package in this case) being sustainable is the people making it, particularly whether:\nhow many contributors a project has, and how contributions are spread across contibutors There are discussions going on about how to increase contributors to any given project. But the first thing to do is to do an assesment of where you\u0026rsquo;re at. One way to do that is visualization.\nWe can look at a sort of proxy of contributions, git commits, to get at this. This isn\u0026rsquo;t perfect because everyone differs in their \u0026ldquo;commit style\u0026rdquo;, where some make a lot of changes in a single commit, while others spread changes across commits. (one could look at additions/deletions of actual code for example)\nIn terms of where to get data, one could get it from the API of any of Github, Gitlab, Bitbucket, or using whatever local git repos you have on your machine. rOpenSci has a nice git R client called git2r maintained by Stefan Widgren. I have a lot of rOpenSci\u0026rsquo;s R packages locally on my machine, though not all of them.\nBelow is a first attempt at starting to look at the distribution of commits across rOpenSci packages. The visualization is meant to get a quick look across packages in terms of a) number of contributors to a package, and b) distribution of commits across each contributor within a package.\nthe actual work Load libraries\nlibrary(git2r) library(ggplot2) library(dplyr) Get directory paths. I was interested in specific packages, so I made a text file of certain repos, rather than getting all repos in my github/ropensci folder on my machine\ndirs \u0026lt;- readLines(\u0026#34;dirs.txt\u0026#34;) paths \u0026lt;- file.path(path.expand(\u0026#34;~/github/ropensci\u0026#34;), dirs) Function to get data.frame of commit authors\nmake_authors_table \u0026lt;- function(x) { repo \u0026lt;- git2r::repository(x) res \u0026lt;- commits(repo) auths \u0026lt;- vapply(res, function(z) z@author@name, character(1)) tmp \u0026lt;- data.frame(table(auths), stringsAsFactors = FALSE) tmp$auths \u0026lt;- as.character(tmp$auths) tmp } Get commit authors for each directory\ndat \u0026lt;- lapply(paths, make_authors_table) dat \u0026lt;- stats::setNames(dat, basename(paths)) Remove those with no rows (i.e., commits)\ndat \u0026lt;- Filter(function(z) NROW(z) \u0026gt; 0, dat) Since person names for commits can vary depending on where the person makes the commit from (a git GUI vs. cli vs. Github web interface, etc.), I made a little table for deduping names, and cleaned up each package\u0026rsquo;s commit summary.\ndups \u0026lt;- read.csv(\u0026#34;github_dups.csv\u0026#34;, stringsAsFactors=FALSE) dups$duplicates \u0026lt;- vapply(dups$duplicates, function(z) gsub(\u0026#34;,\u0026#34;, \u0026#34;|\u0026#34;, z), character(1)) dat \u0026lt;- lapply(dat, function(z) { z$auths \u0026lt;- unname(vapply(z$auths, function(w) { mtch \u0026lt;- grepl(w, dups$duplicates) if (any(mtch)) dups$name_to_use[mtch] else w }, character(1))) aggregate(Freq ~ auths, data = z, FUN = sum) }) Reorder each data.frame by number of commits (the Freq column)\ndat \u0026lt;- lapply(dat, function(x) dplyr::arrange(x, Freq)) Combine into single data.frame, and make a column order so ggplot doesn\u0026rsquo;t mess up our ordering in each facet\ndf \u0026lt;- dplyr::bind_rows(dat, .id = \u0026#39;id\u0026#39;) df$order \u0026lt;- seq_len(NROW(df)) head(df) #\u0026gt; id auths Freq order #\u0026gt; 1 agent jeroen 8 1 #\u0026gt; 2 ALA4R Dave Martin 1 2 #\u0026gt; 3 ALA4R mbohun 1 3 #\u0026gt; 4 ALA4R rforge rforge 1 4 #\u0026gt; 5 ALA4R Tom Saleeba 3 5 #\u0026gt; 6 ALA4R Tasilee 53 6 Make the plot\nEach panel is an ropensci package Each dot is a person for the most part (I tried to remove duplicates, but there\u0026rsquo;s still some) Dots are arranged from less to more commits (from left to right) ggplot(df, aes(order, Freq)) + geom_point(size = 0.5) + facet_wrap(~ id, scales = \u0026#34;free\u0026#34;) + theme( axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), strip.text.x = element_blank() ) Curious what the packages are? Check out the same plot but with facet titles with package names.\nSome observations There\u0026rsquo;s quite a few packages with a single contributor. These could be targeted first possibly for getting at least one additional contrib. Of those that have more than one contributor, there\u0026rsquo;s often a large jump between the person with the most commits and the next most. We could target making that a smoother transition - that is, less of a jump between the main contributor and the others ","permalink":"http://localhost:1313/2018/02/git-commits/","summary":"\u003cp\u003eIn rOpenSci - as in presumably most open source projects - we want the entire project to be sustainable, but also each individual software project to be sustainable.\u003c/p\u003e\n\u003cp\u003eA big part of each software project (aka R package in this case) being sustainable is the people making it, particularly whether:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehow many contributors a project has, and\u003c/li\u003e\n\u003cli\u003ehow contributions are spread across contibutors\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are discussions going on about how to increase contributors to any given project. But the first thing to do is to do an assesment of where you\u0026rsquo;re at. One way to do that is visualization.\u003c/p\u003e","title":"Exploring git commits with git2r"},{"content":"Sublime Text is pretty great.\nLet\u0026rsquo;s start at the beginning.\nWhy would my primary editing tool not be vim? My background is as a biologist, spending way to many years in grad school. My first programming language was R back in 2006; my first text editor about the same year was Notepad++; my first interaction with the cli was probably a year later or so (but that was on Windows).\nAfter using Notepad++ for a few years, I stumbled upon Sublime Text via advice from a friend. I used it for a few years without paying (which you can still do), and after that realized it was worth paying for. They now have an easy to use Discourse forum too.\nAnyway, vim is too intimidating to me, and at this point I like my workflow enough to not want to spend the time learning vim.\nWhy Sublime Text? I have found Sublime Text to be faster than at least Atom, ESPECIALLY when dealing with large files. Atom seems to get bogged down with a very large file and you can take a long time to scroll through it.\nI like the idea of supporting the small team behind Sublime Text rather than the presumably large team behind Atom or the presumably extremely large team behind vscode.\nCan\u0026rsquo;t think of any other reasons - the speed one is enough reason :)\np.s. I\u0026rsquo;ve never tried vscode.\nWhy R and Sublime Text? RStudio has been great, and I still use it from time to time. However, it\u0026rsquo;s started to be buggy here and there, and anyway, I do most everything else in Sublime Text so it doesn\u0026rsquo;t take much to push me towards Sublime Text for every project I do. I definitely prefer RStudio when working on R packages that include C++ code as it helps me out a lot as a C++ newb.\nThe key thing that brought me over to using Sublime Text and R is being able to send code reliably to my cli of choice with a keyboard shortcut (see SendCode below).\nI do development in other languages besides R - primarily Ruby and Python - so having a primary workflow that is agnostic with respect to the programming language is really nice.\nMy Sublime Text packages (check out SB packages at https://packagecontrol.io/)\nGitSavvy lots of good Git goodies MarkdownEditing gotta have markdown support R-Box lots of R tools including building and installing packages SendCode crucial for using Sublime Text and R on the cli - sends code from Sublime Text to your cli of choice. I use iTerm2 as my cli. GotoWindow speeds up your work a lot by quickly switching between projects with a keyboard shortcut Marked integration Marked is a great markdown app - really nice to have an integration with Marked to quickly open it from Sublime Other useful things Do make sure to enable subl - a shortcut cli command to open Sublime Text - see https://stackoverflow.com/questions/16199581/open-sublime-text-from-terminal-in-macos for discussion of how to do it. It\u0026rsquo;s use case for me is opening up the current directory on your cli in Sublime Text. Set Sublime Text as your default editor for git work, check out https://help.github.com/articles/associating-text-editors-with-git/ for guidance Unrelated to Sublime Text, but if you go this route of separate text editor and cli, then arranging windows can start to slow you down a bit. On osx, there\u0026rsquo;s a little app called Spectacle that provides keyboard shortcuts for easily arranging windows in lots of combinations. The setup ","permalink":"http://localhost:1313/2018/01/sublime-text-workflow/","summary":"\u003cp\u003e\u003ca href=\"https://www.sublimetext.com/\"\u003eSublime Text\u003c/a\u003e is pretty great.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start at the beginning.\u003c/p\u003e\n\u003cp\u003eWhy would my primary editing tool not be vim? My background is as a biologist, spending way to many years in grad school. My first programming language was \u003ca href=\"https://www.r-project.org/\"\u003eR\u003c/a\u003e back in 2006; my first text editor about the same year was \u003ca href=\"https://notepad-plus-plus.org/\"\u003eNotepad++\u003c/a\u003e; my first interaction with the cli was probably a year later or so (but that was on Windows).\u003c/p\u003e\n\u003cp\u003eAfter using Notepad++ for a few years, I stumbled upon Sublime Text via advice from a friend. I used it for a few years without paying (which you can still do), and after that realized it was worth paying for. They now have an easy to use \u003ca href=\"https://forum.sublimetext.com/\"\u003eDiscourse forum\u003c/a\u003e too.\u003c/p\u003e","title":"My Sublime Text workflow/setup"},{"content":"I was returning to a long-term project I\u0026rsquo;ve been working on - a package for caching HTTP requests in R called vcr, a port of the Ruby gem vcr - when you do that thing you do when you are porting a library from one language to another. I stumbled upon some methods/functions I wasn\u0026rsquo;t familiar with.\nFor example, take_while I had never seeen before. It iterates over an array, returning the elements of the array that evalulate to true (for those new to Ruby, they use true instead of TRUE as we do in R) when passed through the function given. R has lists and vectors - R\u0026rsquo;s lists are the most similar to Ruby arrays because both can have mixed objects in them (e.g., a string and an integer) while still retaining those objects as is.\nIn another example, I had never seen unshift or it\u0026rsquo;s sister shift. unshift is pretty simple - it prepends objects to the front of the array. shift has more complicated behavior - called without values passed deletes first element of the array, AND returns that deleted value. With shift you can also pass an index that is treated as a range (e.g., 1 is treated as 0 and 1; Ruby has zero based indexing, unlike R\u0026rsquo;s 1 based indexing).\nAnyway, I wanted to explore these new Ruby methods more by trying to implement them in R. Thus, started a bag of functions package called rubfuns for \u0026ldquo;Ruby functions\u0026rdquo; to play while being able to have documentation, etc.\nIt\u0026rsquo;s entirely possible the stuff in rubfuns is already implemented in R elsewhere - the point is for me to learn more about both Ruby and R.\nA big difference between Ruby and R is that Rubys\u0026rsquo;s arrays have methods that can be called on them, e.g.\na = [1, 2, 3] a.count =\u0026gt; 3 Whereas the equivalent in R requires passing the vector to a method, rather than calling the method on the object itself\na = c(1, 2, 3) length(a) #\u0026gt; [1] 3 Of course one could create a R6 object in R and add methods to that object that can be called on a vector:\nlibrary(\u0026#34;R6\u0026#34;) Vec \u0026lt;- R6::R6Class( \u0026#34;Vec\u0026#34;, public = list( x = NULL, initialize = function(x) { self$x \u0026lt;- x }, count = function() length(self$x) ) ) myvec \u0026lt;- Vec$new(1:3) myvec$count() #\u0026gt; [1] 3 But that\u0026rsquo;s not baked into R itself, so not ideal.\nAnyway, on with rubfuns:\ndevtools::install_github(\u0026#34;ropenscilabs/rubfuns\u0026#34;) library(\u0026#34;rubfuns\u0026#34;) take_while\nx \u0026lt;- c(1, 2, 3, 4, 5, 0) x %\u0026gt;% take_while(function(z) z \u0026lt; 3) #\u0026gt; [1] 1 2 0 x \u0026lt;- c(1, 2, 3, 4, 9, -1) x %\u0026gt;% take_while(function(z) z \u0026lt; 3) #\u0026gt; [1] 1 2 -1 drop_while is a similar function to take_while but drops the elements that when passed to the supplied function evaluate to TRUE\nx \u0026lt;- c(1, 2, 3, 4, 5, 0) x %\u0026gt;% drop_while(function(z) z \u0026lt; 3) #\u0026gt; [1] 3 4 5 x \u0026lt;- c(1, 2, 3, 4, 9, -1) x %\u0026gt;% drop_while(function(z) z \u0026lt; 3) #\u0026gt; [1] 3 4 9 delete_at was in interesting function I saw in vcr. It deletes the elements of an array at the positions given (remember, 0 based indexing in Ruby)\nx \u0026lt;- c(1, 2, 3, 4, 5, 0) delete_at(x, 5) #\u0026gt; [1] 1 2 3 4 0 delete_at(x, 4:5) #\u0026gt; [1] 1 2 3 0 delete_if is similar to delete_at but instead you pass a function that when evaluates to TRUE deletes that element\nx \u0026lt;- c(1, 2, 3, 4, 5, 0) delete_if(x, function(z) z \u0026gt; 2) #\u0026gt; [1] 1 2 0 delete_if(x, function(z) z \u0026lt; 4) #\u0026gt; [1] 4 5 unshift is quite simple. it prepends whatever you pass to it to the front of the vector\nx \u0026lt;- c(1, 2, 3) x %\u0026gt;% unshift(4) #\u0026gt; [1] 4 1 2 3 shift is more complicated. called without any values deletes the first element. called with a value deletes all elements up to and including that value\nx \u0026lt;- c(1, 2, 3) x %\u0026gt;% shift #\u0026gt; [1] 2 3 x %\u0026gt;% shift(2) #\u0026gt; [1] 3 That\u0026rsquo;s all I\u0026rsquo;ve got so far. Will likely add more functions as time goes on.\nUnfortunately we can\u0026rsquo;t follow what Ruby does by being able to modify the vector or list while also returning something. There are of course ways to achieve this, e.g., R6 solution above or something like zeallot - but if it\u0026rsquo;s not baked into the R language it seems less likely to get wide adoption.\ntodo: plan to make sure the functions work with vectors and lists\n","permalink":"http://localhost:1313/2018/01/ruby-and-r/","summary":"\u003cp\u003eI was returning to a long-term project I\u0026rsquo;ve been working on - a package for caching HTTP requests in R called \u003ca href=\"https://github.com/ropensci/vcr\"\u003evcr\u003c/a\u003e, a port of the Ruby gem \u003ca href=\"https://github.com/vcr/vcr\"\u003evcr\u003c/a\u003e - when you do that thing you do when you are porting a library from one language to another. I stumbled upon some methods/functions I wasn\u0026rsquo;t familiar with.\u003c/p\u003e\n\u003cp\u003eFor example, \u003ca href=\"https://apidock.com/ruby/Array/take_while\"\u003etake_while\u003c/a\u003e I had never seeen before. It iterates over an array, returning the elements of the array that evalulate to \u003ccode\u003etrue\u003c/code\u003e (for those new to Ruby, they use \u003ccode\u003etrue\u003c/code\u003e instead of \u003ccode\u003eTRUE\u003c/code\u003e as we do in R) when passed through the function given. R has lists and vectors - R\u0026rsquo;s lists are the most similar to Ruby arrays because both can have mixed objects in them (e.g., a string and an integer) while still retaining those objects as is.\u003c/p\u003e","title":"Playing with Ruby Patterns in R"},{"content":"The problem The R community has a package distribution thing called CRAN just like Ruby has Rubygems, and Python has Pypi, etc. On all packages on CRAN, the CRAN maintainers run checks on each package on multiple versions of R and on many operating systems. They report those results on a page associated with the package, like this one.\nYou might be thinking: okay, but we have Travis-CI and friends, so who cares about that? Well, it\u0026rsquo;s these checks that CRAN runs that will determine if your package on CRAN leads to emails to you asking for changes, and possibly the package being taken down if e.g., they email and you don\u0026rsquo;t respond for a period of time.\nSo CRAN provides these package checks. Now what? Ideally, these would be available through an API so that the data is machine readable, which then makes many other things possible (see What\u0026rsquo;s Next below).\nSo how to build the API?\nBuilding the CRAN checks API On GitHub: https://github.com/ropensci/cchecksapi\nMy main goal learning goals with this API tech wise were two fold:\nlearn how to dockerize the application learn how to use MongoDB I hadn\u0026rsquo;t Dockerized a web API myself before, so that was an important goal - and I had actually never used MongoDB, but wanted to give it a shot to get familiar with it.\nThe whole stack is:\nlanguage: Ruby web API framework: Sinatra http Ruby gem: faraday database: mongodb server: caddy container: all wrapped up in docker (docker-compose) hosting: Amazon EC2 scheduling: crontab At a high level, the system is as so:\nOnce a day a few Ruby scripts (for packages, for maintainers): collects the names of packages on CRAN from Gábor Csárdi\u0026rsquo;s https://crandb.r-pkg.org API and maintainer emails from CRAN itself, then goes out to the CRAN website and collects check results for each package, then insert data into a MongoDB database The API provides routes for getting data on specific packages by name, or all packages, and data on all packages for any given maintainers email adddress, or all maintainers API calls make a query into the MongoDB database matching on the package name or maintainer email address data is given back as JSON The API doesn\u0026rsquo;t currently use caching, but may add if it seems needed.\nRuby and Sinatra I really like Ruby. It\u0026rsquo;s a language that is fun to use, the community is great, and there\u0026rsquo;s tons of packages. Ruby is great for making web stuff, including web APIs. When doing web stuff, for me that means web APIs. For web APIs in Ruby, Rails is too heavy for all the stuff I do - that\u0026rsquo;s where Sinatra comes in.\nSinatra is a lightweight framework for making web apps/APIs. I make all my web APIs with Sinatra, and have had few complaints. Some may say \u0026ldquo;you should use X or Y because faster\u0026rdquo;, or whatever, but Sinatra is plenty fast for my use cases. Not every use case is \u0026ldquo;we\u0026rsquo;re Facebook\u0026rdquo;, or \u0026ldquo;we\u0026rsquo;re Google\u0026rdquo;.\nUntil recently I\u0026rsquo;ve been very much manually managing my Sinatra web APIs on servers - that is, installing/updating everything on the server itself, without using containers or any configuration management. This blog post is the blog post I would have wanted to read when I was figuring out how to dockerize my web APIs.\nThe API The main meat of the API is definitions of routes. In addition, I\u0026rsquo;ve included a number of rules about what HTTP verbs are allowed to be used, what headers to send in each response, how to respond to client and server failures, etc.\nThis is what one of the route definitions looks like:\nget \u0026#39;/pkgs/?\u0026#39; do headers_get begin lim = (params[:limit] || 10).to_i off = (params[:offset] || 0).to_i raise Exception.new(\u0026#39;limit too large (max 1000)\u0026#39;) unless lim \u0026lt;= 1000 d = $cks.find({}, {\u0026#34;limit\u0026#34; =\u0026gt; lim, \u0026#34;skip\u0026#34; =\u0026gt; off}) dat = d.to_a raise Exception.new(\u0026#39;no results found\u0026#39;) if d.nil? { found: d.count, count: dat.length, offset: nil, error: nil, data: dat }.to_json rescue Exception =\u0026gt; e halt 400, { count: 0, error: { message: e.message }, data: nil }.to_json end end This code chunk is for the /pkgs route on the API (check it out at https://cranchecks.info/pkgs). The headers_get bit sends a pre-defined set of headers in the response. The begin ... rescue ... end bit is a \u0026ldquo;try catch\u0026rdquo; thing - leading to a JSON failure response in case there is a failure - and a JSON response on success.\nCollecting data and MongoDB As stated above, data is updated once a day. The code for scraping data on the package level and maintainer level is pretty similar. For both, the steps are the following: a) collect all names (for /pkgs that\u0026rsquo;s package names from https://crandb.r-pkg.org; for /maintainers that\u0026rsquo;s maintainer email addresses from https://cran.rstudio.com/web/checks/check_summary_by_maintainer.html), b) for each package name or maintainer email scrape CRAN check results, c) with all data collected drop data in MongoDB and then load all new data (maybe this could be an update step?). You can see the gory details on GitHub for packages and maintainers.\nThose steps above in code for packages is like the following:\ndef scrape_all pkgs = cran_packages; # get all pkg names out = [] # make an array pkgs.each do |x| # for each pkg, scrape check results out \u0026lt;\u0026lt; scrape_pkg(x) end if $cks.count \u0026gt; 0 $cks.drop # drop data in Mongo $cks = $mongo[:checks] # recreate database in Mongo end $cks.insert_many(out.map { |e| prep_mongo(e) }) # load new data into Mongo end $cks is the MongoDB database connection.\nDocker For containerizing the API, I used Docker. A colleague had used Docker Compose, and it was a really easy experience spinning up and taking down the application we were working on. So I wanted to learn how to do that myself. After trial and error, finally got to a solution for this API. Here is my docker-compose.yml file:\nmongo: image: mongo volumes: - $HOME/data/mongodb:/data/db # persists data to disk outside container restart: always ports: - \u0026#34;27017:27017\u0026#34; api: build: . ports: - \u0026#34;8834:8834\u0026#34; links: - mongo This specifies the container for MongoDB and for the API, and specifies in the API container to link to the mongo container.\nTo build and run do\ndocker-compose build \u0026amp;\u0026amp; docker-compose up -d The -d flag is for daemonize, i.e., run in the background. To kill them run\ndocker-compose stop \u0026amp;\u0026amp; docker-compose rm Caddy server Caddy is great server. I never really used Nginx, so I can\u0026rsquo;t compare the two really - I just know that Caddy is super easy. To install, check out the installation page https://caddyserver.com/download, and it\u0026rsquo;s easy as something like curl https://getcaddy.com | bash -s personal (depends on configuration options on that page and license choice).\nI know there\u0026rsquo;s an option to run a separate container with Caddy, but I run Caddy outside containers.\nMy Caddyfile has something similar to the following:\ncranchecks.info { gzip tls email@foobar.com log / logfile.log \u0026#34;{remote} - [{when}] {method} {uri} {query} {proto} {status} {size} {\u0026gt;User-Agent}\u0026#34; { rotate_size 3 } proxy / localhost:8834 { transparent } } gzip tells Caddy to serve gzipp\u0026rsquo;ed content (see https://caddyserver.com/docs/gzip) tls says use the given email for registering with Letsencrypt (see https://caddyserver.com/docs/tls) log line specifies how to log requests (and rotate_size says start a new file when the current one reaches 3 MB) (see https://caddyserver.com/docs/log) proxy is for specifying reverse proxy (see https://caddyserver.com/docs/proxy) What\u0026rsquo;s Next There\u0026rsquo;s still more work to do.\nBetter /maintainers results right now, we have two arrays of results, one from the html table on the CRAN results page and the other from the text below it. This duplication isn\u0026rsquo;t ideal. it would be helpful to have a summary across all packages for any given maintainer Better /pkgs results it would be helpful to have a summary across all R versions and platforms for any given package Include actual CRAN check results - CRAN check results can include output of the failures (whether they\u0026rsquo;re in examples or the test suite, or an installation error). The API doesn\u0026rsquo;t currently include that output, but thinking about how it could. Possibly update data more often. Right now we update once per day. Seems like results do roll in at different times though, perhaps as builds are done for each pkg? Notification service: package maintainers can opt-in to notifications when their checks are failing so they can be on top of fixes quickly. This could be managed through the API itself, with no GUI, but to make it palatable to all types may want to make a super simple web page to sign up. Check out the issue tracker to follow progress or file a feature request or bug.\nThanks Thanks to Gábor Csárdi for the idea to make a /maintainers route.\nFurther reading In a similar post Cloud66 folks talked about deploying an API with the same stack essentially: Sinatra, MongoDB, and Docker.\np.s. I mostly write about R software, so some readers may use R: if you want to make a web API but only know R, try learning Ruby! It can\u0026rsquo;t hurt to learn Ruby, and you\u0026rsquo;ll be happy you did.\n","permalink":"http://localhost:1313/2017/11/sinatra-mongo-docker-caddy/","summary":"\u003ch2 id=\"the-problem\"\u003eThe problem\u003c/h2\u003e\n\u003cp\u003eThe R community has a package distribution thing called \u003ca href=\"https://cran.rstudio.com/web/packages/\"\u003eCRAN\u003c/a\u003e just like Ruby has \u003ca href=\"https://rubygems.org/\"\u003eRubygems\u003c/a\u003e, and Python has \u003ca href=\"https://pypi.python.org/pypi\"\u003ePypi\u003c/a\u003e, etc. On all packages on CRAN, the CRAN maintainers run checks on each package on multiple versions of R and on many operating systems. They report those results on a page associated with the package, like \u003ca href=\"https://cran.rstudio.com/web/checks/check_results_crul.html\"\u003ethis one\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou might be thinking: okay, but we have Travis-CI and friends, so who cares about that?  Well, it\u0026rsquo;s these checks that CRAN runs that will determine if your package on CRAN leads to emails to you asking for changes, and possibly the package being taken down if e.g., they email and you don\u0026rsquo;t respond for a period of time.\u003c/p\u003e","title":"Web APIs with Sinatra, Mongo, Docker, and Caddy"},{"content":"I wrote about Crossref clients back nearly two years ago on this blog: Crossref programmatic clients.\nSince it\u0026rsquo;s been a while, it seems worth talking again about the the many ways to work programmatically with Crossref data - and focus in on the Python client habanero since it has some recent updates.\nThe 3 clients work with the main Crossref API, which lets you do things like search for works by title, author, etc. (e.g., books, articles), search for publishing members, for funders, for journals, for DOI prefixes, and for licenses. It\u0026rsquo;s a powerful API with basically no rate limits, so you can work through lots of data quickly.\nSome deets:\nCrossref API documentation: https://api.crossref.org Python client habanero: https://github.com/sckott/habanero Ruby client serrano: https://github.com/sckott/serrano R client rcrossref: https://github.com/ropensci/rcrossref At rOpenSci we\u0026rsquo;ve maintained the R client for quite a few years now, but the Python and Ruby clients were a result of consulting work I did for Crossref.\nThe R, Ruby, and Python clients are all quite feature complete, although software is never perfect :), and the thing about talking to an API to some other software is they can change stuff on their end - then we have to change suff on our end, on and on \u0026hellip;\nBack when the earlier blog post was written about these Crossref clients, we were at the first versions of both serrano and habanero. As you can see in the changelogs of the three clients (serrano, habanero, rcrossref) alot has changed in the last two years as we\u0026rsquo;ve made improvements and kept up with Crossref API changes.\nRuby and R Nothing new to report for the Ruby (serrano) and R (rcrossref) clients, though both will soon be getting the previous features just mentioned (mailto and select).\nPython: habanero I\u0026rsquo;ve just released a new version of habanero - v0.6. Noteable changes include adding ability to add a mailto to each request to get into the so called \u0026ldquo;polite pool\u0026rdquo;; select parameter added to select certain fields to get back; and the docs got a major overhaul (check em out at https://habanero.readthedocs.io/en/latest/; hope you like it; get in touch if you think docs can be improved).\nTo install:\npip3 install habanero # or pip install habanero To get into the polite pool, add your mailto email address when you instantiate a Crossref object\nfrom habanero import Crossref cr = Crossref(mailto = \u0026#34;foo@bar.com\u0026#34;) Then when you call any methods on cr your email address is sent in the request headers and you\u0026rsquo;ll get into the polite pool.\ncr.works() To use the select parameter, pass a comma separated string or a list of strings (both work):\ncr.works(select = \u0026#34;DOI,title\u0026#34;) habanero use cases I\u0026rsquo;ve seen some cool use cases using habanero lately.\nA bibliographic application at https://taccimo.info/tbl_sector_list.php from Sean Gordon. An application called PyKED from Kyle Niemeyer - \u0026ldquo;a Python-based software package for validating and interacting with ChemKED (Chemical Kinetics Experimental Data format) files that describe fundamental experimental measurements of combustion phenomena\u0026rdquo;. A Django app called TailorDev Biblio from Julien Maupetit that manages references. ","permalink":"http://localhost:1313/2017/10/habanero-update/","summary":"\u003cp\u003eI wrote about Crossref clients back nearly two years ago on this blog: \u003ca href=\"https://recology.info/2015/11/crossref-clients/\"\u003eCrossref programmatic clients\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSince it\u0026rsquo;s been a while, it seems worth talking again about the the many ways to work programmatically with Crossref data - and focus in on the Python client \u003ccode\u003ehabanero\u003c/code\u003e since it has some recent updates.\u003c/p\u003e\n\u003cp\u003eThe 3 clients work with the main \u003ca href=\"https://api.crossref.org\"\u003eCrossref API\u003c/a\u003e, which lets you do things like search for works by title, author, etc. (e.g., books, articles), search for publishing members, for funders, for journals, for DOI prefixes, and for licenses. It\u0026rsquo;s a powerful API with basically no rate limits, so you can work through lots of data quickly.\u003c/p\u003e","title":"habanero update: Crossref data from Python"},{"content":"If you maintain an R package, or even use R packages, you may have looked at CRAN check results. These are essentially the results of running R CMD CHECK on a package. They do these for each package for each of a few different operating systems (debian, fedora, solaris, windows, osx) and different R versions (devel, release and patched).\nsrc: https://github.com/ropensci/cchecksapi base api url: https://cranchecks.info CRAN maintainers look at these, and eventually will email maintainers if checks are bad enough.\nWhich brings us to the motivation for the API: it\u0026rsquo;d be nice to have a modern way (read: an API) to check CRAN check results.\nThe tech looks like so:\nlanguage: Ruby rest framework: Sinatra http requests for scraping: faraday database (storage): mongodb server: caddy scheduled scraping: cron (outside of docker) container: docker-compose The API originally just had rOpenSci pkgs, which is a small 150ish. But it was easy enough to scale it, so the API has all CRAN packages now.\nThe scraping step takes about 40 minutes and happens once a day. To clarify, results are up to date, so you can just use this API and not have to look up results on a cran mirror itself.\nAPI routes Here\u0026rsquo;s the breakdown\n/ /heartbeat /docs /pkgs /pkgs/:pkg_name: /docs Brings you to the docs at https://github.com/ropensci/cchecksapi/blob/master/docs/api_docs.md\n/pkgs Get all packages, paginated 10 at a time by default.\nfor example:\ncurl https://cranchecks.info/pkgs | jq . Params:\nlimit - number of records to return, default 10, max 1000 offset - record to start at, deafult 0 /pkgs/:pkg_name Get a package by name.\nfor example:\ncurl https://cranchecks.info/pkgs/crul | jq . Output looks like:\n{ \u0026#34;error\u0026#34;: null, \u0026#34;data\u0026#34;: { \u0026#34;_id\u0026#34;: \u0026#34;sofa\u0026#34;, \u0026#34;package\u0026#34;: \u0026#34;sofa\u0026#34;, \u0026#34;checks\u0026#34;: [ { \u0026#34;Flavor\u0026#34;: \u0026#34;r-devel-linux-x86_64-debian-clang \u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;0.2.0 \u0026#34;, \u0026#34;Tinstall\u0026#34;: \u0026#34;1.01 \u0026#34;, \u0026#34;Tcheck\u0026#34;: \u0026#34;18.27 \u0026#34;, \u0026#34;Ttotal\u0026#34;: \u0026#34;19.28 \u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;check_url\u0026#34;: \u0026#34;https://www.R-project.org/nosvn/R.check/r-devel-linux-x86_64-debian-clang/sofa-00check.html\u0026#34; }, ... The full URL is given for the check results, so you can go to it and check results, e.g., the top of the one above:\nusing R Under development (unstable) (2017-09-21 r73332) using platform: x86_64-pc-linux-gnu (64-bit) using session charset: UTF-8 checking for file ‘sofa/DESCRIPTION’ ... OK this is package ‘sofa’ version ‘0.2.0’ checking package namespace information ... OK checking package dependencies ... OK checking if this is a source package ... OK checking if there is a namespace ... OK checking for executable files ... OK checking for hidden files and directories ... OK checking for portable file names ... OK checking for sufficient/correct file permissions ... OK checking whether package ‘sofa’ can be installed ... OK ... TO DO maybe caching for /pkgs route lowercase all keys just cause clean up api results: numerics should be actual numerics, make empty strings to null, maybe change a status of OK to true so its more JSONish ","permalink":"http://localhost:1313/2017/09/cranchecks-api/","summary":"\u003cp\u003eIf you maintain an R package, or even use R packages, you may have looked at CRAN check results. These are essentially the results of running \u003ccode\u003eR CMD CHECK\u003c/code\u003e on a package. They do these for each package for each of a few different operating systems (debian, fedora, solaris, windows, osx) and different R versions (devel, release and patched).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esrc: \u003ca href=\"https://github.com/ropensci/cchecksapi\"\u003ehttps://github.com/ropensci/cchecksapi\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ebase api url: \u003ca href=\"https://cranchecks.info\"\u003ehttps://cranchecks.info\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCRAN maintainers look at these, and eventually will email maintainers if checks are bad enough.\u003c/p\u003e","title":"cranchecks: an API for CRAN check results"},{"content":"gbifrb is a new Ruby client for the GBIF API.\ndocs: https://www.rubydoc.info/gems/gbifrb/ rubygems: https://rubygems.org/gems/gbifrb code: https://github.com/sckott/gbifrb I maintain (w/ help) two other GBIF API clients:\nPython: pygbif R: rgbif API Here\u0026rsquo;s the gbifrb methods in relation to GBIF API routes\nregistry\n/node - Gbif::Registry.nodes /network - Gbif::Registry.networks /installations - Gbif::Registry.installations /organizations - Gbif::Registry.organizations /dataset_metrics - Gbif::Registry.dataset_metrics /datasets - Gbif::Registry.datasets /dataset_suggest - Gbif::Registry.dataset_suggest /dataset_search - Gbif::Registry.dataset_search species\n/species/match - Gbif::Species.name_backbone /species/suggest - Gbif::Species.name_suggest /species/search - Gbif::Species.name_lookup /species - Gbif::Species.name_usage occurrences\n/search - Gbif::Occurrences.search /get - Gbif::Occurrences.get /get_verbatim - Gbif::Occurrences.get_verbatim /get_fragment - Gbif::Occurrences.get_fragment Install gem install gbifrb Registry Nodes\nrequire \u0026#39;gbifrb\u0026#39; registry = Gbif::Registry registry.nodes(limit: 5) Networks\nregistry.networks(uuid: \u0026#39;16ab5405-6c94-4189-ac71-16ca3b753df7\u0026#39;) Species GBIF backbone\nspecies = Gbif::Species species.name_backbone(name: \u0026#34;Helianthus\u0026#34;) Suggester\nspecies.name_suggest(\u0026#34;Helianthus\u0026#34;) Occurrences occ = Gbif::Occurrences occ.search(taxonKey: 3329049) occ.search(taxonKey: 3329049, limit: 2) occ.search(scientificName: \u0026#39;Ursus americanus\u0026#39;) curl options You can do verbose curl output by settin verbiose: true. See also the parameter options, passed on to Faraday\nspecies = Gbif::Species species.name_backbone(\u0026#34;Helianthus\u0026#34;, verbose: true) Todo Still need to work on the CLI interface, add occurrence metrics methods, add occurrence downloads methods, and add OAI-PMH interface methods.\nFeedback Let me know what you think. Bugs. Featur requests. Etc.\n","permalink":"http://localhost:1313/2017/09/gbifrb/","summary":"\u003cp\u003e\u003ccode\u003egbifrb\u003c/code\u003e is a new Ruby client for the \u003ca href=\"https://www.gbif.org/developer/summary\"\u003eGBIF API\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edocs: \u003ca href=\"https://www.rubydoc.info/gems/gbifrb/\"\u003ehttps://www.rubydoc.info/gems/gbifrb/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003erubygems: \u003ca href=\"https://rubygems.org/gems/gbifrb\"\u003ehttps://rubygems.org/gems/gbifrb\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ecode: \u003ca href=\"https://github.com/sckott/gbifrb\"\u003ehttps://github.com/sckott/gbifrb\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI maintain (w/ help) two other GBIF API clients:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython: \u003ca href=\"https://github.com/sckott/pygbif\"\u003epygbif\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eR: \u003ca href=\"https://github.com/ropensci/rgbif\"\u003ergbif\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"api\"\u003eAPI\u003c/h2\u003e\n\u003cp\u003eHere\u0026rsquo;s the \u003ccode\u003egbifrb\u003c/code\u003e methods in relation to \u003ca href=\"https://www.gbif.org/developer/summary\"\u003eGBIF API\u003c/a\u003e routes\u003c/p\u003e\n\u003cp\u003eregistry\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/node\u003c/code\u003e - \u003ccode\u003eGbif::Registry.nodes\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/network\u003c/code\u003e - \u003ccode\u003eGbif::Registry.networks\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/installations\u003c/code\u003e - \u003ccode\u003eGbif::Registry.installations\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/organizations\u003c/code\u003e - \u003ccode\u003eGbif::Registry.organizations\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/dataset_metrics\u003c/code\u003e - \u003ccode\u003eGbif::Registry.dataset_metrics\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/datasets\u003c/code\u003e - \u003ccode\u003eGbif::Registry.datasets\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/dataset_suggest\u003c/code\u003e - \u003ccode\u003eGbif::Registry.dataset_suggest\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/dataset_search\u003c/code\u003e - \u003ccode\u003eGbif::Registry.dataset_search\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003especies\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/species/match\u003c/code\u003e - \u003ccode\u003eGbif::Species.name_backbone\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/species/suggest\u003c/code\u003e - \u003ccode\u003eGbif::Species.name_suggest\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/species/search\u003c/code\u003e - \u003ccode\u003eGbif::Species.name_lookup\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/species\u003c/code\u003e - \u003ccode\u003eGbif::Species.name_usage\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eoccurrences\u003c/p\u003e","title":"gbifrb: Ruby client for the GBIF API"},{"content":"hoardr is a client for caching files and managing those files.\nYou can definitely achieve the same tasks without a separate pacakge, and there\u0026rsquo;s a number of packages for caching various objects in R already. However, I didn\u0026rsquo;t think there was a tool for that did everything I needed.\nThe use cases I typically need hoardr for are when dealing with large files, either text (e.g., csv) or binary (e.g., shp) files that would be nice to not make the user of packages I maintain download again if they already have the file. This makes the server\u0026rsquo;s life easier that\u0026rsquo;s serving the files and makes work faster for the user of my package.\nGiven the existence of the awesome R6, hoardr becomes simple to use inside of other packages. Namely, hoardr can export just a single object that another package has to import, then we can call methods on that object, instead of having to import loads of functions.\nInstall From CRAN\ninstall.packages(\u0026#34;hoardr\u0026#34;) Dev version\ndevtools::install_github(\u0026#34;ropensci/hoardr\u0026#34;) library(\u0026#34;hoardr\u0026#34;) Package API There\u0026rsquo;s only a single exported object: hoard. This is a normal function, although is a lite wrapper around the R6 class HoardClient, which contains all the smarts.\nExample usage Initialze an object\n(x \u0026lt;- hoard()) #\u0026gt; \u0026lt;hoard\u0026gt; #\u0026gt; path: #\u0026gt; cache path: /var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar After making the object with hoardr(), it\u0026rsquo;s good to set a cache path. Here, we\u0026rsquo;ll use a temporary directoy, which we can set by doing type = \u0026quot;tempdir\u0026quot;\nx$cache_path_set(path = \u0026#34;foobar\u0026#34;, type = \u0026#39;tempdir\u0026#39;) #\u0026gt; [1] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar\u0026#34; Now our cache path is set to a temp dir\nx #\u0026gt; \u0026lt;hoard\u0026gt; #\u0026gt; path: foobar #\u0026gt; cache path: /var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar And we can request the base cache path as well\nx$cache_path_get() #\u0026gt; [1] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar\u0026#34; The next thing you\u0026rsquo;ll likely want to do is create that base directory since setting the path doesn\u0026rsquo;t create the directory:\nx$mkdir() What files are in the directory (hint: there shouldn\u0026rsquo;t be any):\nx$list() #\u0026gt; character(0) Let\u0026rsquo;s put a file in the cache\ncat(1:10000L, file = file.path(x$cache_path_get(), \u0026#34;foo.txt\u0026#34;)) Now see what\u0026rsquo;s in there\nx$list() #\u0026gt; [1] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar/foo.txt\u0026#34; While list() method lists full file paths, we can get more details with the details() method:\nx$details() #\u0026gt; \u0026lt;cached files\u0026gt; #\u0026gt; directory: /var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar #\u0026gt; #\u0026gt; file: /foo.txt #\u0026gt; size: 0.049 mb You can delete files by name:\nx$delete(\u0026#34;foo.txt\u0026#34;) x$list() #\u0026gt; character(0) As well as delete all files:\ncat(\u0026#34;one\\ntwo\\nthree\u0026#34;, file = file.path(x$cache_path_get(), \u0026#34;foo.txt\u0026#34;)) cat(\u0026#34;asdfasdf asd fasdf\u0026#34;, file = file.path(x$cache_path_get(), \u0026#34;bar.txt\u0026#34;)) x$list() #\u0026gt; [1] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar/bar.txt\u0026#34; #\u0026gt; [2] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar/foo.txt\u0026#34; x$delete_all() x$list() #\u0026gt; character(0) There\u0026rsquo;s also methods for compressing and uncompressing all the files in your cache:\ncat(\u0026#34;one\\ntwo\\nthree\u0026#34;, file = file.path(x$cache_path_get(), \u0026#34;foo.txt\u0026#34;)) cat(\u0026#34;asdfasdf asd fasdf\u0026#34;, file = file.path(x$cache_path_get(), \u0026#34;bar.txt\u0026#34;)) x$compress() x$list() #\u0026gt; [1] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar/compress.zip\u0026#34; x$uncompress() x$list() #\u0026gt; [1] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar/bar.txt\u0026#34; #\u0026gt; [2] \u0026#34;/var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//RtmpPITEm6/R/foobar/foo.txt\u0026#34; How to use in a package I already use hoardr in five R packages I maintain: crminer, rdpla, rerddap, rnoaa, and taxizedb. I\u0026rsquo;m planning to use it in many more packages, especially as it gets more stable.\nThis is how I use hoardr in packages.\nlist hoardr in your Imports in your DESCRIPTION file make on .onLoad method in your package, with the following content (as an example): cache \u0026lt;- NULL .onLoad \u0026lt;- function(libname, pkgname){ x \u0026lt;- hoardr::hoard() x$cache_path_set(\u0026#34;\u0026lt;your package name\u0026gt;\u0026#34;) cache \u0026lt;\u0026lt;- x } Then when the package is loaded, you have a cache object that you can then use to manage cached files.\nUse cache$mkdir() to make the directory Probably use cache$cache_path_get() in combination with e.g., file.path() to make file paths for files you need to cache Write files as needed If you need to delete files you can use delete() method to delete by name, or use unlink() on the complete file path, or you can delet_all() if you need to delete all files. If you need to do compression compress/uncompress are available - may be a nice thing to do for users so files are taking up less disk space. Add a manual file with a description of the various methods available and example usage, e.g, https://github.com/ropensci/crminer/blob/master/R/caching.R The cache object created above is also available to the user of your package so that they can manage files themselves as well - but of course you can choose not to export the cache object with methods to the user. ","permalink":"http://localhost:1313/2017/08/hoardr/","summary":"\u003cp\u003e\u003ccode\u003ehoardr\u003c/code\u003e is a client for caching files and managing those files.\u003c/p\u003e\n\u003cp\u003eYou can definitely achieve the same tasks without a separate pacakge, and there\u0026rsquo;s\na number of packages for caching various objects in R already. However,\nI didn\u0026rsquo;t think there was a tool for that did everything I needed.\u003c/p\u003e\n\u003cp\u003eThe use cases I typically need \u003ccode\u003ehoardr\u003c/code\u003e for are when dealing with large files,\neither text (e.g., csv) or binary (e.g., shp) files that would be nice to not\nmake the user of packages I maintain download again if they already have the\nfile. This makes the server\u0026rsquo;s life easier that\u0026rsquo;s serving the files and makes\nwork faster for the user of my package.\u003c/p\u003e","title":"hoardr: simple file caching"},{"content":"There are a lot of ways to make R packages. Many blog posts have covered making R packages, but for the most part they\u0026rsquo;ve covered only how they make packages, going from the required files for a package, what to put in DESCRIPTION, etc. But what about the tooling? I\u0026rsquo;m not going to talk about the code, etc. - but rather the different ways to approach it.\nThe blog posts/etc. on making R packages:\nWriting an R package from scratch - Hilary Parker Developing R packages - Jeff Leek stat545 - Write your own R package - Jenny Bryan\u0026rsquo;s statistics 545 class at UBC R package primer - Karl Boman Making Your First R Package - Fong Chun Chan R Package Development Pictorial - Matthew Denny Coursera course on building R packages R Packages by Hadley for a full treatment of the subject. From time to time you may need to reference CRAN\u0026rsquo;s Writing R Extensions. the ways The following are not mutually exclusive - some can be combined with others.\nIn process of writing this I figured I should ask other people what they do. I ended up asking 16 people - not a random selection or a big enough n to really say anything conclusively. But it did allow me to cover more ways of doing package dev.\nmason github: https://github.com/metacran/mason (by Gábor Csárdi)\nNote that mason is on CRAN, but it\u0026rsquo;s a completely different package.\nYou can use mason inside of R or via Rscript on the cli.\ndevtools::install_github(\u0026#34;metacran/mason\u0026#34;) library(mason) mason::mason() Then you\u0026rsquo;ll go through a series of prompts asking you for inormation (package name, license, your name, etc.)\nOut of 16 people I talked to, 2 mentioned using mason.\ndevtools github: https://github.com/hadley/devtools (by Hadley Wickham)\nWithin R:\ninstall.packages(\u0026#34;devtools\u0026#34;) library(devtools) devtools::create(\u0026#34;foobar\u0026#34;) On the cli, we can do:\nRscript -e \u0026#39;devtools::create(\u0026#34;foobar\u0026#34;)\u0026#39; devtools::create() adds basic set of files needed for an R package - and also adds files assuming you use the RStudio IDE. Though you can not add RStudio files by choosing rstudio = FALSE.\nBe aware of the default entry in the NAMESPACE file: exportPattern(\u0026quot;^[^\\\\.]\u0026quot;). The first time you generate documentation, e.g., via devtools::document() your NAMESPACE file will be changed to only export those things you want exported, which is ideal.\nOut of 16 people I talked to, 7 mentioned using devtools.\nrcmdcheck rcmdcheck: https://github.com/r-lib/rcmdcheck\nThis is an alternative to running R CMD CHECK or devtools::check(), that gives nice colorized output, at least in the terminal.\nI usually run it like this in the root of an R package directory in my terminal (running with --as-cran to check as CRAN does, and --run-dontrun to run examples wrapped in \\dontrun{}):\nRscript -e \u0026#39;rcmdcheck::rcmdcheck(args = c(\u0026#34;--as-cran\u0026#34;, \u0026#34;--run-dontrun\u0026#34;))\u0026#39; IDE: RStudio rstudio: https://www.rstudio.com/products/rstudio/#Desktop\nThe following is a guide provided by RStudio for creating packages in RStudio IDE: Using RStudio for package development\nWhen in RStudio - New Project in upper left hand corner - choose new or existing directory - choose R package - name the package, and you probably want to initialize git by checking the appropriate box.\nOut of 16 people I talked to, 14 used RStudio all the time or most of the time. It\u0026rsquo;s popular, to say the least.\nA noteable quote from one person I talked to:\nall rstudio all day\nRStudio inside of Docker rstudio server: https://www.rstudio.com/products/rstudio/download-server/ rstudio server docker container: https://hub.docker.com/r/rocker/rstudio/\nI know of at least one person that works this way, and surely there are others.\nAs simple as:\ndocker run -d -p 8787:8787 rocker/rstudio Then visit localhost:8787 in your browser.\nweb For example, we could just create files from the Github website. e.g,.\nNew Repository then add files you\u0026rsquo;d need for an R package and edit those in the browser Out of 16 people I talked to, 2 mentioned starting with creating a GitHub repository, then pulling that down, R development, etc. etc., then push back up. But no one mentioned all in browser - although see phone dev below.\nThe Github web interface is an important starting point for getting people into code in general when they are not familiar with git.\ntext editor If you primarily work in a text editor perhaps this (using Atom editor):\nRscript -e 'devtools::create(\u0026quot;foobar\u0026quot;)' \u0026amp;\u0026amp; cd foobar \u0026amp;\u0026amp; git init \u0026amp;\u0026amp; atom .\nOr the same for Sublime Text with subl . instead of atom .\nTwo of 16 people I talked to mentioned using Emacs exclusively or mostly. One of the 16 people mentione Sublime Text by name - that\u0026rsquo;s also the editor I use (I often have RStudio and Sublime Text open for the same R package - switching between them for the features I like).\ncopy/paste Sometimes when creating a new package I know of a previous package I\u0026rsquo;ve created that may have similar code I want in the new one or so. So I just copy/paste essentially the old package into a new folder. Be careful when doing this: make sure to delete git history, then re-initialize git (rm -rf .git \u0026amp;\u0026amp; git init in the new repository). Ideally you use roxygen/devtools for docs - in which case just delete all files in man/ then when you generate docs, you get all new man files.\nrhub rhub: https://builder.r-hub.io/ https://github.com/r-hub https://www.r-consortium.org/events/2016/10/11/r-hub-public-beta\nR-hub is a project by Gabor Csárdi, funded by the R Consortium, which is a service for developing, building, testing and validating R packages.\nOne of the 16 people I talked to mentioned rhub - but I imagine many of them use it. I use it :)\nI usually use it from the command line (or you can use it from within R, either on CLI or RStudio), like rhub::check_for_cran() to get checks for my package on Windows and two Linux platforms, before sending to CRAN.\nthe Makefile The Makefile is a file containing a set of directives. Some use a Makefile for a few or even most things one does in package development, from re-making man files, to building, installing, checking, building vignettes, making pkgdown docs, and more. Makefiles can include actions that do not just R things, but run other programming/command line tools. It\u0026rsquo;s a good idea when contributing to another R package to look for a Makefile - and to use them in your own package development. I don\u0026rsquo;t personally use them enough, and ideally will use them more in the future.\nHere\u0026rsquo;s an example of a Makefile from Rich FitzJohn:\nruby? To scratch a personal itch, I made a little Ruby gem with a command line tool to run one or more specific tests by fuzzy matching the name of the test file. Reason is, sometimes I work on a test file and I just want to run that test and not any others - and not from within RStudio, but form the terminal.\nhttps://github.com/sckott/rubrb\n➜ rb test config using: tests/testthat/test-config-fxns.R config fxns: ........ DONE =========================================================================== I\u0026rsquo;m sure there\u0026rsquo;s lots of these types of things out there - scratching an itch that helps the person work the way they want to work.\nWhile we\u0026rsquo;re on the topic of Ruby, Travis-CI has a nice Ruby gem travis to interact with Travis for your R packages. There\u0026rsquo;s also one for Circle-CI and I\u0026rsquo;ve written one for Appveyor.\nphone dev DO NOT TRY THIS AT HOME\ntracestack: search Stack Overflow for your most recent error msg. First #rstats package written entirely on a phone? https://t.co/IRX2luiR0N\n\u0026mdash; David Robinson (@drob) April 25, 2015 .@drob just wrote an #rstats package from his phone, in case you wanted to feel even less productive checking twitter\n\u0026mdash; Hilary Parker (@hspter) April 25, 2015 So that idea for throwing traceback to stackoverflow? @drob is actually writing it. As a package. Live on github. ON HIS PHONE. #rstats\n\u0026mdash; Oliver Keyes (@kopshtik) April 25, 2015 putting stuff on the web While we\u0026rsquo;re talking about tooling, I thought I should briefly mention putting code up on the interwebs. There\u0026rsquo;s many code hosting options - for brevity, we\u0026rsquo;ll just cover GitHub.\nIt\u0026rsquo;s a good idea to learn command line git, and related command line tools that make using git easier - if you can get work done faster you have more time to look at cat pictures!\nhub is one git tool that I use a lot. For example, create a folder, initialize a git repo, push to github, then open the just created repo on Github:\nmkdir helloworld \u0026amp;\u0026amp; cd helloworld \u0026amp;\u0026amp; hub init \u0026amp;\u0026amp; hub create sckott/helloworld \u0026amp;\u0026amp; hub browse\nwhere hub create uses the owner/repo pattern\n","permalink":"http://localhost:1313/2017/06/package-dev/","summary":"\u003cp\u003eThere are a lot of ways to make R packages. Many blog posts have covered making\nR packages, but for the most part they\u0026rsquo;ve covered only how they make\npackages, going from the required files for a package, what to put in DESCRIPTION, etc. But what about the tooling? I\u0026rsquo;m not going to talk about the code, etc. - but rather the different ways to approach it.\u003c/p\u003e\n\u003cp\u003eThe blog posts/etc. on making R packages:\u003c/p\u003e","title":"Tooling for R package development"},{"content":"Reading right now or just finished\nThe Nine, Jeffrey Toobin https://www.jeffreytoobin.com/books/the-nine-tr Just finished reading this. synopsis: fucking hell, Scalia and Thomas are awful the Warren court was awesome RBG 4 life Evolutionary Biology of Parasites, Peter W. Price https://press.princeton.edu/titles/645.html In progress. I got this book from my undergrad advisor around 2001 or so - figured I\u0026rsquo;d give it a read. synopsis: parasites are awesome Bike Snob: Systematically \u0026amp; Mercilessly Realigning the World of Cycling, Christopher Koelle https://www.goodreads.com/book/show/7549138-bike-snob In progress. Got from my dad, thx dad synopsis: funny The Genius of Birds, Jennifer Ackerman https://www.jenniferackermanauthor.com/genius-ofbirds In progress. synopsis: birds are smart ","permalink":"http://localhost:1313/2017/05/reading-may/","summary":"\u003cp\u003eReading right now or just finished\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Nine, \u003cem\u003eJeffrey Toobin\u003c/em\u003e \u003ca href=\"https://www.jeffreytoobin.com/books/the-nine-tr\"\u003ehttps://www.jeffreytoobin.com/books/the-nine-tr\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eJust finished reading this.\u003c/li\u003e\n\u003cli\u003esynopsis:\n\u003cul\u003e\n\u003cli\u003efucking hell, Scalia and Thomas are awful\u003c/li\u003e\n\u003cli\u003ethe Warren court was awesome\u003c/li\u003e\n\u003cli\u003eRBG 4 life\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEvolutionary Biology of Parasites, \u003cem\u003ePeter W. Price\u003c/em\u003e \u003ca href=\"https://press.princeton.edu/titles/645.html\"\u003ehttps://press.princeton.edu/titles/645.html\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eIn progress. I got this book from my undergrad advisor around 2001 or so - figured I\u0026rsquo;d give it a read.\u003c/li\u003e\n\u003cli\u003esynopsis: parasites are awesome\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBike Snob: Systematically \u0026amp; Mercilessly Realigning the World of Cycling, \u003cem\u003eChristopher Koelle\u003c/em\u003e \u003ca href=\"https://www.goodreads.com/book/show/7549138-bike-snob\"\u003ehttps://www.goodreads.com/book/show/7549138-bike-snob\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eIn progress. Got from my dad, thx dad\u003c/li\u003e\n\u003cli\u003esynopsis: funny\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe Genius of Birds, \u003cem\u003eJennifer Ackerman\u003c/em\u003e \u003ca href=\"https://www.jenniferackermanauthor.com/genius-ofbirds\"\u003ehttps://www.jenniferackermanauthor.com/genius-ofbirds\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eIn progress.\u003c/li\u003e\n\u003cli\u003esynopsis: birds are smart\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Reading in May"},{"content":" Save the date for CascadiaRConf! Website: cascadiarconf.com Twitter: @cascadiarconf There\u0026rsquo;s not a lot of info available yet - but so far:\nWhen 3 June, 2017\nWhere OHSU Collaborative Life Science Building\nmore details soon on what rooms, etc. Agenda No details yet - but likely to be a series of workshops as well as single track set of talks.\nWe\u0026rsquo;ll be accepting talk submissions soonish.\nTickets We aren\u0026rsquo;t out to make money - tickets will be cheap and probably free for students.\nWhy Why not!? But srsly, there\u0026rsquo;s not that many R conferences, so more is better at this point. In addition, there is an increasingly large R community in Portland at least judged by the ~1,140 members in the Portland R Meetup - so a conference makes sense. In addition, this will be an excuse to hopefully get together people from other nearby cities: Corvallis, Salem, Seattle, etc.\n","permalink":"http://localhost:1313/2017/03/cascadiarconf/","summary":"\u003ccenter\u003e\n  \u003ch1\u003eSave the date for CascadiaRConf!\u003c/h1\u003e\n\n  \u003ch2\u003eWebsite: \u003ca href=\"https://cascadiarconf.com/\"\u003ecascadiarconf.com\u003c/a\u003e\u003c/h2\u003e\n\n  \u003ch2\u003eTwitter: \u003ca href=\"https://twitter.com/cascadiarconf/\"\u003e@cascadiarconf\u003c/a\u003e\u003c/h2\u003e\n\u003c/center\u003e\n\n\u003cp\u003eThere\u0026rsquo;s not a lot of info available yet - but so far:\u003c/p\u003e\n\u003ch2 id=\"when\"\u003eWhen\u003c/h2\u003e\n\u003cp\u003e3 June, 2017\u003c/p\u003e\n\u003ch2 id=\"where\"\u003eWhere\u003c/h2\u003e\n\u003cp\u003eOHSU Collaborative Life Science Building\u003c/p\u003e\n\n\u003csmall\u003emore details soon on what rooms, etc.\u003c/small\u003e\n\n\u003ch2 id=\"agenda\"\u003eAgenda\u003c/h2\u003e\n\u003cp\u003eNo details yet - but likely to be a series of workshops as well as single track set of talks.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be accepting talk submissions soonish.\u003c/p\u003e\n\u003ch2 id=\"tickets\"\u003eTickets\u003c/h2\u003e\n\u003cp\u003eWe aren\u0026rsquo;t out to make money - tickets will be cheap and probably free for students.\u003c/p\u003e","title":"CascadiaRConf"},{"content":"The USDA maintains a database of plant information, some of it trait data, some of it life history. Check it out at https://plants.usda.gov/java/\nThey\u0026rsquo;ve been talking about releasing an API for a long time, but have not done so.\nThus, since at least some version of their data is in the public web, I\u0026rsquo;ve created a RESTful API for the data:\nsource code: https://github.com/sckott/usdaplantsapi/ base URL: https://plantsdb.xyz Check out the API, and open issues for bugs/feature requests in the github repo.\nThe following is an example using it from R, but you can use it from anywhere, the command line, Ruby, Python, a browser, whatevs.\nHere, we\u0026rsquo;ll use request, a higher level http client for R that I\u0026rsquo;ve been working on. A small quirk with request is that when piping, you have to assign the output of the request to an object before you can do any further manipulation. But that\u0026rsquo;s probably good for avoiding too long pipe chains.\nnote, that I\u0026rsquo;ve set tibble.max_extra_cols = 15 to not print the many columns that are returned, for blog post brevity. When you run below you\u0026rsquo;ll get more columns.\nInstall from CRAN\ninstall.packages(\u0026#34;request\u0026#34;) There is a small improvement in the dev version of request to make any data.frame\u0026rsquo;s tibble\u0026rsquo;s (which the below examples uses). To get that install from GitHub:\ndevtools::install_github(\u0026#34;sckott/request\u0026#34;) library(\u0026#39;request\u0026#39;) library(\u0026#39;tibble\u0026#39;) Heartbeat The simplest call to the API is to a route /heartbeat, which just lists the available routes.\nSet the base url we\u0026rsquo;ll use throughout the work below\nroot \u0026lt;- api(\u0026#34;https://plantsdb.xyz\u0026#34;) root %\u0026gt;% api_path(heartbeat) #\u0026gt; $routes #\u0026gt; [1] \u0026#34;/search (HEAD, GET)\u0026#34; \u0026#34;/heartbeat\u0026#34; Okay, so there are just two routes, /search and /heartbeat.\nSearch The search route suppports the following parameters:\nfields, e.g., fields='Genus,Species' (default: all fields returned) limit, e.g., limit=10 (default: 10) offset, e.g., offset=1 (default: 0) search on any fields in the output, e.g, Genus=Pinus or Species=annua basic search Let\u0026rsquo;s first not pass any params\nroot %\u0026gt;% api_path(search) #\u0026gt; $count #\u0026gt; [1] 92171 #\u0026gt; #\u0026gt; $returned #\u0026gt; [1] 10 #\u0026gt; #\u0026gt; $citation #\u0026gt; [1] \u0026#34;USDA, NRCS. 2016. The PLANTS Database (https://plants.usda.gov, 12 July 2016). National Plant Data Team, Greensboro, NC 27401-4901 USA.\u0026#34; #\u0026gt; #\u0026gt; $terms #\u0026gt; [1] \u0026#34;Our plant information, including the distribution maps, lists, and text, is not copyrighted and is free for any use.\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 10 × 134 #\u0026gt; id Symbol Accepted_Symbol_x Synonym_Symbol_x #\u0026gt; * \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 1 ABAB ABAB #\u0026gt; 2 2 ABAB2 ABPR3 ABAB2 #\u0026gt; 3 3 ABAB3 ABTH ABAB3 #\u0026gt; 4 4 ABAB70 ABAB70 #\u0026gt; 5 5 ABAC ABUMB ABAC #\u0026gt; 6 6 ABAL ABAL #\u0026gt; 7 7 ABAL2 ABUMU ABAL2 #\u0026gt; 8 8 ABAL3 ABAL3 #\u0026gt; 9 9 ABAM ABAM #\u0026gt; 10 10 ABAM2 ABAM2 #\u0026gt; # ... with 130 more variables: Scientific_Name_x \u0026lt;chr\u0026gt;, #\u0026gt; # Hybrid_Genus_Indicator \u0026lt;chr\u0026gt;, Hybrid_Species_Indicator \u0026lt;chr\u0026gt;, #\u0026gt; # Species \u0026lt;chr\u0026gt;, Subspecies_Prefix \u0026lt;chr\u0026gt;, #\u0026gt; # Hybrid_Subspecies_Indicator \u0026lt;chr\u0026gt;, Subspecies \u0026lt;chr\u0026gt;, #\u0026gt; # Variety_Prefix \u0026lt;chr\u0026gt;, Hybrid_Variety_Indicator \u0026lt;chr\u0026gt;, Variety \u0026lt;chr\u0026gt;, #\u0026gt; # Subvariety_Prefix \u0026lt;chr\u0026gt;, Subvariety \u0026lt;chr\u0026gt;, Forma_Prefix \u0026lt;chr\u0026gt;, #\u0026gt; # Forma \u0026lt;chr\u0026gt;, Genera_Binomial_Author \u0026lt;chr\u0026gt;, ... #\u0026gt; #\u0026gt; $error #\u0026gt; NULL You get slots:\ncount: number of results found returned: numbef of results returned citation: suggested citation, from USDA terms: terms of use, from USDA data: the results error: if an error occurred, you\u0026rsquo;ll see the message here Note that if any data.frame\u0026rsquo;s are found, we make them into tibble\u0026rsquo;s, nicely formatted data.frame\u0026rsquo;s that make it easy to deal with large data.\nPagination limit number of results\nroot %\u0026gt;% api_path(search) %\u0026gt;% api_query(limit = 5) #\u0026gt; $count #\u0026gt; [1] 92171 #\u0026gt; #\u0026gt; $returned #\u0026gt; [1] 5 #\u0026gt; #\u0026gt; $citation #\u0026gt; [1] \u0026#34;USDA, NRCS. 2016. The PLANTS Database (https://plants.usda.gov, 12 July 2016). National Plant Data Team, Greensboro, NC 27401-4901 USA.\u0026#34; #\u0026gt; #\u0026gt; $terms #\u0026gt; [1] \u0026#34;Our plant information, including the distribution maps, lists, and text, is not copyrighted and is free for any use.\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 5 × 134 #\u0026gt; id Symbol Accepted_Symbol_x Synonym_Symbol_x #\u0026gt; * \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 1 ABAB ABAB #\u0026gt; 2 2 ABAB2 ABPR3 ABAB2 #\u0026gt; 3 3 ABAB3 ABTH ABAB3 #\u0026gt; 4 4 ABAB70 ABAB70 #\u0026gt; 5 5 ABAC ABUMB ABAC #\u0026gt; # ... with 130 more variables: Scientific_Name_x \u0026lt;chr\u0026gt;, #\u0026gt; # Hybrid_Genus_Indicator \u0026lt;chr\u0026gt;, Hybrid_Species_Indicator \u0026lt;chr\u0026gt;, #\u0026gt; # Species \u0026lt;chr\u0026gt;, Subspecies_Prefix \u0026lt;chr\u0026gt;, #\u0026gt; # Hybrid_Subspecies_Indicator \u0026lt;chr\u0026gt;, Subspecies \u0026lt;chr\u0026gt;, #\u0026gt; # Variety_Prefix \u0026lt;chr\u0026gt;, Hybrid_Variety_Indicator \u0026lt;chr\u0026gt;, Variety \u0026lt;chr\u0026gt;, #\u0026gt; # Subvariety_Prefix \u0026lt;chr\u0026gt;, Subvariety \u0026lt;chr\u0026gt;, Forma_Prefix \u0026lt;chr\u0026gt;, #\u0026gt; # Forma \u0026lt;chr\u0026gt;, Genera_Binomial_Author \u0026lt;chr\u0026gt;, ... #\u0026gt; #\u0026gt; $error #\u0026gt; NULL change record to start at\nroot %\u0026gt;% api_path(search) %\u0026gt;% api_query(limit = 5, offset = 10) #\u0026gt; $count #\u0026gt; [1] 92161 #\u0026gt; #\u0026gt; $returned #\u0026gt; [1] 5 #\u0026gt; #\u0026gt; $citation #\u0026gt; [1] \u0026#34;USDA, NRCS. 2016. The PLANTS Database (https://plants.usda.gov, 12 July 2016). National Plant Data Team, Greensboro, NC 27401-4901 USA.\u0026#34; #\u0026gt; #\u0026gt; $terms #\u0026gt; [1] \u0026#34;Our plant information, including the distribution maps, lists, and text, is not copyrighted and is free for any use.\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 5 × 134 #\u0026gt; id Symbol Accepted_Symbol_x Synonym_Symbol_x #\u0026gt; * \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 11 ABAM3 ABAM3 #\u0026gt; 2 12 ABAM4 NAAM ABAM4 #\u0026gt; 3 13 ABAM5 ABAB ABAM5 #\u0026gt; 4 14 ABAN ABAN #\u0026gt; 5 15 ABANA ABAN ABANA #\u0026gt; # ... with 130 more variables: Scientific_Name_x \u0026lt;chr\u0026gt;, #\u0026gt; # Hybrid_Genus_Indicator \u0026lt;chr\u0026gt;, Hybrid_Species_Indicator \u0026lt;chr\u0026gt;, #\u0026gt; # Species \u0026lt;chr\u0026gt;, Subspecies_Prefix \u0026lt;chr\u0026gt;, #\u0026gt; # Hybrid_Subspecies_Indicator \u0026lt;chr\u0026gt;, Subspecies \u0026lt;chr\u0026gt;, #\u0026gt; # Variety_Prefix \u0026lt;chr\u0026gt;, Hybrid_Variety_Indicator \u0026lt;chr\u0026gt;, Variety \u0026lt;chr\u0026gt;, #\u0026gt; # Subvariety_Prefix \u0026lt;chr\u0026gt;, Subvariety \u0026lt;chr\u0026gt;, Forma_Prefix \u0026lt;chr\u0026gt;, #\u0026gt; # Forma \u0026lt;chr\u0026gt;, Genera_Binomial_Author \u0026lt;chr\u0026gt;, ... #\u0026gt; #\u0026gt; $error #\u0026gt; NULL Return fields You can say what fields you want returned, useful when you just want a subset of fields\nroot %\u0026gt;% api_path(search) %\u0026gt;% api_query(fields = \u0026#39;Genus,Species,Symbol\u0026#39;) #\u0026gt; $count #\u0026gt; [1] 92171 #\u0026gt; #\u0026gt; $returned #\u0026gt; [1] 10 #\u0026gt; #\u0026gt; $citation #\u0026gt; [1] \u0026#34;USDA, NRCS. 2016. The PLANTS Database (https://plants.usda.gov, 12 July 2016). National Plant Data Team, Greensboro, NC 27401-4901 USA.\u0026#34; #\u0026gt; #\u0026gt; $terms #\u0026gt; [1] \u0026#34;Our plant information, including the distribution maps, lists, and text, is not copyrighted and is free for any use.\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 10 × 3 #\u0026gt; Symbol Species Genus #\u0026gt; * \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 ABAB abutiloides Abutilon #\u0026gt; 2 ABAB2 abrus Abrus #\u0026gt; 3 ABAB3 abutilon Abutilon #\u0026gt; 4 ABAB70 abietina Abietinella #\u0026gt; 5 ABAC acutalata Abronia #\u0026gt; 6 ABAL alpina Abronia #\u0026gt; 7 ABAL2 alba Abronia #\u0026gt; 8 ABAL3 alba Abies #\u0026gt; 9 ABAM amabilis Abies #\u0026gt; 10 ABAM2 ameliae Abronia #\u0026gt; #\u0026gt; $error #\u0026gt; NULL Query You can query on individual fields\nroot %\u0026gt;% api_path(search) %\u0026gt;% api_query(Genus = Pinus, fields = \u0026#34;Genus,Species\u0026#34;) #\u0026gt; $count #\u0026gt; [1] 185 #\u0026gt; #\u0026gt; $returned #\u0026gt; [1] 10 #\u0026gt; #\u0026gt; $citation #\u0026gt; [1] \u0026#34;USDA, NRCS. 2016. The PLANTS Database (https://plants.usda.gov, 12 July 2016). National Plant Data Team, Greensboro, NC 27401-4901 USA.\u0026#34; #\u0026gt; #\u0026gt; $terms #\u0026gt; [1] \u0026#34;Our plant information, including the distribution maps, lists, and text, is not copyrighted and is free for any use.\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 10 × 2 #\u0026gt; Species Genus #\u0026gt; * \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 albicaulis Pinus #\u0026gt; 2 apacheca Pinus #\u0026gt; 3 aristata Pinus #\u0026gt; 4 arizonica Pinus #\u0026gt; 5 armandii Pinus #\u0026gt; 6 arizonica Pinus #\u0026gt; 7 aristata Pinus #\u0026gt; 8 arizonica Pinus #\u0026gt; 9 arizonica Pinus #\u0026gt; 10 attenuata Pinus #\u0026gt; #\u0026gt; $error #\u0026gt; NULL Another query example\nroot %\u0026gt;% api_path(search) %\u0026gt;% api_query(Species = annua, fields = \u0026#34;Genus,Species\u0026#34;) #\u0026gt; $count #\u0026gt; [1] 30 #\u0026gt; #\u0026gt; $returned #\u0026gt; [1] 10 #\u0026gt; #\u0026gt; $citation #\u0026gt; [1] \u0026#34;USDA, NRCS. 2016. The PLANTS Database (https://plants.usda.gov, 12 July 2016). National Plant Data Team, Greensboro, NC 27401-4901 USA.\u0026#34; #\u0026gt; #\u0026gt; $terms #\u0026gt; [1] \u0026#34;Our plant information, including the distribution maps, lists, and text, is not copyrighted and is free for any use.\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 10 × 2 #\u0026gt; Species Genus #\u0026gt; * \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 annua Adonis #\u0026gt; 2 annua Artemisia #\u0026gt; 3 annua Bulbostylis #\u0026gt; 4 annua Castilleja #\u0026gt; 5 annua Craniolaria #\u0026gt; 6 annua Dimorphotheca #\u0026gt; 7 annua Drosera #\u0026gt; 8 annua Eleocharis #\u0026gt; 9 annua Fimbristylis #\u0026gt; 10 annua Heliomeris #\u0026gt; #\u0026gt; $error #\u0026gt; NULL And one more example, here we\u0026rsquo;re interested in finding taxa that are perennials\nroot %\u0026gt;% api_path(search) %\u0026gt;% api_query(Duration = Perennial, fields = \u0026#34;Genus,Species,Symbol,Duration\u0026#34;) #\u0026gt; $count #\u0026gt; [1] 25296 #\u0026gt; #\u0026gt; $returned #\u0026gt; [1] 10 #\u0026gt; #\u0026gt; $citation #\u0026gt; [1] \u0026#34;USDA, NRCS. 2016. The PLANTS Database (https://plants.usda.gov, 12 July 2016). National Plant Data Team, Greensboro, NC 27401-4901 USA.\u0026#34; #\u0026gt; #\u0026gt; $terms #\u0026gt; [1] \u0026#34;Our plant information, including the distribution maps, lists, and text, is not copyrighted and is free for any use.\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; # A tibble: 10 × 4 #\u0026gt; Symbol Species Duration Genus #\u0026gt; * \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 ABAB abutiloides Perennial Abutilon #\u0026gt; 2 ABAL alpina Perennial Abronia #\u0026gt; 3 ABAL3 alba Perennial Abies #\u0026gt; 4 ABAM amabilis Perennial Abies #\u0026gt; 5 ABAM2 ameliae Perennial Abronia #\u0026gt; 6 ABAM3 ammophila Perennial Abronia #\u0026gt; 7 ABAR argillosa Perennial Abronia #\u0026gt; 8 ABAU auritum Perennial Abutilon #\u0026gt; 9 ABBA balsamea Perennial Abies #\u0026gt; 10 ABBAB balsamea Perennial Abies #\u0026gt; #\u0026gt; $error #\u0026gt; NULL ","permalink":"http://localhost:1313/2016/10/usda-plants-database-r/","summary":"\u003cp\u003eThe USDA maintains a database of plant information, some of it trait data, some\nof it life history. Check it out at \u003ca href=\"https://plants.usda.gov/java/\"\u003ehttps://plants.usda.gov/java/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThey\u0026rsquo;ve been talking about releasing an API for a long time, but have not done so.\u003c/p\u003e\n\u003cp\u003eThus, since at least some version of their data is in the public web,\nI\u0026rsquo;ve created a RESTful API for the data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esource code: \u003ca href=\"https://github.com/sckott/usdaplantsapi/\"\u003ehttps://github.com/sckott/usdaplantsapi/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ebase URL: \u003ca href=\"https://plantsdb.xyz\"\u003ehttps://plantsdb.xyz\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck out the API, and open issues for bugs/feature requests in the github repo.\u003c/p\u003e","title":"USDA plants database API in R"},{"content":"GBIDS API is back Back in March this year I wrote a post about a new API for working with GenBank IDs.\nI had to take the API down because it was too expensive to keep up. Expensive because the dump of data is very large (3.8 GB compressed), and I need disk space on the server to uncompress that to I think about 18 GB, then load into MySQL, which is another maybe 30 GB or so. Anyway, it\u0026rsquo;s not expensive because of high traffic - although I wish that was the case - but because of needing lots of disk space.\nI was fortuntate to recently receive some Amazon Cloud Credits for Research. The credits expire in 1 year. With these credits, I\u0026rsquo;ve put the GBIDS API back up. In the next year I\u0026rsquo;m hoping to gain user traction suggesting that\u0026rsquo;s is useful to enough people to keep maintaining - in which case I\u0026rsquo;ll seek ways to fund it.\nBut that means I need people to use it! So please to give it a try. Let me know what could be better; what could be faster; what API routes/features/etc. you\u0026rsquo;d like to see.\nPlans Plans for the future of the GBIDS API:\nAuto-update the Genbank data. This is quite complicated since the dump is so large. I can either keep an EC2 attached disc large enough to do the dump download/expansion/load/etc, or spin up a new instance each Sunday when they do their data release, do the SQL load, make a dump, then shuttle the SQL dump to the instance running, then load in the new data from the dump. I haven\u0026rsquo;t got this bit running yet, so data is from Aug 7. 2016. Add taxonomic IDs. Genbank also dumps their taxonomic IDs. I think it should be possible to get taxonomic IDs into the API, so that users can do accession number to taxon IDs and vice versa. Performance: as anyone would want, I want to continually improve performance. I\u0026rsquo;ll watch out for things I can do, but also let me know what seems too slow. Links API base url: https://gbids.xyz API docs: https://recology.info/gbidsdocs - Let me know if these could be improved API status page: https://recology.info/gbidsstatus - I update this page whenever there\u0026rsquo;s some down time, then when it\u0026rsquo;s back up, etc. API source code: https://github.com/sckott/gbids - You can file issues here about the API Try it Get 5 accession numbers\ncurl \u0026#39;https://gbids.xyz/acc?limit=5\u0026#39; | jq . #\u0026gt; { #\u0026gt; \u0026#34;matched\u0026#34;: 692006925, #\u0026gt; \u0026#34;returned\u0026#34;: 5, #\u0026gt; \u0026#34;data\u0026#34;: [ #\u0026gt; \u0026#34;A00002\u0026#34;, #\u0026gt; \u0026#34;A00003\u0026#34;, #\u0026gt; \u0026#34;X17276\u0026#34;, #\u0026gt; \u0026#34;X60065\u0026#34;, #\u0026gt; \u0026#34;CAA42669\u0026#34; #\u0026gt; ], #\u0026gt; \u0026#34;error\u0026#34;: null #\u0026gt; } Request to match accession identifiers, some exist, while some do not\ncurl \u0026#39;https://gbids.xyz/acc/AACY024124486,AACY024124483,asdfd,asdf,AACY024124476\u0026#39; | jq . #\u0026gt; { #\u0026gt; \u0026#34;matched\u0026#34;: 3, #\u0026gt; \u0026#34;returned\u0026#34;: 5, #\u0026gt; \u0026#34;data\u0026#34;: { #\u0026gt; \u0026#34;AACY024124486\u0026#34;: true, #\u0026gt; \u0026#34;AACY024124483\u0026#34;: true, #\u0026gt; \u0026#34;asdfd\u0026#34;: false, #\u0026gt; \u0026#34;asdf\u0026#34;: false, #\u0026gt; \u0026#34;AACY024124476\u0026#34;: true #\u0026gt; }, #\u0026gt; \u0026#34;error\u0026#34;: null #\u0026gt; } There\u0026rsquo;s many more examples in the API docs\n","permalink":"http://localhost:1313/2016/09/gbids-is-back/","summary":"\u003ch2 id=\"gbids-api-is-back\"\u003eGBIDS API is back\u003c/h2\u003e\n\u003cp\u003eBack in March this year I wrote \u003ca href=\"https://recology.info/2016/03/genbank-ids/\"\u003ea post about a new API for working with GenBank IDs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI had to take the API down because it was too expensive to keep up. Expensive because the dump of data is very large (3.8 GB compressed), and I need disk space on the server to uncompress that to I think about 18 GB, then load into MySQL, which is another maybe 30 GB or so. Anyway, it\u0026rsquo;s not expensive because of high traffic - although I wish that was the case - but because of needing lots of disk space.\u003c/p\u003e","title":"gbids - GenBank IDs API is back up!"},{"content":" Setup URLs Episode names Transcripts Summary word usage Sentiment Most common positive and negative words Reply All is a great podcast. I\u0026rsquo;ve been wanting to learn some text analysis tools, and transcripts from the podcast are on their site.\nTook some approaches outlined in the tidytext package in this vignette, and used the tokenizers package, and some of the tidyverse.\nCode on github at sckott/nonoyes\nAlso check out the html version\nSetup Load deps\nlibrary(\u0026quot;httr\u0026quot;) library(\u0026quot;xml2\u0026quot;) library(\u0026quot;stringi\u0026quot;) library(\u0026quot;dplyr\u0026quot;) library(\u0026quot;ggplot2\u0026quot;) library(\u0026quot;tokenizers\u0026quot;) library(\u0026quot;tidytext\u0026quot;) library(\u0026quot;tidyr\u0026quot;) source helper functions\nsource(\u0026quot;funs.R\u0026quot;) set base url\nra_base \u0026lt;- \u0026quot;https://gimletmedia.com/show/reply-all/episodes\u0026quot; URLs Make all urls for each page of episodes\nurls \u0026lt;- c(ra_base, file.path(ra_base, \u0026quot;page\u0026quot;, 2:8)) Get urls for each episode\nres \u0026lt;- lapply(urls, get_urls) Remove those that are rebroadcasts, updates, or revisited\nres \u0026lt;- grep(\u0026quot;rebroadcast|update|revisited\u0026quot;, unlist(res), value = TRUE, invert = TRUE) Episode names Give some episodes numbers that don\u0026rsquo;t have them\nepnames \u0026lt;- sub(\u0026quot;/$\u0026quot;, \u0026quot;\u0026quot;, sub(\u0026quot;https://gimletmedia.com/episode/\u0026quot;, \u0026quot;\u0026quot;, res)) epnames \u0026lt;- sub(\u0026quot;the-anxiety-box\u0026quot;, \u0026quot;8-the-anxiety-box\u0026quot;, epnames) epnames \u0026lt;- sub(\u0026quot;french-connection\u0026quot;, \u0026quot;10-french-connection\u0026quot;, epnames) epnames \u0026lt;- sub(\u0026quot;ive-killed-people-and-i-have-hostages\u0026quot;, \u0026quot;15-ive-killed-people-and-i-have-hostages\u0026quot;, epnames) epnames \u0026lt;- sub(\u0026quot;6-this-proves-everything\u0026quot;, \u0026quot;75-this-proves-everything\u0026quot;, epnames) epnames \u0026lt;- sub(\u0026quot;zardulu\u0026quot;, \u0026quot;56-zardulu\u0026quot;, epnames) Transcripts Get transcripts\ntxts \u0026lt;- lapply(res, transcript_fetch, sleep = 1) Parse transcripts\ntxtsp \u0026lt;- lapply(txts, transcript_parse) Summary word usage Summarise data for each transcript\ndat \u0026lt;- stats::setNames(lapply(txtsp, function(m) { bind_rows(lapply(m, function(v) { tmp \u0026lt;- unname(vapply(v, nchar, 1)) data_frame( n = length(tmp), mean = mean(tmp), n_laugh = count_word(v, \u0026quot;laugh\u0026quot;), n_groan = count_word(v, \u0026quot;groan\u0026quot;) ) }), .id = \u0026quot;name\u0026quot;) }), epnames) Bind data together to single dataframe, and filter, summarise\ndata \u0026lt;- bind_rows(dat, .id = \u0026quot;episode\u0026quot;) %\u0026gt;% filter(!is.na(episode)) %\u0026gt;% filter(grepl(\u0026quot;^PJ$|^ALEX GOLDMAN$\u0026quot;, name)) %\u0026gt;% mutate(ep_no = as.numeric(strextract(episode, \u0026quot;^[0-9]+\u0026quot;))) %\u0026gt;% group_by(ep_no) %\u0026gt;% mutate(nrow = NROW(ep_no)) %\u0026gt;% ungroup() %\u0026gt;% filter(nrow == 2) data #\u0026gt; # A tibble: 114 × 8 #\u0026gt; episode name n mean n_laugh n_groan ep_no #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 73-sandbox PJ 89 130.65169 9 0 73 #\u0026gt; 2 73-sandbox ALEX GOLDMAN 25 44.00000 1 1 73 #\u0026gt; 3 72-dead-is-paul PJ 137 67.77372 17 0 72 #\u0026gt; 4 72-dead-is-paul ALEX GOLDMAN 90 61.82222 8 0 72 #\u0026gt; 5 71-the-picture-taker PJ 74 77.70270 3 0 71 #\u0026gt; 6 71-the-picture-taker ALEX GOLDMAN 93 105.94624 6 0 71 #\u0026gt; 7 69-disappeared PJ 72 76.50000 2 0 69 #\u0026gt; 8 69-disappeared ALEX GOLDMAN 50 135.90000 5 0 69 #\u0026gt; 9 68-vampire-rules PJ 142 88.00704 6 0 68 #\u0026gt; 10 68-vampire-rules ALEX GOLDMAN 117 73.16239 13 0 68 #\u0026gt; # ... with 104 more rows, and 1 more variables: nrow \u0026lt;int\u0026gt; Number of words - seems PJ talks more, but didn\u0026rsquo;t do quantiative comparison\nggplot(data, aes(ep_no, n, colour = name)) + geom_point(size = 3, alpha = 0.5) + geom_line(aes(group = ep_no), colour = \u0026quot;black\u0026quot;) + scale_color_discrete(labels = c('Alex', 'PJ')) Laughs per episode - take home: PJ laughs a lot\nggplot(data, aes(ep_no, n_laugh, colour = name)) + geom_point(size = 3, alpha = 0.5) + geom_line(aes(group = ep_no), colour = \u0026quot;black\u0026quot;) + scale_color_discrete(labels = c('Alex', 'PJ')) Sentiment zero \u0026lt;- which(vapply(txtsp, length, 1) == 0) txtsp_ \u0026lt;- Filter(function(x) length(x) != 0, txtsp) Tokenize words, and create data_frame\nwordz \u0026lt;- stats::setNames( lapply(txtsp_, function(z) { bind_rows( if (is.null(try_tokenize(z$`ALEX GOLDMAN`))) { data_frame() } else { data_frame( name = \u0026quot;Alex\u0026quot;, word = try_tokenize(z$`ALEX GOLDMAN`) ) }, if (is.null(try_tokenize(z$PJ))) { data_frame() } else { data_frame( name = \u0026quot;PJ\u0026quot;, word = try_tokenize(z$PJ) ) } ) }), epnames[-zero]) Combine to single data_frame\n(wordz_df \u0026lt;- bind_rows(wordz, .id = \u0026quot;episode\u0026quot;)) #\u0026gt; # A tibble: 104,713 × 3 #\u0026gt; episode name word #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 73-sandbox Alex alex #\u0026gt; 2 73-sandbox Alex goldman #\u0026gt; 3 73-sandbox Alex i #\u0026gt; 4 73-sandbox Alex generally #\u0026gt; 5 73-sandbox Alex don’t #\u0026gt; 6 73-sandbox Alex alex #\u0026gt; 7 73-sandbox Alex really #\u0026gt; 8 73-sandbox Alex alex #\u0026gt; 9 73-sandbox Alex groans #\u0026gt; 10 73-sandbox Alex so #\u0026gt; # ... with 104,703 more rows Calculate sentiment using tidytext\nbing \u0026lt;- sentiments %\u0026gt;% filter(lexicon == \u0026quot;bing\u0026quot;) %\u0026gt;% select(-score) sent \u0026lt;- wordz_df %\u0026gt;% inner_join(bing) %\u0026gt;% count(name, episode, sentiment) %\u0026gt;% spread(sentiment, n, fill = 0) %\u0026gt;% mutate(sentiment = positive - negative) %\u0026gt;% ungroup() %\u0026gt;% filter(!is.na(episode)) %\u0026gt;% complete(episode, name) %\u0026gt;% mutate(ep_no = as.numeric(strextract(episode, \u0026quot;^[0-9]+\u0026quot;))) sent #\u0026gt; # A tibble: 148 × 6 #\u0026gt; episode name negative positive #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 1-an-app-sends-a-stranger-to-say-i-love-you Alex 19 30 #\u0026gt; 2 1-an-app-sends-a-stranger-to-say-i-love-you PJ 14 14 #\u0026gt; 3 10-french-connection Alex 15 32 #\u0026gt; 4 10-french-connection PJ 16 36 #\u0026gt; 5 11-did-errol-morris-brother-invent-email Alex NA NA #\u0026gt; 6 11-did-errol-morris-brother-invent-email PJ 25 30 #\u0026gt; 7 12-backend-trouble Alex 20 15 #\u0026gt; 8 12-backend-trouble PJ 40 59 #\u0026gt; 9 13-love-is-lies Alex NA NA #\u0026gt; 10 13-love-is-lies PJ 45 64 #\u0026gt; # ... with 138 more rows, and 2 more variables: sentiment \u0026lt;dbl\u0026gt;, #\u0026gt; # ep_no \u0026lt;dbl\u0026gt; Names separate\nggplot(sent, aes(ep_no, sentiment, fill = name)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + facet_wrap(~name, ncol = 2, scales = \u0026quot;free_x\u0026quot;) Compare for each episode\nggplot(sent, aes(ep_no, sentiment, fill = name)) + geom_bar(stat = \u0026quot;identity\u0026quot;, position = position_dodge(width = 0.5), width = 0.6) Most common positive and negative words Clearly, the word like is surely rarely used as a positive word meaning e.g., that they like something, but rather as the colloquial like, totally usage. So it\u0026rsquo;s removed.\nAlex\nsent_cont_plot(wordz_df, \u0026quot;Alex\u0026quot;) PJ\nsent_cont_plot(wordz_df, \u0026quot;PJ\u0026quot;) ","permalink":"http://localhost:1313/2016/08/no-no-yes/","summary":"\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#setup\"\u003eSetup\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#urls\"\u003eURLs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#episode-names\"\u003eEpisode names\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#transcripts\"\u003eTranscripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#summary-word-usage\"\u003eSummary word usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sentiment\"\u003eSentiment\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#most-common-positive-and-negative-words\"\u003eMost common positive and negative\nwords\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://gimletmedia.com/show/reply-all/\"\u003eReply All\u003c/a\u003e is a great podcast.\nI\u0026rsquo;ve been wanting to learn some text analysis tools, and transcripts\nfrom the podcast are on their site.\u003c/p\u003e\n\u003cp\u003eTook some approaches outlined in the\n\u003ca href=\"https://cran.rstudio.com/web/packages/tidytext/\"\u003etidytext\u003c/a\u003e package in\n\u003ca href=\"https://cran.rstudio.com/web/packages/tidytext/vignettes/tidytext.html\"\u003ethis\nvignette\u003c/a\u003e,\nand used the\n\u003ca href=\"https://cran.rstudio.com/web/packages/tokenizers\"\u003etokenizers\u003c/a\u003e package,\nand some of the tidyverse.\u003c/p\u003e\n\u003cp\u003eCode on github at \u003ca href=\"https://github.com/sckott/nonoyes\"\u003esckott/nonoyes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAlso check out the \u003ca href=\"https://htmlpreview.github.io/?https://github.com/sckott/nonoyes/blob/master/script.html\"\u003ehtml version\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"setup\"\u003eSetup\u003c/h2\u003e\n\u003cp\u003eLoad deps\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elibrary(\u0026quot;httr\u0026quot;)\nlibrary(\u0026quot;xml2\u0026quot;)\nlibrary(\u0026quot;stringi\u0026quot;)\nlibrary(\u0026quot;dplyr\u0026quot;)\nlibrary(\u0026quot;ggplot2\u0026quot;)\nlibrary(\u0026quot;tokenizers\u0026quot;)\nlibrary(\u0026quot;tidytext\u0026quot;)\nlibrary(\u0026quot;tidyr\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003esource helper functions\u003c/p\u003e","title":"nonoyes - text analysis of Reply All podcast transcripts"},{"content":"This is how I edit videos of talks that I need to incorporate slides and video together\nI\u0026rsquo;m on a Mac\nimport to iMovie (using v10 something) drop movie into editing section split pdf slides into individual files pdfseparate foobar.pdf %d.pdf convert individual pdf slides into png sips -s format png --out \u0026quot;${pdf%%.*}.png\u0026quot; \u0026quot;$pdf\u0026quot; import png\u0026rsquo;s into imovie for each image, drop into editing area where you want it when focused on the png of the slide: select crop, then - choose fit, say okay select \u0026ldquo;add as overlay\u0026rdquo; (very most left symbol), then choose picture in picture then choose swap then move inset to where you want it say okay rinse and repeat for all slides export - via File option share to youtube e.g. of the result ","permalink":"http://localhost:1313/2016/08/video-editing/","summary":"\u003cp\u003eThis is how I edit videos of talks that I need to incorporate slides and video together\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m on a Mac\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eimport to iMovie (using v10 something)\u003c/li\u003e\n\u003cli\u003edrop movie into editing section\u003c/li\u003e\n\u003cli\u003esplit pdf slides into individual files \u003ccode\u003epdfseparate foobar.pdf %d.pdf\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003econvert individual pdf slides into png \u003ccode\u003esips -s format png --out \u0026quot;${pdf%%.*}.png\u0026quot; \u0026quot;$pdf\u0026quot;\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eimport png\u0026rsquo;s into imovie\u003c/li\u003e\n\u003cli\u003efor each image, drop into editing area where you want it\u003c/li\u003e\n\u003cli\u003ewhen focused on the png of the slide:\n\u003cul\u003e\n\u003cli\u003eselect crop, then - choose \u003ccode\u003efit\u003c/code\u003e, say okay\u003c/li\u003e\n\u003cli\u003eselect \u0026ldquo;add as overlay\u0026rdquo; (very most left symbol), then choose \u003ccode\u003epicture in picture\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ethen choose \u003ccode\u003eswap\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ethen move inset to where you want it\u003c/li\u003e\n\u003cli\u003esay okay\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003erinse and repeat for all slides\u003c/li\u003e\n\u003cli\u003eexport - via \u003ccode\u003eFile\u003c/code\u003e option\u003c/li\u003e\n\u003cli\u003eshare to youtube\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"eg-of-the-result\"\u003ee.g. of the result\u003c/h3\u003e\n\n\n    \n    \u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/MHWX0f3TG4I?autoplay=0\u0026controls=1\u0026end=0\u0026loop=0\u0026mute=0\u0026start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\n      \u003e\u003c/iframe\u003e\n    \u003c/div\u003e","title":"video editing notes"},{"content":" UPDATE: pkg API has changed - updated the post below to work with the current CRAN version, submitted 2016-08-02\nI was at a hackathon focused on Ocean Biogeographic Information System (OBIS) data back in November last year in Belgium. One project idea was to make it easier to get at data based on one or more marine regions. I was told that Marineregions.org is often used for shape files to get different regions to then do other work with.\nDuring the event I started a package mregions. Here I\u0026rsquo;ll show how to get different marine regions, then use those outputs to get species occurrence data.\nWe\u0026rsquo;ll use WKT (Well-Known Text) to define spatial dimensions in this post. If you don\u0026rsquo;t know what it is, Wikipedia to the rescue: https://en.wikipedia.org/wiki/Well-known_text\nInstall install.packages(\u0026#34;mregions\u0026#34;) devtools::install_github(\u0026#34;iobis/robis\u0026#34;) Or get the dev version\ndevtools::install_github(\u0026#34;ropenscilabs/mregions\u0026#34;) library(\u0026#34;mregions\u0026#34;) Get list of place types res \u0026lt;- mr_place_types() head(res$type) #\u0026gt; [1] \u0026#34;Town\u0026#34; \u0026#34;Arrondissement\u0026#34; #\u0026gt; [3] \u0026#34;Department\u0026#34; \u0026#34;Province (administrative)\u0026#34; #\u0026gt; [5] \u0026#34;Country\u0026#34; \u0026#34;Continent\u0026#34; Get Marineregions records by place type res \u0026lt;- mr_records_by_type(type = \u0026#34;EEZ\u0026#34;) head(res) #\u0026gt; MRGID gazetteerSource #\u0026gt; 1 3293 Maritime Boundaries Geodatabase, Flanders Marine Institute #\u0026gt; 2 5668 Maritime Boundaries Geodatabase, Flanders Marine Institute #\u0026gt; 3 5669 Maritime Boundaries Geodatabase, Flanders Marine Institute #\u0026gt; 4 5670 Maritime Boundaries Geodatabase, Flanders Marine Institute #\u0026gt; 5 5672 Maritime Boundaries Geodatabase, Flanders Marine Institute #\u0026gt; 6 5673 Maritime Boundaries Geodatabase, Flanders Marine Institute #\u0026gt; placeType latitude longitude minLatitude minLongitude maxLatitude #\u0026gt; 1 EEZ 51.46483 2.704458 51.09111 2.238118 51.87000 #\u0026gt; 2 EEZ 53.61508 4.190675 51.26203 2.539443 55.76500 #\u0026gt; 3 EEZ 54.55970 8.389231 53.24281 3.349999 55.91928 #\u0026gt; 4 EEZ 40.87030 19.147094 39.63863 18.461940 41.86124 #\u0026gt; 5 EEZ 42.94272 29.219062 41.97820 27.449580 43.74779 #\u0026gt; 6 EEZ 43.42847 15.650844 41.62201 13.001390 45.59079 #\u0026gt; maxLongitude precision preferredGazetteerName #\u0026gt; 1 3.364907 58302.49 Belgian Exclusive Economic Zone #\u0026gt; 2 7.208364 294046.10 Dutch Exclusive Economic Zone #\u0026gt; 3 14.750000 395845.50 German Exclusive Economic Zone #\u0026gt; 4 20.010030 139751.70 Albanian Exclusive Economic Zone #\u0026gt; 5 31.345280 186792.50 Bulgarian Exclusive Economic Zone #\u0026gt; 6 18.552360 313990.30 Croatian Exclusive Economic Zone #\u0026gt; preferredGazetteerNameLang status accepted #\u0026gt; 1 English standard 3293 #\u0026gt; 2 English standard 5668 #\u0026gt; 3 English standard 5669 #\u0026gt; 4 English standard 5670 #\u0026gt; 5 English standard 5672 #\u0026gt; 6 English standard 5673 Get and search region names (res \u0026lt;- mr_names()) #\u0026gt; # A tibble: 676 x 4 #\u0026gt; name #\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 Morocco:elevation_10m #\u0026gt; 2 Emodnet:emodnet1x1grid #\u0026gt; 3 Emodnet:grid #\u0026gt; 4 Morocco:dam #\u0026gt; 5 WoRMS:azmp_sections #\u0026gt; 6 Morocco:accomodationcapacity #\u0026gt; 7 Morocco:admin_boundary #\u0026gt; 8 Lifewatch:ovam_afvalverwerking #\u0026gt; 9 Eurobis:eurobis_points #\u0026gt; 10 Morocco:roads #\u0026gt; # ... with 666 more rows, and 3 more variables: title \u0026lt;chr\u0026gt;, #\u0026gt; # name_first \u0026lt;chr\u0026gt;, name_second \u0026lt;chr\u0026gt; mr_names_search(res, q = \u0026#34;IHO\u0026#34;) #\u0026gt; # A tibble: 5 x 4 #\u0026gt; name #\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 MarineRegions:iho #\u0026gt; 2 MarineRegions:iho_quadrants_20150810 #\u0026gt; 3 World:iosregions #\u0026gt; 4 MarineRegions:eez_iho_union_v2 #\u0026gt; 5 Belgium:vl_venivon #\u0026gt; # ... with 3 more variables: title \u0026lt;chr\u0026gt;, name_first \u0026lt;chr\u0026gt;, #\u0026gt; # name_second \u0026lt;chr\u0026gt; Get a region - geojson res \u0026lt;- mr_geojson(name = \u0026#34;Turkmen Exclusive Economic Zone\u0026#34;) class(res) #\u0026gt; [1] \u0026#34;mr_geojson\u0026#34; names(res) #\u0026gt; [1] \u0026#34;type\u0026#34; \u0026#34;totalFeatures\u0026#34; \u0026#34;features\u0026#34; \u0026#34;crs\u0026#34; #\u0026gt; [5] \u0026#34;bbox\u0026#34; Get a region - shp res \u0026lt;- mr_shp(name = \u0026#34;Belgian Exclusive Economic Zone\u0026#34;) class(res) #\u0026gt; [1] \u0026#34;SpatialPolygonsDataFrame\u0026#34; #\u0026gt; attr(,\u0026#34;package\u0026#34;) #\u0026gt; [1] \u0026#34;sp\u0026#34; Get OBIS EEZ ID res \u0026lt;- mr_names() res \u0026lt;- res[grepl(\u0026#34;eez\u0026#34;, res$name, ignore.case = TRUE),] mr_obis_eez_id(res$title[2]) #\u0026gt; [1] 84 Convert to WKT From geojson or shp. Here, geojson\nres \u0026lt;- mr_geojson(key = \u0026#34;MarineRegions:eez_33176\u0026#34;) mr_as_wkt(res, fmt = 5) #\u0026gt; [1] \u0026#34;MULTIPOLYGON (((41.573732 -1.659444, 45.891882 ... cutoff Get regions, then OBIS data Using Well-Known Text Both shp and geojson data returned from region_shp() and region_geojson(), respectively, can be passed to as_wkt() to get WKT.\nshp \u0026lt;- mr_shp(name = \u0026#34;Belgian Exclusive Economic Zone\u0026#34;) wkt \u0026lt;- mr_as_wkt(shp) library(\u0026#39;httr\u0026#39;) library(\u0026#39;data.table\u0026#39;) args \u0026lt;- list(scientificname = \u0026#34;Abra alba\u0026#34;, geometry = wkt, limit = 100) res \u0026lt;- httr::GET(\u0026#39;https://api.iobis.org/occurrence\u0026#39;, query = args) xx \u0026lt;- data.table::setDF(data.table::rbindlist(httr::content(res)$results, use.names = TRUE, fill = TRUE)) xx \u0026lt;- xx[, c(\u0026#39;scientificName\u0026#39;, \u0026#39;decimalLongitude\u0026#39;, \u0026#39;decimalLatitude\u0026#39;)] names(xx)[2:3] \u0026lt;- c(\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;) Plot\nlibrary(\u0026#39;leaflet\u0026#39;) leaflet() %\u0026gt;% addTiles() %\u0026gt;% addCircleMarkers(data = xx) %\u0026gt;% addPolygons(data = shp) Using EEZ ID EEZ is a Exclusive Economic Zone\n(eez \u0026lt;- mr_obis_eez_id(\u0026#34;Belgian Exclusive Economic Zone\u0026#34;)) #\u0026gt; [1] 59 You currently can\u0026rsquo;t search for OBIS occurrences by EEZ ID, but hopefully soon\u0026hellip;\nDealing with bigger WKT What if you\u0026rsquo;re WKT string is super long? It\u0026rsquo;s often a problem because some online species occurrence databases that accept WKT to search by geometry bork due to limitations on length of URLs if your WKT string is too long (about 8000 characters, including remainder of URL). One way to deal with it is to reduce detail - simplify.\ninstall.packages(\u0026#34;rmapshaper\u0026#34;) Using rmapshaper we can simplify a spatial object, then search with that.\nshp \u0026lt;- mr_shp(name = \u0026#34;Dutch Exclusive Economic Zone\u0026#34;) Visualize\nleaflet() %\u0026gt;% addTiles() %\u0026gt;% addPolygons(data = shp) Simplify\nlibrary(\u0026#34;rmapshaper\u0026#34;) shp \u0026lt;- ms_simplify(shp) It\u0026rsquo;s simplified:\nleaflet() %\u0026gt;% addTiles() %\u0026gt;% addPolygons(data = shp) Pass to GBIF if (!requireNamespace(\u0026#34;rgbif\u0026#34;)) { install.packages(\u0026#34;rgbif\u0026#34;) } library(\u0026#34;rgbif\u0026#34;) occ_search(geometry = mr_as_wkt(shp), limit = 400) #\u0026gt; Records found [2395936] #\u0026gt; Records returned [400] #\u0026gt; No. unique hierarchies [17] #\u0026gt; No. media records [3] #\u0026gt; Args [geometry=POLYGON ((7.2083632399999997 53.2428091399999985, #\u0026gt; 6.6974995100000001 53.4619365499999972, 5.89083650, limit=400, #\u0026gt; offset=0, fields=all] #\u0026gt; # A tibble: 400 x 102 #\u0026gt; name key decimalLatitude decimalLongitude #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Haematopus ostralegus 1265900666 52.13467 4.21306 #\u0026gt; 2 Limosa limosa 1265577408 53.03249 4.88665 #\u0026gt; 3 Corvus splendens 1269536058 51.98297 4.12982 #\u0026gt; 4 Corvus splendens 1269536065 51.98297 4.12982 #\u0026gt; 5 Lanius excubitor 1211320606 52.57223 4.62569 #\u0026gt; 6 Lanius excubitor 1211320862 52.67419 4.63645 #\u0026gt; 7 Lanius excubitor 1211320806 53.05790 4.72974 #\u0026gt; 8 Lanius excubitor 1211321040 52.57540 4.63651 #\u0026gt; 9 Lanius excubitor 1211320590 52.41180 5.19500 #\u0026gt; 10 Lanius excubitor 1211320401 52.57535 4.63654 #\u0026gt; # ... with 390 more rows, and 98 more variables: issues \u0026lt;chr\u0026gt;, #\u0026gt; # datasetKey \u0026lt;chr\u0026gt;, publishingOrgKey \u0026lt;chr\u0026gt;, publishingCountry \u0026lt;chr\u0026gt;, #\u0026gt; # protocol \u0026lt;chr\u0026gt;, lastCrawled \u0026lt;chr\u0026gt;, lastParsed \u0026lt;chr\u0026gt;, extensions \u0026lt;chr\u0026gt;, #\u0026gt; # basisOfRecord \u0026lt;chr\u0026gt;, taxonKey \u0026lt;int\u0026gt;, kingdomKey \u0026lt;int\u0026gt;, #\u0026gt; # phylumKey \u0026lt;int\u0026gt;, classKey \u0026lt;int\u0026gt;, orderKey \u0026lt;int\u0026gt;, familyKey \u0026lt;int\u0026gt;, #\u0026gt; # genusKey \u0026lt;int\u0026gt;, speciesKey \u0026lt;int\u0026gt;, scientificName \u0026lt;chr\u0026gt;, kingdom \u0026lt;chr\u0026gt;, #\u0026gt; # phylum \u0026lt;chr\u0026gt;, order \u0026lt;chr\u0026gt;, family \u0026lt;chr\u0026gt;, genus \u0026lt;chr\u0026gt;, species \u0026lt;chr\u0026gt;, #\u0026gt; # genericName \u0026lt;chr\u0026gt;, specificEpithet \u0026lt;chr\u0026gt;, taxonRank \u0026lt;chr\u0026gt;, #\u0026gt; # dateIdentified \u0026lt;chr\u0026gt;, coordinateUncertaintyInMeters \u0026lt;dbl\u0026gt;, year \u0026lt;int\u0026gt;, #\u0026gt; # month \u0026lt;int\u0026gt;, day \u0026lt;int\u0026gt;, eventDate \u0026lt;chr\u0026gt;, modified \u0026lt;chr\u0026gt;, #\u0026gt; # lastInterpreted \u0026lt;chr\u0026gt;, references \u0026lt;chr\u0026gt;, identifiers \u0026lt;chr\u0026gt;, #\u0026gt; # facts \u0026lt;chr\u0026gt;, relations \u0026lt;chr\u0026gt;, geodeticDatum \u0026lt;chr\u0026gt;, class \u0026lt;chr\u0026gt;, #\u0026gt; # countryCode \u0026lt;chr\u0026gt;, country \u0026lt;chr\u0026gt;, rightsHolder \u0026lt;chr\u0026gt;, #\u0026gt; # identifier \u0026lt;chr\u0026gt;, informationWithheld \u0026lt;chr\u0026gt;, verbatimEventDate \u0026lt;chr\u0026gt;, #\u0026gt; # datasetName \u0026lt;chr\u0026gt;, gbifID \u0026lt;chr\u0026gt;, collectionCode \u0026lt;chr\u0026gt;, #\u0026gt; # verbatimLocality \u0026lt;chr\u0026gt;, occurrenceID \u0026lt;chr\u0026gt;, taxonID \u0026lt;chr\u0026gt;, #\u0026gt; # license \u0026lt;chr\u0026gt;, recordedBy \u0026lt;chr\u0026gt;, catalogNumber \u0026lt;chr\u0026gt;, #\u0026gt; # http...unknown.org.occurrenceDetails \u0026lt;chr\u0026gt;, institutionCode \u0026lt;chr\u0026gt;, #\u0026gt; # rights \u0026lt;chr\u0026gt;, eventTime \u0026lt;chr\u0026gt;, identificationID \u0026lt;chr\u0026gt;, #\u0026gt; # individualCount \u0026lt;int\u0026gt;, continent \u0026lt;chr\u0026gt;, stateProvince \u0026lt;chr\u0026gt;, #\u0026gt; # nomenclaturalCode \u0026lt;chr\u0026gt;, locality \u0026lt;chr\u0026gt;, language \u0026lt;chr\u0026gt;, #\u0026gt; # taxonomicStatus \u0026lt;chr\u0026gt;, type \u0026lt;chr\u0026gt;, preparations \u0026lt;chr\u0026gt;, #\u0026gt; # disposition \u0026lt;chr\u0026gt;, originalNameUsage \u0026lt;chr\u0026gt;, #\u0026gt; # bibliographicCitation \u0026lt;chr\u0026gt;, identifiedBy \u0026lt;chr\u0026gt;, sex \u0026lt;chr\u0026gt;, #\u0026gt; # lifeStage \u0026lt;chr\u0026gt;, otherCatalogNumbers \u0026lt;chr\u0026gt;, habitat \u0026lt;chr\u0026gt;, #\u0026gt; # georeferencedBy \u0026lt;chr\u0026gt;, vernacularName \u0026lt;chr\u0026gt;, elevation \u0026lt;dbl\u0026gt;, #\u0026gt; # minimumDistanceAboveSurfaceInMeters \u0026lt;chr\u0026gt;, dynamicProperties \u0026lt;chr\u0026gt;, #\u0026gt; # samplingEffort \u0026lt;chr\u0026gt;, organismName \u0026lt;chr\u0026gt;, georeferencedDate \u0026lt;chr\u0026gt;, #\u0026gt; # georeferenceProtocol \u0026lt;chr\u0026gt;, georeferenceVerificationStatus \u0026lt;chr\u0026gt;, #\u0026gt; # organismID \u0026lt;chr\u0026gt;, ownerInstitutionCode \u0026lt;chr\u0026gt;, samplingProtocol \u0026lt;chr\u0026gt;, #\u0026gt; # datasetID \u0026lt;chr\u0026gt;, accessRights \u0026lt;chr\u0026gt;, georeferenceSources \u0026lt;chr\u0026gt;, #\u0026gt; # elevationAccuracy \u0026lt;dbl\u0026gt;, depth \u0026lt;dbl\u0026gt;, depthAccuracy \u0026lt;dbl\u0026gt;, #\u0026gt; # waterBody \u0026lt;chr\u0026gt; ","permalink":"http://localhost:1313/2016/06/marine-regions/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUPDATE: pkg API has changed - updated the post below to work with the current CRAN version, submitted 2016-08-02\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI was at a hackathon focused on Ocean Biogeographic Information System (OBIS) data back in November last year in Belgium. One project idea was to make it easier to get at data based on one or more marine regions. I was told that Marineregions.org is often used for shape files to get different regions to then do other work with.\u003c/p\u003e","title":"Marine Regions data in R"},{"content":"We (rOpenSci) just held our 3rd annual rOpenSci unconference (https://unconf16.ropensci.org/) in San Francisco. There were a lot of ideas, and lots of awesome projects from awesome people came out of the 2 day event.\nOne weird idea I had comes from looking at the Node world, where there are lots of tiny packages, instead of the often larger packages we have in the R world. One reason for tiny in Node is that of course you want a library to be tiny if running in the browser for faster load times (esp. on mobile).\nSo the idea is, what if we could separate all the functions in a package, or any particular function of your choice, into new packages, with all the internal functions and dependencies. And automatically as well, not manually.\nSo what are the use cases? I can\u0026rsquo;t imagine this being used to create stable packages to disperse to the world on CRAN, but it could be really useful for development purposes, or for R users/analysts that want lighter weight dependencies (e.g., a package with just the one function needed from a larger package).\nThis approach of course has drawbacks. The new created package is now broken apart from the original - however, beause it\u0026rsquo;s automated, you can just re-create it.\nAnother pain point would surely be with packages that have C/C++ code in them.\nThe package: atomize.\nThe package was made possible by the awesome functionMap package by Gábor Csárdi, and the more well-known devtools.\nExpect bugs, the package has no tests. Sorry :(\nInstallation devtools::install_github(\u0026#34;ropenscilabs/atomize\u0026#34;) library(\u0026#34;atomize\u0026#34;) usage atomize a fxn into separate package You can select one function, or many. Here, I\u0026rsquo;m using another package I maintain, rredlist, a pkg to interact with the IUCN Redlist API.\nIn this example, I want a new package called foobar with just the function rl_citation(). The function atomize::atomizer() takes the path for the package to extract from, then a path for the new package, then the function names.\natomizer(path_ref = \u0026#34;../rredlist\u0026#34;, path_new = \u0026#34;../foobar\u0026#34;, funcs = \u0026#34;rl_citation\u0026#34;) This creates a new package in the path_new directory\ninstall Now we need to install the new package, can do easily with devtools::install()\ndevtools::install(\u0026#34;../foobar\u0026#34;) load Then load the new package\nlibrary(\u0026#34;foobar\u0026#34;) call function Now call the function in the new package\nfoobar::rl_citation() #\u0026gt; [1] \u0026#34;IUCN 2015. IUCN Red List of Threatened Species. Version 2015-4 \u0026lt;www.iucnredlist.org\u0026gt;\u0026#34; it\u0026rsquo;s identical to the same function in the old package\nidentical(rredlist::rl_citation(), foobar::rl_citation()) #\u0026gt; [1] TRUE ","permalink":"http://localhost:1313/2016/04/atomize/","summary":"\u003cp\u003eWe (rOpenSci) just held our 3rd annual rOpenSci unconference (\u003ca href=\"https://unconf16.ropensci.org/\"\u003ehttps://unconf16.ropensci.org/\u003c/a\u003e) in San Francisco. There were \u003ca href=\"https://github.com/ropensci/unconf16/issues\"\u003ea lot of ideas\u003c/a\u003e, and lots of awesome projects from awesome people came out of the 2 day event.\u003c/p\u003e\n\u003cp\u003eOne weird idea I had comes from looking at the Node world, where there are lots of tiny packages, instead of the often larger packages we have in the R world. One reason for tiny in Node is that of course you want a library to be tiny if running in the browser for faster load times (esp. on mobile).\u003c/p\u003e","title":"atomize - make new packages from other packages"},{"content":"GenBank IDs, accession numbers and GI identifiers, are the two types of identifiers for entries in GenBank. (see this page for why there are two types of identifiers). Actually, recent news from NCBI is that GI identifiers will be phased out by September this year, which affects what I\u0026rsquo;ll talk about below.\nThere are a lot of sequences in GenBank. Sometimes you have identifiers and you want to check if they exist in GenBank, or want to get one type from another (accession from GI, or vice versa; although GI phase out will make this use case no longer needed), or just get a bunch of identifiers for software testing purposes perhaps.\nCurrently, the ENTREZ web services aren\u0026rsquo;t super performant or easy to use for the above use cases. Thus, I\u0026rsquo;ve built out a RESTful API for these use cases, called gbids.\ngbids on GitHub: sckott/gbids\nHere\u0026rsquo;s the tech stack:\nAPI: Ruby/Sinatra Storage: MySQL Caching: Redis each key cached for 3 hours Server: Caddy https Authentication: none Will soon have a cron job update when new dump is available every Sunday, but for now we\u0026rsquo;re about a month behind the current dump of identifiers. If usage of the API picks up, I\u0026rsquo;ll know it\u0026rsquo;s worth maintaining and make sure data is up to date.\nThe base url is https://gbids.xyz\nHere\u0026rsquo;s a rundown of the API routes:\n/ re-routes to /heartbeat /heartbeat - list routes /acc - GET - list accession ids /acc/:id,:id,... - GET - submit many accession numbers, get back boolean (match or no match) /acc - POST /gi - GET - list gi numbers /gi/:id,:id,... - GET - submit many gi numbers, get back boolean (match or no match) /gi - POST /acc2gi/:id,:id,... - GET - get gi numbers from accession numbers /acc2gi - POST /gi2acc/:id,:id,... - GET - get accession numbers from gi numbers /gi2acc - POST Of course after GI identifiers are phased out, all gi routes will be removed.\nThe API docs are at recology.info/gbidsdocs - let me know if you have any feedback on those.\nI also have a status page up at recology.info/gbidsstatus - it\u0026rsquo;s not automated, I have to update the status manually, but I do update that page whenever I\u0026rsquo;m doing maintenance and the API will be down, or if it goes down due to any other reason.\nexamples Request to list accession identifiers, limit to 5\ncurl \u0026#39;https://gbids.xyz/acc?limit=5\u0026#39; | jq . { \u0026#34;matched\u0026#34;: 692006925, \u0026#34;returned\u0026#34;: 5, \u0026#34;data\u0026#34;: [ \u0026#34;A00002\u0026#34;, \u0026#34;A00003\u0026#34;, \u0026#34;X17276\u0026#34;, \u0026#34;X60065\u0026#34;, \u0026#34;CAA42669\u0026#34; ], \u0026#34;error\u0026#34;: null } Request to match accession identifiers, some exist, while some do not\ncurl \u0026#39;https://gbids.xyz/acc/AACY024124486,AACY024124483,asdfd,asdf,AACY024124476\u0026#39; | jq . { \u0026#34;matched\u0026#34;: 3, \u0026#34;returned\u0026#34;: 5, \u0026#34;data\u0026#34;: { \u0026#34;AACY024124486\u0026#34;: true, \u0026#34;AACY024124483\u0026#34;: true, \u0026#34;asdfd\u0026#34;: false, \u0026#34;asdf\u0026#34;: false, \u0026#34;AACY024124476\u0026#34;: true }, \u0026#34;error\u0026#34;: null } to do I think it\u0026rsquo;d probably be worth adding routes for getting accession from taxonomy id and vice versa since that\u0026rsquo;s something that is currently not very easy. We could use the dumped accession2taxid data from ftp://ftp.ncbi.nih.gov/pub/taxonomy/accession2taxid/\nfeedback! If you think this could be potentially useful, do try it out and send any feedback. I watch logs from the API, so I\u0026rsquo;m hoping for an increase in usage so I know people find it useful.\nSince servers aren\u0026rsquo;t free, costs add up, and if I don\u0026rsquo;t see usage pick up I\u0026rsquo;ll discontinue the service at some point. So do use it!\nAnd if anyone can offer free servers, I\u0026rsquo;d gladly take advantage of that. I\u0026rsquo;ve applied for AWS research grant, but won\u0026rsquo;t hear back for a few months.\n","permalink":"http://localhost:1313/2016/03/genbank-ids/","summary":"\u003cp\u003eGenBank IDs, accession numbers and GI identifiers, are the two types of identifiers for entries in GenBank. (see \u003ca href=\"https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html\"\u003ethis page\u003c/a\u003e for why there are two types of identifiers). Actually, \u003ca href=\"https://www.ncbi.nlm.nih.gov/news/03-02-2016-phase-out-of-GI-numbers/\"\u003erecent news\u003c/a\u003e from NCBI is that GI identifiers will be phased out by September this year, which affects what I\u0026rsquo;ll talk about below.\u003c/p\u003e\n\u003cp\u003eThere are a lot of sequences in GenBank. Sometimes you have identifiers and you want to check if they exist in GenBank, or want to get one type from another (accession from GI, or vice versa; although GI phase out will make this use case no longer needed), or just get a bunch of identifiers for software testing purposes perhaps.\u003c/p\u003e","title":"GenBank IDs API - get, match, swap id types"},{"content":"GitHub issues are great for humans to correspond over software, or any other project. At rOpenSci we use an issue based software review system (ropensci/onboarding). Software authors and reviewers go back and forth on the software, making a better product in the end.\nWe have a relatively small number of pieces of software under review at any one time compared to e.g., scientific journals - however, even with the small number, we as organizers, and authors and reviewers can forget things. For example:\nan organizer can forget to remind a reviewer to get a review in a reviewer can forget about a review, and may benefit from a friendly reminder an author may forget about updating software based on the review As we are managing more package submissions through our system, automated things done by machine, or robot, will be increasingly helpful to keep the system moving smoothly.\nA big red flag with automated systems is the annoyance factor. We can try to be smart about this and only remind people when it\u0026rsquo;s really needed.\nI\u0026rsquo;ve been working on a thing for a while now, it\u0026rsquo;s called heythere. It\u0026rsquo;s a Ruby application that is currently set up to run on Heroku, though you could run it anywhere you want. It\u0026rsquo;s running right now once per day to check to see if it should send any reminders to organizers, authors, reviewers.\nheythere on GitHub: ropenscilabs/heythere\nHow it works heythere is controlled through a series of environment variables that controls GitHub authentication, the first day post reviewer assignment when a reminder should be sent, how many days after reviews are submitted to ask if the author needs any help, and more. Check out the repo for all the env var options.\nThe Ruby app can be run via a rake task from the command line, or triggered with a scheduler on something like Heroku.\nWhen the app runs, we look for environment variables that you set. If we don\u0026rsquo;t find them we use sensible defaults.\nUsing the env vars, we grab the issues for the repository you chose, limit to a subset of your choosing based on a series of labels, then compare dates on comments compared to your env vars, and either skip or send of comments on issues.\nWe use ockokit under the hood to work with GitHub issues.\nHow to use it clone git clone git@github.com:ropenscilabs/heythere.git cd heythere setup Change the repo in Rakefile to whatever your repository is.\nHeythere.hey_there(repo = \u0026#39;ropensci/onboarding\u0026#39;) Create the app (use a different name, of course)\nheroku apps:create ropensci-hey-there Add the repository that you are targeting:\nheroku config:add HEYTHERE_REPOSITORY=\u0026lt;github-repository\u0026gt; (like `owner/repo`) Create a GitHub personal access token just for this application. You\u0026rsquo;ll need to set a env var for your username and the token. We read these in the app.\nheroku config:add GITHUB_USERNAME=\u0026lt;github-user\u0026gt; heroku config:add GITHUB_PAT_OCTOKIT=\u0026lt;github-pat-for-octokit\u0026gt; Optionally, set env vars for various options. You don\u0026rsquo;t have to set these - we\u0026rsquo;ll use defaults\nheroku config:add HEYTHERE_PRE_DEADLINE_DAYS=\u0026lt;number-of-days-integer\u0026gt; heroku config:add HEYTHERE_DEADLINE_DAYS=\u0026lt;number-of-days-integer\u0026gt; heroku config:add HEYTHERE_POST_DEADLINE_EVERY_DAYS=\u0026lt;number-of-days-integer\u0026gt; heroku config:add HEYTHERE_POST_REVIEW_IN_DAYS=\u0026lt;number-of-days-integer\u0026gt; heroku config:add HEYTHERE_POST_REVIEW_TOGGLE=\u0026lt;boolean\u0026gt; heroku config:add HEYTHERE_BOT_NICKNAME=\u0026lt;string\u0026gt; Also save all these env vars in your .bash_profile, .zshrc, or similar so you can run the app locally. E.g. with entries like export HEYTHERE_PRE_DEADLINE_DAYS=15\nYou can see all your Heroku config vars using heroku config or use rake envs\nPush your app to Heroku\ngit push heroku master Add the scheduler to your heroku app\nheroku addons:create scheduler:standard heroku addons:open scheduler Add the task rake hey to your heroku scheduler and set to whatever schedule you want.\nusage If you have your repo in an env var as above, run the rake task hey\nrake hey If not, then pass the repo to hey like\nrake hey repo=owner/repo what does it look like? This is what a comment looks like in a thread\nYou could set up a different GitHub account as your robot so it\u0026rsquo;s clearly not coming from a real person.\nfeedback I\u0026rsquo;ll continue to improve heythere as we get feedback on its use in ropensci/onboarding. Would also love any feedback from you, internet. Thanks!\n","permalink":"http://localhost:1313/2016/03/hey-there/","summary":"\u003cp\u003eGitHub issues are great for humans to correspond over software, or any other project. At rOpenSci we use an issue based software review system (\u003ca href=\"https://github.com/ropensci/onboarding\"\u003eropensci/onboarding\u003c/a\u003e). Software authors and reviewers go back and forth on the software, making a better product in the end.\u003c/p\u003e\n\u003cp\u003eWe have a relatively small number of pieces of software under review at any one time compared to e.g., scientific journals - however, even with the small number, we as organizers, and authors and reviewers can forget things. For example:\u003c/p\u003e","title":"heythere - a robot to automate GitHub issue comments"},{"content":"scrubr is an R library for cleaning species occurrence records. It\u0026rsquo;s general purpose, and has the following approach:\nWe think using a piping workflow (%\u0026gt;%) makes code easier to build up, and easier to understand. However, you don\u0026rsquo;t have to use pipes in this package. All inputs and outputs are data.frame\u0026rsquo;s - which makes the above point easier Records trimmed off due to various filters are retained as attributes, so can still be accessed for later inspection, but don\u0026rsquo;t get in the way of the data.frame that gets modified for downstream use User interface vs. speed: This is the kind of package that surely can get faster. However, we\u0026rsquo;re focusing on the UI first, then make speed improvements down the road. Since occurrence record datasets should all have columns with lat/long information, we automatically look for those columns for you. If identified, we use them, but you can supply lat/long column names manually as well. We have many packages that fetch species occurrence records from GBIF, iNaturalist, VertNet, iDigBio, Ecoengine, and more. scrubr fills a crucial missing niche as likely all uses of occurrence data requires cleaning of some kind. When using GBIF data via rgbif, that package has some utilities for cleaning data based on the issues returned with GBIF data - scrubr is a companion to do the rest of the cleaning.\nscrubr use cases Those covered Impossible lat/long values: e.g., latitude 75 Incomplete cases: one or the other of lat/long missing Unlikely lat/long values: e.g., points at 0,0 Deduplication: try to identify duplicates, esp. when pulling data from multiple sources, e.g., can try to use occurrence IDs, if provided Date based cleaning Outside political boundary: User input to check for points in the wrong country, or points outside of a known country Taxonomic name based cleaning: via taxize (one method so far) To be covered Political centroids: unlikely that occurrences fall exactly on these points, more likely a default position (Draft function started, but not exported, and commented out) Herbaria/Museums: many specimens may have location of the collection they are housed in Habitat type filtering: e.g., fish should not be on land; marine fish should not be in fresh water Check for contextually wrong values: That is, if 99 out of 100 lat/long coordinates are within the continental US, but 1 is in China, then perhaps something is wrong with that one point and many more\u0026hellip; What else do you want included? Open an issue in the repo to chat about use cases.\nInstall From CRAN (binaries may not be up yet, but source is)\ninstall.packages(\u0026#34;scrubr\u0026#34;) Or from GitHub\ndevtools::install_github(\u0026#34;ropenscilabs/scrubr\u0026#34;) library(\u0026#34;scrubr\u0026#34;) dframe dframe() is a tool to convert your data.frame to a compact dplyr like data.frame so that you can get a quick peek at your data each time you call a function - BUT, you don\u0026rsquo;t have to use it.\nCompare mtcars\nmtcars #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb #\u0026gt; Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #\u0026gt; Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #\u0026gt; Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #\u0026gt; Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #\u0026gt; Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 #\u0026gt; Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 #\u0026gt; Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 #\u0026gt; Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #\u0026gt; Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #\u0026gt; Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 #\u0026gt; Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 #\u0026gt; Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 #\u0026gt; Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 #\u0026gt; Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 #\u0026gt; Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 #\u0026gt; Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 #\u0026gt; Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 #\u0026gt; Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 #\u0026gt; Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 #\u0026gt; Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 #\u0026gt; Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 #\u0026gt; Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 #\u0026gt; AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 #\u0026gt; Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 #\u0026gt; Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 #\u0026gt; Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 #\u0026gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 #\u0026gt; Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 #\u0026gt; Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 #\u0026gt; Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 #\u0026gt; Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 #\u0026gt; Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 To\ndframe(mtcars) #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 32 X 11 #\u0026gt; #\u0026gt; #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb #\u0026gt; (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) #\u0026gt; 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #\u0026gt; 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #\u0026gt; 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #\u0026gt; 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #\u0026gt; 5 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 #\u0026gt; 6 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 #\u0026gt; 7 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 #\u0026gt; 8 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #\u0026gt; 9 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #\u0026gt; 10 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 #\u0026gt; .. ... ... ... ... ... ... ... ... ... ... ... Coordinate based cleaning Load some sample data that comes with the package\ndata(\u0026#34;sampledata1\u0026#34;) Remove impossible coordinates (using sample data included in the pkg)\ndframe(sample_data_1) %\u0026gt;% coord_impossible() #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 1500 X 5 #\u0026gt; Lat/Lon vars: latitude/longitude #\u0026gt; #\u0026gt; name longitude latitude date key #\u0026gt; (chr) (dbl) (dbl) (time) (int) #\u0026gt; 1 Ursus americanus -79.68283 38.36662 2015-01-14 16:36:45 1065590124 #\u0026gt; 2 Ursus americanus -82.42028 35.73304 2015-01-13 00:25:39 1065588899 #\u0026gt; 3 Ursus americanus -99.09625 23.66893 2015-02-20 23:00:00 1098894889 #\u0026gt; 4 Ursus americanus -72.77432 43.94883 2015-02-13 16:16:41 1065611122 #\u0026gt; 5 Ursus americanus -72.34617 43.86464 2015-03-01 20:20:45 1088908315 #\u0026gt; 6 Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238 #\u0026gt; 7 Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273 #\u0026gt; 8 Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409 #\u0026gt; 9 Ursus americanus -78.25027 36.93018 2015-03-20 21:11:24 1088923534 #\u0026gt; 10 Ursus americanus -76.78671 35.53079 2015-04-05 23:00:00 1088954559 #\u0026gt; .. ... ... ... ... ... Remove incomplete coordinates\ndframe(sample_data_1) %\u0026gt;% coord_incomplete() #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 1306 X 5 #\u0026gt; Lat/Lon vars: latitude/longitude #\u0026gt; #\u0026gt; name longitude latitude date key #\u0026gt; (chr) (dbl) (dbl) (time) (int) #\u0026gt; 1 Ursus americanus -79.68283 38.36662 2015-01-14 16:36:45 1065590124 #\u0026gt; 2 Ursus americanus -82.42028 35.73304 2015-01-13 00:25:39 1065588899 #\u0026gt; 3 Ursus americanus -99.09625 23.66893 2015-02-20 23:00:00 1098894889 #\u0026gt; 4 Ursus americanus -72.77432 43.94883 2015-02-13 16:16:41 1065611122 #\u0026gt; 5 Ursus americanus -72.34617 43.86464 2015-03-01 20:20:45 1088908315 #\u0026gt; 6 Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238 #\u0026gt; 7 Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273 #\u0026gt; 8 Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409 #\u0026gt; 9 Ursus americanus -78.25027 36.93018 2015-03-20 21:11:24 1088923534 #\u0026gt; 10 Ursus americanus -76.78671 35.53079 2015-04-05 23:00:00 1088954559 #\u0026gt; .. ... ... ... ... ... Remove unlikely coordinates (e.g., those at 0,0)\ndframe(sample_data_1) %\u0026gt;% coord_unlikely() #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 1488 X 5 #\u0026gt; Lat/Lon vars: latitude/longitude #\u0026gt; #\u0026gt; name longitude latitude date key #\u0026gt; (chr) (dbl) (dbl) (time) (int) #\u0026gt; 1 Ursus americanus -79.68283 38.36662 2015-01-14 16:36:45 1065590124 #\u0026gt; 2 Ursus americanus -82.42028 35.73304 2015-01-13 00:25:39 1065588899 #\u0026gt; 3 Ursus americanus -99.09625 23.66893 2015-02-20 23:00:00 1098894889 #\u0026gt; 4 Ursus americanus -72.77432 43.94883 2015-02-13 16:16:41 1065611122 #\u0026gt; 5 Ursus americanus -72.34617 43.86464 2015-03-01 20:20:45 1088908315 #\u0026gt; 6 Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238 #\u0026gt; 7 Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273 #\u0026gt; 8 Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409 #\u0026gt; 9 Ursus americanus -78.25027 36.93018 2015-03-20 21:11:24 1088923534 #\u0026gt; 10 Ursus americanus -76.78671 35.53079 2015-04-05 23:00:00 1088954559 #\u0026gt; .. ... ... ... ... ... Do all three\ndframe(sample_data_1) %\u0026gt;% coord_impossible() %\u0026gt;% coord_incomplete() %\u0026gt;% coord_unlikely() #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 1294 X 5 #\u0026gt; Lat/Lon vars: latitude/longitude #\u0026gt; #\u0026gt; name longitude latitude date key #\u0026gt; (chr) (dbl) (dbl) (time) (int) #\u0026gt; 1 Ursus americanus -79.68283 38.36662 2015-01-14 16:36:45 1065590124 #\u0026gt; 2 Ursus americanus -82.42028 35.73304 2015-01-13 00:25:39 1065588899 #\u0026gt; 3 Ursus americanus -99.09625 23.66893 2015-02-20 23:00:00 1098894889 #\u0026gt; 4 Ursus americanus -72.77432 43.94883 2015-02-13 16:16:41 1065611122 #\u0026gt; 5 Ursus americanus -72.34617 43.86464 2015-03-01 20:20:45 1088908315 #\u0026gt; 6 Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238 #\u0026gt; 7 Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273 #\u0026gt; 8 Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409 #\u0026gt; 9 Ursus americanus -78.25027 36.93018 2015-03-20 21:11:24 1088923534 #\u0026gt; 10 Ursus americanus -76.78671 35.53079 2015-04-05 23:00:00 1088954559 #\u0026gt; .. ... ... ... ... ... Do vs. don\u0026rsquo;t drop bad data\n# do dframe(sample_data_1) %\u0026gt;% coord_incomplete(drop = TRUE) %\u0026gt;% NROW #\u0026gt; [1] 1306 # don\u0026#39;t dframe(sample_data_1) %\u0026gt;% coord_incomplete(drop = FALSE) %\u0026gt;% NROW #\u0026gt; [1] 1500 Deduplicate Get a smaller subset of a data.frame\nsmalldf \u0026lt;- sample_data_1[1:20, ] create a duplicate record\nsmalldf \u0026lt;- rbind(smalldf, smalldf[10,]) row.names(smalldf) \u0026lt;- NULL make it slightly different\nsmalldf[21, \u0026#34;key\u0026#34;] \u0026lt;- 1088954555 NROW(smalldf) #\u0026gt; [1] 21 It\u0026rsquo;s 21 rows, including 1 duplicate. Do the deduplication\n(dp \u0026lt;- dframe(smalldf) %\u0026gt;% dedup()) #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 20 X 5 #\u0026gt; #\u0026gt; #\u0026gt; name longitude latitude date key #\u0026gt; (chr) (dbl) (dbl) (time) (dbl) #\u0026gt; 1 Ursus americanus -79.68283 38.36662 2015-01-14 16:36:45 1065590124 #\u0026gt; 2 Ursus americanus -82.42028 35.73304 2015-01-13 00:25:39 1065588899 #\u0026gt; 3 Ursus americanus -99.09625 23.66893 2015-02-20 23:00:00 1098894889 #\u0026gt; 4 Ursus americanus -72.77432 43.94883 2015-02-13 16:16:41 1065611122 #\u0026gt; 5 Ursus americanus -72.34617 43.86464 2015-03-01 20:20:45 1088908315 #\u0026gt; 6 Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238 #\u0026gt; 7 Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273 #\u0026gt; 8 Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409 #\u0026gt; 9 Ursus americanus -78.25027 36.93018 2015-03-20 21:11:24 1088923534 #\u0026gt; 10 Ursus americanus -103.30058 29.27042 2015-04-29 22:00:00 1088964797 #\u0026gt; .. ... ... ... ... ... Now its 20 rows, duplicate removed\nHere\u0026rsquo;s the duplicates\nattr(dp, \u0026#34;dups\u0026#34;) #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 1 X 5 #\u0026gt; #\u0026gt; #\u0026gt; name longitude latitude date key #\u0026gt; (chr) (dbl) (dbl) (time) (dbl) #\u0026gt; 1 Ursus americanus -76.78671 35.53079 2015-04-05 23:00:00 1088954555 Dates Standardize/convert dates\ndf \u0026lt;- sample_data_1 dframe(df) %\u0026gt;% date_standardize(\u0026#34;%d%b%Y\u0026#34;) #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 1500 X 5 #\u0026gt; #\u0026gt; #\u0026gt; name longitude latitude date key #\u0026gt; (chr) (dbl) (dbl) (chr) (int) #\u0026gt; 1 Ursus americanus -79.68283 38.36662 14Jan2015 1065590124 #\u0026gt; 2 Ursus americanus -82.42028 35.73304 13Jan2015 1065588899 #\u0026gt; 3 Ursus americanus -99.09625 23.66893 20Feb2015 1098894889 #\u0026gt; 4 Ursus americanus -72.77432 43.94883 13Feb2015 1065611122 #\u0026gt; 5 Ursus americanus -72.34617 43.86464 01Mar2015 1088908315 #\u0026gt; 6 Ursus americanus -108.53674 32.65219 29Mar2015 1088932238 #\u0026gt; 7 Ursus americanus -108.53691 32.65237 29Mar2015 1088932273 #\u0026gt; 8 Ursus americanus -123.82900 40.13240 28Mar2015 1132403409 #\u0026gt; 9 Ursus americanus -78.25027 36.93018 20Mar2015 1088923534 #\u0026gt; 10 Ursus americanus -76.78671 35.53079 05Apr2015 1088954559 #\u0026gt; .. ... ... ... ... ... Drop records without dates\nNROW(df) #\u0026gt; [1] 1500 NROW(dframe(df) %\u0026gt;% date_missing()) #\u0026gt; [1] 1498 Create date field from other fields\ndframe(sample_data_2) %\u0026gt;% date_create(year, month, day) #\u0026gt; \u0026lt;scrubr dframe\u0026gt; #\u0026gt; Size: 1500 X 8 #\u0026gt; #\u0026gt; #\u0026gt; name longitude latitude key year month day #\u0026gt; (chr) (dbl) (dbl) (int) (chr) (chr) (chr) #\u0026gt; 1 Ursus americanus -79.68283 38.36662 1065590124 2015 01 14 #\u0026gt; 2 Ursus americanus -82.42028 35.73304 1065588899 2015 01 13 #\u0026gt; 3 Ursus americanus -99.09625 23.66893 1098894889 2015 02 20 #\u0026gt; 4 Ursus americanus -72.77432 43.94883 1065611122 2015 02 13 #\u0026gt; 5 Ursus americanus -72.34617 43.86464 1088908315 2015 03 01 #\u0026gt; 6 Ursus americanus -108.53674 32.65219 1088932238 2015 03 29 #\u0026gt; 7 Ursus americanus -108.53691 32.65237 1088932273 2015 03 29 #\u0026gt; 8 Ursus americanus -123.82900 40.13240 1132403409 2015 03 28 #\u0026gt; 9 Ursus americanus -78.25027 36.93018 1088923534 2015 03 20 #\u0026gt; 10 Ursus americanus -76.78671 35.53079 1088954559 2015 04 05 #\u0026gt; .. ... ... ... ... ... ... ... #\u0026gt; Variables not shown: date (chr). bugs and such Report them in the scrubr issue tracker\n","permalink":"http://localhost:1313/2016/03/scrubr/","summary":"\u003cp\u003e\u003ccode\u003escrubr\u003c/code\u003e is an R library for cleaning species occurrence records. It\u0026rsquo;s general purpose, and has the following approach:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe think using a piping workflow (\u003ccode\u003e%\u0026gt;%\u003c/code\u003e) makes code easier to build up, and easier to understand. However, you don\u0026rsquo;t have to use pipes in this package.\u003c/li\u003e\n\u003cli\u003eAll inputs and outputs are data.frame\u0026rsquo;s - which makes the above point easier\u003c/li\u003e\n\u003cli\u003eRecords trimmed off due to various filters are retained as attributes, so can still be accessed for later inspection, but don\u0026rsquo;t get in the way of the data.frame that gets modified for downstream use\u003c/li\u003e\n\u003cli\u003eUser interface vs. speed: This is the kind of package that surely can get faster. However, we\u0026rsquo;re focusing on the UI first, then make speed improvements down the road.\u003c/li\u003e\n\u003cli\u003eSince occurrence record datasets should all have columns with lat/long information, we automatically look for those columns for you. If identified, we use them, but you can supply lat/long column names manually as well.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe have many packages that fetch species occurrence records from GBIF, iNaturalist, VertNet, iDigBio, Ecoengine, and more. \u003ccode\u003escrubr\u003c/code\u003e fills a crucial missing niche as likely all uses of occurrence data requires cleaning of some kind. When using GBIF data via \u003ccode\u003ergbif\u003c/code\u003e, that package has some utilities for cleaning data based on the issues returned with GBIF data - \u003ccode\u003escrubr\u003c/code\u003e is a companion to do the rest of the cleaning.\u003c/p\u003e","title":"scrubr - clean species occurrence records"},{"content":"request is DSL for http requests for R, and is inspired by the CLI tool httpie. It\u0026rsquo;s built on httr.\nThe following were driving principles for this package:\nThe web is increasingly a JSON world, so we assume applications/json by default, but give back other types if not The workflow follows logically, or at least should, from, hey, I got this url, to i need to add some options, to execute request - and functions support piping so that you can execute functions in this order Whenever possible, we transform output to data.frame\u0026rsquo;s - facilitating downstream manipulation via dplyr, etc. We do GET requests by default. Specify a different type if you don\u0026rsquo;t want GET. Given GET by default, this client is optimized for consumption of data, rather than creating new things on servers You can use non-standard evaluation to easily pass in query parameters without worrying about \u0026amp;\u0026rsquo;s, URL escaping, etc. (see api_query()) Same for body params (see api_body()) The following is a brief demo of some of the package functionality:\nInstall From CRAN\ninstall.packages(\u0026#34;request\u0026#34;) Or from GitHub\ndevtools::install_github(\u0026#34;sckott/request\u0026#34;) library(\u0026#34;request\u0026#34;) Execute on last pipe When using pipes (%\u0026gt;%) in request, we autodetect last piped command, and execute http() if it\u0026rsquo;s the last. If not the last, the output gets passed to the next command, and so on. This feature (and magrittr) were done by Stefan Milton Bache.\nThis feature is really nice because a) it\u0026rsquo;s one less thing you need to do, and b) you only need to care about the request itself.\nYou can escape auto-execution if you use the function peep(), which prints out a summary of the request you\u0026rsquo;ve created, but does not execute an HTTP request.\nHTTP Requests A high level function http() wraps a lower level R6 object RequestIterator, which holds a series of variables and functions to execute GET and POST requests, and will hold other HTTP verbs as well. In addition, it can hold state, which will allow us to do paging internally for you (see below). You have direct access to the R6 object if you call http_client() instead of http().\nNSE and SE Most if not all functions in request support non-standard evaluation (NSE) as well as standard evaluation (SE). If a function supports both, there\u0026rsquo;s a version without an underscore for NSE, while a version with an underscore is for SE. For example, here, we make a HTTP request by passing a base URL, then a series of paths that get combined together. First the NSE version\napi(\u0026#39;https://api.github.com/\u0026#39;) %\u0026gt;% api_path(repos, ropensci, rgbif, issues) Then the SE version\napi(\u0026#39;https://api.github.com/\u0026#39;) %\u0026gt;% api_path_(\u0026#39;repos\u0026#39;, \u0026#39;ropensci\u0026#39;, \u0026#39;rgbif\u0026#39;, \u0026#39;issues\u0026#39;) Building API routes The first thing you\u0026rsquo;ll want to do is lay out the base URL for your request. The function api() is your friend.\napi() works with full or partial URLs:\napi(\u0026#39;https://api.github.com/\u0026#39;) #\u0026gt; URL: https://api.github.com/ api(\u0026#39;https://api.gbif.org/v1\u0026#39;) #\u0026gt; URL: https://api.gbif.org/v1 api(\u0026#39;api.gbif.org/v1\u0026#39;) #\u0026gt; URL: api.gbif.org/v1 And works with ports, full or partial\napi(\u0026#39;http://localhost:9200\u0026#39;) #\u0026gt; URL: http://localhost:9200 api(\u0026#39;localhost:9200\u0026#39;) #\u0026gt; URL: http://localhost:9200 api(\u0026#39;:9200\u0026#39;) #\u0026gt; URL: http://localhost:9200 api(\u0026#39;9200\u0026#39;) #\u0026gt; URL: http://localhost:9200 api(\u0026#39;9200/stuff\u0026#39;) #\u0026gt; URL: http://localhost:9200/stuff Make HTTP requests The above examples with api() are not passed through a pipe, so only define a URL, but don\u0026rsquo;t do an HTTP request. To make an HTTP request, you can either pipe a url or partial url to e.g., api(), or call http() at the end of a string of function calls:\n\u0026#39;https://api.github.com/\u0026#39; %\u0026gt;% api() #\u0026gt; $current_user_url #\u0026gt; [1] \u0026#34;https://api.github.com/user\u0026#34; #\u0026gt; #\u0026gt; $current_user_authorizations_html_url #\u0026gt; [1] \u0026#34;https://github.com/settings/connections/applications{/client_id}\u0026#34; #\u0026gt; #\u0026gt; $authorizations_url #\u0026gt; [1] \u0026#34;https://api.github.com/authorizations\u0026#34; #\u0026gt; #\u0026gt; $code_search_url ... Or\napi(\u0026#39;https://api.github.com/\u0026#39;) %\u0026gt;% http() #\u0026gt; $current_user_url #\u0026gt; [1] \u0026#34;https://api.github.com/user\u0026#34; #\u0026gt; #\u0026gt; $current_user_authorizations_html_url #\u0026gt; [1] \u0026#34;https://github.com/settings/connections/applications{/client_id}\u0026#34; #\u0026gt; #\u0026gt; $authorizations_url #\u0026gt; [1] \u0026#34;https://api.github.com/authorizations\u0026#34; #\u0026gt; #\u0026gt; $code_search_url ... http() is called at the end of a chain of piped commands, so no need to invoke it. However, you can if you like.\nTemplating repo_info \u0026lt;- list(username = \u0026#39;craigcitro\u0026#39;, repo = \u0026#39;r-travis\u0026#39;) api(\u0026#39;https://api.github.com/\u0026#39;) %\u0026gt;% api_template(template = \u0026#39;repos/{{username}}/{{repo}}/issues\u0026#39;, data = repo_info) #\u0026gt; [[1]] #\u0026gt; [[1]]$url #\u0026gt; [1] \u0026#34;https://api.github.com/repos/craigcitro/r-travis/issues/164\u0026#34; #\u0026gt; #\u0026gt; [[1]]$labels_url #\u0026gt; [1] \u0026#34;https://api.github.com/repos/craigcitro/r-travis/issues/164/labels{/name}\u0026#34; #\u0026gt; #\u0026gt; [[1]]$comments_url #\u0026gt; [1] \u0026#34;https://api.github.com/repos/craigcitro/r-travis/issues/164/comments\u0026#34; #\u0026gt; ... Set paths api_path() adds paths to the base URL\napi(\u0026#39;https://api.github.com/\u0026#39;) %\u0026gt;% api_path(repos, ropensci, rgbif, issues) %\u0026gt;% peep #\u0026gt; \u0026lt;http request\u0026gt; #\u0026gt; url: https://api.github.com/ #\u0026gt; paths: repos/ropensci/rgbif/issues #\u0026gt; query: #\u0026gt; body: #\u0026gt; paging: #\u0026gt; headers: #\u0026gt; rate limit: #\u0026gt; retry (n/delay (s)): / #\u0026gt; error handler: #\u0026gt; config: Query api(\u0026#34;https://api.plos.org/search\u0026#34;) %\u0026gt;% api_query(q = ecology, wt = json, fl = journal) %\u0026gt;% peep #\u0026gt; \u0026lt;http request\u0026gt; #\u0026gt; url: https://api.plos.org/search #\u0026gt; paths: #\u0026gt; query: q=ecology, wt=json, fl=journal #\u0026gt; body: #\u0026gt; paging: #\u0026gt; headers: #\u0026gt; rate limit: #\u0026gt; retry (n/delay (s)): / #\u0026gt; error handler: #\u0026gt; config: Headers api(\u0026#39;https://httpbin.org/headers\u0026#39;) %\u0026gt;% api_headers(`X-FARGO-SEASON` = 3, `X-NARCOS-SEASON` = 5) %\u0026gt;% peep #\u0026gt; \u0026lt;http request\u0026gt; #\u0026gt; url: https://httpbin.org/headers #\u0026gt; paths: #\u0026gt; query: #\u0026gt; body: #\u0026gt; paging: #\u0026gt; headers: #\u0026gt; X-FARGO-SEASON: 3 #\u0026gt; X-NARCOS-SEASON: 5 #\u0026gt; rate limit: #\u0026gt; retry (n/delay (s)): / #\u0026gt; error handler: #\u0026gt; config: curl configuration httr is exported in request, so you can use httr functions like verbose() to get verbose curl output\napi(\u0026#39;https://httpbin.org/headers\u0026#39;) %\u0026gt;% api_config(verbose()) #\u0026gt; -\u0026gt; GET /headers HTTP/1.1 #\u0026gt; -\u0026gt; Host: httpbin.org #\u0026gt; -\u0026gt; User-Agent: curl/7.43.0 curl/0.9.4 httr/1.0.0 request/0.1.0 #\u0026gt; -\u0026gt; Accept-Encoding: gzip, deflate #\u0026gt; -\u0026gt; Accept: application/json, text/xml, application/xml, */* #\u0026gt; -\u0026gt; #\u0026gt; \u0026lt;- HTTP/1.1 200 OK #\u0026gt; \u0026lt;- Server: nginx #\u0026gt; \u0026lt;- Date: Sun, 03 Jan 2016 16:56:29 GMT #\u0026gt; \u0026lt;- Content-Type: application/json #\u0026gt; \u0026lt;- Content-Length: 227 #\u0026gt; \u0026lt;- Connection: keep-alive #\u0026gt; \u0026lt;- Access-Control-Allow-Origin: * #\u0026gt; \u0026lt;- Access-Control-Allow-Credentials: true #\u0026gt; \u0026lt;- #\u0026gt; $headers #\u0026gt; $headers$Accept #\u0026gt; [1] \u0026#34;application/json, text/xml, application/xml, */*\u0026#34; #\u0026gt; ... Coming soon There\u0026rsquo;s a number of interesting features that should be coming soon to request.\nPaging - a paging helper will make it easy to do paing, and will attempt to handle any parameters used for paging. Some user input will be required, like what parameter names are, and how many records you want returned sckott/request#2 Retry - a retry helper will make it easy to retry http requests on any failure, and execute a user defined function on failure sckott/request#6 Rate limit - a rate limit helper will add info to a set of many requests - still in early design stages sckott/request#5 Caching - a caching helper - may use in the background the in development vcr R client when on CRAN or perhaps storr sckott/request#4 ","permalink":"http://localhost:1313/2016/01/request-hello-world/","summary":"\u003cp\u003e\u003ccode\u003erequest\u003c/code\u003e is DSL for http requests for R, and is inspired by the CLI tool \u003ca href=\"https://github.com/jakubroztocil/httpie\"\u003ehttpie\u003c/a\u003e. It\u0026rsquo;s built on \u003ccode\u003ehttr\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe following were driving principles for this package:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe web is increasingly a JSON world, so we assume \u003ccode\u003eapplications/json\u003c/code\u003e by default, but give back other types if not\u003c/li\u003e\n\u003cli\u003eThe workflow follows logically, or at least should, from, \u003cem\u003ehey, I got this url\u003c/em\u003e, to \u003cem\u003ei need to add some options\u003c/em\u003e, to \u003cem\u003eexecute request\u003c/em\u003e - and functions support piping so that you can execute functions in this order\u003c/li\u003e\n\u003cli\u003eWhenever possible, we transform output to data.frame\u0026rsquo;s - facilitating downstream manipulation via \u003ccode\u003edplyr\u003c/code\u003e, etc.\u003c/li\u003e\n\u003cli\u003eWe do \u003ccode\u003eGET\u003c/code\u003e requests by default. Specify a different type if you don\u0026rsquo;t want \u003ccode\u003eGET\u003c/code\u003e. Given \u003ccode\u003eGET\u003c/code\u003e by default, this client is optimized for consumption of data, rather than creating new things on servers\u003c/li\u003e\n\u003cli\u003eYou can use non-standard evaluation to easily pass in query parameters without worrying about \u003ccode\u003e\u0026amp;\u003c/code\u003e\u0026rsquo;s, URL escaping, etc. (see \u003ccode\u003eapi_query()\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eSame for body params (see \u003ccode\u003eapi_body()\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe following is a brief demo of some of the package functionality:\u003c/p\u003e","title":"request - a high level HTTP client for R"},{"content":"The first version of binomen is now up on CRAN. It provides various taxonomic classes for defining a single taxon, multiple taxa, and a taxonomic data.frame. It is designed as a companion to taxize, where you can get taxonomic data on taxonomic names from the web.\nThe classes (S3):\ntaxon taxonref taxonrefs binomial grouping (i.e., classification - used different term to avoid conflict with classification in taxize) For example, the binomial class is defined by a genus, epithet, authority, and optional full species name and canonical version.\nbinomial(\u0026#34;Poa\u0026#34;, \u0026#34;annua\u0026#34;, authority=\u0026#34;L.\u0026#34;) \u0026lt;binomial\u0026gt; genus: Poa epithet: annua canonical: species: authority: L. The package has a suite of functions to work on these taxonomic classes:\ngethier() - get hierarchy from a taxon class scatter() - make each row in taxonomic data.frame (taxondf) a separate taxon object within a single taxa object assemble() - make a taxa object into a taxondf data.frame pick() - pick out one or more taxonomic groups pop() - pop out (drop) one or more taxonomic groups span() - pick a range between two taxonomic groups (inclusive) strain() - filter by taxonomic groups, like dplyr\u0026rsquo;s filter name() - get the taxon name for each taxonref object uri() - get the reference uri for each taxonref object rank() - get the taxonomic rank for each taxonref object id() - get the reference uri for each taxonref object The approach in this package I suppose is sort of like split-apply-combine from plyr/dplyr, whereas this is aims to make it easy to do with taxonomic names.\nInstall For examples below, you\u0026rsquo;ll need the development version:\ninstall.packages(\u0026#34;binomen\u0026#34;) library(\u0026#34;binomen\u0026#34;) Make a taxon Make a taxon object\n(obj \u0026lt;- make_taxon(genus=\u0026#34;Poa\u0026#34;, epithet=\u0026#34;annua\u0026#34;, authority=\u0026#34;L.\u0026#34;, family=\u0026#39;Poaceae\u0026#39;, clazz=\u0026#39;Poales\u0026#39;, kingdom=\u0026#39;Plantae\u0026#39;, variety=\u0026#39;annua\u0026#39;)) #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa annua #\u0026gt; grouping: #\u0026gt; kingdom: Plantae #\u0026gt; clazz: Poales #\u0026gt; family: Poaceae #\u0026gt; genus: Poa #\u0026gt; species: Poa annua #\u0026gt; variety: annua Index to various parts of the object\nThe binomial\nobj$binomial #\u0026gt; \u0026lt;binomial\u0026gt; #\u0026gt; genus: Poa #\u0026gt; epithet: annua #\u0026gt; canonical: Poa annua #\u0026gt; species: Poa annua L. #\u0026gt; authority: L. The authority\nobj$binomial$authority #\u0026gt; [1] \u0026#34;L.\u0026#34; The classification\nobj$grouping #\u0026gt; \u0026lt;grouping\u0026gt; #\u0026gt; kingdom: Plantae #\u0026gt; clazz: Poales #\u0026gt; family: Poaceae #\u0026gt; genus: Poa #\u0026gt; species: Poa annua #\u0026gt; variety: annua The family\nobj$grouping$family #\u0026gt; \u0026lt;taxonref\u0026gt; #\u0026gt; rank: family #\u0026gt; name: Poaceae #\u0026gt; id: none #\u0026gt; uri: none Subset taxon objects Get one or more ranks via pick()\nobj %\u0026gt;% pick(family) #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa annua #\u0026gt; grouping: #\u0026gt; family: Poaceae obj %\u0026gt;% pick(family, genus) #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa annua #\u0026gt; grouping: #\u0026gt; family: Poaceae #\u0026gt; genus: Poa Drop one or more ranks via pop()\nobj %\u0026gt;% pop(family) #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa annua #\u0026gt; grouping: #\u0026gt; kingdom: Plantae #\u0026gt; clazz: Poales #\u0026gt; genus: Poa #\u0026gt; species: Poa annua #\u0026gt; variety: annua obj %\u0026gt;% pop(family, genus) #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa annua #\u0026gt; grouping: #\u0026gt; kingdom: Plantae #\u0026gt; clazz: Poales #\u0026gt; species: Poa annua #\u0026gt; variety: annua Get a range of ranks via span()\nobj %\u0026gt;% span(kingdom, family) #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa annua #\u0026gt; grouping: #\u0026gt; kingdom: Plantae #\u0026gt; clazz: Poales #\u0026gt; family: Poaceae Extract classification as a data.frame\ngethier(obj) #\u0026gt; rank name #\u0026gt; 1 kingdom Plantae #\u0026gt; 2 clazz Poales #\u0026gt; 3 family Poaceae #\u0026gt; 4 genus Poa #\u0026gt; 5 species Poa annua #\u0026gt; 6 variety annua Taxonomic data.frame\u0026rsquo;s Make one\ndf \u0026lt;- data.frame(order = c(\u0026#39;Asterales\u0026#39;,\u0026#39;Asterales\u0026#39;,\u0026#39;Fagales\u0026#39;,\u0026#39;Poales\u0026#39;,\u0026#39;Poales\u0026#39;,\u0026#39;Poales\u0026#39;), family = c(\u0026#39;Asteraceae\u0026#39;,\u0026#39;Asteraceae\u0026#39;,\u0026#39;Fagaceae\u0026#39;,\u0026#39;Poaceae\u0026#39;,\u0026#39;Poaceae\u0026#39;,\u0026#39;Poaceae\u0026#39;), genus = c(\u0026#39;Helianthus\u0026#39;,\u0026#39;Helianthus\u0026#39;,\u0026#39;Quercus\u0026#39;,\u0026#39;Poa\u0026#39;,\u0026#39;Festuca\u0026#39;,\u0026#39;Holodiscus\u0026#39;), stringsAsFactors = FALSE) (df2 \u0026lt;- taxon_df(df)) #\u0026gt; order family genus #\u0026gt; 1 Asterales Asteraceae Helianthus #\u0026gt; 2 Asterales Asteraceae Helianthus #\u0026gt; 3 Fagales Fagaceae Quercus #\u0026gt; 4 Poales Poaceae Poa #\u0026gt; 5 Poales Poaceae Festuca #\u0026gt; 6 Poales Poaceae Holodiscus Parse - get rank order via pick()\ndf2 %\u0026gt;% pick(order) #\u0026gt; order #\u0026gt; 1 Asterales #\u0026gt; 2 Asterales #\u0026gt; 3 Fagales #\u0026gt; 4 Poales #\u0026gt; 5 Poales #\u0026gt; 6 Poales get ranks order, family, and genus via pick()\ndf2 %\u0026gt;% pick(order, family, genus) #\u0026gt; order family genus #\u0026gt; 1 Asterales Asteraceae Helianthus #\u0026gt; 2 Asterales Asteraceae Helianthus #\u0026gt; 3 Fagales Fagaceae Quercus #\u0026gt; 4 Poales Poaceae Poa #\u0026gt; 5 Poales Poaceae Festuca #\u0026gt; 6 Poales Poaceae Holodiscus get range of names via span(), from rank X to rank Y\ndf2 %\u0026gt;% span(family, genus) #\u0026gt; family genus #\u0026gt; 1 Asteraceae Helianthus #\u0026gt; 2 Asteraceae Helianthus #\u0026gt; 3 Fagaceae Quercus #\u0026gt; 4 Poaceae Poa #\u0026gt; 5 Poaceae Festuca #\u0026gt; 6 Poaceae Holodiscus Separate each row into a taxon class (many taxon objects are a taxa class)\nscatter(df2) #\u0026gt; [[1]] #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Helianthus none #\u0026gt; grouping: #\u0026gt; order: Asterales #\u0026gt; family: Asteraceae #\u0026gt; genus: Helianthus #\u0026gt; species: Helianthus none #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Helianthus none #\u0026gt; grouping: #\u0026gt; order: Asterales #\u0026gt; family: Asteraceae #\u0026gt; genus: Helianthus #\u0026gt; species: Helianthus none #\u0026gt; #\u0026gt; [[3]] #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Quercus none #\u0026gt; grouping: #\u0026gt; order: Fagales #\u0026gt; family: Fagaceae #\u0026gt; genus: Quercus #\u0026gt; species: Quercus none #\u0026gt; #\u0026gt; [[4]] #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa none #\u0026gt; grouping: #\u0026gt; order: Poales #\u0026gt; family: Poaceae #\u0026gt; genus: Poa #\u0026gt; species: Poa none #\u0026gt; #\u0026gt; [[5]] #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Festuca none #\u0026gt; grouping: #\u0026gt; order: Poales #\u0026gt; family: Poaceae #\u0026gt; genus: Festuca #\u0026gt; species: Festuca none #\u0026gt; #\u0026gt; [[6]] #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Holodiscus none #\u0026gt; grouping: #\u0026gt; order: Poales #\u0026gt; family: Poaceae #\u0026gt; genus: Holodiscus #\u0026gt; species: Holodiscus none #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;taxa\u0026#34; And you can re-assemble a data.frame from the output of scatter() with assemble()\nout \u0026lt;- scatter(df2) assemble(out) #\u0026gt; order family genus species #\u0026gt; 1 Asterales Asteraceae Helianthus Helianthus none #\u0026gt; 2 Asterales Asteraceae Helianthus Helianthus none #\u0026gt; 3 Fagales Fagaceae Quercus Quercus none #\u0026gt; 4 Poales Poaceae Poa Poa none #\u0026gt; 5 Poales Poaceae Festuca Festuca none #\u0026gt; 6 Poales Poaceae Holodiscus Holodiscus none Thoughts? I\u0026rsquo;m really curious what people think of binomen. I\u0026rsquo;m not sure how useful this will be in the wild. Try it. Let me know. Thanks much :)\n","permalink":"http://localhost:1313/2015/12/binomen-taxonomy-tools/","summary":"\u003cp\u003eThe first version of \u003ccode\u003ebinomen\u003c/code\u003e is now up on \u003ca href=\"https://cran.rstudio.com/web/packages/binomen\"\u003eCRAN\u003c/a\u003e. It provides various taxonomic classes for defining a single taxon, multiple taxa, and a taxonomic data.frame. It is designed as a companion to \u003ca href=\"https://github.com/ropensci/taxize\"\u003etaxize\u003c/a\u003e, where you can get taxonomic data on taxonomic names from the web.\u003c/p\u003e\n\u003cp\u003eThe classes (S3):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003etaxon\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etaxonref\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etaxonrefs\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ebinomial\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003egrouping\u003c/code\u003e (i.e., classification - used different term to avoid conflict with classification in \u003ccode\u003etaxize\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, the \u003ccode\u003ebinomial\u003c/code\u003e class is defined by a genus, epithet, authority, and optional full species name and canonical version.\u003c/p\u003e","title":"binomen - Tools for slicing and dicing taxonomic names"},{"content":"I gave two talks recently at the annual Crossref meeting, one of which was a somewhat technical overview of programmatic clients for Crossref APIs. Check out the talk here. I talked about the motivation for working with Crossref data by writing code/etc. rather than going the GUI route, then went over the various clients, with brief examples.\nWe (rOpenSci) have been working on the R client rcrossref for a while now, but I\u0026rsquo;m also working on the Python and Ruby clients for Crossref. In addition, the Ruby client has a CLI client inside. The Javascript client is worked on independently by ScienceAI.\nThe R, Ruby, and Python clients are useable but not feature complete yet, and would benefit from lots of users surfacing bugs and highlighting nice to have features.\nThe main Crossref API used in all the clients is documented at api.crossref.org.\nI\u0026rsquo;ve tried to make the APIs similar-ish across clients. Functions in each client match the main Crossref search API (api.crossref.org) routes:\n/works /members /funders /journals /types /licenses Other methods in all three clients:\nGet DOI minting agency Uses api.crossref.org API Get random DOIs Uses api.crossref.org API Content negotiation Documented at https://www.crosscite.org/cn Get full text other clients in each language will focus on this use case Get citation count Uses service at https://www.crossref.org/openurl - though this functionality may be in the api.crossref.org API at some point The following shows how to install, and then examples from each client for a few use cases.\nInstallation Python pip install habanero Ruby gem install serrano R Inside R:\ninstall.packages(\u0026#34;rcrossref\u0026#34;) Javascript npm install crossref I won\u0026rsquo;t do any examples with the js library, as I don\u0026rsquo;t maintain it.\nUse case: get ORCID IDs for authors Python\nfrom habanero import Crossref cr = Crossref() res = cr.works(filter = {\u0026#39;has_orcid\u0026#39;: True}, limit = 10) res2 = [ [ z.get(\u0026#39;ORCID\u0026#39;) for z in x[\u0026#39;author\u0026#39;] ] for x in res.result[\u0026#39;message\u0026#39;][\u0026#39;items\u0026#39;] ] filter(None, reduce(lambda x, y: x+y, res2)) [u\u0026#39;https://orcid.org/0000-0003-4087-8021\u0026#39;, u\u0026#39;https://orcid.org/0000-0002-2076-5452\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-4087-8021\u0026#39;, u\u0026#39;https://orcid.org/0000-0002-2076-5452\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-1710-1580\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-1710-1580\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-4637-238X\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-4637-238X\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-4637-238X\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-4637-238X\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-4637-238X\u0026#39;, u\u0026#39;https://orcid.org/0000-0003-2510-4271\u0026#39;] Ruby\nrequire \u0026#39;serrano\u0026#39; res = Serrano.works(filter: {\u0026#39;has_orcid\u0026#39;: true}, limit: 10) res2 = res[\u0026#39;message\u0026#39;][\u0026#39;items\u0026#39;].collect { |x| x[\u0026#39;author\u0026#39;].collect { |z| z[\u0026#39;ORCID\u0026#39;] } } res2.flatten.compact =\u0026gt; [\u0026#34;https://orcid.org/0000-0003-4087-8021\u0026#34;, \u0026#34;https://orcid.org/0000-0002-2076-5452\u0026#34;, \u0026#34;https://orcid.org/0000-0003-4087-8021\u0026#34;, \u0026#34;https://orcid.org/0000-0002-2076-5452\u0026#34;, \u0026#34;https://orcid.org/0000-0003-1710-1580\u0026#34;, \u0026#34;https://orcid.org/0000-0003-1710-1580\u0026#34;, \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34;, \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34;, \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34;, \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34;, \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34;, \u0026#34;https://orcid.org/0000-0003-2510-4271\u0026#34;] R\nlibrary(\u0026#34;rcrossref\u0026#34;) res \u0026lt;- cr_works(filter=c(has_orcid=TRUE), limit = 10) orcids \u0026lt;- unlist(lapply(res$data$author, function(z) z$ORCID)) Filter(function(x) !is.na(x), orcids) [1] \u0026#34;https://orcid.org/0000-0003-4087-8021\u0026#34; [2] \u0026#34;https://orcid.org/0000-0002-2076-5452\u0026#34; [3] \u0026#34;https://orcid.org/0000-0003-4087-8021\u0026#34; [4] \u0026#34;https://orcid.org/0000-0002-2076-5452\u0026#34; [5] \u0026#34;https://orcid.org/0000-0003-1710-1580\u0026#34; [6] \u0026#34;https://orcid.org/0000-0003-1710-1580\u0026#34; [7] \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; [8] \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; [9] \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; [10] \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; [11] \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; [12] \u0026#34;https://orcid.org/0000-0003-2510-4271\u0026#34; CLI\nserrano works --filter=has_orcid:true --json --limit=12 | jq \u0026#39;.message.items[].author[].ORCID | select(. != null)\u0026#39; \u0026#34;https://orcid.org/0000-0003-4087-8021\u0026#34; \u0026#34;https://orcid.org/0000-0002-2076-5452\u0026#34; \u0026#34;https://orcid.org/0000-0003-4087-8021\u0026#34; \u0026#34;https://orcid.org/0000-0002-2076-5452\u0026#34; \u0026#34;https://orcid.org/0000-0003-1710-1580\u0026#34; \u0026#34;https://orcid.org/0000-0003-1710-1580\u0026#34; \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; \u0026#34;https://orcid.org/0000-0003-4637-238X\u0026#34; \u0026#34;https://orcid.org/0000-0003-2510-4271\u0026#34; \u0026#34;https://orcid.org/0000-0001-9408-8207\u0026#34; \u0026#34;https://orcid.org/0000-0002-2076-5452\u0026#34; Use case: content negotation Python\nfrom habanero import cn cn.content_negotiation(ids = \u0026#39;10.1126/science.169.3946.635\u0026#39;, format = \u0026#34;text\u0026#34;) u\u0026#39;Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635\\xe2\\x80\\x93641. doi:10.1126/science.169.3946.635\\n\u0026#39; Ruby\nrequire \u0026#39;serrano\u0026#39; Serrano.content_negotiation(ids: \u0026#39;10.1126/science.169.3946.635\u0026#39;, format: \u0026#34;text\u0026#34;) =\u0026gt; [\u0026#34;Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635\\xE2\\x80\\x93641. doi:10.1126/science.169.3946.635\\n\u0026#34;] R\nlibrary(\u0026#34;rcrossref\u0026#34;) cr_cn(dois=\u0026#34;10.1126/science.169.3946.635\u0026#34;, \u0026#34;text\u0026#34;) [1] \u0026#34;Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635–641. doi:10.1126/science.169.3946.635\u0026#34; CLI\nserrano contneg 10.1890/13-0590.1 --format=text Murtaugh, P. A. (2014). In defense of P values . Ecology, 95(3), 611–617. doi:10.1890/13-0590.1 More There are definitely issues with data in the Crossref search API, some of which I cover in my talks. However, it is still the best place to go for scholarly metadata.\nLet us know of other use cases - there are others not covered here for brevity sake.\nThere are lots of examples in the docs for each client. If you can think of any doc improvements file an issue.\nIf you find any bugs, please do file an issue.\n","permalink":"http://localhost:1313/2015/11/crossref-clients/","summary":"\u003cp\u003eI gave two talks recently at the annual \u003ca href=\"https://www.crossref.org/annualmeeting/agenda.html\"\u003eCrossref meeting\u003c/a\u003e, one of which was a somewhat technical overview of programmatic clients for Crossref APIs. Check out the talk \u003ca href=\"https://crossref.wistia.com/medias/8rh0jm5eda\"\u003ehere\u003c/a\u003e. I talked about the motivation for working with Crossref data by writing code/etc. rather than going the GUI route, then went over the various clients, with brief examples.\u003c/p\u003e\n\u003cp\u003eWe (rOpenSci) have been working on the R client \u003ca href=\"https://github.com/ropensci/rcrossref\"\u003ercrossref\u003c/a\u003e for a while now, but I\u0026rsquo;m also working on the Python and Ruby clients for Crossref. In addition, the Ruby client has a CLI client inside. The Javascript client is worked on independently by \u003ca href=\"https://science.ai/\"\u003eScienceAI\u003c/a\u003e.\u003c/p\u003e","title":"Crossref programmatic clients"},{"content":"I maintain an R client for the GBIF API, at rgbif. Been working on it for a few years, and recently been thinking that there should be a nice low level client for Python as well. I didn\u0026rsquo;t see one searching Github, etc. so I started working on one recently: pygbif\nIt\u0026rsquo;s up on pypi.\nThere\u0026rsquo;s not much in pygbif yet - I wanted to get something up to start getting some users to more quickly make the library useful to people.\nThere\u0026rsquo;s three modules, with a few methods each:\nspecies name_backbone() name_suggest() registry nodes() dataset_metrics() datasets() occurrences search() get() get_verbatim() get_fragment() count() count_basisofrecord() count_year() count_datasets() count_countries() count_publishingcountries() count_schema() Here\u0026rsquo;s a quick intro (in a Jupyter notebook):\nInstall pip install pygbif Registry/datasets from pygbif import registry registry.dataset_metrics(uuid=\u0026#39;3f8a1297-3259-4700-91fc-acc4170b27ce\u0026#39;) {u\u0026#39;colCoveragePct\u0026#39;: 79, u\u0026#39;colMatchingCount\u0026#39;: 24335, u\u0026#39;countByConstituent\u0026#39;: {}, u\u0026#39;countByIssue\u0026#39;: {u\u0026#39;BACKBONE_MATCH_FUZZY\u0026#39;: 573, u\u0026#39;BACKBONE_MATCH_NONE\u0026#39;: 1306, u\u0026#39;VERNACULAR_NAME_INVALID\u0026#39;: 7777}, u\u0026#39;countByKingdom\u0026#39;: {u\u0026#39;ANIMALIA\u0026#39;: 30, u\u0026#39;FUNGI\u0026#39;: 3, u\u0026#39;INCERTAE_SEDIS\u0026#39;: 26, u\u0026#39;PLANTAE\u0026#39;: 10997, u\u0026#39;PROTOZOA\u0026#39;: 1}, ... } Taxonomic names from pygbif import species species.name_suggest(q=\u0026#39;Puma concolor\u0026#39;, limit = 1) {\u0026#39;data\u0026#39;: [{u\u0026#39;canonicalName\u0026#39;: u\u0026#39;Puma concolor\u0026#39;, u\u0026#39;class\u0026#39;: u\u0026#39;Mammalia\u0026#39;, u\u0026#39;classKey\u0026#39;: 359, u\u0026#39;family\u0026#39;: u\u0026#39;Felidae\u0026#39;, u\u0026#39;familyKey\u0026#39;: 9703, u\u0026#39;genus\u0026#39;: u\u0026#39;Puma\u0026#39;, u\u0026#39;genusKey\u0026#39;: 2435098, u\u0026#39;key\u0026#39;: 2435099, u\u0026#39;kingdom\u0026#39;: u\u0026#39;Animalia\u0026#39;, u\u0026#39;kingdomKey\u0026#39;: 1, u\u0026#39;nubKey\u0026#39;: 2435099, u\u0026#39;order\u0026#39;: u\u0026#39;Carnivora\u0026#39;, u\u0026#39;orderKey\u0026#39;: 732, u\u0026#39;parent\u0026#39;: u\u0026#39;Puma\u0026#39;, u\u0026#39;parentKey\u0026#39;: 2435098, u\u0026#39;phylum\u0026#39;: u\u0026#39;Chordata\u0026#39;, u\u0026#39;phylumKey\u0026#39;: 44, u\u0026#39;rank\u0026#39;: u\u0026#39;SPECIES\u0026#39;, u\u0026#39;species\u0026#39;: u\u0026#39;Puma concolor\u0026#39;, u\u0026#39;speciesKey\u0026#39;: 2435099}], \u0026#39;hierarchy\u0026#39;: [{u\u0026#39;1\u0026#39;: u\u0026#39;Animalia\u0026#39;, u\u0026#39;2435098\u0026#39;: u\u0026#39;Puma\u0026#39;, u\u0026#39;359\u0026#39;: u\u0026#39;Mammalia\u0026#39;, u\u0026#39;44\u0026#39;: u\u0026#39;Chordata\u0026#39;, u\u0026#39;732\u0026#39;: u\u0026#39;Carnivora\u0026#39;, u\u0026#39;9703\u0026#39;: u\u0026#39;Felidae\u0026#39;}]} Occurrence data Search\nfrom pygbif import occurrences res = occurrences.search(taxonKey = 3329049, limit = 10) [ x[\u0026#39;phylum\u0026#39;] for x in res[\u0026#39;results\u0026#39;] ] [u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;, u\u0026#39;Basidiomycota\u0026#39;] Fetch specific occurrences\noccurrences.get(key = 252408386) {u\u0026#39;basisOfRecord\u0026#39;: u\u0026#39;OBSERVATION\u0026#39;, u\u0026#39;catalogNumber\u0026#39;: u\u0026#39;70875196\u0026#39;, u\u0026#39;collectionCode\u0026#39;: u\u0026#39;7472\u0026#39;, u\u0026#39;continent\u0026#39;: u\u0026#39;EUROPE\u0026#39;, u\u0026#39;country\u0026#39;: u\u0026#39;United Kingdom\u0026#39;, u\u0026#39;countryCode\u0026#39;: u\u0026#39;GB\u0026#39;, u\u0026#39;datasetKey\u0026#39;: u\u0026#39;26a49731-9457-45b2-9105-1b96063deb26\u0026#39;, u\u0026#39;day\u0026#39;: 30, ... } Occurrence counts API\noccurrences.count(isGeoreferenced = True) 500283031 feedback Would love any feedback\u0026hellip;\n","permalink":"http://localhost:1313/2015/11/pygbif/","summary":"\u003cp\u003eI maintain an R client for the GBIF API, at \u003ca href=\"https://github.com/ropensci/rgbif\"\u003ergbif\u003c/a\u003e. Been working on it for a few years, and recently been thinking that there should be a nice low level client for Python as well. I didn\u0026rsquo;t see one searching Github, etc. so I started working on one recently: \u003ca href=\"https://github.com/sckott/pygbif\"\u003epygbif\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s up on \u003ca href=\"https://pypi.python.org/pypi/pygbif/0.1.1\"\u003epypi\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThere\u0026rsquo;s not much in \u003ccode\u003epygbif\u003c/code\u003e yet - I wanted to get something up to start getting some users to more quickly make the library useful to people.\u003c/p\u003e","title":"pygbif - GBIF client for Python"},{"content":"I\u0026rsquo;ve recently made some improvements to the functions that work with ISD (Integrated Surface Database) data.\nisd data\nThe isd() function now caches more intelligently. We now cache using .rds files via saveRDS/readRDS, whereas we used to use .csv files, which take up much more disk space, and we have to worry about not changing data formats on reading data back into an R session. This has the downside that you can\u0026rsquo;t just go directly to open up a cached file in your favorite spreadsheet viewer, but you can do that manually after reading in to R. In addition, isd() now has a function cleanup, if TRUE after downloading the data file from NOAA\u0026rsquo;s ftp server and processing, we delete the file. That\u0026rsquo;s fine since we have the cached processed file. But you can choose not to cleanup the original data files. Data processing in isd() is improved as well. We convert key variables to appropriate classes to be more useful. isd stations\nIn isd_stations(), there\u0026rsquo;s now a cached version of the station data in the package, or you can get optionally get fresh station data from NOAA\u0026rsquo;s FTP server. There\u0026rsquo;s a new function isd_stations_search() that uses the station data to allow you to search for stations via either: A bounding box Radius froma point Install For examples below, you\u0026rsquo;ll need the development version:\ndevtools::install_github(\u0026#34;ropensci/rnoaa\u0026#34;) Load rnoaa\nlibrary(\u0026#34;rnoaa\u0026#34;) ISD stations Get stations There\u0026rsquo;s a cached version of the station data in the package, or you can get fresh station data from NOAA\u0026rsquo;s FTP server.\nstations \u0026lt;- isd_stations() head(stations) #\u0026gt; usaf wban station_name ctry state icao lat lon elev_m begin end #\u0026gt; 1 7005 99999 CWOS 07005 NA NA NA 20120127 20120127 #\u0026gt; 2 7011 99999 CWOS 07011 NA NA NA 20111025 20121129 #\u0026gt; 3 7018 99999 WXPOD 7018 0 0 7018 20110309 20130730 #\u0026gt; 4 7025 99999 CWOS 07025 NA NA NA 20120127 20120127 #\u0026gt; 5 7026 99999 WXPOD 7026 AF 0 0 7026 20120713 20141120 #\u0026gt; 6 7034 99999 CWOS 07034 NA NA NA 20121024 20121106 Filter and visualize stations In addition to getting the entire station data.frame, you can also search for stations, either with a bounding box or within a radius from a point. First, the bounding box\nbbox \u0026lt;- c(-125.0, 38.4, -121.8, 40.9) out \u0026lt;- isd_stations_search(bbox = bbox) head(out) #\u0026gt; usaf wban station_name ctry state icao #\u0026gt; 1 720193 99999 LONNIE POOL FLD / WEAVERVILLE AIRPORT US CA KO54 #\u0026gt; 2 724834 99999 POINT CABRILLO US CA #\u0026gt; 3 724953 99999 RIO NIDO US CA #\u0026gt; 4 724957 23213 SONOMA COUNTY AIRPORT US CA KSTS #\u0026gt; 5 724957 99999 C M SCHULZ SONOMA CO US CA KSTS #\u0026gt; 6 724970 99999 CHICO CALIFORNIA MAP US CA CIC #\u0026gt; elev_m begin end lon lat #\u0026gt; 1 716.0 20101030 20150831 -122.922 40.747 #\u0026gt; 2 20.0 19810906 19871007 -123.820 39.350 #\u0026gt; 3 -999.0 19891111 19900303 -122.917 38.517 #\u0026gt; 4 34.8 20000101 20150831 -122.810 38.504 #\u0026gt; 5 38.0 19430404 19991231 -122.817 38.517 #\u0026gt; 6 69.0 19420506 19760305 -121.850 39.783 Where is the bounding box? (you\u0026rsquo;ll need lawn, or you can vizualize some other way)\nlibrary(\u0026#34;lawn\u0026#34;) lawn::lawn_bbox_polygon(bbox) %\u0026gt;% view Vizualize station subset - yep, looks right\nlibrary(\u0026#34;leaflet\u0026#34;) leaflet(data = out) %\u0026gt;% addTiles() %\u0026gt;% addCircles() Next, search with a lat/lon coordinate, with a radius. That is, we search for stations within X km from the coordinate.\nout \u0026lt;- isd_stations_search(lat = 38.4, lon = -123, radius = 250) head(out) #\u0026gt; usaf wban station_name ctry state icao elev_m begin #\u0026gt; 1 690070 93217 FRITZSCHE AAF US CA KOAR 43.0 19600404 #\u0026gt; 2 720267 23224 AUBURN MUNICIPAL AIRPORT US CA KAUN 466.7 20060101 #\u0026gt; 3 720267 99999 AUBURN MUNICIPAL US CA KAUN 468.0 20040525 #\u0026gt; 4 720406 99999 GNOSS FIELD AIRPORT US CA KDVO 0.6 20071114 #\u0026gt; 5 720576 174 UNIVERSITY AIRPORT US CA KEDU 21.0 20130101 #\u0026gt; 6 720576 99999 DAVIS US CA KEDU 21.0 20080721 #\u0026gt; end lon lat #\u0026gt; 1 19930831 -121.767 36.683 #\u0026gt; 2 20150831 -121.082 38.955 #\u0026gt; 3 20051231 -121.082 38.955 #\u0026gt; 4 20150831 -122.550 38.150 #\u0026gt; 5 20150831 -121.783 38.533 #\u0026gt; 6 20121231 -121.783 38.533 Again, compare search area to stations found\nsearch area\npt \u0026lt;- lawn::lawn_point(c(-123, 38.4)) lawn::lawn_buffer(pt, dist = 250) %\u0026gt;% view stations found\nleaflet(data = out) %\u0026gt;% addTiles() %\u0026gt;% addCircles() ISD data Get ISD data Here, I get data for four stations.\nres1 \u0026lt;- isd(usaf=\u0026#34;011690\u0026#34;, wban=\u0026#34;99999\u0026#34;, year=1993) res2 \u0026lt;- isd(usaf=\u0026#34;172007\u0026#34;, wban=\u0026#34;99999\u0026#34;, year=2015) res3 \u0026lt;- isd(usaf=\u0026#34;702700\u0026#34;, wban=\u0026#34;00489\u0026#34;, year=2015) res4 \u0026lt;- isd(usaf=\u0026#34;109711\u0026#34;, wban=99999, year=1970) Then, combine data, with rnoaa:::rbind.isd()\nres_all \u0026lt;- rbind(res1, res2, res3, res4) Add date time\nlibrary(\u0026#34;lubridate\u0026#34;) res_all$date_time \u0026lt;- ymd_hm( sprintf(\u0026#34;%s %s\u0026#34;, as.character(res_all$date), res_all$time) ) Remove 999\u0026rsquo;s (NOAA\u0026rsquo;s way to indicate missing/no data)\nlibrary(\u0026#34;dplyr\u0026#34;) res_all \u0026lt;- res_all %\u0026gt;% filter(temperature \u0026lt; 900) Visualize ISD data library(\u0026#34;ggplot2\u0026#34;) ggplot(res_all, aes(date_time, temperature)) + geom_line() + facet_wrap(~usaf_station, scales = \u0026#34;free_x\u0026#34;) ","permalink":"http://localhost:1313/2015/10/noaa-isd/","summary":"\u003cp\u003eI\u0026rsquo;ve recently made some improvements to the functions that work with ISD\n(Integrated Surface Database) data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eisd data\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ccode\u003eisd()\u003c/code\u003e function now caches more intelligently. We now cache using\n\u003ccode\u003e.rds\u003c/code\u003e files via \u003ccode\u003esaveRDS\u003c/code\u003e/\u003ccode\u003ereadRDS\u003c/code\u003e, whereas we used to use \u003ccode\u003e.csv\u003c/code\u003e files,\nwhich take up much more disk space, and we have to worry about not changing\ndata formats on reading data back into an R session. This has the downside\nthat you can\u0026rsquo;t just go directly to open up a cached file in your favorite\nspreadsheet viewer, but you can do that manually after reading in to R.\u003c/li\u003e\n\u003cli\u003eIn addition, \u003ccode\u003eisd()\u003c/code\u003e now has a function \u003ccode\u003ecleanup\u003c/code\u003e, if \u003ccode\u003eTRUE\u003c/code\u003e after\ndownloading the data file from NOAA\u0026rsquo;s ftp server and processing, we delete\nthe file. That\u0026rsquo;s fine since we have the cached processed file. But you\ncan choose not to cleanup the original data files.\u003c/li\u003e\n\u003cli\u003eData processing in \u003ccode\u003eisd()\u003c/code\u003e is improved as well. We convert key variables\nto appropriate classes to be more useful.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eisd stations\u003c/strong\u003e\u003c/p\u003e","title":"noaa - Integrated Surface Database data"},{"content":"Measuring use of open source software isn\u0026rsquo;t always straightforward. The problem is especially acute for software targeted largely at academia, where usage is not measured just by software downloads, but also by citations.\nCitations are a well-known pain point because the citation graph is privately held by iron doors (e.g., Scopus, Google Scholar). New ventures aim to open up citation data, but of course it\u0026rsquo;s an immense amount of work, and so does not come quickly.\nThe following is a laundry list of metrics on software of which I am aware, and some of which I use in our rOpenSci twice monthly updates.\nI primarily develop software for the R language, so some of the metrics are specific to R, but many are not. In addition, we (rOpenSci) don\u0026rsquo;t develop web apps, which may bring in an additional set of metrics not covered below.\nI organize by source instead of type of data because some sources give multiple kinds of data - I note what kinds of data they give with labels.\nCRAN downloads downloads Link: https://github.com/metacran/cranlogs.app This is a REST API for CRAN downloads from the RStudio CRAN CDN. Note however, that the RStudio CDN is only one of many - there are other mirrors users can insall packages from, and are not included in this count. However, a significant portion of downloads probably come from the RStudio CDN. Other programming languages have similar support, e.g., Ruby and Node. Lagotto citations github social-media\nLink: https://software.lagotto.io/works Lagotto is a Rails application, developed by Martin Fenner, originally designed to collect and provide article level metrics for scientific publications at Public Library of Science. It is now used by many publishers, and there are installations of Lagotto targeting datasets and software. Discussion forum: https://discuss.lagotto.io/ Depsy citations github\nLink: https://depsy.org This is a nascent venture by the ImpactStory team that seeks to uncover the impact of research software. As far as I can tell, they\u0026rsquo;ll collect usage via software downloads and citations in the literature. Web Site Analytics page-views\nIf you happen to have a website for your project, collecting analytics is a way to gauge views of the landing page, and any help/tutorial pages you may have. A good easy way to do this is a deploy a basic site on your gh-pages branch of your GitHub repo, and use the easily integrated Google Analytics. Whatever analytics you use, in my experience this mostly brings up links from google searches and blog posts that may mention your project Google Analytics beacon (for README views): https://github.com/igrigorik/ga-beacon. I haven\u0026rsquo;t tried this yet, but seems promising. Auomated tracking: SSNMP citations github\nLink: https://scisoft-net-map.isri.cmu.edu Scientific Software Network Map Project This is a cool NSF funded project by Chris Bogart that tracks software usage via GitHub and citations in literature. Google Scholar citations\nLink: https://scholar.google.com/ Searching Google Scholar for software citations manually is fine at a small scale, but at a larger scale scraping is best. However, you\u0026rsquo;re not legally supposed to do this, and Google will shut you down. Could try using g-scholar alerts as well, especially if new citations of your work are infrequent. If you have institutional access to Scopus/Web of Science, you could search those, but I don\u0026rsquo;t push this as an option since it\u0026rsquo;s available to so few. GitHub github\nLinks: https://developer.github.com/v3/ I keep a list of rOpenSci uses found in GitHub repos at https://discuss.ropensci.org/t/use-of-some-ropensci-packages-on-github/137 GitHub does collect traffic data on each repo (clones, downloads, page views), but they are not exposed in the API. I\u0026rsquo;ve bugged them a bit about this - hopefully we\u0026rsquo;ll be able to get that dat in their API soon. Bitbucket/Gitlab - don\u0026rsquo;t use them, but I assume they also provide some metrics via their APIs Other Support forums: Whether you use UserVoice, Discourse, Google Groups, Gitter, etc., depending on your viewpoint, these interactions could be counted as metrics of software usage. Emails: I personally get a lot of emails asking for help with software I maintain. I imagine this is true for most software developers. Counting these could be another metric of software usage, although I never have counted mine. Social media: See Lagotto above, which tracks some social media outlets. Code coverage: There are many options now for code coverage, integrated with each Travis-CI build. A good option is CodeCov. CodeCov gives percentage test coverage, which one could use as one measure of code quality. Reviews: There isn\u0026rsquo;t a lot of code review going on that I\u0026rsquo;m aware of. Even if there was, I suppose this would just be a logical TRUE/FALSE. Cash money y\u0026rsquo;all: Grants/consulting income/etc. could be counted as a metric. Users: If you require users to create an account or similar before getting your software, you have a sense of number of users and perhaps their demographics. Promising Some software metrics things on the horizon that look interesting:\nSoftware Attribution for Geoscience Applications (SAGA) Crossref: They have a very nice API, but they don\u0026rsquo;t yet provide citation counts - but they may soon. njsmith/sempervirens - a prototype for gathering anonymous, opt-in usage data for open scientific software Force11 Software Citation Working Group - \u0026hellip;produce a consolidated set of citation principles in order to encourage broad adoption of a consistent policy for software citation across disciplines and venues Missed? I\u0026rsquo;m sure I missed things. Let me know.\n","permalink":"http://localhost:1313/2015/10/open-source-metrics/","summary":"\u003cp\u003eMeasuring use of open source software isn\u0026rsquo;t always straightforward. The problem is especially acute for software targeted largely at academia, where usage is not measured just by software downloads, but also by citations.\u003c/p\u003e\n\u003cp\u003eCitations are a well-known pain point because the citation graph is privately held by iron doors (e.g., \u003ca href=\"https://www.scopus.com/\"\u003eScopus\u003c/a\u003e, \u003ca href=\"https://scholar.google.com/\"\u003eGoogle Scholar\u003c/a\u003e). New ventures aim to open up citation data, but of course it\u0026rsquo;s an immense amount of work, and so does not come quickly.\u003c/p\u003e","title":"Metrics for open source projects"},{"content":"analogsea is now on CRAN. We started developing the pkg back in May 2014, but just now getting the first version on CRAN. It\u0026rsquo;s a collaboration with Hadley and Winston Chang.\nMost of analogsea package is for interacting with the Digital Ocean API, including:\nManage domains Manage ssh keys Get actions Manage images Manage droplets (servers) A number of convenience functions are included for doing tasks (e.g., resizing a droplet) that aren\u0026rsquo;t supported by Digital Ocean\u0026rsquo;s API out of the box (i.e., there\u0026rsquo;s no API route for it).\nIn addition to wrapping their API routes, we provide other functionality, e.g.:\nexecute shell commands on a droplet (server) execute R commands on a droplet install R install RStudio server install Shiny server Other functionality we\u0026rsquo;re working on, not yet available:\ninstall OpenCPU use packrat to move projects from local to server, and vice versa See also: two previous blog posts on this package https://recology.info/2014/05/analogsea/ and https://recology.info/2014/06/analogsea-v01/\nInstall Binaries are not yet on CRAN, but you can install from source.\n# install.packages(\u0026#34;analogsea\u0026#34;) # when binaries available install.packages(\u0026#34;analogsea\u0026#34;, repos = \u0026#34;https://cran.r-project.org\u0026#34;, type = \u0026#34;source\u0026#34;) Or install development version from GitHub\ndevtools::install_github(\u0026#34;sckott/analogsea\u0026#34;) Load analogsea\nlibrary(\u0026#34;analogsea\u0026#34;) Etc. As this post is mostly to announce that this pkg is on CRAN now, I won\u0026rsquo;t go through examples, but instead point you to the package README and vignette in which we cover creating a Digital Ocean account, authenticating, and have many examples.\nFeedback Let us know what you think. We\u0026rsquo;d love to hear about any problems, use cases, feature requests.\n","permalink":"http://localhost:1313/2015/10/analogsea-cran/","summary":"\u003cp\u003e\u003ccode\u003eanalogsea\u003c/code\u003e is now on CRAN. We started developing the pkg back in \u003ca href=\"https://github.com/sckott/analogsea/commit/b129164dd87969d2fc6bcf3b51576fe1da932fdb\"\u003eMay 2014\u003c/a\u003e, but just\nnow getting the first version on CRAN. It\u0026rsquo;s a collaboration with \u003ca href=\"https://had.co.nz/\"\u003eHadley\u003c/a\u003e and \u003ca href=\"https://github.com/wch/\"\u003eWinston Chang\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eMost of \u003ccode\u003eanalogsea\u003c/code\u003e package is for interacting with the \u003ca href=\"https://developers.digitalocean.com/documentation/v2/\"\u003eDigital Ocean API\u003c/a\u003e, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eManage domains\u003c/li\u003e\n\u003cli\u003eManage ssh keys\u003c/li\u003e\n\u003cli\u003eGet actions\u003c/li\u003e\n\u003cli\u003eManage images\u003c/li\u003e\n\u003cli\u003eManage droplets (servers)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA number of convenience functions are included for doing tasks (e.g., resizing\na droplet) that aren\u0026rsquo;t supported by Digital Ocean\u0026rsquo;s API out of the box (i.e.,\nthere\u0026rsquo;s no API route for it).\u003c/p\u003e","title":"analogsea - an R client for the Digital Ocean API"},{"content":"oai is a general purpose client to work with any \u0026lsquo;OAI-PMH\u0026rsquo; service. The \u0026lsquo;OAI-PMH\u0026rsquo; protocol is described at https://www.openarchives.org/OAI/openarchivesprotocol.html. The main functions follow the OAI-PMH verbs:\nGetRecord Identify ListIdentifiers ListMetadataFormats ListRecords ListSets The repo is at https://github.com/sckott/oai\nI will be using this in a number of packages I maintain that use OAI-PMH data services. If you try it, let me know what you think.\nThis package is heading to rOpenSci soon: https://github.com/ropensci/onboarding/issues/19\nHere\u0026rsquo;s a few usage examples:\nInstall Is on CRAN now, but binaries may not be available yet.\ninstall.packages(\u0026#34;oai\u0026#34;) Or install development version from GitHub\ndevtools::install_github(\u0026#34;sckott/oai\u0026#34;) Load oai\nlibrary(\u0026#34;oai\u0026#34;) Identify id(\u0026#34;https://oai.datacite.org/oai\u0026#34;) #\u0026gt; repositoryName baseURL protocolVersion #\u0026gt; 1 DataCite MDS https://oai.datacite.org/oai 2.0 #\u0026gt; adminEmail earliestDatestamp deletedRecord #\u0026gt; 1 admin@datacite.org 2011-01-01T00:00:00Z persistent #\u0026gt; granularity compression compression.1 #\u0026gt; 1 YYYY-MM-DDThh:mm:ssZ gzip deflate #\u0026gt; description #\u0026gt; 1 oaioai.datacite.org:oai:oai.datacite.org:12425 ListIdentifiers list_identifiers(from = \u0026#39;2011-05-01T\u0026#39;, until = \u0026#39;2011-09-01T\u0026#39;) #\u0026gt; \u0026lt;ListRecords\u0026gt; 925 X 6 #\u0026gt; #\u0026gt; identifier datestamp setSpec setSpec.1 #\u0026gt; 1 oai:oai.datacite.org:32153 2011-06-08T08:57:11Z TIB TIB.WDCC #\u0026gt; 2 oai:oai.datacite.org:32200 2011-06-20T08:12:41Z TIB TIB.DAGST #\u0026gt; 3 oai:oai.datacite.org:32220 2011-06-28T14:11:08Z TIB TIB.DAGST #\u0026gt; 4 oai:oai.datacite.org:32241 2011-06-30T13:24:45Z TIB TIB.DAGST #\u0026gt; 5 oai:oai.datacite.org:32255 2011-07-01T12:09:24Z TIB TIB.DAGST #\u0026gt; 6 oai:oai.datacite.org:32282 2011-07-05T09:08:10Z TIB TIB.DAGST #\u0026gt; 7 oai:oai.datacite.org:32309 2011-07-06T12:30:54Z TIB TIB.DAGST #\u0026gt; 8 oai:oai.datacite.org:32310 2011-07-06T12:42:32Z TIB TIB.DAGST #\u0026gt; 9 oai:oai.datacite.org:32325 2011-07-07T11:17:46Z TIB TIB.DAGST #\u0026gt; 10 oai:oai.datacite.org:32326 2011-07-07T11:18:47Z TIB TIB.DAGST #\u0026gt; .. ... ... ... ... #\u0026gt; Variables not shown: setSpec.2 (chr), setSpec.3 (chr) Count Identifiers count_identifiers() #\u0026gt; url count #\u0026gt; 1 https://oai.datacite.org/oai 6350706 ListRecords list_records(from = \u0026#39;2011-05-01T\u0026#39;, until = \u0026#39;2011-08-15T\u0026#39;) #\u0026gt; \u0026lt;ListRecords\u0026gt; 126 X 46 #\u0026gt; #\u0026gt; identifier datestamp setSpec setSpec.1 #\u0026gt; 1 oai:oai.datacite.org:32153 2011-06-08T08:57:11Z TIB TIB.WDCC #\u0026gt; 2 oai:oai.datacite.org:32200 2011-06-20T08:12:41Z TIB TIB.DAGST #\u0026gt; 3 oai:oai.datacite.org:32220 2011-06-28T14:11:08Z TIB TIB.DAGST #\u0026gt; 4 oai:oai.datacite.org:32241 2011-06-30T13:24:45Z TIB TIB.DAGST #\u0026gt; 5 oai:oai.datacite.org:32255 2011-07-01T12:09:24Z TIB TIB.DAGST #\u0026gt; 6 oai:oai.datacite.org:32282 2011-07-05T09:08:10Z TIB TIB.DAGST #\u0026gt; 7 oai:oai.datacite.org:32309 2011-07-06T12:30:54Z TIB TIB.DAGST #\u0026gt; 8 oai:oai.datacite.org:32310 2011-07-06T12:42:32Z TIB TIB.DAGST #\u0026gt; 9 oai:oai.datacite.org:32325 2011-07-07T11:17:46Z TIB TIB.DAGST #\u0026gt; 10 oai:oai.datacite.org:32326 2011-07-07T11:18:47Z TIB TIB.DAGST #\u0026gt; .. ... ... ... ... #\u0026gt; Variables not shown: title (chr), creator (chr), creator.1 (chr), #\u0026gt; creator.2 (chr), creator.3 (chr), creator.4 (chr), creator.5 (chr), #\u0026gt; creator.6 (chr), creator.7 (chr), publisher (chr), date (chr), #\u0026gt; identifier.2 (chr), identifier.1 (chr), subject (chr), description #\u0026gt; (chr), description.1 (chr), contributor (chr), language (chr), type #\u0026gt; (chr), type.1 (chr), format (chr), format.1 (chr), rights (chr), #\u0026gt; subject.1 (chr), relation (chr), subject.2 (chr), subject.3 (chr), #\u0026gt; subject.4 (chr), setSpec.2 (chr), setSpec.3 (chr), format.2 (chr), #\u0026gt; subject.5 (chr), subject.6 (chr), subject.7 (chr), description.2 #\u0026gt; (chr), description.3 (chr), description.4 (chr), description.5 (chr), #\u0026gt; title.1 (chr), relation.1 (chr), relation.2 (chr), contributor.1 #\u0026gt; (chr) GetRecords get_records(c(\u0026#34;oai:oai.datacite.org:32255\u0026#34;, \u0026#34;oai:oai.datacite.org:32325\u0026#34;)) #\u0026gt; \u0026lt;GetRecord\u0026gt; 2 X 23 #\u0026gt; #\u0026gt; identifier datestamp setSpec setSpec.1 #\u0026gt; 1 oai:oai.datacite.org:32255 2011-07-01T12:09:24Z TIB TIB.DAGST #\u0026gt; 2 oai:oai.datacite.org:32325 2011-07-07T11:17:46Z TIB TIB.DAGST #\u0026gt; Variables not shown: title (chr), creator (chr), creator.1 (chr), #\u0026gt; creator.2 (chr), creator.3 (chr), publisher (chr), date (chr), #\u0026gt; identifier.1 (chr), subject (chr), subject.1 (chr), description #\u0026gt; (chr), description.1 (chr), contributor (chr), language (chr), type #\u0026gt; (chr), type.1 (chr), format (chr), format.1 (chr), rights (chr) List MetadataFormats list_metadataformats(id = \u0026#34;oai:oai.datacite.org:32348\u0026#34;) #\u0026gt; $`oai:oai.datacite.org:32348` #\u0026gt; metadataPrefix #\u0026gt; 1 oai_dc #\u0026gt; 2 datacite #\u0026gt; 3 oai_datacite #\u0026gt; schema #\u0026gt; 1 https://www.openarchives.org/OAI/2.0/oai_dc.xsd #\u0026gt; 2 https://schema.datacite.org/meta/nonexistant/nonexistant.xsd #\u0026gt; 3 https://schema.datacite.org/oai/oai-1.0/oai.xsd #\u0026gt; metadataNamespace #\u0026gt; 1 https://www.openarchives.org/OAI/2.0/oai_dc/ #\u0026gt; 2 https://datacite.org/schema/nonexistant #\u0026gt; 3 https://schema.datacite.org/oai/oai-1.0/ List Sets list_sets(\u0026#34;https://oai.datacite.org/oai\u0026#34;) #\u0026gt; \u0026lt;ListSets\u0026gt; 1227 X 2 #\u0026gt; #\u0026gt; setSpec #\u0026gt; 1 REFQUALITY #\u0026gt; 2 ANDS #\u0026gt; 3 ANDS.REFQUALITY #\u0026gt; 4 ANDS.CENTRE-1 #\u0026gt; 5 ANDS.CENTRE-1.REFQUALITY #\u0026gt; 6 ANDS.CENTRE-2 #\u0026gt; 7 ANDS.CENTRE-2.REFQUALITY #\u0026gt; 8 ANDS.CENTRE-3 #\u0026gt; 9 ANDS.CENTRE-3.REFQUALITY #\u0026gt; 10 ANDS.CENTRE-5 #\u0026gt; .. ... #\u0026gt; Variables not shown: setName (chr) ","permalink":"http://localhost:1313/2015/09/oai-client/","summary":"\u003cp\u003e\u003ccode\u003eoai\u003c/code\u003e is a general purpose client to work with any \u0026lsquo;OAI-PMH\u0026rsquo; service. The \u0026lsquo;OAI-PMH\u0026rsquo; protocol is described at \u003ca href=\"https://www.openarchives.org/OAI/openarchivesprotocol.html\"\u003ehttps://www.openarchives.org/OAI/openarchivesprotocol.html\u003c/a\u003e. The main functions follow the OAI-PMH verbs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eGetRecord\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eIdentify\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eListIdentifiers\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eListMetadataFormats\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eListRecords\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eListSets\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe repo is at \u003ca href=\"https://github.com/sckott/oai\"\u003ehttps://github.com/sckott/oai\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eI will be using this in a number of packages I maintain that use OAI-PMH data services. If you try it, let me know what you think.\u003c/p\u003e\n\u003cp\u003eThis package is heading to rOpenSci soon: \u003ca href=\"https://github.com/ropensci/onboarding/issues/19\"\u003ehttps://github.com/ropensci/onboarding/issues/19\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s a few usage examples:\u003c/p\u003e","title":"oai - an OAI-PMH client"},{"content":"Finally, we got fulltext up on CRAN - our first commit was May last year. fulltext is a package to facilitate text mining. It focuses on open access journals. This package makes it easier to search for articles, download those articles in full text if available, convert pdf format to plain text, and extract text chunks for vizualization/analysis. We are planning to add bits for analysis in future versions. We\u0026rsquo;ve been working on this package for a while now. It has a lot of moving parts and package dependencies, so it took a while to get a first useable version.\nThe tasks facilitated by fulltext in bullet form:\nSearch - search for articles Retrieve - get full text Convert - convert from format X to Y Text - if needed, get text from pdfs/etc. Extract - pull out the bits of articles that you want I won\u0026rsquo;t be surprised if users uncover a lot of bugs in this package given the huge number of publishers/journals users want to get literature data from, and the surely wide diversity of use cases. But I thought it was important to get out a first version to get feedback on the user interface, and gather use cases.\nWe hope that this package can help bring text-mining to the masses - making it easy for anyone to do do, not just text-mining experts.\nIf you have any feedback, please do get in touch in the issue tracker for fulltext at https://github.com/ropensci/fulltext/issues - If you have use case thoughts, the rOpenSci discussion forum might be a good place to go.\nLet\u0026rsquo;s kick the tires, shall we?\nInstall Will be on CRAN soon, not as of AM PDT on 2015-08-07.\ninstall.packages(\u0026#34;fulltext\u0026#34;) # if binaries not avail. yet on your favorite CRAN mirror install.packages(\u0026#34;https://cran.rstudio.com/src/contrib/fulltext_0.1.0.tar.gz\u0026#34;, repos = NULL, type = \u0026#34;source\u0026#34;) Or install development version from GitHub\ndevtools::install_github(\u0026#34;ropensci/fulltext\u0026#34;) Load fulltext\nlibrary(\u0026#34;fulltext\u0026#34;) Search for articles Currently, there are hooks for searching for articles from PLOS, BMC, Crossref, Entrez, arXiv, and BioRxiv. We\u0026rsquo;ll add more in the future, but that does cover a lot of articles, especially given inclusion of Crossref (which mints most DOIs) and Entrez (which houses PMC and Pubmed).\nAn example: Search for the term ecology in PLOS journals.\n(res1 \u0026lt;- ft_search(query = \u0026#39;ecology\u0026#39;, from = \u0026#39;plos\u0026#39;)) #\u0026gt; Query: #\u0026gt; [ecology] #\u0026gt; Found: #\u0026gt; [PLoS: 28589; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] #\u0026gt; Returned: #\u0026gt; [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] Each publisher/search-engine has a slot with metadata and data, saying how many articles were found and how many were returned. We can dig into what PLOS gave us:\nres1$plos #\u0026gt; Query: [ecology] #\u0026gt; Records found, returned: [28589, 10] #\u0026gt; License: [CC-BY] #\u0026gt; id #\u0026gt; 1 10.1371/journal.pone.0059813 #\u0026gt; 2 10.1371/journal.pone.0001248 #\u0026gt; 3 10.1371/annotation/69333ae7-757a-4651-831c-f28c5eb02120 #\u0026gt; 4 10.1371/journal.pone.0080763 #\u0026gt; 5 10.1371/journal.pone.0102437 #\u0026gt; 6 10.1371/journal.pone.0017342 #\u0026gt; 7 10.1371/journal.pone.0091497 #\u0026gt; 8 10.1371/journal.pone.0092931 #\u0026gt; 9 10.1371/annotation/28ac6052-4f87-4b88-a817-0cd5743e83d6 #\u0026gt; 10 10.1371/journal.pcbi.1003594 For each of the data sources to search on you can pass in additional options (basically, you can use the query parameters in the functions that hit each service). Here, we can modify our search to PLOS by requesting a particular set of fields with the fl parameter (PLOS uses a Solr backed search engine, and fl is short for fields in Solr land):\nft_search(query = \u0026#39;ecology\u0026#39;, from = \u0026#39;plos\u0026#39;, plosopts = list( fl = c(\u0026#39;id\u0026#39;,\u0026#39;author\u0026#39;,\u0026#39;eissn\u0026#39;,\u0026#39;journal\u0026#39;,\u0026#39;counter_total_all\u0026#39;,\u0026#39;alm_twitterCount\u0026#39;))) #\u0026gt; Query: #\u0026gt; [ecology] #\u0026gt; Found: #\u0026gt; [PLoS: 28589; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] #\u0026gt; Returned: #\u0026gt; [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] Note that PLOS is a bit unique in allowing you to request specific parts of articles. Other sources in ft_search() don\u0026rsquo;t let you do that.\nGet full text After you\u0026rsquo;ve found the set of articles you want to get full text for, we can use the results from ft_search() to grab full text. ft_get() accepts a character vector of list of DOIs (or PMC IDs if fetching from Entrez), or the output of ft_search().\n(out \u0026lt;- ft_get(res1)) #\u0026gt; [Docs] 8 #\u0026gt; [Source] R session #\u0026gt; [IDs] 10.1371/journal.pone.0059813 10.1371/journal.pone.0001248 #\u0026gt; 10.1371/journal.pone.0080763 10.1371/journal.pone.0102437 #\u0026gt; 10.1371/journal.pone.0017342 10.1371/journal.pone.0091497 #\u0026gt; 10.1371/journal.pone.0092931 10.1371/journal.pcbi.1003594 ... We got eight articles in full text in the result. We didn\u0026rsquo;t get 10, even though 10 were returned from ft_search() because PLOS often returns records for annotations, that is, comments on articles, which we auto-seive out within ft_get().\nDig in to the PLOS data\nout$plos #\u0026gt; $found #\u0026gt; [1] 8 #\u0026gt; #\u0026gt; $dois #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0059813\u0026#34; \u0026#34;10.1371/journal.pone.0001248\u0026#34; #\u0026gt; [3] \u0026#34;10.1371/journal.pone.0080763\u0026#34; \u0026#34;10.1371/journal.pone.0102437\u0026#34; #\u0026gt; [5] \u0026#34;10.1371/journal.pone.0017342\u0026#34; \u0026#34;10.1371/journal.pone.0091497\u0026#34; #\u0026gt; [7] \u0026#34;10.1371/journal.pone.0092931\u0026#34; \u0026#34;10.1371/journal.pcbi.1003594\u0026#34; #\u0026gt; #\u0026gt; $data #\u0026gt; $data$backend #\u0026gt; NULL #\u0026gt; #\u0026gt; $data$path #\u0026gt; [1] \u0026#34;session\u0026#34; #\u0026gt; #\u0026gt; $data$data #\u0026gt; 8 full-text articles retrieved #\u0026gt; Min. Length: 3828 - Max. Length: 104702 #\u0026gt; DOIs: 10.1371/journal.pone.0059813 10.1371/journal.pone.0001248 #\u0026gt; 10.1371/journal.pone.0080763 10.1371/journal.pone.0102437 #\u0026gt; 10.1371/journal.pone.0017342 10.1371/journal.pone.0091497 #\u0026gt; 10.1371/journal.pone.0092931 10.1371/journal.pcbi.1003594 ... #\u0026gt; #\u0026gt; NOTE: extract xml strings like output[\u0026#39;\u0026lt;doi\u0026gt;\u0026#39;] #\u0026gt; #\u0026gt; $opts #\u0026gt; $opts$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0059813\u0026#34; \u0026#34;10.1371/journal.pone.0001248\u0026#34; #\u0026gt; [3] \u0026#34;10.1371/journal.pone.0080763\u0026#34; \u0026#34;10.1371/journal.pone.0102437\u0026#34; #\u0026gt; [5] \u0026#34;10.1371/journal.pone.0017342\u0026#34; \u0026#34;10.1371/journal.pone.0091497\u0026#34; #\u0026gt; [7] \u0026#34;10.1371/journal.pone.0092931\u0026#34; \u0026#34;10.1371/journal.pcbi.1003594\u0026#34; #\u0026gt; #\u0026gt; $opts$callopts #\u0026gt; list() Dig in further to get to one of the articles in XML format\nlibrary(\u0026#34;xml2\u0026#34;) xml2::read_xml(out$plos$data$data$`10.1371/journal.pone.0059813`) #\u0026gt; {xml_document} #\u0026gt; \u0026lt;article\u0026gt; #\u0026gt; [1] \u0026lt;front\u0026gt;\\n\u0026lt;journal-meta\u0026gt;\\n\u0026lt;journal-id journal-id-type=\u0026#34;nlm-ta\u0026#34;\u0026gt;PLoS O ... #\u0026gt; [2] \u0026lt;body\u0026gt;\\n \u0026lt;sec id=\u0026#34;s1\u0026#34;\u0026gt;\\n\u0026lt;title\u0026gt;Introduction\u0026lt;/title\u0026gt;\\n\u0026lt;p\u0026gt;Ecologists ... #\u0026gt; [3] \u0026lt;back\u0026gt;\\n\u0026lt;ack\u0026gt;\\n\u0026lt;p\u0026gt;Curtis Flather, Mark Burgman, Leon Blaustein, Yaac ... Now with the xml, you can dig into whatever you like, e.g., using xml2 or rvest.\nExtract text from pdfs Ideally for text mining you have access to XML or other text based formats. However, sometimes you only have access to PDFs. In this case you want to extract text from PDFs. fulltext can help with that.\nYou can extract from any pdf from a file path, like:\npath \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;example1.pdf\u0026#34;, package = \u0026#34;fulltext\u0026#34;) ft_extract(path) #\u0026gt; \u0026lt;document\u0026gt;/Library/Frameworks/R.framework/Versions/3.2/Resources/library/fulltext/examples/example1.pdf #\u0026gt; Pages: 18 #\u0026gt; Title: Suffering and mental health among older people living in nursing homes---a mixed-methods study #\u0026gt; Producer: pdfTeX-1.40.10 #\u0026gt; Creation date: 2015-07-17 Let\u0026rsquo;s search for articles from arXiv, a preprint service. Here, get pdf from an article with ID cond-mat/9309029:\nres \u0026lt;- ft_get(\u0026#39;cond-mat/9309029\u0026#39;, from = \u0026#34;arxiv\u0026#34;) res2 \u0026lt;- ft_extract(res) res2$arxiv$data #\u0026gt; $backend #\u0026gt; NULL #\u0026gt; #\u0026gt; $path #\u0026gt; $path$`cond-mat/9309029` #\u0026gt; [1] \u0026#34;~/.fulltext/cond-mat_9309029.pdf\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $data #\u0026gt; $data[[1]] #\u0026gt; \u0026lt;document\u0026gt;/Users/sacmac/.fulltext/cond-mat_9309029.pdf #\u0026gt; Pages: 14 #\u0026gt; Title: arXiv:cond-mat/9309029v8 26 Jan 1994 #\u0026gt; Producer: GPL Ghostscript SVN PRE-RELEASE 8.62 #\u0026gt; Creation date: 2008-02-06 And a short snippet of the full text\nres2$arxiv$data$data[[1]]$data #\u0026gt; \u0026#34;arXiv:cond-mat/9309029v8 26 Jan 1994, , FERMILAB-PUB-93/15-T March 1993, Revised: #\u0026gt; January 1994, The Thermodynamics and Economics of Waste, Dallas C. Kennedy, Research #\u0026gt; Associate, Fermi National Accelerator Laboratory, P.O. Box 500 MS106, Batavia, Illinois #\u0026gt; 60510 USA, Abstract, The increasingly relevant problem of natural resource use and #\u0026gt; waste production, disposal, and reuse is examined from several viewpoints: economic, #\u0026gt; technical, and thermodynamic. Alternative economies are studied, with emphasis on #\u0026gt; recycling of waste to close the natural resource cycle. The physical nature of human #\u0026gt; economies and constraints on recycling and energy efficiency are stated in terms #\u0026gt; ...\u0026#34; Extract text chunks We have a few functions to help you pull out certain parts of an article. For example, perhaps you want to get just the authors from your articles, or just the abstracts.\nHere, we\u0026rsquo;ll search for some PLOS articles, then get their full text, then extract various parts of each article with chunks().\nres \u0026lt;- ft_search(query = \u0026#34;ecology\u0026#34;, from = \u0026#34;plos\u0026#34;) (x \u0026lt;- ft_get(res)) #\u0026gt; [Docs] 8 #\u0026gt; [Source] R session #\u0026gt; [IDs] 10.1371/journal.pone.0059813 10.1371/journal.pone.0001248 #\u0026gt; 10.1371/journal.pone.0080763 10.1371/journal.pone.0102437 #\u0026gt; 10.1371/journal.pone.0017342 10.1371/journal.pone.0091497 #\u0026gt; 10.1371/journal.pone.0092931 10.1371/journal.pcbi.1003594 ... Extract DOIs\nx %\u0026gt;% chunks(\u0026#34;doi\u0026#34;) #\u0026gt; $plos #\u0026gt; $plos$`10.1371/journal.pone.0059813` #\u0026gt; $plos$`10.1371/journal.pone.0059813`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0059813\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0001248` #\u0026gt; $plos$`10.1371/journal.pone.0001248`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0001248\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0080763` #\u0026gt; $plos$`10.1371/journal.pone.0080763`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0080763\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0102437` #\u0026gt; $plos$`10.1371/journal.pone.0102437`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0102437\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0017342` #\u0026gt; $plos$`10.1371/journal.pone.0017342`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0017342\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0091497` #\u0026gt; $plos$`10.1371/journal.pone.0091497`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0091497\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0092931` #\u0026gt; $plos$`10.1371/journal.pone.0092931`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0092931\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pcbi.1003594` #\u0026gt; $plos$`10.1371/journal.pcbi.1003594`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pcbi.1003594\u0026#34; Extract DOIs and categories\nx %\u0026gt;% chunks(c(\u0026#34;doi\u0026#34;,\u0026#34;categories\u0026#34;)) #\u0026gt; $plos #\u0026gt; $plos$`10.1371/journal.pone.0059813` #\u0026gt; $plos$`10.1371/journal.pone.0059813`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0059813\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0059813`$categories #\u0026gt; [1] \u0026#34;Research Article\u0026#34; \u0026#34;Biology\u0026#34; #\u0026gt; [3] \u0026#34;Ecology\u0026#34; \u0026#34;Community ecology\u0026#34; #\u0026gt; [5] \u0026#34;Species interactions\u0026#34; \u0026#34;Science policy\u0026#34; #\u0026gt; [7] \u0026#34;Research assessment\u0026#34; \u0026#34;Research monitoring\u0026#34; #\u0026gt; [9] \u0026#34;Research funding\u0026#34; \u0026#34;Government funding of science\u0026#34; #\u0026gt; [11] \u0026#34;Research laboratories\u0026#34; \u0026#34;Science policy and economics\u0026#34; #\u0026gt; [13] \u0026#34;Science and technology workforce\u0026#34; \u0026#34;Careers in research\u0026#34; #\u0026gt; [15] \u0026#34;Social and behavioral sciences\u0026#34; \u0026#34;Sociology\u0026#34; #\u0026gt; [17] \u0026#34;Sociology of knowledge\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0001248` #\u0026gt; $plos$`10.1371/journal.pone.0001248`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0001248\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0001248`$categories #\u0026gt; [1] \u0026#34;Research Article\u0026#34; \u0026#34;Ecology\u0026#34; #\u0026gt; [3] \u0026#34;Ecology/Ecosystem Ecology\u0026#34; \u0026#34;Ecology/Evolutionary Ecology\u0026#34; #\u0026gt; [5] \u0026#34;Ecology/Theoretical Ecology\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0080763` #\u0026gt; $plos$`10.1371/journal.pone.0080763`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0080763\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0080763`$categories #\u0026gt; [1] \u0026#34;Research Article\u0026#34; \u0026#34;Biology\u0026#34; \u0026#34;Ecology\u0026#34; #\u0026gt; [4] \u0026#34;Autecology\u0026#34; \u0026#34;Behavioral ecology\u0026#34; \u0026#34;Community ecology\u0026#34; #\u0026gt; [7] \u0026#34;Evolutionary ecology\u0026#34; \u0026#34;Population ecology\u0026#34; \u0026#34;Evolutionary biology\u0026#34; #\u0026gt; [10] \u0026#34;Behavioral ecology\u0026#34; \u0026#34;Evolutionary ecology\u0026#34; \u0026#34;Population biology\u0026#34; #\u0026gt; [13] \u0026#34;Population ecology\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0102437` #\u0026gt; $plos$`10.1371/journal.pone.0102437`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0102437\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0102437`$categories #\u0026gt; [1] \u0026#34;Research Article\u0026#34; #\u0026gt; [2] \u0026#34;Biology and life sciences\u0026#34; #\u0026gt; [3] \u0026#34;Biogeography\u0026#34; #\u0026gt; [4] \u0026#34;Ecology\u0026#34; #\u0026gt; [5] \u0026#34;Ecosystems\u0026#34; #\u0026gt; [6] \u0026#34;Ecosystem engineering\u0026#34; #\u0026gt; [7] \u0026#34;Ecosystem functioning\u0026#34; #\u0026gt; [8] \u0026#34;Industrial ecology\u0026#34; #\u0026gt; [9] \u0026#34;Spatial and landscape ecology\u0026#34; #\u0026gt; [10] \u0026#34;Urban ecology\u0026#34; #\u0026gt; [11] \u0026#34;Computer and information sciences\u0026#34; #\u0026gt; [12] \u0026#34;Geoinformatics\u0026#34; #\u0026gt; [13] \u0026#34;Spatial analysis\u0026#34; #\u0026gt; [14] \u0026#34;Earth sciences\u0026#34; #\u0026gt; [15] \u0026#34;Geography\u0026#34; #\u0026gt; [16] \u0026#34;Human geography\u0026#34; #\u0026gt; [17] \u0026#34;Cultural geography\u0026#34; #\u0026gt; [18] \u0026#34;Social geography\u0026#34; #\u0026gt; [19] \u0026#34;Ecology and environmental sciences\u0026#34; #\u0026gt; [20] \u0026#34;Conservation science\u0026#34; #\u0026gt; [21] \u0026#34;Environmental protection\u0026#34; #\u0026gt; [22] \u0026#34;Nature-society interactions\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0017342` #\u0026gt; $plos$`10.1371/journal.pone.0017342`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0017342\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0017342`$categories #\u0026gt; [1] \u0026#34;Research Article\u0026#34; \u0026#34;Biology\u0026#34; \u0026#34;Ecology\u0026#34; #\u0026gt; [4] \u0026#34;Community ecology\u0026#34; \u0026#34;Community assembly\u0026#34; \u0026#34;Community structure\u0026#34; #\u0026gt; [7] \u0026#34;Niche construction\u0026#34; \u0026#34;Ecological metrics\u0026#34; \u0026#34;Species diversity\u0026#34; #\u0026gt; [10] \u0026#34;Species richness\u0026#34; \u0026#34;Biodiversity\u0026#34; \u0026#34;Biogeography\u0026#34; #\u0026gt; [13] \u0026#34;Population ecology\u0026#34; \u0026#34;Mathematics\u0026#34; \u0026#34;Statistics\u0026#34; #\u0026gt; [16] \u0026#34;Biostatistics\u0026#34; \u0026#34;Statistical theories\u0026#34; \u0026#34;Ecology\u0026#34; #\u0026gt; [19] \u0026#34;Mathematics\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0091497` #\u0026gt; $plos$`10.1371/journal.pone.0091497`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0091497\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0091497`$categories #\u0026gt; [1] \u0026#34;Correction\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0092931` #\u0026gt; $plos$`10.1371/journal.pone.0092931`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pone.0092931\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pone.0092931`$categories #\u0026gt; [1] \u0026#34;Correction\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pcbi.1003594` #\u0026gt; $plos$`10.1371/journal.pcbi.1003594`$doi #\u0026gt; [1] \u0026#34;10.1371/journal.pcbi.1003594\u0026#34; #\u0026gt; #\u0026gt; $plos$`10.1371/journal.pcbi.1003594`$categories #\u0026gt; [1] \u0026#34;Research Article\u0026#34; \u0026#34;Biology and life sciences\u0026#34; #\u0026gt; [3] \u0026#34;Computational biology\u0026#34; \u0026#34;Microbiology\u0026#34; #\u0026gt; [5] \u0026#34;Theoretical biology\u0026#34; tabularize attempts to help you put the data that comes out of chunks() in to a data.frame, that we all know and love.\nx %\u0026gt;% chunks(c(\u0026#34;doi\u0026#34;, \u0026#34;history\u0026#34;)) %\u0026gt;% tabularize() #\u0026gt; $plos #\u0026gt; doi history.received history.accepted #\u0026gt; 1 10.1371/journal.pone.0059813 2012-09-16 2013-02-19 #\u0026gt; 2 10.1371/journal.pone.0001248 2007-07-02 2007-11-06 #\u0026gt; 3 10.1371/journal.pone.0080763 2013-08-15 2013-10-16 #\u0026gt; 4 10.1371/journal.pone.0102437 2013-11-27 2014-06-19 #\u0026gt; 5 10.1371/journal.pone.0017342 2010-08-24 2011-01-31 #\u0026gt; 6 10.1371/journal.pone.0091497 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\u0026gt; 7 10.1371/journal.pone.0092931 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\u0026gt; 8 10.1371/journal.pcbi.1003594 2014-01-09 2014-03-14 Bring it all together With the pieces above, let\u0026rsquo;s see what it looks like all in one go. Here, we\u0026rsquo;ll search for articles on climate change, then visualize word usage in those articles.\nSearch (out \u0026lt;- ft_search(query = \u0026#39;climate change\u0026#39;, from = \u0026#39;plos\u0026#39;, limit = 100)) #\u0026gt; Query: #\u0026gt; [climate change] #\u0026gt; Found: #\u0026gt; [PLoS: 11737; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] #\u0026gt; Returned: #\u0026gt; [PLoS: 100; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] Get full text (texts \u0026lt;- ft_get(out)) #\u0026gt; [Docs] 99 #\u0026gt; [Source] R session #\u0026gt; [IDs] 10.1371/journal.pone.0054839 10.1371/journal.pone.0045683 #\u0026gt; 10.1371/journal.pone.0050182 10.1371/journal.pone.0118489 #\u0026gt; 10.1371/journal.pone.0053646 10.1371/journal.pone.0015103 #\u0026gt; 10.1371/journal.pone.0008320 10.1371/journal.pmed.1001227 #\u0026gt; 10.1371/journal.pmed.1001374 10.1371/journal.pone.0097480 ... Because PLOS returns XML, we don\u0026rsquo;t need to do a PDF extraction step. However, if we got full text from arXiv or bioRxiv, we\u0026rsquo;d need to extract from PDFs first.\nPull out chunks abs \u0026lt;- texts %\u0026gt;% chunks(\u0026#34;abstract\u0026#34;) Let\u0026rsquo;s pull out just the text\nabs \u0026lt;- lapply(abs$plos, function(z) { paste0(z$abstract, collapse = \u0026#34; \u0026#34;) }) Analyze Using the tm package, we can analyze our articles\nlibrary(\u0026#34;tm\u0026#34;) corp \u0026lt;- VCorpus(VectorSource(abs)) # remove stop words, strip whitespace, remove punctuation corp \u0026lt;- tm_map(corp, removeWords, stopwords(\u0026#34;english\u0026#34;)) corp \u0026lt;- tm_map(corp, stripWhitespace) corp \u0026lt;- tm_map(corp, removePunctuation) # Make a term document matrix tdm \u0026lt;- TermDocumentMatrix(corp) # remove sparse terms tdm \u0026lt;- removeSparseTerms(tdm, sparse = 0.8) # get data rs \u0026lt;- rowSums(as.matrix(tdm)) df \u0026lt;- data.frame(word = names(rs), n = unname(rs), stringsAsFactors = FALSE) Visualize library(\u0026#34;ggplot2\u0026#34;) ggplot(df, aes(reorder(word, n), n)) + geom_point() + coord_flip() + labs(y = \u0026#34;Count\u0026#34;, x = \u0026#34;Word\u0026#34;) ","permalink":"http://localhost:1313/2015/08/full-text/","summary":"\u003cp\u003eFinally, we got \u003ccode\u003efulltext\u003c/code\u003e up on CRAN - our first commit was \u003ca href=\"https://github.com/ropensci/fulltext/commit/2d4f7e270040b2c8914853113073fc4d3134445e\"\u003eMay last year\u003c/a\u003e. \u003ccode\u003efulltext\u003c/code\u003e is a package to facilitate text mining. It focuses on open access journals. This package makes it easier to search for articles, download those articles in full text if available, convert pdf format to plain text, and extract text chunks for vizualization/analysis. We are planning to add bits for analysis in future versions. We\u0026rsquo;ve been working on this package for a while now. It has a lot of moving parts and package dependencies, so it took a while to get a first useable version.\u003c/p\u003e","title":"fulltext - a package to help you mine text"},{"content":"NOAA provides a lot of weather data, across many different websites under different project names. The R package rnoaa accesses many of these, including:\nNOAA NCDC climate data, using the NCDC API version 2 GHCND FTP data ISD FTP data Severe weather data docs are at https://www.ncdc.noaa.gov/swdiws/ Sea ice data NOAA buoy data Tornadoes! Data from the NOAA Storm Prediction Center HOMR - Historical Observing Metadata Repository - from NOAA NCDC Storm data - from the International Best Track Archive for Climate Stewardship (IBTrACS) rnoaa used to provide access to ERDDAP servers, but a separate package rerddap focuses on just those data sources.\nWe focus on getting you the data, so there\u0026rsquo;s very little in rnoaa for visualizing, statistics, etc.\nInstallation The newest version should be on CRAN in the next few days. In the meantime, let\u0026rsquo;s install from GitHub\ndevtools::install_github(\u0026#34;ropensci/rnoaa\u0026#34;) library(\u0026#34;rnoaa\u0026#34;) There\u0026rsquo;s an example using the lawn, sp, and dplyr packages. If you want to try those, install like\ninstall.packages(c(\u0026#34;lawn\u0026#34;, \u0026#34;dplyr\u0026#34;, \u0026#34;sp\u0026#34;)) NCDC NCDC = National Climatic Data Center Data comes from a RESTful API described at https://www.ncdc.noaa.gov/cdo-web/webservices/v2 This web service requires an API key - get one at https://www.ncdc.noaa.gov/cdo-web/token if you don\u0026rsquo;t already have one. NCDC provides access to many different datasets:\nThe main function to get data from NCDC is ncdc(). datasetid, startdate, and enddate are required parameters. A quick example, here getting data from the GHCND dataset, from a particular station, and from Oct 1st 2013 to Dec 12th 2013:\nncdc(datasetid = \u0026#39;GHCND\u0026#39;, stationid = \u0026#39;GHCND:USW00014895\u0026#39;, startdate = \u0026#39;2013-10-01\u0026#39;, enddate = \u0026#39;2013-12-01\u0026#39;) #\u0026gt; $meta #\u0026gt; $meta$totalCount #\u0026gt; [1] 697 #\u0026gt; #\u0026gt; $meta$pageCount #\u0026gt; [1] 25 #\u0026gt; #\u0026gt; $meta$offset #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; #\u0026gt; $data #\u0026gt; Source: local data frame [25 x 8] #\u0026gt; #\u0026gt; date datatype station value fl_m fl_q fl_so #\u0026gt; 1 2013-10-01T00:00:00 AWND GHCND:USW00014895 29 W #\u0026gt; 2 2013-10-01T00:00:00 PRCP GHCND:USW00014895 0 W #\u0026gt; 3 2013-10-01T00:00:00 SNOW GHCND:USW00014895 0 W #\u0026gt; 4 2013-10-01T00:00:00 SNWD GHCND:USW00014895 0 W #\u0026gt; 5 2013-10-01T00:00:00 TAVG GHCND:USW00014895 179 H S #\u0026gt; 6 2013-10-01T00:00:00 TMAX GHCND:USW00014895 250 W #\u0026gt; 7 2013-10-01T00:00:00 TMIN GHCND:USW00014895 133 W #\u0026gt; 8 2013-10-01T00:00:00 WDF2 GHCND:USW00014895 210 W #\u0026gt; 9 2013-10-01T00:00:00 WDF5 GHCND:USW00014895 230 W #\u0026gt; 10 2013-10-01T00:00:00 WSF2 GHCND:USW00014895 76 W #\u0026gt; .. ... ... ... ... ... ... ... #\u0026gt; Variables not shown: fl_t (chr) #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;ncdc_data\u0026#34; You probably won\u0026rsquo;t know what station you want data from off hand though, so you can first search for stations, in this example using a bounding box that defines a rectangular area near Seattle\nlibrary(\u0026#34;lawn\u0026#34;) lawn_bbox_polygon(c(-122.2047, 47.5204, -122.1065, 47.6139)) %\u0026gt;% view We\u0026rsquo;ll search within that bounding box for weather stations.\nncdc_stations(extent = c(47.5204, -122.2047, 47.6139, -122.1065)) #\u0026gt; $meta #\u0026gt; $meta$totalCount #\u0026gt; [1] 9 #\u0026gt; #\u0026gt; $meta$pageCount #\u0026gt; [1] 25 #\u0026gt; #\u0026gt; $meta$offset #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; #\u0026gt; $data #\u0026gt; Source: local data frame [9 x 9] #\u0026gt; #\u0026gt; elevation mindate maxdate latitude name #\u0026gt; 1 199.6 2008-06-01 2015-06-29 47.5503 EASTGATE 1.7 SSW, WA US #\u0026gt; 2 240.8 2010-05-01 2015-07-05 47.5604 EASTGATE 1.1 SW, WA US #\u0026gt; 3 85.6 2008-07-01 2015-07-05 47.5916 BELLEVUE 0.8 S, WA US #\u0026gt; 4 104.2 2008-06-01 2015-07-05 47.5211 NEWPORT HILLS 1.9 SSE, WA US #\u0026gt; 5 58.5 2008-08-01 2009-04-12 47.6138 BELLEVUE 2.3 ENE, WA US #\u0026gt; 6 199.9 2008-06-01 2009-11-22 47.5465 NEWPORT HILLS 1.4 E, WA US #\u0026gt; 7 27.1 2008-07-01 2015-07-05 47.6046 BELLEVUE 1.8 W, WA US #\u0026gt; 8 159.4 2008-11-01 2015-07-05 47.5694 BELLEVUE 2.3 SSE, WA US #\u0026gt; 9 82.3 2008-12-01 2010-09-17 47.6095 BELLEVUE 0.6 NE, WA US #\u0026gt; Variables not shown: datacoverage (dbl), id (chr), elevationUnit (chr), #\u0026gt; longitude (dbl) #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;ncdc_stations\u0026#34; And there are 9 found. We could then use their station ids (e.g., GHCND:US1WAKG0024) to search for data using ncdc(), or search for what kind of data that station has with ncdc_datasets(), or other functions.\nGHCND GHCND = Global Historical Climatology Network Daily (Data) Data comes from an FTP server library(\u0026#34;dplyr\u0026#34;) dat \u0026lt;- ghcnd(stationid = \u0026#34;AGE00147704\u0026#34;) dat$data %\u0026gt;% filter(element == \u0026#34;PRCP\u0026#34;, year == 1909) #\u0026gt; id year month element VALUE1 MFLAG1 QFLAG1 SFLAG1 VALUE2 MFLAG2 #\u0026gt; 1 AGE00147704 1909 11 PRCP -9999 NA -9999 NA #\u0026gt; 2 AGE00147704 1909 12 PRCP 23 NA E 0 NA #\u0026gt; QFLAG2 SFLAG2 VALUE3 MFLAG3 QFLAG3 SFLAG3 VALUE4 MFLAG4 QFLAG4 SFLAG4 #\u0026gt; 1 -9999 NA -9999 NA #\u0026gt; 2 E 0 NA E 0 NA E #\u0026gt; VALUE5 MFLAG5 QFLAG5 SFLAG5 VALUE6 MFLAG6 QFLAG6 SFLAG6 VALUE7 MFLAG7 #\u0026gt; 1 -9999 NA -9999 NA -9999 NA #\u0026gt; 2 0 NA E 0 NA E 0 NA #\u0026gt; QFLAG7 SFLAG7 VALUE8 MFLAG8 QFLAG8 SFLAG8 VALUE9 MFLAG9 QFLAG9 SFLAG9 #\u0026gt; 1 NA -9999 NA -9999 NA #\u0026gt; 2 NA E 250 NA E 75 NA E #\u0026gt; VALUE10 MFLAG10 QFLAG10 SFLAG10 VALUE11 MFLAG11 QFLAG11 SFLAG11 VALUE12 #\u0026gt; 1 -9999 NA -9999 NA -9999 #\u0026gt; 2 131 NA E 0 NA E 0 #\u0026gt; MFLAG12 QFLAG12 SFLAG12 VALUE13 MFLAG13 QFLAG13 SFLAG13 VALUE14 MFLAG14 #\u0026gt; 1 NA -9999 NA -9999 NA #\u0026gt; 2 NA E 0 NA E 0 NA #\u0026gt; QFLAG14 SFLAG14 VALUE15 MFLAG15 QFLAG15 SFLAG15 VALUE16 MFLAG16 QFLAG16 #\u0026gt; 1 -9999 NA -9999 NA #\u0026gt; 2 E 0 NA E 0 NA #\u0026gt; SFLAG16 VALUE17 MFLAG17 QFLAG17 SFLAG17 VALUE18 MFLAG18 QFLAG18 SFLAG18 #\u0026gt; 1 -9999 NA -9999 NA #\u0026gt; 2 E 0 NA E 0 NA E #\u0026gt; VALUE19 MFLAG19 QFLAG19 SFLAG19 VALUE20 MFLAG20 QFLAG20 SFLAG20 VALUE21 #\u0026gt; 1 -9999 NA NA -9999 NA NA -9999 #\u0026gt; 2 0 NA NA E 0 NA NA E 0 #\u0026gt; MFLAG21 QFLAG21 SFLAG21 VALUE22 MFLAG22 QFLAG22 SFLAG22 VALUE23 MFLAG23 #\u0026gt; 1 NA -9999 NA 22 NA #\u0026gt; 2 NA E 0 NA E 0 NA #\u0026gt; QFLAG23 SFLAG23 VALUE24 MFLAG24 QFLAG24 SFLAG24 VALUE25 MFLAG25 QFLAG25 #\u0026gt; 1 NA E 9 NA NA E 5 NA NA #\u0026gt; 2 NA E 0 NA NA E 0 NA NA #\u0026gt; SFLAG25 VALUE26 MFLAG26 QFLAG26 SFLAG26 VALUE27 MFLAG27 QFLAG27 SFLAG27 #\u0026gt; 1 E 0 NA E 86 NA NA E #\u0026gt; 2 E 0 NA E 0 NA NA E #\u0026gt; VALUE28 MFLAG28 QFLAG28 SFLAG28 VALUE29 MFLAG29 QFLAG29 SFLAG29 VALUE30 #\u0026gt; 1 0 NA NA E 28 NA NA E 0 #\u0026gt; 2 0 NA NA E 0 NA NA E 0 #\u0026gt; MFLAG30 QFLAG30 SFLAG30 VALUE31 MFLAG31 QFLAG31 SFLAG31 #\u0026gt; 1 NA E -9999 NA NA #\u0026gt; 2 NA E 57 NA NA E You can also get to datasets by searching by station id, date min, date max, and variable. E.g.\nghcnd_search(\u0026#34;AGE00147704\u0026#34;, var = \u0026#34;PRCP\u0026#34;) #\u0026gt; $prcp #\u0026gt; Source: local data frame [9,803 x 6] #\u0026gt; #\u0026gt; id prcp date mflag qflag sflag #\u0026gt; 1 AGE00147704 -9999 1909-11-01 NA #\u0026gt; 2 AGE00147704 23 1909-12-01 NA E #\u0026gt; 3 AGE00147704 81 1910-01-01 NA E #\u0026gt; 4 AGE00147704 0 1910-02-01 NA E #\u0026gt; 5 AGE00147704 18 1910-03-01 NA E #\u0026gt; 6 AGE00147704 0 1910-04-01 NA E #\u0026gt; 7 AGE00147704 223 1910-05-01 NA E #\u0026gt; 8 AGE00147704 0 1910-06-01 NA E #\u0026gt; 9 AGE00147704 0 1910-07-01 NA E #\u0026gt; 10 AGE00147704 0 1910-08-01 NA E #\u0026gt; .. ... ... ... ... ... ... ISD ISD = Integrated Surface Database Data comes from an FTP server You\u0026rsquo;ll likely first want to run isd_stations() to get list of stations\nstations \u0026lt;- isd_stations() head(stations) #\u0026gt; usaf wban station_name ctry state icao lat lon elev_m begin end #\u0026gt; 1 7005 99999 CWOS 07005 NA NA NA 20120127 20120127 #\u0026gt; 2 7011 99999 CWOS 07011 NA NA NA 20111025 20121129 #\u0026gt; 3 7018 99999 WXPOD 7018 0 0 7018 20110309 20130730 #\u0026gt; 4 7025 99999 CWOS 07025 NA NA NA 20120127 20120127 #\u0026gt; 5 7026 99999 WXPOD 7026 AF 0 0 7026 20120713 20141120 #\u0026gt; 6 7034 99999 CWOS 07034 NA NA NA 20121024 20121106 Then get data from particular stations, like\n(res \u0026lt;- isd(usaf = \u0026#34;011490\u0026#34;, wban = \u0026#34;99999\u0026#34;, year = 1986)) #\u0026gt; \u0026lt;ISD Data\u0026gt; #\u0026gt; Size: 1328 X 85 #\u0026gt; #\u0026gt; total_chars usaf_station wban_station date time date_flag latitude #\u0026gt; 1 50 11490 99999 19860101 0 4 66267 #\u0026gt; 2 123 11490 99999 19860101 600 4 66267 #\u0026gt; 3 50 11490 99999 19860101 1200 4 66267 #\u0026gt; 4 94 11490 99999 19860101 1800 4 66267 #\u0026gt; 5 50 11490 99999 19860102 0 4 66267 #\u0026gt; 6 123 11490 99999 19860102 600 4 66267 #\u0026gt; 7 50 11490 99999 19860102 1200 4 66267 #\u0026gt; 8 94 11490 99999 19860102 1800 4 66267 #\u0026gt; 9 50 11490 99999 19860103 0 4 66267 #\u0026gt; 10 123 11490 99999 19860103 600 4 66267 #\u0026gt; .. ... ... ... ... ... ... ... #\u0026gt; Variables not shown: longitude (int), type_code (chr), elevation (int), #\u0026gt; call_letter (int), quality (chr), wind_direction (int), #\u0026gt; wind_direction_quality (int), wind_code (chr), wind_speed (int), #\u0026gt; wind_speed_quality (int), ceiling_height (int), #\u0026gt; ceiling_height_quality (int), ceiling_height_determination (chr), #\u0026gt; ceiling_height_cavok (chr), visibility_distance (int), #\u0026gt; visibility_distance_quality (int), visibility_code (chr), #\u0026gt; visibility_code_quality (int), temperature (int), temperature_quality #\u0026gt; (int), temperature_dewpoint (int), temperature_dewpoint_quality #\u0026gt; (int), air_pressure (int), air_pressure_quality (int), #\u0026gt; AG1.precipitation (chr), AG1.discrepancy (int), AG1.est_water_depth #\u0026gt; (int), GF1.sky_condition (chr), GF1.coverage (int), #\u0026gt; GF1.opaque_coverage (int), GF1.coverage_quality (int), #\u0026gt; GF1.lowest_cover (int), GF1.lowest_cover_quality (int), #\u0026gt; GF1.low_cloud_genus (int), GF1.low_cloud_genus_quality (int), #\u0026gt; GF1.lowest_cloud_base_height (int), #\u0026gt; GF1.lowest_cloud_base_height_quality (int), GF1.mid_cloud_genus #\u0026gt; (int), GF1.mid_cloud_genus_quality (int), GF1.high_cloud_genus (int), #\u0026gt; GF1.high_cloud_genus_quality (int), MD1.atmospheric_change (chr), #\u0026gt; MD1.tendency (int), MD1.tendency_quality (int), MD1.three_hr (int), #\u0026gt; MD1.three_hr_quality (int), MD1.twentyfour_hr (int), #\u0026gt; MD1.twentyfour_hr_quality (int), REM.remarks (chr), REM.identifier #\u0026gt; (chr), REM.length_quantity (int), REM.comment (chr), KA1.extreme_temp #\u0026gt; (chr), KA1.period_quantity (int), KA1.max_min (chr), KA1.temp (int), #\u0026gt; KA1.temp_quality (int), AY1.manual_occurrence (chr), #\u0026gt; AY1.condition_code (int), AY1.condition_quality (int), AY1.period #\u0026gt; (int), AY1.period_quality (int), AY2.manual_occurrence (chr), #\u0026gt; AY2.condition_code (int), AY2.condition_quality (int), AY2.period #\u0026gt; (int), AY2.period_quality (int), MW1.first_weather_reported (chr), #\u0026gt; MW1.condition (int), MW1.condition_quality (int), #\u0026gt; EQD.observation_identifier (chr), EQD.observation_text (int), #\u0026gt; EQD.reason_code (int), EQD.parameter (chr), #\u0026gt; EQD.observation_identifier.1 (chr), EQD.observation_text.1 (int), #\u0026gt; EQD.reason_code.1 (int), EQD.parameter.1 (chr) Severe weather SWDI = Severe Weather Data Inventory From the SWDI site The Severe Weather Data Inventory (SWDI) is an integrated database of severe weather records for the United States. The records in SWDI come from a variety of sources in the NCDC archive.\nThe swdi() function allows you to get data in xml, csv, shp, or kmz format. You can get data from many different datasets:\nnx3tvs NEXRAD Level-3 Tornado Vortex Signatures (point) nx3meso NEXRAD Level-3 Mesocyclone Signatures (point) nx3hail NEXRAD Level-3 Hail Signatures (point) nx3structure NEXRAD Level-3 Storm Cell Structure Information (point) plsr Preliminary Local Storm Reports (point) warn Severe Thunderstorm, Tornado, Flash Flood and Special Marine warnings (polygon) nldn Lightning strikes from Vaisala (.gov and .mil ONLY) (point) An example: Get all plsr within the bounding box (-91,30,-90,31)\nswdi(dataset = \u0026#39;plsr\u0026#39;, startdate = \u0026#39;20060505\u0026#39;, enddate = \u0026#39;20060510\u0026#39;, bbox = c(-91, 30, -90, 31)) #\u0026gt; $meta #\u0026gt; $meta$totalCount #\u0026gt; numeric(0) #\u0026gt; #\u0026gt; $meta$totalTimeInSeconds #\u0026gt; [1] 0 #\u0026gt; #\u0026gt; #\u0026gt; $data #\u0026gt; Source: local data frame [5 x 8] #\u0026gt; #\u0026gt; ztime id event magnitude city #\u0026gt; 1 2006-05-09T02:20:00Z 427540 HAIL 1 5 E KENTWOOD #\u0026gt; 2 2006-05-09T02:40:00Z 427536 HAIL 1 MOUNT HERMAN #\u0026gt; 3 2006-05-09T02:40:00Z 427537 TSTM WND DMG -9999 MOUNT HERMAN #\u0026gt; 4 2006-05-09T03:00:00Z 427199 HAIL 0 FRANKLINTON #\u0026gt; 5 2006-05-09T03:17:00Z 427200 TORNADO -9999 5 S FRANKLINTON #\u0026gt; Variables not shown: county (chr), state (chr), source (chr) #\u0026gt; #\u0026gt; $shape #\u0026gt; shape #\u0026gt; 1 POINT (-90.43 30.93) #\u0026gt; 2 POINT (-90.3 30.96) #\u0026gt; 3 POINT (-90.3 30.96) #\u0026gt; 4 POINT (-90.14 30.85) #\u0026gt; 5 POINT (-90.14 30.78) #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;swdi\u0026#34; Sea ice The seaice() function simply grabs shape files that describe sea ice cover at the Northa and South poles, and can be useful for examining change through time in sea ice cover, among other things.\nAn example: Plot sea ice cover for April 1990 for the North pole.\nurls \u0026lt;- seaiceeurls(mo = \u0026#39;Apr\u0026#39;, pole = \u0026#39;N\u0026#39;, yr = 1990) out \u0026lt;- seaice(urls) library(\u0026#39;ggplot2\u0026#39;) ggplot(out, aes(long, lat, group = group)) + geom_polygon(fill = \u0026#34;steelblue\u0026#34;) + theme_ice() Buoys Get NOAA buoy data from the National Buoy Data Center Using buoy data requires the ncdf package. Make sure you have that installed, like install.packages(\u0026quot;ncdf\u0026quot;). buoy() and buoys() will fail if you don\u0026rsquo;t have ncdf installed.\nbuoys() - Get available buoys given a dataset name\nhead(buoys(dataset = \u0026#39;cwind\u0026#39;)) #\u0026gt; id #\u0026gt; 1 41001 #\u0026gt; 2 41002 #\u0026gt; 3 41004 #\u0026gt; 4 41006 #\u0026gt; 5 41008 #\u0026gt; 6 41009 #\u0026gt; url #\u0026gt; 1 https://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41001/catalog.html #\u0026gt; 2 https://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41002/catalog.html #\u0026gt; 3 https://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41004/catalog.html #\u0026gt; 4 https://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41006/catalog.html #\u0026gt; 5 https://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41008/catalog.html #\u0026gt; 6 https://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41009/catalog.html buoy() - Get data for a buoy - if no year or datatype specified, we get the first file\nbuoy(dataset = \u0026#39;cwind\u0026#39;, buoyid = 46085) #\u0026gt; Dimensions (rows/cols): [33486 X 5] #\u0026gt; 2 variables: [wind_dir, wind_spd] #\u0026gt; #\u0026gt; time latitude longitude wind_dir wind_spd #\u0026gt; 1 2007-05-05T02:00:00Z 55.855 -142.559 331 2.8 #\u0026gt; 2 2007-05-05T02:10:00Z 55.855 -142.559 328 2.6 #\u0026gt; 3 2007-05-05T02:20:00Z 55.855 -142.559 329 2.2 #\u0026gt; 4 2007-05-05T02:30:00Z 55.855 -142.559 356 2.1 #\u0026gt; 5 2007-05-05T02:40:00Z 55.855 -142.559 360 1.5 #\u0026gt; 6 2007-05-05T02:50:00Z 55.855 -142.559 10 1.9 #\u0026gt; 7 2007-05-05T03:00:00Z 55.855 -142.559 10 2.2 #\u0026gt; 8 2007-05-05T03:10:00Z 55.855 -142.559 14 2.2 #\u0026gt; 9 2007-05-05T03:20:00Z 55.855 -142.559 16 2.1 #\u0026gt; 10 2007-05-05T03:30:00Z 55.855 -142.559 22 1.6 #\u0026gt; .. ... ... ... ... ... Tornadoes The function tornadoes() gets tornado data from https://www.spc.noaa.gov/gis/svrgis/.\nshp \u0026lt;- tornadoes() library(\u0026#39;sp\u0026#39;) plot(shp) Historical Observing Metadata Repository HOMR = Historical Observing Metadata Repository Data from RESTful API at https://www.ncdc.noaa.gov/homr/api homr_definitions() gets you definitions and metadata for datasets\nhead(homr_definitions()) #\u0026gt; Source: local data frame [6 x 7] #\u0026gt; #\u0026gt; defType abbr fullName displayName #\u0026gt; 1 ids GHCND GHCND IDENTIFIER GHCND ID #\u0026gt; 2 ids COOP COOP NUMBER COOP ID #\u0026gt; 3 ids WBAN WBAN NUMBER WBAN ID #\u0026gt; 4 ids FAA FAA LOCATION IDENTIFIER FAA ID #\u0026gt; 5 ids ICAO ICAO ID ICAO ID #\u0026gt; 6 ids TRANS TRANSMITTAL ID Transmittal ID #\u0026gt; Variables not shown: description (chr), cssaName (chr), ghcndName (chr) homr() gets you metadata for stations given query parameters. In this example, search for data for the state of Delaware\nres \u0026lt;- homr(state = \u0026#39;DE\u0026#39;) names(res) # the stations #\u0026gt; [1] \u0026#34;10001871\u0026#34; \u0026#34;10100162\u0026#34; \u0026#34;10100164\u0026#34; \u0026#34;10100166\u0026#34; \u0026#34;20004155\u0026#34; \u0026#34;20004158\u0026#34; #\u0026gt; [7] \u0026#34;20004160\u0026#34; \u0026#34;20004162\u0026#34; \u0026#34;20004163\u0026#34; \u0026#34;20004168\u0026#34; \u0026#34;20004171\u0026#34; \u0026#34;20004176\u0026#34; #\u0026gt; [13] \u0026#34;20004178\u0026#34; \u0026#34;20004179\u0026#34; \u0026#34;20004180\u0026#34; \u0026#34;20004182\u0026#34; \u0026#34;20004184\u0026#34; \u0026#34;20004185\u0026#34; #\u0026gt; [19] \u0026#34;30001831\u0026#34; \u0026#34;30017384\u0026#34; \u0026#34;30020917\u0026#34; \u0026#34;30021161\u0026#34; \u0026#34;30021998\u0026#34; \u0026#34;30022674\u0026#34; #\u0026gt; [25] \u0026#34;30026770\u0026#34; \u0026#34;30027455\u0026#34; \u0026#34;30032423\u0026#34; \u0026#34;30032685\u0026#34; \u0026#34;30034222\u0026#34; \u0026#34;30039554\u0026#34; #\u0026gt; [31] \u0026#34;30043742\u0026#34; \u0026#34;30046662\u0026#34; \u0026#34;30046814\u0026#34; \u0026#34;30051475\u0026#34; \u0026#34;30057217\u0026#34; \u0026#34;30063570\u0026#34; #\u0026gt; [37] \u0026#34;30064900\u0026#34; \u0026#34;30065901\u0026#34; \u0026#34;30067636\u0026#34; \u0026#34;30069663\u0026#34; \u0026#34;30075067\u0026#34; \u0026#34;30077378\u0026#34; #\u0026gt; [43] \u0026#34;30077857\u0026#34; \u0026#34;30077923\u0026#34; \u0026#34;30077988\u0026#34; \u0026#34;30079088\u0026#34; \u0026#34;30079240\u0026#34; \u0026#34;30082430\u0026#34; #\u0026gt; [49] \u0026#34;30084216\u0026#34; \u0026#34;30084262\u0026#34; \u0026#34;30084537\u0026#34; \u0026#34;30084796\u0026#34; \u0026#34;30094582\u0026#34; \u0026#34;30094639\u0026#34; #\u0026gt; [55] \u0026#34;30094664\u0026#34; \u0026#34;30094670\u0026#34; \u0026#34;30094683\u0026#34; \u0026#34;30094730\u0026#34; \u0026#34;30094806\u0026#34; \u0026#34;30094830\u0026#34; #\u0026gt; [61] \u0026#34;30094917\u0026#34; \u0026#34;30094931\u0026#34; \u0026#34;30094936\u0026#34; \u0026#34;30094991\u0026#34; You can index to each one to get more data\nStorms Data from: International Best Track Archive for Climate Stewardship (IBTrACS) Data comes from an FTP server Flat files (csv\u0026rsquo;s) are served up as well as shp files. In this example, plot storm data for the year 1940\n(res3 \u0026lt;- storm_shp(year = 1940)) #\u0026gt; \u0026lt;NOAA Storm Shp Files\u0026gt; #\u0026gt; Path: ~/.rnoaa/storms/year/Year.1940.ibtracs_all_points.v03r06.shp #\u0026gt; Basin: \u0026lt;NA\u0026gt; #\u0026gt; Storm: \u0026lt;NA\u0026gt; #\u0026gt; Year: 1940 #\u0026gt; Type: points res3shp \u0026lt;- storm_shp_read(res3) sp::plot(res3shp) ","permalink":"http://localhost:1313/2015/07/weather-data-with-rnoaa/","summary":"\u003cp\u003eNOAA provides a lot of weather data, across many different websites under different project names. The R package \u003ccode\u003ernoaa\u003c/code\u003e accesses many of these, including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNOAA NCDC climate data, using the \u003ca href=\"https://www.ncdc.noaa.gov/cdo-web/webservices/v2\"\u003eNCDC API version 2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/\"\u003eGHCND FTP data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"ftp://ftp.ncdc.noaa.gov/pub/data/noaa/\"\u003eISD FTP data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSevere weather data docs are at \u003ca href=\"https://www.ncdc.noaa.gov/swdiws/\"\u003ehttps://www.ncdc.noaa.gov/swdiws/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"ftp://sidads.colorado.edu/DATASETS/NOAA/G02135/shapefiles\"\u003eSea ice data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.ndbc.noaa.gov/\"\u003eNOAA buoy data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTornadoes! Data from the \u003ca href=\"https://www.spc.noaa.gov/gis/svrgis/\"\u003eNOAA Storm Prediction Center\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eHOMR - Historical Observing Metadata Repository - from \u003ca href=\"https://www.ncdc.noaa.gov/homr/api\"\u003eNOAA NCDC\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eStorm data - from the \u003ca href=\"https://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data\"\u003eInternational Best Track Archive for Climate Stewardship (IBTrACS)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003ernoaa\u003c/code\u003e used to provide access to \u003ca href=\"https://upwell.pfeg.noaa.gov/erddap/index.html\"\u003eERDDAP servers\u003c/a\u003e, but a separate package \u003ca href=\"https://github.com/ropensci/rerddap\"\u003ererddap\u003c/a\u003e focuses on just those data sources.\u003c/p\u003e","title":"rnoaa - Weather data in R"},{"content":"ERDDAP is a data server that gives you a simple, consistent way to download subsets of gridded and tabular scientific datasets in common file formats and make graphs and maps. Besides it’s own RESTful interface, much of which is designed based on OPeNDAP, ERDDAP can act as an OPeNDAP server and as a WMS server for gridded data.\nERDDAP is a powerful tool - in a world of heterogeneous data, it\u0026rsquo;s often hard to combine data and serve it through the same interface, with tools for querying/filtering/subsetting the data. That is exactly what ERDDAP does. Heterogeneous data sets often have some similarities, such as latitude/longitude data and usually a time component, but other variables vary widely.\nNetCDF rerddap supports NetCDF format, and is the default when using the griddap() function. We use ncdf by default, but you can choose to use ncdf4 instead.\nCaching Data files downloaded are cached in a single hidden directory ~/.rerddap on your machine. It\u0026rsquo;s hidden so that you don\u0026rsquo;t accidentally delete the data, but you can still easily delete the data if you like, open files, move them around, etc.\nWhen you use griddap() or tabledap() functions, we construct a MD5 hash from the base URL, and any query parameters - this way each query is separately cached. Once we have the hash, we look in ~/.rerddap for a matching hash. If there\u0026rsquo;s a match we use that file on disk - if no match, we make a http request for the data to the ERDDAP server you specify.\nERDDAP servers You can get a data.frame of ERDDAP servers using the function servers(). Most I think serve some kind of NOAA data, but there are a few that aren\u0026rsquo;t NOAA data. Here are a few:\nhead(servers()) #\u0026gt; name #\u0026gt; 1 Marine Domain Awareness (MDA) - Italy #\u0026gt; 2 Marine Institute - Ireland #\u0026gt; 3 CoastWatch Caribbean/Gulf of Mexico Node #\u0026gt; 4 CoastWatch West Coast Node #\u0026gt; 5 NOAA IOOS CeNCOOS (Central and Northern California Ocean Observing System) #\u0026gt; 6 NOAA IOOS NERACOOS (Northeastern Regional Association of Coastal and Ocean Observing Systems) #\u0026gt; url #\u0026gt; 1 https://bluehub.jrc.ec.europa.eu/erddap/ #\u0026gt; 2 https://erddap.marine.ie/erddap/ #\u0026gt; 3 https://cwcgom.aoml.noaa.gov/erddap/ #\u0026gt; 4 https://coastwatch.pfeg.noaa.gov/erddap/ #\u0026gt; 5 https://erddap.axiomalaska.com/erddap/ #\u0026gt; 6 https://www.neracoos.org/erddap/ Install From CRAN\ninstall.packages(\u0026#34;rerddap\u0026#34;) Or development version from GitHub\ndevtools::install_github(\u0026#34;ropensci/rerddap\u0026#34;) library(\u0026#39;rerddap\u0026#39;) Search First, you likely want to search for data, specifying whether to search for either griddadp or tabledap datasets. The default is griddap.\ned_search(query = \u0026#39;size\u0026#39;, which = \u0026#34;table\u0026#34;) #\u0026gt; 11 results, showing first 20 #\u0026gt; title #\u0026gt; 1 CalCOFI Fish Sizes #\u0026gt; 2 CalCOFI Larvae Sizes #\u0026gt; 3 Channel Islands, Kelp Forest Monitoring, Size and Frequency, Natural Habitat #\u0026gt; 4 CalCOFI Larvae Counts Positive Tows #\u0026gt; 5 CalCOFI Tows #\u0026gt; 7 OBIS - ARGOS Satellite Tracking of Animals #\u0026gt; 8 GLOBEC NEP MOCNESS Plankton (MOC1) Data #\u0026gt; 9 GLOBEC NEP Vertical Plankton Tow (VPT) Data #\u0026gt; 10 NWFSC Observer Fixed Gear Data, off West Coast of US, 2002-2006 #\u0026gt; 11 NWFSC Observer Trawl Data, off West Coast of US, 2002-2006 #\u0026gt; 12 AN EXPERIMENTAL DATASET: Underway Sea Surface Temperature and Salinity Aboard the Oleander #\u0026gt; dataset_id #\u0026gt; 1 erdCalCOFIfshsiz #\u0026gt; 2 erdCalCOFIlrvsiz #\u0026gt; 3 erdCinpKfmSFNH #\u0026gt; 4 erdCalCOFIlrvcntpos #\u0026gt; 5 erdCalCOFItows #\u0026gt; 7 aadcArgos #\u0026gt; 8 erdGlobecMoc1 #\u0026gt; 9 erdGlobecVpt #\u0026gt; 10 nwioosObsFixed2002 #\u0026gt; 11 nwioosObsTrawl2002 #\u0026gt; 12 nodcPJJU ed_search(query = \u0026#39;size\u0026#39;, which = \u0026#34;grid\u0026#34;) #\u0026gt; 6 results, showing first 20 #\u0026gt; title #\u0026gt; 6 NOAA Global Coral Bleaching Monitoring Products #\u0026gt; 13 USGS COAWST Forecast, US East Coast and Gulf of Mexico (Experimental) [time][eta_rho][xi_rho] #\u0026gt; 14 USGS COAWST Forecast, US East Coast and Gulf of Mexico (Experimental) [time][eta_u][xi_u] #\u0026gt; 15 USGS COAWST Forecast, US East Coast and Gulf of Mexico (Experimental) [time][eta_v][xi_v] #\u0026gt; 16 USGS COAWST Forecast, US East Coast and Gulf of Mexico (Experimental) [time][s_rho][eta_rho][xi_rho] #\u0026gt; 17 USGS COAWST Forecast, US East Coast and Gulf of Mexico (Experimental) [time][Nbed][eta_rho][xi_rho] #\u0026gt; dataset_id #\u0026gt; 6 NOAA_DHW #\u0026gt; 13 whoi_ed12_89ce_9592 #\u0026gt; 14 whoi_61c3_0b5d_cd61 #\u0026gt; 15 whoi_62d0_9d64_c8ff #\u0026gt; 16 whoi_7dd7_db97_4bbe #\u0026gt; 17 whoi_a4fb_2c9c_16a7 This gives back dataset titles and identifiers - with which you should be able to get a sense for which dataset you may want to fetch.\nInformation After searching you can get more information on a single dataset\ninfo(\u0026#39;whoi_62d0_9d64_c8ff\u0026#39;) #\u0026gt; \u0026lt;ERDDAP info\u0026gt; whoi_62d0_9d64_c8ff #\u0026gt; Dimensions (range): #\u0026gt; time: (2012-06-25T01:00:00Z, 2015-06-24T00:00:00Z) #\u0026gt; eta_v: (0, 334) #\u0026gt; xi_v: (0, 895) #\u0026gt; Variables: #\u0026gt; bedload_Vsand_01: #\u0026gt; Units: kilogram meter-1 s-1 #\u0026gt; bedload_Vsand_02: #\u0026gt; Units: kilogram meter-1 s-1 ... Which is a simple S3 list but prints out pretty, so it\u0026rsquo;s easy to quickly scan the printed output and see what you need to see to proceed. That is, in the next step you want to get the dataset, and you\u0026rsquo;ll want to specify your search using some combination of values for latitude, longitude, and time.\ngriddap (gridded) data First, get information on a dataset to see time range, lat/long range, and variables.\n(out \u0026lt;- info(\u0026#39;noaa_esrl_027d_0fb5_5d38\u0026#39;)) #\u0026gt; \u0026lt;ERDDAP info\u0026gt; noaa_esrl_027d_0fb5_5d38 #\u0026gt; Dimensions (range): #\u0026gt; time: (1850-01-01T00:00:00Z, 2014-05-01T00:00:00Z) #\u0026gt; latitude: (87.5, -87.5) #\u0026gt; longitude: (-177.5, 177.5) #\u0026gt; Variables: #\u0026gt; air: #\u0026gt; Range: -20.9, 19.5 #\u0026gt; Units: degC Then query for gridded data using the griddap() function\n(res \u0026lt;- griddap(out, time = c(\u0026#39;2012-01-01\u0026#39;, \u0026#39;2012-01-30\u0026#39;), latitude = c(21, 10), longitude = c(-80, -70) )) #\u0026gt; \u0026lt;ERDDAP griddap\u0026gt; noaa_esrl_027d_0fb5_5d38 #\u0026gt; Path: [~/.rerddap/648ed11e8b911b65e39eb63c8df339df.nc] #\u0026gt; Last updated: [2015-05-09 08:31:10] #\u0026gt; File size: [0 mb] #\u0026gt; Dimensions (dims/vars): [3 X 1] #\u0026gt; Dim names: time, latitude, longitude #\u0026gt; Variable names: CRUTEM3: Surface Air Temperature Monthly Anomaly #\u0026gt; data.frame (rows/columns): [18 X 4] #\u0026gt; time latitude longitude air #\u0026gt; 1 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 2 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 3 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 4 2012-01-01T00:00:00Z 22.5 -77.5 -0.1 #\u0026gt; 5 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 6 2012-01-01T00:00:00Z 22.5 -77.5 -0.2 #\u0026gt; 7 2012-01-01T00:00:00Z 17.5 -72.5 0.2 #\u0026gt; 8 2012-01-01T00:00:00Z 17.5 -72.5 NA #\u0026gt; 9 2012-01-01T00:00:00Z 17.5 -72.5 0.3 #\u0026gt; 10 2012-02-01T00:00:00Z 17.5 -72.5 NA #\u0026gt; .. ... ... ... ... The output of griddap() is a list that you can explore further. Get the summary\nres$summary #\u0026gt; [1] \u0026#34;file ~/.rerddap/648ed11e8b911b65e39eb63c8df339df.nc has 3 dimensions:\u0026#34; #\u0026gt; [1] \u0026#34;time Size: 2\u0026#34; #\u0026gt; [1] \u0026#34;latitude Size: 3\u0026#34; #\u0026gt; [1] \u0026#34;longitude Size: 3\u0026#34; #\u0026gt; [1] \u0026#34;------------------------\u0026#34; #\u0026gt; [1] \u0026#34;file ~/.rerddap/648ed11e8b911b65e39eb63c8df339df.nc has 1 variables:\u0026#34; #\u0026gt; [1] \u0026#34;float air[longitude,latitude,time] Longname:CRUTEM3: Surface Air Temperature Monthly Anomaly Missval:-9.96920996838687e+36\u0026#34; Or get the dimension variables (just the names of the variables for brevity here)\nnames(res$summary$dim) #\u0026gt; [1] \u0026#34;time\u0026#34; \u0026#34;latitude\u0026#34; \u0026#34;longitude\u0026#34; Get the data.frame (beware: you may want to just look at the head of the data.frame if large)\nres$data #\u0026gt; time latitude longitude air #\u0026gt; 1 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 2 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 3 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 4 2012-01-01T00:00:00Z 22.5 -77.5 -0.10 #\u0026gt; 5 2012-01-01T00:00:00Z 22.5 -77.5 NA #\u0026gt; 6 2012-01-01T00:00:00Z 22.5 -77.5 -0.20 #\u0026gt; 7 2012-01-01T00:00:00Z 17.5 -72.5 0.20 #\u0026gt; 8 2012-01-01T00:00:00Z 17.5 -72.5 NA #\u0026gt; 9 2012-01-01T00:00:00Z 17.5 -72.5 0.30 #\u0026gt; 10 2012-02-01T00:00:00Z 17.5 -72.5 NA #\u0026gt; 11 2012-02-01T00:00:00Z 17.5 -72.5 NA #\u0026gt; 12 2012-02-01T00:00:00Z 17.5 -72.5 NA #\u0026gt; 13 2012-02-01T00:00:00Z 12.5 -67.5 0.40 #\u0026gt; 14 2012-02-01T00:00:00Z 12.5 -67.5 NA #\u0026gt; 15 2012-02-01T00:00:00Z 12.5 -67.5 0.20 #\u0026gt; 16 2012-02-01T00:00:00Z 12.5 -67.5 0.00 #\u0026gt; 17 2012-02-01T00:00:00Z 12.5 -67.5 NA #\u0026gt; 18 2012-02-01T00:00:00Z 12.5 -67.5 0.32 You can actually still explore the original netcdf summary object, e.g.,\nres$summary$dim$time #\u0026gt; $name #\u0026gt; [1] \u0026#34;time\u0026#34; #\u0026gt; #\u0026gt; $len #\u0026gt; [1] 2 #\u0026gt; #\u0026gt; $unlim #\u0026gt; [1] FALSE #\u0026gt; #\u0026gt; $id #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $dimvarid #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $units #\u0026gt; [1] \u0026#34;seconds since 1970-01-01T00:00:00Z\u0026#34; #\u0026gt; #\u0026gt; $vals #\u0026gt; [1] 1325376000 1328054400 #\u0026gt; #\u0026gt; $create_dimvar #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;dim.ncdf\u0026#34; tabledap (tabular) data tabledap is data that is not gridded by lat/lon/time. In addition, the query interface is a bit different. Notice that you can do less than, more than, equal to type queries, but they are specified as character strings.\n(out \u0026lt;- info(\u0026#39;erdCalCOFIfshsiz\u0026#39;)) #\u0026gt; \u0026lt;ERDDAP info\u0026gt; erdCalCOFIfshsiz #\u0026gt; Variables: #\u0026gt; calcofi_species_code: #\u0026gt; Range: 19, 1550 #\u0026gt; common_name: #\u0026gt; cruise: #\u0026gt; fish_1000m3: #\u0026gt; Units: Fish per 1,000 cubic meters of water sampled #\u0026gt; fish_count: #\u0026gt; fish_size: ... (dat \u0026lt;- tabledap(out, \u0026#39;time\u0026gt;=2001-07-07\u0026#39;, \u0026#39;time\u0026lt;=2001-07-10\u0026#39;, fields = c(\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;fish_size\u0026#39;, \u0026#39;itis_tsn\u0026#39;, \u0026#39;scientific_name\u0026#39;))) #\u0026gt; \u0026lt;ERDDAP tabledap\u0026gt; erdCalCOFIfshsiz #\u0026gt; Path: [~/.rerddap/f013f9ee09bdb4184928d533e575e948.csv] #\u0026gt; Last updated: [2015-05-09 08:31:21] #\u0026gt; File size: [0.03 mb] #\u0026gt; Dimensions: [558 X 5] #\u0026gt; #\u0026gt; longitude latitude fish_size itis_tsn scientific_name #\u0026gt; 2 -118.26 33.255 22.9 623745 Nannobrachium ritteri #\u0026gt; 3 -118.26 33.255 22.9 623745 Nannobrachium ritteri #\u0026gt; 4 -118.10667 32.738335 31.5 623625 Lipolagus ochotensis #\u0026gt; 5 -118.10667 32.738335 48.3 623625 Lipolagus ochotensis #\u0026gt; 6 -118.10667 32.738335 15.5 162221 Argyropelecus sladeni #\u0026gt; 7 -118.10667 32.738335 16.3 162221 Argyropelecus sladeni #\u0026gt; 8 -118.10667 32.738335 17.8 162221 Argyropelecus sladeni #\u0026gt; 9 -118.10667 32.738335 18.2 162221 Argyropelecus sladeni #\u0026gt; 10 -118.10667 32.738335 19.2 162221 Argyropelecus sladeni #\u0026gt; 11 -118.10667 32.738335 20.0 162221 Argyropelecus sladeni #\u0026gt; .. ... ... ... ... ... Since both griddap() and tabledap() give back data.frame\u0026rsquo;s, it\u0026rsquo;s easy to do downstream manipulation. For example, we can use dplyr to filter, summarize, group, and sort:\nlibrary(\u0026#34;dplyr\u0026#34;) dat$fish_size \u0026lt;- as.numeric(dat$fish_size) df \u0026lt;- tbl_df(dat) %\u0026gt;% filter(fish_size \u0026gt; 30) %\u0026gt;% group_by(scientific_name) %\u0026gt;% summarise(mean_size = mean(fish_size)) %\u0026gt;% arrange(desc(mean_size)) df #\u0026gt; Source: local data frame [20 x 2] #\u0026gt; #\u0026gt; scientific_name mean_size #\u0026gt; 1 Idiacanthus antrostomus 253.00000 #\u0026gt; 2 Stomias atriventer 189.25000 #\u0026gt; 3 Lestidiops ringens 98.70000 #\u0026gt; 4 Tarletonbeania crenularis 56.50000 #\u0026gt; 5 Ceratoscopelus townsendi 53.70000 #\u0026gt; 6 Stenobrachius leucopsarus 47.74538 #\u0026gt; 7 Sardinops sagax 47.00000 #\u0026gt; 8 Nannobrachium ritteri 43.30250 #\u0026gt; 9 Bathylagoides wesethi 43.09167 #\u0026gt; 10 Vinciguerria lucetia 42.00000 #\u0026gt; 11 Cyclothone acclinidens 40.80000 #\u0026gt; 12 Lipolagus ochotensis 39.72500 #\u0026gt; 13 Leuroglossus stilbius 38.35385 #\u0026gt; 14 Triphoturus mexicanus 38.21342 #\u0026gt; 15 Diaphus theta 37.88571 #\u0026gt; 16 Trachipterus altivelis 37.70000 #\u0026gt; 17 Symbolophorus californiensis 37.66000 #\u0026gt; 18 Nannobrachium regale 37.50000 #\u0026gt; 19 Merluccius productus 36.61333 #\u0026gt; 20 Argyropelecus sladeni 32.43333 Then make a cute little plot\nlibrary(\u0026#34;ggplot2\u0026#34;) ggplot(df, aes(reorder(scientific_name, mean_size), mean_size)) + geom_bar(stat = \u0026#34;identity\u0026#34;) + coord_flip() + theme_grey(base_size = 20) + labs(y = \u0026#34;Mean Size\u0026#34;, x = \u0026#34;Species\u0026#34;) ","permalink":"http://localhost:1313/2015/06/rerddap/","summary":"\u003cp\u003e\u003ca href=\"https://upwell.pfeg.noaa.gov/erddap/information.html\"\u003eERDDAP\u003c/a\u003e is a data server that gives you a simple, consistent way to download subsets of gridded and tabular scientific datasets in common file formats and make graphs and maps.  Besides it’s own \u003ca href=\"https://upwell.pfeg.noaa.gov/erddap/rest.html\"\u003eRESTful interface\u003c/a\u003e, much of which is designed based on \u003ca href=\"https://en.wikipedia.org/wiki/OPeNDAP\"\u003eOPeNDAP\u003c/a\u003e, ERDDAP can act as an OPeNDAP server and as a \u003ca href=\"https://en.wikipedia.org/wiki/Web_Map_Service\"\u003eWMS\u003c/a\u003e server for gridded data.\u003c/p\u003e\n\u003cp\u003eERDDAP is a powerful tool - in a world of heterogeneous data, it\u0026rsquo;s often hard to combine data and serve it through the same interface, with tools for querying/filtering/subsetting the data. That is exactly what ERDDAP does. Heterogeneous data sets often have some similarities, such as latitude/longitude data and usually a time component, but other variables vary widely.\u003c/p\u003e","title":"rerddap - General purpose R client for ERDDAP servers"},{"content":"iDigBio, or Integrated Digitized Biocollections, collects and provides access to species occurrence data, and associated metadata (e.g., images of specimens, when provided). They collect data from a lot of different providers. They have a nice web interface for searching, check out idigbio.org/portal/search.\nspocc is a package we\u0026rsquo;ve been working on at rOpenSci for a while now - it is a one stop shop for retrieving species ocurrence data. As new sources of species occurrence data come to our attention, and are available via a RESTful API, we incorporate them into spocc.\nI attended last week a hackathon put on by iDigBio. One of the projects I worked on was integrating iDigBio into spocc.\nWith the addition of iDigBio, we now have in spocc:\nGBIF iNaturalist USGS Bison eBird Ecoengine Vertnet iDigBio The following is a quick demo of getting iDigBio data in spocc\nInstall Get updated versions of rgbif and ridigbio first. And get leaflet to make an interactive map.\ndevtools::install_github(\u0026#34;ropensci/rgbif\u0026#34;, \u0026#34;iDigBio/ridigbio\u0026#34;, \u0026#34;rstudio/leaflet\u0026#34;) devtools::install_github(\u0026#34;ropensci/spocc\u0026#34;) library(\u0026#34;spocc\u0026#34;) Use ridigbio - the R client for iDigBio library(\u0026#34;ridigbio\u0026#34;) idig_search_records(rq = list(genus = \u0026#34;acer\u0026#34;), limit = 5) #\u0026gt; uuid #\u0026gt; 1 00041678-5df1-4a23-ba78-8c12f60af369 #\u0026gt; 2 00072caf-0f24-447f-b68e-a20299f6afc7 #\u0026gt; 3 000a6b9b-0bbd-46f6-82cb-848c30c46313 #\u0026gt; 4 001d05e0-9c86-466d-957d-e73e2ce64fbe #\u0026gt; 5 0022a2da-bc97-4bef-b2a5-b8a9944fc677 #\u0026gt; occurrenceid catalognumber family #\u0026gt; 1 urn:uuid:b275f928-5c0d-4832-ae82-fde363d8fde1 \u0026lt;NA\u0026gt; sapindaceae #\u0026gt; 2 40428b90-27a5-11e3-8d47-005056be0003 lsu00049997 aceraceae #\u0026gt; 3 02ca5aae-d8ab-492f-af10-e005b96c2295 191243 sapindaceae #\u0026gt; 4 urn:catalog:cas:ds:679715 ds679715 sapindaceae #\u0026gt; 5 b12bd651-2c6b-11e3-b3b8-180373cac83e 41898 sapindaceae #\u0026gt; genus scientificname country stateprovince geopoint.lat #\u0026gt; 1 acer acer rubrum united states illinois \u0026lt;NA\u0026gt; #\u0026gt; 2 acer acer negundo united states louisiana \u0026lt;NA\u0026gt; #\u0026gt; 3 acer \u0026lt;NA\u0026gt; united states new york \u0026lt;NA\u0026gt; #\u0026gt; 4 acer acer circinatum united states california 41.8714 #\u0026gt; 5 acer acer rubrum united states maryland 39.4197222 #\u0026gt; geopoint.lon datecollected collector #\u0026gt; 1 \u0026lt;NA\u0026gt; 1967-06-25T00:00:00+00:00 john e. ebinger #\u0026gt; 2 \u0026lt;NA\u0026gt; 1991-04-19T00:00:00+00:00 alan w. lievens #\u0026gt; 3 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; stephen f. hilfiker #\u0026gt; 4 -123.8503 1930-10-27T00:00:00+00:00 carl b. wolf #\u0026gt; 5 -77.1227778 1980-04-29T00:00:00+00:00 doweary, d. Use spocc Scientific name search Same search as above with ridigbio\nocc(query = \u0026#34;Acer\u0026#34;, from = \u0026#34;idigbio\u0026#34;, limit = 5) #\u0026gt; Searched: idigbio #\u0026gt; Occurrences - Found: 379, Returned: 5 #\u0026gt; Search type: Scientific #\u0026gt; idigbio: Acer (5) Geographic search iDigBio uses Elasticsearch syntax to define a geographic search, but all you need to do is give a numeric vector of length 4 defining a bounding box, and you\u0026rsquo;re good to go.\nbounds \u0026lt;- c(-120, 40, -100, 45) occ(from = \u0026#34;idigbio\u0026#34;, geometry = bounds, limit = 10) #\u0026gt; Searched: idigbio #\u0026gt; Occurrences - Found: 346,737, Returned: 10 #\u0026gt; Search type: Geometry W/ or W/O Coordinates Don\u0026rsquo;t pass has_coords (gives data w/ and w/o coordinates data)\nocc(query = \u0026#34;Acer\u0026#34;, from = \u0026#34;idigbio\u0026#34;, limit = 5) #\u0026gt; Searched: idigbio #\u0026gt; Occurrences - Found: 379, Returned: 5 #\u0026gt; Search type: Scientific #\u0026gt; idigbio: Acer (5) Only records with coordinates data\nocc(query = \u0026#34;Acer\u0026#34;, from = \u0026#34;idigbio\u0026#34;, limit = 5, has_coords = TRUE) #\u0026gt; Searched: idigbio #\u0026gt; Occurrences - Found: 16, Returned: 5 #\u0026gt; Search type: Scientific #\u0026gt; idigbio: Acer (5) Only records without coordinates data\nocc(query = \u0026#34;Acer\u0026#34;, from = \u0026#34;idigbio\u0026#34;, limit = 5, has_coords = FALSE) #\u0026gt; Searched: idigbio #\u0026gt; Occurrences - Found: 363, Returned: 5 #\u0026gt; Search type: Scientific #\u0026gt; idigbio: Acer (5) Make an interactive map library(\u0026#34;leaflet\u0026#34;) bounds \u0026lt;- c(-120, 40, -100, 45) leaflet(data = dat) %\u0026gt;% addTiles() %\u0026gt;% addMarkers(~longitude, ~latitude, popup = ~name) %\u0026gt;% addRectangles( lng1 = bounds[1], lat1 = bounds[4], lng2 = bounds[3], lat2 = bounds[2], fillColor = \u0026#34;transparent\u0026#34; ) ","permalink":"http://localhost:1313/2015/06/idigbio-in-spocc/","summary":"\u003cp\u003e\u003ca href=\"https://www.idigbio.org/\"\u003eiDigBio\u003c/a\u003e, or \u003cem\u003eIntegrated Digitized Biocollections\u003c/em\u003e, collects and provides access to species occurrence data, and associated metadata (e.g., images of specimens, when provided). They collect data from \u003ca href=\"https://www.idigbio.org/portal/publishers\"\u003ea lot of different providers\u003c/a\u003e. They have a nice web interface for searching, check out \u003ca href=\"https://www.idigbio.org/portal/search\"\u003eidigbio.org/portal/search\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003espocc\u003c/code\u003e is a package we\u0026rsquo;ve been working on at \u003ca href=\"https://ropensci.org/\"\u003erOpenSci\u003c/a\u003e for a while now - it is a one stop shop for retrieving species ocurrence data. As new sources of species occurrence data come to our attention, and are available via a RESTful API, we incorporate them into \u003ccode\u003espocc\u003c/code\u003e.\u003c/p\u003e","title":"iDigBio - a new data source in spocc"},{"content":"lawn is an R wrapper for the Javascript library turf.js for advanced geospatial analysis. In addition, we have a few functions to interface with the geojson-random Javascript library.\nlawn includes traditional spatial operations, helper functions for creating GeoJSON data, and data classification and statistics tools.\nThere is an additional helper function (see view()) in this package to help visualize data with interactive maps via the leaflet package (https://github.com/rstudio/leaflet). Note that leaflet is not required to install lawn - it\u0026rsquo;s in Suggests, not Imports or Depends.\nUse cases for this package include (but not limited to, obs.) the following (all below assumes GeoJSON format):\nCreate random spatial data. Convert among spatial data types (e.g. Polygon to FeatureCollection) Transform objects, including merging many, simplifying, calculating hulls, etc. Measuring objects Performing interpolation of objects Aggregating data (aka properties) associated with objects Install Stable lawn version from CRAN - this should fetch leaflet, which is not on CRAN, but in a drat repo (let me know if it doesn\u0026rsquo;t)\ninstall.packages(\u0026#34;lawn\u0026#34;) Or, the development version from Github\ndevtools::install_github(\u0026#34;ropensci/lawn\u0026#34;) library(\u0026#34;lawn\u0026#34;) view lawn includes a tiny helper function for visualizing geojson. For examples below, we\u0026rsquo;ll make liberal use of the lawn::view() function to visualize what it is the heck we\u0026rsquo;re doing. mkay, lets roll\u0026hellip;\nWe\u0026rsquo;ve tried to make view() work with as many inputs as possible, from class character containing json to the class json from the jsonlite package, to the class list to all of the GeoJSON outputs from functions in lawn.\nview(lawn_data$points_average) Here, we sample at random two points from the same dataset just viewed.\nlawn_sample(lawn_data$points_average, 2) %\u0026gt;% view() Make some geojson data Point\nlawn_point(c(-74.5, 40)) #\u0026gt; $type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $geometry #\u0026gt; $geometry$type #\u0026gt; [1] \u0026#34;Point\u0026#34; #\u0026gt; #\u0026gt; $geometry$coordinates #\u0026gt; [1] -74.5 40.0 #\u0026gt; #\u0026gt; #\u0026gt; $properties #\u0026gt; named list() #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;point\u0026#34; lawn_point(c(-74.5, 40)) %\u0026gt;% view Polygon\nrings \u0026lt;- list(list( c(-2.275543, 53.464547), c(-2.275543, 53.489271), c(-2.215118, 53.489271), c(-2.215118, 53.464547), c(-2.275543, 53.464547) )) lawn_polygon(rings) #\u0026gt; $type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $geometry #\u0026gt; $geometry$type #\u0026gt; [1] \u0026#34;Polygon\u0026#34; #\u0026gt; #\u0026gt; $geometry$coordinates #\u0026gt; , , 1 #\u0026gt; #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] -2.275543 -2.275543 -2.215118 -2.215118 -2.275543 #\u0026gt; #\u0026gt; , , 2 #\u0026gt; #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 53.46455 53.48927 53.48927 53.46455 53.46455 #\u0026gt; #\u0026gt; #\u0026gt; #\u0026gt; $properties #\u0026gt; named list() #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;polygon\u0026#34; lawn_polygon(rings) %\u0026gt;% view Random set of points\nlawn_random(n = 2) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; type geometry.type geometry.coordinates #\u0026gt; 1 Feature Point -137.46327, -63.46154 #\u0026gt; 2 Feature Point -110.68426, 83.10533 #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;featurecollection\u0026#34; lawn_random(n = 5) %\u0026gt;% view Or, use a different Javascript library (geojson-random) to create random features.\nPositions\ngr_position() #\u0026gt; [1] -179.77996 45.99018 Points\ngr_point(2) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; type geometry.type geometry.coordinates #\u0026gt; 1 Feature Point 5.83895, -27.77218 #\u0026gt; 2 Feature Point 78.50177, 14.95840 #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;featurecollection\u0026#34; gr_point(2) %\u0026gt;% view Polygons\ngr_polygon(n = 1, vertices = 5, max_radial_length = 5) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; type geometry.type #\u0026gt; 1 Feature Polygon #\u0026gt; geometry.coordinates #\u0026gt; 1 67.58827, 67.68551, 67.00091, 66.70156, 65.72578, 67.58827, -42.11340, -42.69850, -43.54866, -42.42758, -41.76731, -42.11340 #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;featurecollection\u0026#34; gr_polygon(n = 1, vertices = 5, max_radial_length = 5) %\u0026gt;% view count Count number of points within polygons, appends a new field to properties (see the count field)\nlawn_count(polygons = lawn_data$polygons_count, points = lawn_data$points_count) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; type pt_count geometry.type #\u0026gt; 1 Feature 2 Polygon #\u0026gt; 2 Feature 0 Polygon #\u0026gt; geometry.coordinates #\u0026gt; 1 -112.07239, -112.07239, -112.02810, -112.02810, -112.07239, 46.58659, 46.61761, 46.61761, 46.58659, 46.58659 #\u0026gt; 2 -112.02398, -112.02398, -111.96613, -111.96613, -112.02398, 46.57043, 46.61502, 46.61502, 46.57043, 46.57043 #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;featurecollection\u0026#34; distance Define two points\nfrom \u0026lt;- \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, \u0026#34;properties\u0026#34;: {}, \u0026#34;geometry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34;, \u0026#34;coordinates\u0026#34;: [-75.343, 39.984] } }\u0026#39; to \u0026lt;- \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, \u0026#34;properties\u0026#34;: {}, \u0026#34;geometry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34;, \u0026#34;coordinates\u0026#34;: [-75.534, 39.123] } }\u0026#39; Calculate distance, default units is kilometers (default output: km)\nlawn_distance(from, to) #\u0026gt; [1] 97.15958 sample from a FeatureCollection dat \u0026lt;- lawn_data$points_average cat(dat) #\u0026gt; { #\u0026gt; \u0026#34;type\u0026#34;: \u0026#34;FeatureCollection\u0026#34;, #\u0026gt; \u0026#34;features\u0026#34;: [ #\u0026gt; { #\u0026gt; \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, #\u0026gt; \u0026#34;properties\u0026#34;: { #\u0026gt; \u0026#34;population\u0026#34;: 200 #\u0026gt; }, #\u0026gt; \u0026#34;geometry\u0026#34;: { #\u0026gt; \u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34;, ... Sample 2 points at random\nlawn_sample(dat, 2) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; type population geometry.type geometry.coordinates #\u0026gt; 1 Feature 200 Point 10.80643, 59.90891 #\u0026gt; 2 Feature 600 Point 10.71579, 59.90478 #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;featurecollection\u0026#34; extent Calculates the extent of all input features in a FeatureCollection, and returns a bounding box.\nlawn_extent(lawn_data$points_average) #\u0026gt; [1] 10.71579 59.90478 10.80643 59.93162 buffer Calculates a buffer for input features for a given radius.\ndat \u0026lt;- \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, \u0026#34;properties\u0026#34;: {}, \u0026#34;geometry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [[ [-112.072391,46.586591], [-112.072391,46.61761], [-112.028102,46.61761], [-112.028102,46.586591], [-112.072391,46.586591] ]] } }\u0026#39; view(dat) lawn_buffer(dat, 1, \u0026#34;miles\u0026#34;) %\u0026gt;% view Union polygons together poly1 \u0026lt;- \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#0f0\u0026#34; }, \u0026#34;geometry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [[ [-122.801742, 45.48565], [-122.801742, 45.60491], [-122.584762, 45.60491], [-122.584762, 45.48565], [-122.801742, 45.48565] ]] } }\u0026#39; poly2 \u0026lt;- \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;#00f\u0026#34; }, \u0026#34;geometry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Polygon\u0026#34;, \u0026#34;coordinates\u0026#34;: [[ [-122.520217, 45.535693], [-122.64038, 45.553967], [-122.720031, 45.526554], [-122.669906, 45.507309], [-122.723464, 45.446643], [-122.532577, 45.408574], [-122.487258, 45.477466], [-122.520217, 45.535693] ]] } }\u0026#39; view(poly1) view(poly2) Visualize union-ed polygons\nlawn_union(poly1, poly2) %\u0026gt;% view See also lawn_merge() and lawn_intersect().\nlint input geojson For most functions, you can lint your input geojson data to make sure it is proper geojson. We use the javascript library geojsonhint. See the lint parameter.\nGood GeoJSON\ndat \u0026lt;- \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;FeatureCollection\u0026#34;, \u0026#34;features\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;population\u0026#34;: 200 }, \u0026#34;geometry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34;, \u0026#34;coordinates\u0026#34;: [10.724029, 59.926807] } } ] }\u0026#39; lawn_extent(dat) #\u0026gt; [1] 10.72403 59.92681 10.72403 59.92681 Bad GeoJSON\ndat \u0026lt;- \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;FeatureCollection\u0026#34;, \u0026#34;features\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Feature\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;population\u0026#34;: 200 }, \u0026#34;geometry\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34; } } ] }\u0026#39; lawn_extent(dat, lint = TRUE) #\u0026gt; Error: Line 1 - \u0026#34;coordinates\u0026#34; property required To do As Turf.js changes, we\u0026rsquo;ll update lawn Performance improvements. We realize that this package is slower than the C based rgdal/rgeos - we are looking into ways to increaes performance to get closer to the performance of those packages. ","permalink":"http://localhost:1313/2015/05/mow-the-lawn/","summary":"\u003cp\u003e\u003ccode\u003elawn\u003c/code\u003e is an R wrapper for the Javascript library \u003ca href=\"http://turfjs.org/\"\u003eturf.js\u003c/a\u003e for advanced geospatial analysis. In addition, we have a few functions to interface with the \u003ca href=\"https://github.com/mapbox/geojson-random\"\u003egeojson-random\u003c/a\u003e Javascript library.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003elawn\u003c/code\u003e includes traditional spatial operations, helper functions for creating GeoJSON data, and data classification and statistics tools.\u003c/p\u003e\n\u003cp\u003eThere is an additional helper function (see \u003ccode\u003eview()\u003c/code\u003e) in this package to help visualize data with interactive maps via the \u003ccode\u003eleaflet\u003c/code\u003e package (\u003ca href=\"https://github.com/rstudio/leaflet\"\u003ehttps://github.com/rstudio/leaflet\u003c/a\u003e). Note that \u003ccode\u003eleaflet\u003c/code\u003e is not required to install \u003ccode\u003elawn\u003c/code\u003e - it\u0026rsquo;s in Suggests, not Imports or Depends.\u003c/p\u003e","title":"lawn - a new package to do geospatial analysis"},{"content":"openadds talks to Openaddresses.io. a run down of its things:\nInstall devtools::install_github(\u0026#34;sckott/openadds\u0026#34;) library(\u0026#34;openadds\u0026#34;) List datasets Scrapes links to datasets from the openaddresses site\ndat \u0026lt;- oa_list() dat[2:6] #\u0026gt; [1] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150511/au-tas-launceston.csv\u0026#34; #\u0026gt; [2] \u0026#34;https://s3.amazonaws.com/data.openaddresses.io/20141127/au-victoria.zip\u0026#34; #\u0026gt; [3] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150511/be-flanders.zip\u0026#34; #\u0026gt; [4] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150417/ca-ab-calgary.zip\u0026#34; #\u0026gt; [5] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150511/ca-ab-grande_prairie.zip\u0026#34; Search for datasets Uses oa_list() internally, then searches through columns requested.\noa_search(country = \u0026#34;us\u0026#34;, state = \u0026#34;ca\u0026#34;) #\u0026gt; Source: local data frame [68 x 5] #\u0026gt; #\u0026gt; country state city ext #\u0026gt; 1 us ca san_mateo_county .zip #\u0026gt; 2 us ca alameda_county .zip #\u0026gt; 3 us ca alameda_county .zip #\u0026gt; 4 us ca amador .zip #\u0026gt; 5 us ca amador .zip #\u0026gt; 6 us ca bakersfield .zip #\u0026gt; 7 us ca bakersfield .zip #\u0026gt; 8 us ca berkeley .zip #\u0026gt; 9 us ca berkeley .zip #\u0026gt; 10 us ca butte_county .zip #\u0026gt; .. ... ... ... ... #\u0026gt; Variables not shown: url (chr) Get data Passing in a URL\n(out1 \u0026lt;- oa_get(dat[5])) #\u0026gt; \u0026lt;Openaddresses data\u0026gt; ~/.openadds/ca-ab-calgary.zip #\u0026gt; Dimensions [350962, 13] #\u0026gt; #\u0026gt; OBJECTID ADDRESS_TY ADDRESS STREET_NAM STREET_TYP #\u0026gt; 0 757023 Parcel 249 SAGE MEADOWS CI NW SAGE MEADOWS CI #\u0026gt; 1 757022 Parcel 2506 17 ST SE 17 ST #\u0026gt; 2 757021 Parcel 305 EVANSPARK GD NW EVANSPARK GD #\u0026gt; 3 757020 Parcel 321 EVANSPARK GD NW EVANSPARK GD #\u0026gt; 4 757019 Parcel 204 EVANSBROOKE LD NW EVANSBROOKE LD #\u0026gt; 5 757018 Parcel 200 EVANSBROOKE LD NW EVANSBROOKE LD #\u0026gt; 6 757017 Parcel 219 HIDDEN VALLEY LD NW HIDDEN VALLEY LD #\u0026gt; 7 757016 Parcel 211 HIDDEN VALLEY LD NW HIDDEN VALLEY LD #\u0026gt; 8 757015 Parcel 364 HIDDEN VALLEY LD NW HIDDEN VALLEY LD #\u0026gt; 9 757014 Parcel 348 HIDDEN VALLEY LD NW HIDDEN VALLEY LD #\u0026gt; .. ... ... ... ... ... #\u0026gt; Variables not shown: STREET_QUA (fctr), HOUSE_NUMB (int), HOUSE_ALPH #\u0026gt; (fctr), SUITE_NUMB (int), SUITE_ALPH (fctr), LONGITUDE (dbl), #\u0026gt; LATITUDE (dbl), COMM_NAME (fctr) First getting URL for dataset through as_openadd(), then passing to oa_get()\n(x \u0026lt;- as_openadd(\u0026#34;us\u0026#34;, \u0026#34;nm\u0026#34;, \u0026#34;hidalgo\u0026#34;)) #\u0026gt; \u0026lt;\u0026lt;OpenAddreses\u0026gt;\u0026gt; #\u0026gt; \u0026lt;\u0026lt;country\u0026gt;\u0026gt; us #\u0026gt; \u0026lt;\u0026lt;state\u0026gt;\u0026gt; nm #\u0026gt; \u0026lt;\u0026lt;city\u0026gt;\u0026gt; hidalgo #\u0026gt; \u0026lt;\u0026lt;extension\u0026gt;\u0026gt; .csv oa_get(x) #\u0026gt; \u0026lt;Openaddresses data\u0026gt; ~/.openadds/us-nm-hidalgo.csv #\u0026gt; Dimensions [170659, 37] #\u0026gt; #\u0026gt; OBJECTID Shape ADD_NUM ADD_SUF PRE_MOD PRE_DIR PRE_TYPE ST_NAME #\u0026gt; 1 1 NA 422 S 2ND #\u0026gt; 2 2 NA 1413 S 4TH #\u0026gt; 3 3 NA 412 E CHAMPION #\u0026gt; 4 4 NA 110 E SAMANO #\u0026gt; 5 5 NA 2608 W FREDDY GONZALEZ #\u0026gt; 6 6 NA 2604 W FREDDY GONZALEZ #\u0026gt; 7 7 NA 1123 W FAY #\u0026gt; 8 8 NA 417 S 2ND #\u0026gt; 9 9 NA 4551 E TEXAS #\u0026gt; 10 10 NA 810 DRIFTWOOD #\u0026gt; .. ... ... ... ... ... ... ... ... #\u0026gt; Variables not shown: ST_TYPE (chr), POS_DIR (chr), POS_MOD (chr), ESN #\u0026gt; (int), MSAG_COMM (chr), PARCEL_ID (chr), PLACE_TYPE (chr), LANDMARK #\u0026gt; (chr), BUILDING (chr), UNIT (chr), ROOM (chr), FLOOR (int), LOC_NOTES #\u0026gt; (chr), ST_ALIAS (chr), FULL_ADDR (chr), ZIP (chr), POSTAL_COM (chr), #\u0026gt; MUNICIPAL (chr), COUNTY (chr), STATE (chr), SOURCE (chr), REGION #\u0026gt; (chr), EXCH (chr), LAT (dbl), LONG (dbl), PICTURE (chr), OA:x (dbl), #\u0026gt; OA:y (dbl), OA:geom (chr) Combine multiple datasets combine attemps to guess lat/long and address columns, but definitely more work to do to make this work for most cases. Lat/long and address columns vary among every dataset - some datasets have no lat/long data, some have no address data.\nout2 \u0026lt;- oa_get(dat[32]) (alldat \u0026lt;- oa_combine(out1, out2)) #\u0026gt; Source: local data frame [418,623 x 4] #\u0026gt; #\u0026gt; lon lat address dataset #\u0026gt; 1 -114.1303 51.17188 249 SAGE MEADOWS CI NW ca-ab-calgary.zip #\u0026gt; 2 -114.0190 51.03168 2506 17 ST SE ca-ab-calgary.zip #\u0026gt; 3 -114.1175 51.17497 305 EVANSPARK GD NW ca-ab-calgary.zip #\u0026gt; 4 -114.1175 51.17461 321 EVANSPARK GD NW ca-ab-calgary.zip #\u0026gt; 5 -114.1212 51.16268 204 EVANSBROOKE LD NW ca-ab-calgary.zip #\u0026gt; 6 -114.1213 51.16264 200 EVANSBROOKE LD NW ca-ab-calgary.zip #\u0026gt; 7 -114.1107 51.14784 219 HIDDEN VALLEY LD NW ca-ab-calgary.zip #\u0026gt; 8 -114.1108 51.14768 211 HIDDEN VALLEY LD NW ca-ab-calgary.zip #\u0026gt; 9 -114.1121 51.14780 364 HIDDEN VALLEY LD NW ca-ab-calgary.zip #\u0026gt; 10 -114.1117 51.14800 348 HIDDEN VALLEY LD NW ca-ab-calgary.zip #\u0026gt; .. ... ... ... ... Map data Get some data\n(out \u0026lt;- oa_get(dat[400])) #\u0026gt; \u0026lt;Openaddresses data\u0026gt; ~/.openadds/us-ca-sonoma_county.zip #\u0026gt; Dimensions [217243, 5] #\u0026gt; #\u0026gt; LON LAT NUMBER STREET POSTCODE #\u0026gt; 1 -122.5327 38.29779 3771 A Cory Lane NA #\u0026gt; 2 -122.5422 38.30354 18752 White Oak Drive NA #\u0026gt; 3 -122.5412 38.30327 18749 White Oak Drive NA #\u0026gt; 4 -122.3997 38.26122 3552 Napa Road NA #\u0026gt; 5 -122.5425 38.30404 3998 White Oak Court NA #\u0026gt; 6 -122.5429 38.30434 4026 White Oak Court NA #\u0026gt; 7 -122.5430 38.30505 4039 White Oak Court NA #\u0026gt; 8 -122.5417 38.30504 4017 White Oak Court NA #\u0026gt; 9 -122.5409 38.30436 18702 White Oak Drive NA #\u0026gt; 10 -122.5403 38.30392 18684 White Oak Drive NA #\u0026gt; .. ... ... ... ... ... Make an interactive map (not all data)\nlibrary(\u0026#34;leaflet\u0026#34;) x \u0026lt;- oa_get(oa_search(country = \u0026#34;us\u0026#34;, city = \u0026#34;boulder\u0026#34;)[1,]$url) y \u0026lt;- oa_get(oa_search(country = \u0026#34;us\u0026#34;, city = \u0026#34;gunnison\u0026#34;)[1,]$url) oa_combine(x, y) %\u0026gt;% leaflet() %\u0026gt;% addTiles() %\u0026gt;% addCircles(lat = ~lat, lng = ~lon, popup = ~address) To do Surely there are many datasets that won\u0026rsquo;t work in oa_combine() - gotta go through many more. An easy viz function wrapping leaflet Since you can get a lot of spatial data quickly, easy way to visualize big data, maybe marker clusters? ","permalink":"http://localhost:1313/2015/05/openadds/","summary":"\u003cp\u003e\u003ccode\u003eopenadds\u003c/code\u003e talks to \u003ca href=\"https://openaddresses.io/\"\u003eOpenaddresses.io\u003c/a\u003e. a run down of its things:\u003c/p\u003e\n\u003ch2 id=\"install\"\u003eInstall\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevtools\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall_github\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sckott/openadds\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;openadds\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"list-datasets\"\u003eList datasets\u003c/h2\u003e\n\u003cp\u003eScrapes links to datasets from the openaddresses site\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eoa_list\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat[2\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [1] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150511/au-tas-launceston.csv\u0026#34;   \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [2] \u0026#34;https://s3.amazonaws.com/data.openaddresses.io/20141127/au-victoria.zip\u0026#34;         \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [3] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150511/be-flanders.zip\u0026#34;         \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [4] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150417/ca-ab-calgary.zip\u0026#34;       \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [5] \u0026#34;https://data.openaddresses.io.s3.amazonaws.com/20150511/ca-ab-grande_prairie.zip\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"search-for-datasets\"\u003eSearch for datasets\u003c/h2\u003e\n\u003cp\u003eUses \u003ccode\u003eoa_list()\u003c/code\u003e internally, then searches through columns requested.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eoa_search\u003c/span\u003e(country \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;us\u0026#34;\u003c/span\u003e, state \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ca\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; Source: local data frame [68 x 5]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;    country state             city  ext\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 1       us    ca san_mateo_county .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 2       us    ca   alameda_county .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 3       us    ca   alameda_county .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 4       us    ca           amador .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 5       us    ca           amador .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 6       us    ca      bakersfield .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 7       us    ca      bakersfield .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 8       us    ca         berkeley .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 9       us    ca         berkeley .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 10      us    ca     butte_county .zip\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; ..     ...   ...              ...  ...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; Variables not shown: url (chr)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"get-data\"\u003eGet data\u003c/h2\u003e\n\u003cp\u003ePassing in a URL\u003c/p\u003e","title":"openadds - open addresses client"},{"content":"geojsonio converts geographic data to GeoJSON and TopoJSON formats - though the focus is mostly on GeoJSON\nFor those not familiar GeoJSON it is a format for encoding a variety of geographic data structures. GeoJSON supports the following geometry types: Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection. These geometry types are also found in well known text (WKT), and have equivalents in R\u0026rsquo;s spatial classes. Read the spec for more detailed information.\nOther great geojson resources:\nGeoJSON lint - lint your geojson - https://geojsonlint.com/ GeoJSON.io - make maps with geojson input or draw maps and get geojson - https://geojson.io/ Functions in this package are organized first around what you\u0026rsquo;re working with or want to get, geojson or topojson, then convert to or read from various formats:\ngeojson_list() - convert to GeoJSON as R list format geojson_json() - convert to GeoJSON as json geojson_read()/topojson_read() - read a GeoJSON/TopoJSON file from file path or URL geojson_write() - write a GeoJSON file locally (no write TopoJSON yet) Each of the above functions have methods for various objects/classes, including numeric, data.frame, list, SpatialPolygons, SpatialLines, SpatialPoints, etc. (including the classes in rgeos)\nUse cases for this package include (but not limited to, obs.) the following:\nGet data in GeoJSON json format, and you want to get it into a list in R. Get data into GeoJSON format to use downstream to make a interactive map in R (e.g., with leaflet) or in another context (e.g., using javascript with mapbox/leaflet) Data is in a data.frame/matrix/list and you want to make GeoJSON format data. Data is in one of the many spatial classes (e.g., SpatialPoints) and you want GeoJSON You need to add styling to your data - can do with this package for certain data types. You want to check that your GeoJSON data is valid - two ways to do it in geojsonio. Combine objects together (e.g., a point and a line), either from two geo_list objects, or two json objects. See ?geojson-add Install See the github repo for notes about dependencies https://github.com/ropensci/geojsonio#install\nCRAN version or the dev version from GitHub\ninstall.packages(\u0026#34;geojsonio\u0026#34;) devtools::install_github(\u0026#34;sckott/geojsonio\u0026#34;) library(\u0026#34;geojsonio\u0026#34;) GeoJSON Convert various formats to geojson From a numeric vector of length 2\nas json\ngeojson_json(c(32.45, -99.74)) #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;,\u0026#34;features\u0026#34;:[{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[32.45,-99.74]},\u0026#34;properties\u0026#34;:{}}]} as a list\ngeojson_list(c(32.45, -99.74)) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; $features[[1]] #\u0026gt; $features[[1]]$type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$geometry #\u0026gt; $features[[1]]$geometry$type ... From a data.frame\nas json\ngeojson_json(us_cities[1:2, ], lat = \u0026#39;lat\u0026#39;, lon = \u0026#39;long\u0026#39;) #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;,\u0026#34;features\u0026#34;:[{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[-99.74,32.45]},\u0026#34;properties\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Abilene TX\u0026#34;,\u0026#34;country.etc\u0026#34;:\u0026#34;TX\u0026#34;,\u0026#34;pop\u0026#34;:\u0026#34;113888\u0026#34;,\u0026#34;capital\u0026#34;:\u0026#34;0\u0026#34;}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[-81.52,41.08]},\u0026#34;properties\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Akron OH\u0026#34;,\u0026#34;country.etc\u0026#34;:\u0026#34;OH\u0026#34;,\u0026#34;pop\u0026#34;:\u0026#34;206634\u0026#34;,\u0026#34;capital\u0026#34;:\u0026#34;0\u0026#34;}}]} as a list\ngeojson_list(us_cities[1:2, ], lat = \u0026#39;lat\u0026#39;, lon = \u0026#39;long\u0026#39;) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; $features[[1]] #\u0026gt; $features[[1]]$type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$geometry #\u0026gt; $features[[1]]$geometry$type ... From SpatialPolygons class\nlibrary(\u0026#39;sp\u0026#39;) poly1 \u0026lt;- Polygons(list(Polygon(cbind(c(-100, -90, -85, -100), c(40, 50, 45, 40)))), \u0026#34;1\u0026#34;) poly2 \u0026lt;- Polygons(list(Polygon(cbind(c(-90, -80, -75, -90), c(30, 40, 35, 30)))), \u0026#34;2\u0026#34;) (sp_poly \u0026lt;- SpatialPolygons(list(poly1, poly2), 1:2)) #\u0026gt; An object of class \u0026#34;SpatialPolygons\u0026#34; #\u0026gt; Slot \u0026#34;polygons\u0026#34;: #\u0026gt; [[1]] #\u0026gt; An object of class \u0026#34;Polygons\u0026#34; #\u0026gt; Slot \u0026#34;Polygons\u0026#34;: #\u0026gt; [[1]] #\u0026gt; An object of class \u0026#34;Polygon\u0026#34; #\u0026gt; Slot \u0026#34;labpt\u0026#34;: #\u0026gt; [1] -91.66667 45.00000 #\u0026gt; ... to json\ngeojson_json(sp_poly) #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;,\u0026#34;features\u0026#34;:[{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;id\u0026#34;:1,\u0026#34;properties\u0026#34;:{\u0026#34;dummy\u0026#34;:0},\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Polygon\u0026#34;,\u0026#34;coordinates\u0026#34;:[[[-100,40],[-90,50],[-85,45],[-100,40]]]}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;id\u0026#34;:2,\u0026#34;properties\u0026#34;:{\u0026#34;dummy\u0026#34;:0},\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Polygon\u0026#34;,\u0026#34;coordinates\u0026#34;:[[[-90,30],[-80,40],[-75,35],[-90,30]]]}}]} to a list\ngeojson_list(sp_poly) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; $features[[1]] #\u0026gt; $features[[1]]$type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$id #\u0026gt; [1] 1 ... From SpatialPoints class\nx \u0026lt;- c(1, 2, 3, 4, 5) y \u0026lt;- c(3, 2, 5, 1, 4) (s \u0026lt;- SpatialPoints(cbind(x, y))) #\u0026gt; SpatialPoints: #\u0026gt; x y #\u0026gt; [1,] 1 3 #\u0026gt; [2,] 2 2 #\u0026gt; [3,] 3 5 #\u0026gt; [4,] 4 1 #\u0026gt; [5,] 5 4 #\u0026gt; Coordinate Reference System (CRS) arguments: NA to json\ngeojson_json(s) #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;,\u0026#34;features\u0026#34;:[{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;id\u0026#34;:1,\u0026#34;properties\u0026#34;:{\u0026#34;dat\u0026#34;:1},\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[1,3]}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;id\u0026#34;:2,\u0026#34;properties\u0026#34;:{\u0026#34;dat\u0026#34;:2},\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[2,2]}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;id\u0026#34;:3,\u0026#34;properties\u0026#34;:{\u0026#34;dat\u0026#34;:3},\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[3,5]}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;id\u0026#34;:4,\u0026#34;properties\u0026#34;:{\u0026#34;dat\u0026#34;:4},\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[4,1]}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;id\u0026#34;:5,\u0026#34;properties\u0026#34;:{\u0026#34;dat\u0026#34;:5},\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[5,4]}}]} to a list\ngeojson_list(s) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; $features[[1]] #\u0026gt; $features[[1]]$type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$id #\u0026gt; [1] 1 ... Combine objects geo_list + geo_list\nNote: geo_list is the output type from geojson_list(), it\u0026rsquo;s just a list with a class attached so we know it\u0026rsquo;s geojson :)\nvec \u0026lt;- c(-99.74, 32.45) a \u0026lt;- geojson_list(vec) vecs \u0026lt;- list(c(100.0, 0.0), c(101.0, 0.0), c(100.0, 0.0)) b \u0026lt;- geojson_list(vecs, geometry = \u0026#34;polygon\u0026#34;) a + b #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; $features[[1]] #\u0026gt; $features[[1]]$type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$geometry #\u0026gt; $features[[1]]$geometry$type ... json + json\nc \u0026lt;- geojson_json(c(-99.74, 32.45)) vecs \u0026lt;- list(c(100.0, 0.0), c(101.0, 0.0), c(101.0, 1.0), c(100.0, 1.0), c(100.0, 0.0)) d \u0026lt;- geojson_json(vecs, geometry = \u0026#34;polygon\u0026#34;) c + d #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;,\u0026#34;features\u0026#34;:[{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[-99.74,32.45]},\u0026#34;properties\u0026#34;:{}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Polygon\u0026#34;,\u0026#34;coordinates\u0026#34;:[[[100,0],[101,0],[101,1],[100,1],[100,0]]]},\u0026#34;properties\u0026#34;:[]}]} Write geojson geojson_write(us_cities[1:2, ], lat = \u0026#39;lat\u0026#39;, lon = \u0026#39;long\u0026#39;) #\u0026gt; \u0026lt;geojson\u0026gt; #\u0026gt; Path: myfile.geojson #\u0026gt; From class: data.frame Topojson In the current version of this package you can read topojson. Writing topojson was in this package, but is gone for now - will come back later as in interface to topojson via V8.\nRead from a file\nfile \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;us_states.topojson\u0026#34;, package = \u0026#34;geojsonio\u0026#34;) out \u0026lt;- topojson_read(file) Read from a URL\nurl \u0026lt;- \u0026#34;https://raw.githubusercontent.com/shawnbot/d3-cartogram/master/data/us-states.topojson\u0026#34; out \u0026lt;- topojson_read(url) Lint geojson There are two ways to do this in this package.\nlint, locally Uses the javascript library geojsonhint from Mapbox. We\u0026rsquo;re running this locally via the V8 package.\nGood\nlint(\u0026#39;{\u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34;, \u0026#34;coordinates\u0026#34;: [-100, 80]}\u0026#39;) #\u0026gt; [1] \u0026#34;valid\u0026#34; Bad\nlint(\u0026#39;{\u0026#34;type\u0026#34;: \u0026#34;Rhombus\u0026#34;, \u0026#34;coordinates\u0026#34;: [[1, 2], [3, 4], [5, 6]]}\u0026#39;) #\u0026gt; $message #\u0026gt; [1] \u0026#34;The type Rhombus is unknown\u0026#34; #\u0026gt; #\u0026gt; $line #\u0026gt; [1] 1 validate, with a web service Uses the web service at https://geojsonlint.com/\nGood\nvalidate(\u0026#39;{\u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34;, \u0026#34;coordinates\u0026#34;: [-100, 80]}\u0026#39;) #\u0026gt; $status #\u0026gt; [1] \u0026#34;ok\u0026#34; Bad\nvalidate(\u0026#39;{\u0026#34;type\u0026#34;: \u0026#34;Rhombus\u0026#34;, \u0026#34;coordinates\u0026#34;: [[1, 2], [3, 4], [5, 6]]}\u0026#39;) #\u0026gt; $message #\u0026gt; [1] \u0026#34;\\\u0026#34;Rhombus\\\u0026#34; is not a valid GeoJSON type.\u0026#34; #\u0026gt; #\u0026gt; $status #\u0026gt; [1] \u0026#34;error\u0026#34; To do I\u0026rsquo;d like to replace rgdal with javascript libraries to read from various file types (kml, shp, etc.) and convert to geojson. This is in development, and will come in the next version of this package most likely. This should make installation a bit easier as we won\u0026rsquo;t have to depend on rgdal and GDAL Performance improvements. Some operations already use the gdal or geos C libraries, so are quite fast, though the round trip to disk and back does take significant time. I\u0026rsquo;d like to speed this up. More input types. We already have operations (json, list, etc.) for lots of input types (data.frame, list, sp classes), but likely there will be more added. Most likely add functions topojson_list(), topojson_json() ","permalink":"http://localhost:1313/2015/04/geojson-io/","summary":"\u003cp\u003e\u003ccode\u003egeojsonio\u003c/code\u003e converts geographic data to GeoJSON and TopoJSON formats - though the focus is mostly on GeoJSON\u003c/p\u003e\n\u003cp\u003eFor those not familiar GeoJSON it is a format for encoding a variety of geographic data structures. GeoJSON supports the following geometry types: Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection. These geometry types are also found in \u003ca href=\"https://en.wikipedia.org/wiki/Well-known_text\"\u003ewell known text (WKT)\u003c/a\u003e, and have equivalents in R\u0026rsquo;s spatial classes. Read the \u003ca href=\"https://geojson.org/geojson-spec.html\"\u003espec\u003c/a\u003e for more detailed information.\u003c/p\u003e","title":"geojsonio - a new package to do geojson things"},{"content":"Inspired by httpie, a Python command line client as a sort of drop in replacement for curl, I am playing around with something similar-ish in R, at least in spirit. I started a little R pkg called httsnap with the following ideas:\nThe web is increasingly a JSON world, so set content-type and accept headers to applications/json by default The workflow follows logically, or at least should, from, hey, I got this url, to i need to add some options, to execute request Whenever possible, transform output to data.frame\u0026rsquo;s - facilitating downstream manipulation via dplyr, etc. Do GET requests by default. Specify a different type if you don\u0026rsquo;t want GET. Some functionality does GET by default, though in some cases you need to specify GET You can use non-standard evaluation to easily pass in query parameters without worrying about \u0026amp;\u0026rsquo;s, URL escaping, etc. (see Query()) Same for body params (see Body()) Install Install and load httsnap\ndevtools::install_github(\u0026#34;sckott/httsnap\u0026#34;) library(\u0026#34;httsnap\u0026#34;) library(\u0026#34;dplyr\u0026#34;) Functions so far Get - GET request Query - add query parameters Authenticate - add authentication details Progress - add progress bar Timeout - add a timeout User_agent - add a user agent Verbose - give verbose output Body - add a body h - add headers by key-value pair These are named to avoid conflict with httr\nIntro A simple GET request\n\u0026#34;https://httpbin.org/get\u0026#34; %\u0026gt;% Get() #\u0026gt; $args #\u0026gt; named list() #\u0026gt; #\u0026gt; $headers #\u0026gt; $headers$Accept #\u0026gt; [1] \u0026#34;application/json, text/xml, application/xml, */*\u0026#34; #\u0026gt; #\u0026gt; $headers$`Accept-Encoding` #\u0026gt; [1] \u0026#34;gzip\u0026#34; #\u0026gt; #\u0026gt; $headers$Host #\u0026gt; [1] \u0026#34;httpbin.org\u0026#34; #\u0026gt; #\u0026gt; $headers$`User-Agent` #\u0026gt; [1] \u0026#34;curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $origin #\u0026gt; [1] \u0026#34;24.21.209.71\u0026#34; #\u0026gt; #\u0026gt; $url #\u0026gt; [1] \u0026#34;https://httpbin.org/get\u0026#34; You\u0026rsquo;ll notice that Get() doesn\u0026rsquo;t just get the response, but also checks for whether it was a good response (the HTTP status code), and extracts the data.\nOr you can just pass the URL into the function itself\nGet(\u0026#34;https://httpbin.org/get\u0026#34;) #\u0026gt; $args #\u0026gt; named list() #\u0026gt; #\u0026gt; $headers #\u0026gt; $headers$Accept #\u0026gt; [1] \u0026#34;application/json, text/xml, application/xml, */*\u0026#34; #\u0026gt; #\u0026gt; $headers$`Accept-Encoding` #\u0026gt; [1] \u0026#34;gzip\u0026#34; #\u0026gt; #\u0026gt; $headers$Host #\u0026gt; [1] \u0026#34;httpbin.org\u0026#34; #\u0026gt; #\u0026gt; $headers$`User-Agent` #\u0026gt; [1] \u0026#34;curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $origin #\u0026gt; [1] \u0026#34;24.21.209.71\u0026#34; #\u0026gt; #\u0026gt; $url #\u0026gt; [1] \u0026#34;https://httpbin.org/get\u0026#34; You can buid up options by calling functions via pipes, and see what the options look like\n\u0026#34;https://httpbin.org/get\u0026#34; %\u0026gt;% Progress() %\u0026gt;% Verbose() #\u0026gt; \u0026lt;http request\u0026gt; #\u0026gt; url: https://httpbin.org/get #\u0026gt; config: #\u0026gt; Config: #\u0026gt; List of 4 #\u0026gt; $ noprogress :FALSE #\u0026gt; $ progressfunction:function (...) #\u0026gt; $ debugfunction :function (...) #\u0026gt; $ verbose :TRUE Then execute the GET request when you\u0026rsquo;re ready\n\u0026#34;https://httpbin.org/get\u0026#34; %\u0026gt;% Progress() %\u0026gt;% Verbose() %\u0026gt;% Get() #\u0026gt; $args #\u0026gt; named list() #\u0026gt; #\u0026gt; $headers #\u0026gt; $headers$Accept #\u0026gt; [1] \u0026#34;application/json, text/xml, application/xml, */*\u0026#34; #\u0026gt; #\u0026gt; $headers$`Accept-Encoding` #\u0026gt; [1] \u0026#34;gzip\u0026#34; #\u0026gt; #\u0026gt; $headers$Host #\u0026gt; [1] \u0026#34;httpbin.org\u0026#34; #\u0026gt; #\u0026gt; $headers$`User-Agent` #\u0026gt; [1] \u0026#34;curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $origin #\u0026gt; [1] \u0026#34;24.21.209.71\u0026#34; #\u0026gt; #\u0026gt; $url #\u0026gt; [1] \u0026#34;https://httpbin.org/get\u0026#34; Example 1 Get scholarly article metadata from the Crossref API\n\u0026#34;https://api.crossref.org/works\u0026#34; %\u0026gt;% Query(query = \u0026#34;ecology\u0026#34;) %\u0026gt;% .$message %\u0026gt;% .$items %\u0026gt;% select(DOI, title, publisher) #\u0026gt; DOI title #\u0026gt; 1 10.4996/fireecology Fire Ecology #\u0026gt; 2 10.5402/ecology ISRN Ecology #\u0026gt; 3 10.1155/8641 ISRN Ecology #\u0026gt; 4 10.1111/(issn)1526-100x Restoration Ecology #\u0026gt; 5 10.1007/248.1432-184x Microbial Ecology #\u0026gt; 6 10.1007/10144.1438-390x Population Ecology #\u0026gt; 7 10.1007/10452.1573-5125 Aquatic Ecology #\u0026gt; 8 10.1007/10682.1573-8477 Evolutionary Ecology #\u0026gt; 9 10.1007/10745.1572-9915 Human Ecology #\u0026gt; 10 10.1007/10980.1572-9761 Landscape Ecology #\u0026gt; 11 10.1007/11258.1573-5052 Plant Ecology #\u0026gt; 12 10.1007/12080.1874-1746 Theoretical Ecology #\u0026gt; 13 10.1111/(issn)1442-9993 Austral Ecology #\u0026gt; 14 10.1111/(issn)1439-0485 Marine Ecology #\u0026gt; 15 10.1111/(issn)1365-2435 Functional Ecology #\u0026gt; 16 10.1111/(issn)1365-294x Molecular Ecology #\u0026gt; 17 10.1111/(issn)1461-0248 Ecology Letters #\u0026gt; 18 10.1002/9780470979365.ch7 Behavioural Ecology #\u0026gt; 19 10.1111/fec.2007.21.issue-5 #\u0026gt; 20 10.1111/rec.0.0.issue-0 #\u0026gt; publisher #\u0026gt; 1 Association for Fire Ecology #\u0026gt; 2 Hindawi Publishing Corporation #\u0026gt; 3 Hindawi Publishing Corporation #\u0026gt; 4 Wiley-Blackwell #\u0026gt; 5 Springer Science + Business Media #\u0026gt; 6 Springer Science + Business Media #\u0026gt; 7 Springer Science + Business Media #\u0026gt; 8 Springer Science + Business Media #\u0026gt; 9 Springer Science + Business Media #\u0026gt; 10 Springer Science + Business Media #\u0026gt; 11 Springer Science + Business Media #\u0026gt; 12 Springer Science + Business Media #\u0026gt; 13 Wiley-Blackwell #\u0026gt; 14 Wiley-Blackwell #\u0026gt; 15 Wiley-Blackwell #\u0026gt; 16 Wiley-Blackwell #\u0026gt; 17 Wiley-Blackwell #\u0026gt; 18 Wiley-Blackwell #\u0026gt; 19 Wiley-Blackwell #\u0026gt; 20 Wiley-Blackwell Example 2 Get Public Library of Science article metadata via their API, make a histogram of number of tweets for each article\n\u0026#34;https://api.plos.org/search\u0026#34; %\u0026gt;% Query(q = \u0026#34;*:*\u0026#34;, wt = \u0026#34;json\u0026#34;, rows = 100, fl = \u0026#34;id,journal,alm_twitterCount\u0026#34;, fq = \u0026#39;alm_twitterCount:[100 TO 10000]\u0026#39;) %\u0026gt;% .$response %\u0026gt;% .$docs %\u0026gt;% .$alm_twitterCount %\u0026gt;% hist() Notes Okay, so this isn\u0026rsquo;t drastically different from what httr already does, but its early days.\n","permalink":"http://localhost:1313/2015/04/the-new-way/","summary":"\u003cp\u003eInspired by \u003ccode\u003ehttpie\u003c/code\u003e, a Python command line client as a sort of drop in replacement for \u003ccode\u003ecurl\u003c/code\u003e, I am playing around with something similar-ish in R, at least in spirit. I started a little R pkg called \u003ccode\u003ehttsnap\u003c/code\u003e with the following ideas:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe web is increasingly a JSON world, so set \u003ccode\u003econtent-type\u003c/code\u003e and \u003ccode\u003eaccept\u003c/code\u003e headers to \u003ccode\u003eapplications/json\u003c/code\u003e by default\u003c/li\u003e\n\u003cli\u003eThe workflow follows logically, or at least should, from, \u003cem\u003ehey, I got this url\u003c/em\u003e, to \u003cem\u003ei need to add some options\u003c/em\u003e, to \u003cem\u003eexecute request\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eWhenever possible, transform output to data.frame\u0026rsquo;s - facilitating downstream manipulation via \u003ccode\u003edplyr\u003c/code\u003e, etc.\u003c/li\u003e\n\u003cli\u003eDo \u003ccode\u003eGET\u003c/code\u003e requests by default. Specify a different type if you don\u0026rsquo;t want \u003ccode\u003eGET\u003c/code\u003e. Some functionality does GET by default, though in some cases you need to specify GET\u003c/li\u003e\n\u003cli\u003eYou can use non-standard evaluation to easily pass in query parameters without worrying about \u003ccode\u003e\u0026amp;\u003c/code\u003e\u0026rsquo;s, URL escaping, etc. (see \u003ccode\u003eQuery()\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eSame for body params (see \u003ccode\u003eBody()\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"install\"\u003eInstall\u003c/h2\u003e\n\u003cp\u003eInstall and load \u003ccode\u003ehttsnap\u003c/code\u003e\u003c/p\u003e","title":"the new way - httsnap"},{"content":"With the help of user input, I\u0026rsquo;ve tweaked solr just a bit to make things faster using default setings. I imagine the main interface for people using the solr R client is via solr_search(), which used to have wt=json by default. Changing this to wt=csv gives better performance. And it sorta makes sense to use csv, as the point of using an R client is probably do get data eventually into a data.frame, so it makes sense to go csv format (Already in tabular format) if it\u0026rsquo;s faster too.\nInstall Install and load solr\ndevtools::install_github(\u0026#34;ropensci/solr\u0026#34;) library(\u0026#34;solr\u0026#34;) library(\u0026#34;microbenchmark\u0026#34;) Setup Define base url and fields to return\nurl \u0026lt;- \u0026#39;https://api.plos.org/search\u0026#39; fields \u0026lt;- c(\u0026#39;id\u0026#39;,\u0026#39;cross_published_journal_name\u0026#39;,\u0026#39;cross_published_journal_key\u0026#39;, \u0026#39;cross_published_journal_eissn\u0026#39;,\u0026#39;pmid\u0026#39;,\u0026#39;pmcid\u0026#39;,\u0026#39;publisher\u0026#39;,\u0026#39;journal\u0026#39;, \u0026#39;publication_date\u0026#39;,\u0026#39;article_type\u0026#39;,\u0026#39;article_type_facet\u0026#39;,\u0026#39;author\u0026#39;, \u0026#39;author_facet\u0026#39;,\u0026#39;volume\u0026#39;,\u0026#39;issue\u0026#39;,\u0026#39;elocation_id\u0026#39;,\u0026#39;author_display\u0026#39;, \u0026#39;competing_interest\u0026#39;,\u0026#39;copyright\u0026#39;) json The previous default for solr_search() used json\nsolr_search(q=\u0026#39;*:*\u0026#39;, rows=10, fl=fields, base=url, wt = \u0026#34;json\u0026#34;) #\u0026gt; Source: local data frame [10 x 19] #\u0026gt; #\u0026gt; id #\u0026gt; 1 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4 #\u0026gt; 2 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/title #\u0026gt; 3 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/abstract #\u0026gt; 4 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/references #\u0026gt; 5 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/body #\u0026gt; 6 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525 #\u0026gt; 7 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/title #\u0026gt; 8 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/abstract #\u0026gt; 9 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/references #\u0026gt; 10 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/body #\u0026gt; Variables not shown: cross_published_journal_name (chr), #\u0026gt; cross_published_journal_key (chr), cross_published_journal_eissn (chr), #\u0026gt; pmid (chr), pmcid (chr), publisher (chr), journal (chr), #\u0026gt; publication_date (chr), article_type (chr), article_type_facet (chr), #\u0026gt; author (chr), author_facet (chr), volume (int), issue (int), #\u0026gt; elocation_id (chr), author_display (chr), competing_interest (chr), #\u0026gt; copyright (chr) csv The default wt setting is now csv\nsolr_search(q=\u0026#39;*:*\u0026#39;, rows=10, fl=fields, base=url, wt = \u0026#34;json\u0026#34;) #\u0026gt; Source: local data frame [10 x 19] #\u0026gt; #\u0026gt; id #\u0026gt; 1 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4 #\u0026gt; 2 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/title #\u0026gt; 3 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/abstract #\u0026gt; 4 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/references #\u0026gt; 5 10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/body #\u0026gt; 6 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525 #\u0026gt; 7 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/title #\u0026gt; 8 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/abstract #\u0026gt; 9 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/references #\u0026gt; 10 10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/body #\u0026gt; Variables not shown: cross_published_journal_name (chr), #\u0026gt; cross_published_journal_key (chr), cross_published_journal_eissn (chr), #\u0026gt; pmid (chr), pmcid (chr), publisher (chr), journal (chr), #\u0026gt; publication_date (chr), article_type (chr), article_type_facet (chr), #\u0026gt; author (chr), author_facet (chr), volume (int), issue (int), #\u0026gt; elocation_id (chr), author_display (chr), competing_interest (chr), #\u0026gt; copyright (chr) Compare times When parsing to a data.frame (which solr_search() does by default), csv is quite a bit faster.\nmicrobenchmark( json = solr_search(q=\u0026#39;*:*\u0026#39;, rows=500, fl=fields, base=url, wt = \u0026#34;json\u0026#34;, verbose = FALSE), csv = solr_search(q=\u0026#39;*:*\u0026#39;, rows=500, fl=fields, base=url, wt = \u0026#34;csv\u0026#34;, verbose = FALSE), times = 20 ) #\u0026gt; Unit: milliseconds #\u0026gt; expr min lq mean median uq max neval cld #\u0026gt; json 965.7043 1013.014 1124.1229 1086.3225 1227.9054 1441.8332 20 b #\u0026gt; csv 509.6573 520.089 541.5784 532.4546 548.0303 723.7575 20 a json vs xml vs csv When getting raw data, csv is best, json next, then xml pulling up the rear.\nmicrobenchmark( json = solr_search(q=\u0026#39;*:*\u0026#39;, rows=1000, fl=fields, base=url, wt = \u0026#34;json\u0026#34;, verbose = FALSE, raw = TRUE), csv = solr_search(q=\u0026#39;*:*\u0026#39;, rows=1000, fl=fields, base=url, wt = \u0026#34;csv\u0026#34;, verbose = FALSE, raw = TRUE), xml = solr_search(q=\u0026#39;*:*\u0026#39;, rows=1000, fl=fields, base=url, wt = \u0026#34;xml\u0026#34;, verbose = FALSE, raw = TRUE), times = 10 ) #\u0026gt; Unit: milliseconds #\u0026gt; expr min lq mean median uq max neval cld #\u0026gt; json 1110.9515 1142.478 1198.9981 1169.0808 1195.5709 1518.7412 10 b #\u0026gt; csv 801.6871 802.516 826.0655 819.1532 835.0512 873.4266 10 a #\u0026gt; xml 1507.1111 1554.002 1618.5963 1617.5208 1671.0026 1740.4448 10 c Notes Note that wt=csv is only available in solr_search() and solr_all() because csv writer only returns the docs element in csv, dropping other elements, including facets, mlt, groups, stats, etc.\nAlso, note the http client used in solr is httr, which passes in a gzip compression header by default, so as long as the server serving up the Solr data has compression turned on, that\u0026rsquo;s all set.\nAnother way I\u0026rsquo;ve sped things up is if you use wt=json then parse to a data.frame, it uses dplyr which sped things up considerably.\n","permalink":"http://localhost:1313/2015/03/faster-solr/","summary":"\u003cp\u003eWith the \u003ca href=\"https://github.com/ropensci/solr/issues/47\"\u003ehelp of user input\u003c/a\u003e, I\u0026rsquo;ve tweaked \u003ccode\u003esolr\u003c/code\u003e just a bit to make things faster using default setings. I imagine the main interface for people using the \u003ccode\u003esolr\u003c/code\u003e R client is via \u003ccode\u003esolr_search()\u003c/code\u003e, which used to have \u003ccode\u003ewt=json\u003c/code\u003e by default. Changing this to \u003ccode\u003ewt=csv\u003c/code\u003e gives better performance. And it sorta makes sense to use csv, as the point of using an R client is probably do get data eventually into a data.frame, so it makes sense to go csv format (Already in tabular format) if it\u0026rsquo;s faster too.\u003c/p\u003e","title":"Faster solr with csv"},{"content":"It would be nice to easily push each row or column of a data.frame into CouchDB instead of having to prepare them yourself into JSON, then push in to couch. I recently added ability to push data.frame\u0026rsquo;s into couch using the normal PUT /{db} method, and added support for the couch bulk API.\nInstall install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;sckott/sofa\u0026#34;) library(\u0026#34;sofa\u0026#34;) PUT /db You can write directly from a data.frame, either by rows or columns. First, rows:\n#\u0026gt; $ok #\u0026gt; [1] TRUE Create a database\ndb_create(dbname=\u0026#34;mtcarsdb\u0026#34;) #\u0026gt; $ok #\u0026gt; [1] TRUE out \u0026lt;- doc_create(mtcars, dbname=\u0026#34;mtcarsdb\u0026#34;, how=\u0026#34;rows\u0026#34;) out[1:2] #\u0026gt; $`Mazda RX4` #\u0026gt; $`Mazda RX4`$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $`Mazda RX4`$id #\u0026gt; [1] \u0026#34;0063109bfb1c15765854cbc9525c3a7a\u0026#34; #\u0026gt; #\u0026gt; $`Mazda RX4`$rev #\u0026gt; [1] \u0026#34;1-3946941c894a874697554e3e6d9bc176\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $`Mazda RX4 Wag` #\u0026gt; $`Mazda RX4 Wag`$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $`Mazda RX4 Wag`$id #\u0026gt; [1] \u0026#34;0063109bfb1c15765854cbc9525c461d\u0026#34; #\u0026gt; #\u0026gt; $`Mazda RX4 Wag`$rev #\u0026gt; [1] \u0026#34;1-273ff17a938cb956cba21051ab428b95\u0026#34; Then by columns\nout \u0026lt;- doc_create(mtcars, dbname=\u0026#34;mtcarsdb\u0026#34;, how=\u0026#34;columns\u0026#34;) out[1:2] #\u0026gt; $mpg #\u0026gt; $mpg$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $mpg$id #\u0026gt; [1] \u0026#34;0063109bfb1c15765854cbc9525d4f1f\u0026#34; #\u0026gt; #\u0026gt; $mpg$rev #\u0026gt; [1] \u0026#34;1-4b83d0ef53a28849a872d47ad03fef9a\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $cyl #\u0026gt; $cyl$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $cyl$id #\u0026gt; [1] \u0026#34;0063109bfb1c15765854cbc9525d57d3\u0026#34; #\u0026gt; #\u0026gt; $cyl$rev #\u0026gt; [1] \u0026#34;1-c21bfa5425c67743f0826fd4b44b0dbf\u0026#34; Bulk API The bulk API will/should be faster for larger data.frames\n#\u0026gt; $ok #\u0026gt; [1] TRUE We\u0026rsquo;ll use part of the diamonds dataset\nlibrary(\u0026#34;ggplot2\u0026#34;) dat \u0026lt;- diamonds[1:20000,] Create a database\ndb_create(dbname=\u0026#34;bulktest\u0026#34;) #\u0026gt; $ok #\u0026gt; [1] TRUE Load by row (could instead do each column, see how parameter), printing the time it takes\nsystem.time(out \u0026lt;- bulk_create(dat, dbname=\u0026#34;bulktest\u0026#34;)) #\u0026gt; user system elapsed #\u0026gt; 16.832 6.039 24.432 The returned data is the same as with doc_create()\nout[1:2] #\u0026gt; [[1]] #\u0026gt; [[1]]$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; [[1]]$id #\u0026gt; [1] \u0026#34;0063109bfb1c15765854cbc9525d8b8d\u0026#34; #\u0026gt; #\u0026gt; [[1]]$rev #\u0026gt; [1] \u0026#34;1-f407fe4935af7fd17c101f13d3c81679\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; [[2]] #\u0026gt; [[2]]$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; [[2]]$id #\u0026gt; [1] \u0026#34;0063109bfb1c15765854cbc9525d998b\u0026#34; #\u0026gt; #\u0026gt; [[2]]$rev #\u0026gt; [1] \u0026#34;1-cf8b9a9dcdc026052a663d6fef8a36fe\u0026#34; So that\u0026rsquo;s 20,000 rows in not that much time, not bad.\nnot dataframes You can also pass in lists or vectors of json as character strings, e.g.,\nlists\n#\u0026gt; $ok #\u0026gt; [1] TRUE row.names(mtcars) \u0026lt;- NULL # get rid of row.names lst \u0026lt;- parse_df(mtcars, tojson=FALSE) db_create(dbname=\u0026#34;bulkfromlist\u0026#34;) #\u0026gt; $ok #\u0026gt; [1] TRUE out \u0026lt;- bulk_create(lst, dbname=\u0026#34;bulkfromlist\u0026#34;) out[1:2] #\u0026gt; [[1]] #\u0026gt; [[1]]$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; [[1]]$id #\u0026gt; [1] \u0026#34;ba70c46d73707662b1e204a90fcd9bb8\u0026#34; #\u0026gt; #\u0026gt; [[1]]$rev #\u0026gt; [1] \u0026#34;1-3946941c894a874697554e3e6d9bc176\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; [[2]] #\u0026gt; [[2]]$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; [[2]]$id #\u0026gt; [1] \u0026#34;ba70c46d73707662b1e204a90fcda9f6\u0026#34; #\u0026gt; #\u0026gt; [[2]]$rev #\u0026gt; [1] \u0026#34;1-273ff17a938cb956cba21051ab428b95\u0026#34; json\n#\u0026gt; $ok #\u0026gt; [1] TRUE strs \u0026lt;- as.character(parse_df(mtcars, \u0026#34;columns\u0026#34;)) db_create(dbname=\u0026#34;bulkfromchr\u0026#34;) #\u0026gt; $ok #\u0026gt; [1] TRUE out \u0026lt;- bulk_create(strs, dbname=\u0026#34;bulkfromchr\u0026#34;) out[1:2] #\u0026gt; [[1]] #\u0026gt; [[1]]$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; [[1]]$id #\u0026gt; [1] \u0026#34;ba70c46d73707662b1e204a90fce8c20\u0026#34; #\u0026gt; #\u0026gt; [[1]]$rev #\u0026gt; [1] \u0026#34;1-4b83d0ef53a28849a872d47ad03fef9a\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; [[2]] #\u0026gt; [[2]]$ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; [[2]]$id #\u0026gt; [1] \u0026#34;ba70c46d73707662b1e204a90fce9bc1\u0026#34; #\u0026gt; #\u0026gt; [[2]]$rev #\u0026gt; [1] \u0026#34;1-c21bfa5425c67743f0826fd4b44b0dbf\u0026#34; ","permalink":"http://localhost:1313/2015/03/couch-dataframes/","summary":"\u003cp\u003eIt would be nice to easily push each row or column of a data.frame into CouchDB instead of having to prepare them yourself into JSON, then push in to couch. I recently added ability to push data.frame\u0026rsquo;s into couch using the normal \u003ccode\u003ePUT /{db}\u003c/code\u003e method, and added support for the couch bulk API.\u003c/p\u003e\n\u003ch2 id=\"install\"\u003eInstall\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall.packages\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;devtools\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevtools\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall_github\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sckott/sofa\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sofa\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"put-db\"\u003ePUT /db\u003c/h2\u003e\n\u003cp\u003eYou can write directly from a data.frame, either by rows or columns. First, rows:\u003c/p\u003e","title":"PUT dataframes on your couch"},{"content":"CSL (Citation Style Language) is used quite widely now to specify citations in a standard fashion. csl is an R client for exploring CSL styles, and is inspired by the Ruby gem csl. For example, csl is given back in the PLOS Lagotto article level metric API (follow https://alm.plos.org/api/v5/articles?ids=10.1371%252Fjournal.pone.0025110\u0026amp;info=detail\u0026amp;source_id=crossref).\nLet me know if you have any feedback at the repo https://github.com/ropensci/csl\nInstall install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/csl\u0026#34;) library(\u0026#34;csl\u0026#34;) Load CSL style from a URL You can load CSL styles from either a URL or a local file on your machine. Firt, from a URL. In this case from the Zotero style repository, for the American Journal or Political Science.\njps \u0026lt;- style_load(\u0026#39;https://www.zotero.org/styles/american-journal-of-political-science\u0026#39;) A list is returned, which you can index to various parts of the style specification.\njps$info #\u0026gt; $title #\u0026gt; [1] \u0026#34;American Journal of Political Science\u0026#34; #\u0026gt; #\u0026gt; $title_short #\u0026gt; [1] \u0026#34;AJPS\u0026#34; #\u0026gt; #\u0026gt; $id #\u0026gt; [1] \u0026#34;https://www.zotero.org/styles/american-journal-of-political-science\u0026#34; #\u0026gt; #\u0026gt; $author ... jps$title #\u0026gt; [1] \u0026#34;American Journal of Political Science\u0026#34; jps$citation_format #\u0026gt; [1] \u0026#34;author-date\u0026#34; jps$links_template #\u0026gt; [1] \u0026#34;https://www.zotero.org/styles/american-political-science-association\u0026#34; jps$editor #\u0026gt; $editor #\u0026gt; $editor$variable #\u0026gt; [1] \u0026#34;editor\u0026#34; #\u0026gt; #\u0026gt; $editor$delimiter #\u0026gt; [1] \u0026#34;, \u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $label #\u0026gt; $label$form ... jps$author #\u0026gt; $author #\u0026gt; $author$variable #\u0026gt; [1] \u0026#34;author\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $label #\u0026gt; $label$form #\u0026gt; [1] \u0026#34;short\u0026#34; #\u0026gt; #\u0026gt; $label$prefix ... Get raw XML You can also get raw XML if you\u0026rsquo;d rather deal with that format.\nstyle_xml(\u0026#39;https://www.zotero.org/styles/american-journal-of-political-science\u0026#39;) #\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; #\u0026gt; \u0026lt;style xmlns=\u0026#34;https://purl.org/net/xbiblio/csl\u0026#34; class=\u0026#34;in-text\u0026#34; version=\u0026#34;1.0\u0026#34; demote-non-dropping-particle=\u0026#34;sort-only\u0026#34; default-locale=\u0026#34;en-US\u0026#34;\u0026gt; #\u0026gt; \u0026lt;info\u0026gt; #\u0026gt; \u0026lt;title\u0026gt;American Journal of Political Science\u0026lt;/title\u0026gt; #\u0026gt; \u0026lt;title-short\u0026gt;AJPS\u0026lt;/title-short\u0026gt; #\u0026gt; \u0026lt;id\u0026gt;https://www.zotero.org/styles/american-journal-of-political-science\u0026lt;/id\u0026gt; #\u0026gt; \u0026lt;link href=\u0026#34;https://www.zotero.org/styles/american-journal-of-political-science\u0026#34; rel=\u0026#34;self\u0026#34;/\u0026gt; #\u0026gt; \u0026lt;link href=\u0026#34;https://www.zotero.org/styles/american-political-science-association\u0026#34; rel=\u0026#34;template\u0026#34;/\u0026gt; #\u0026gt; \u0026lt;link href=\u0026#34;https://www.ajps.org/AJPS%20Style%20Guide.pdf\u0026#34; rel=\u0026#34;documentation\u0026#34;/\u0026gt; #\u0026gt; \u0026lt;author\u0026gt; ... Get styles There is a GitHub repository of CSL styles at https://github.com/citation-style-language/styles-distribution. These don\u0026rsquo;t come with the csl package, so you have to run get_styles() to get them on your machine. The default path is Sys.getenv(\u0026quot;HOME\u0026quot;)/styles, which for me is /Users/sacmac/styles. You can change where files are saved by using the path parameter.\nget_styles() #\u0026gt; #\u0026gt; Done! Files put in /Users/sacmac/styles After getting styles locally you can load them just as we did with style_load(), but from your machine. However, since the file is local, we can make this easier by allowing just the name of the style, like\nstyle_load(\u0026#34;apa\u0026#34;) #\u0026gt; $info #\u0026gt; $info$title #\u0026gt; [1] \u0026#34;American Psychological Association 6th edition\u0026#34; #\u0026gt; #\u0026gt; $info$title_short #\u0026gt; [1] \u0026#34;APA\u0026#34; #\u0026gt; #\u0026gt; $info$id #\u0026gt; [1] \u0026#34;https://www.zotero.org/styles/apa\u0026#34; #\u0026gt; ... If you are unsure if a style exists, you can use style_exists()\nstyle_exists(\u0026#34;helloworld\u0026#34;) #\u0026gt; [1] FALSE style_exists(\u0026#34;acs-nano\u0026#34;) #\u0026gt; [1] TRUE In addition, you can list the path for a single style, more than 1, or all styles with styles()\nstyles(\u0026#34;apa\u0026#34;) #\u0026gt; [1] \u0026#34;/Users/sacmac/styles/apa.csl\u0026#34; All of them, truncated for blog brevity\nstyles() #\u0026gt; $independent #\u0026gt; [1] \u0026#34;academy-of-management-review\u0026#34; #\u0026gt; [2] \u0026#34;acm-sig-proceedings-long-author-list\u0026#34; #\u0026gt; [3] \u0026#34;acm-sig-proceedings\u0026#34; #\u0026gt; [4] \u0026#34;acm-sigchi-proceedings-extended-abstract-format\u0026#34; #\u0026gt; [5] \u0026#34;acm-sigchi-proceedings\u0026#34; #\u0026gt; [6] \u0026#34;acm-siggraph\u0026#34; #\u0026gt; [7] \u0026#34;acs-nano\u0026#34; #\u0026gt; [8] \u0026#34;acta-anaesthesiologica-scandinavica\u0026#34; #\u0026gt; [9] \u0026#34;acta-anaesthesiologica-taiwanica\u0026#34; ... Get locales In addition to styles, there is a GitHub repo for locales at https://github.com/citation-style-language/locales. These also don\u0026rsquo;t come with the csl package, so you have to run get_locales() to get them on your machine. Same goes here for paths as above for styles.\nget_locales() #\u0026gt; #\u0026gt; Done! Files put in /Users/sacmac/locales ","permalink":"http://localhost:1313/2015/03/csl-client/","summary":"\u003cp\u003eCSL (Citation Style Language) is used quite widely now to specify citations in a standard fashion. \u003ccode\u003ecsl\u003c/code\u003e is an R client for exploring CSL styles, and is inspired by the Ruby gem \u003ca href=\"https://github.com/inukshuk/csl-ruby\"\u003ecsl\u003c/a\u003e. For example, csl is given back in the PLOS Lagotto article level metric API (follow \u003ca href=\"https://alm.plos.org/api/v5/articles?ids=10.1371%252Fjournal.pone.0025110\u0026amp;info=detail\u0026amp;source_id=crossref\"\u003ehttps://alm.plos.org/api/v5/articles?ids=10.1371%252Fjournal.pone.0025110\u0026amp;info=detail\u0026amp;source_id=crossref\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eLet me know if you have any feedback at the repo \u003ca href=\"https://github.com/ropensci/csl\"\u003ehttps://github.com/ropensci/csl\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"install\"\u003eInstall\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall.packages\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;devtools\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevtools\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall_github\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ropensci/csl\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;csl\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"load-csl-style-from-a-url\"\u003eLoad CSL style from a URL\u003c/h2\u003e\n\u003cp\u003eYou can load CSL styles from either a URL or a local file on your machine. Firt, from a URL. In this case from the Zotero style repository, for the American Journal or Political Science.\u003c/p\u003e","title":"csl - an R client for Citation Style Language data"},{"content":"setup backup curl -XPUT \u0026#39;http://localhost:9200/_snapshot/my_backup/\u0026#39; -d \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/Users/sacmac/esbackups/my_backup\u0026#34;, \u0026#34;compress\u0026#34;: true } }\u0026#39; create backup http PUT \u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2?wait_for_completion=true\u0026#34; get info on snapshot http \u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2\u0026#34; restore curl -XPOST \u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2/_restore\u0026#34; partial restore, including various options that can be used curl -XPOST \u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2/_restore\u0026#34; -d \u0026#39;{ \u0026#34;indices\u0026#34;: \u0026#34;index_1,index_2\u0026#34;, \u0026#34;ignore_unavailable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;include_global_state\u0026#34;: false, \u0026#34;rename_pattern\u0026#34;: \u0026#34;index_(.+)\u0026#34;, \u0026#34;rename_replacement\u0026#34;: \u0026#34;restored_index_$1\u0026#34; }\u0026#39; ","permalink":"http://localhost:1313/2015/02/elasticsearch-backup-restore/","summary":"\u003ch2 id=\"setup-backup\"\u003esetup backup\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -XPUT \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;http://localhost:9200/_snapshot/my_backup/\u0026#39;\u003c/span\u003e -d \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;{\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;settings\u0026#34;: {\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e        \u0026#34;location\u0026#34;: \u0026#34;/Users/sacmac/esbackups/my_backup\u0026#34;,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e        \u0026#34;compress\u0026#34;: true\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"create-backup\"\u003ecreate backup\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehttp PUT \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2?wait_for_completion=true\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"get-info-on-snapshot\"\u003eget info on snapshot\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehttp \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"restore\"\u003erestore\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -XPOST \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2/_restore\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"partial-restore-including-various-options-that-can-be-used\"\u003epartial restore, including various options that can be used\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -XPOST \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;localhost:9200/_snapshot/my_backup/snapshot_2/_restore\u0026#34;\u003c/span\u003e -d \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;{\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;indices\u0026#34;: \u0026#34;index_1,index_2\u0026#34;,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;ignore_unavailable\u0026#34;: \u0026#34;true\u0026#34;,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;include_global_state\u0026#34;: false,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;rename_pattern\u0026#34;: \u0026#34;index_(.+)\u0026#34;,\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;rename_replacement\u0026#34;: \u0026#34;restored_index_$1\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Elasticsearch backup and restore"},{"content":"Recently I spun up a box on a cloud hosting provider planning to make a tens of thousdands of queries to an Elasticsearch instance on the same box. I could have done this on my own machine, but didn\u0026rsquo;t want to take up compute resources.\nI installed R and Elasticsearch on the box, then went about doing my thang.\nA day later when things were still running, the hosting provider sent me a message that apparently my box had been serving up a DDoS attack.\nThis was incredibly surprising, as I don\u0026rsquo;t even know how to do such a thing.\nAfter some digging it seems that the culprit was likely Elasticsearch, as a number of tutorials/blog posts state that Elaticsearch is insecure by default, so if it\u0026rsquo;s exposed on a public port, someone can hack in. I had only used Elasticsearch locally on my own machine, so I hadn\u0026rsquo;t thought about security. Here\u0026rsquo;s a few resources for security help:\nDigitalOcean tutorial no. 1 DigitalOcean tutorial no. 2 Blog post on securing ES SO answer on securing ES Trying to narrow down the various pieces of advice for securing Elasticsearch, here\u0026rsquo;s a list:\nUse iptables (or rather nftables?) to firewall the box Whitelist certain trusted IPs Use the elasticsearch-http-basic plugin, adds basic username/pwd login Remove public access: use network.bind_host: localhost and script.disable_dynamic: true in the elasticsearch.yml config file from Elasticsearch provides a new feature for security that\u0026rsquo;s built into Elasticsearch, Shield, but I believe it\u0026rsquo;s only available to enterprise customers. Boo.\n","permalink":"http://localhost:1313/2015/02/secure-elasticsearch/","summary":"\u003cp\u003eRecently I spun up a box on a cloud hosting provider planning to make a tens of thousdands of queries to an Elasticsearch instance on the same box. I could have done this on my own machine, but didn\u0026rsquo;t want to take up compute resources.\u003c/p\u003e\n\u003cp\u003eI installed R and Elasticsearch on the box, then went about doing my thang.\u003c/p\u003e\n\u003cp\u003eA day later when things were still running, the hosting provider sent me a message that apparently my box had been serving up a DDoS attack.\u003c/p\u003e","title":"note to self, secure elasticsearch"},{"content":"I\u0026rsquo;ve been working on a little thing called httping - a small R package that started as a pkg to Ping urls and time requests. It\u0026rsquo;s a port of the Ruby gem httping. The httr package is in Depends in this package, so its functions can be called directly, without having to load httr explicitly yourself.\nIn addition to timing requests, I\u0026rsquo;ve been tinkering with how to make http requests, with curl options accepting and returning the same object so they can be chained together, and then that object passed to a http verb like GET. Maybe this is a bad idea, but maybe not.\nInstallation Install:\nOne non-CRAN dep (httpcode) needs to be installed first.\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;sckott/httpcode\u0026#34;) devtools::install_github(\u0026#34;sckott/httping\u0026#34;) Then load package\nlibrary(\u0026#34;httping\u0026#34;) Time requests The idea with time() is to provide easy to use and understand information on how long http requests take to run. You should be able to pass in any httr verbs (GET(), POST(), etc.) to time(). time() repeats whatever http request you pass to it by default 10 times, but you can set the number of times to repeat in the count parameter. In addition, the flood parameter controls whether there is a delay between requests or not, and delay controls length of the delay.\nA GET request\nGET(\u0026#34;http://google.com\u0026#34;) %\u0026gt;% time(count=3) #\u0026gt; 29.392 kb - http://www.google.com/ code:200 time(ms):92.444 #\u0026gt; 29.392 kb - http://www.google.com/ code:200 time(ms):82.127 #\u0026gt; 29.392 kb - http://www.google.com/ code:200 time(ms):85.587 #\u0026gt; \u0026lt;http time\u0026gt; #\u0026gt; Avg. min (ms): 82.127 #\u0026gt; Avg. max (ms): 92.444 #\u0026gt; Avg. mean (ms): 86.71933 A POST request\nPOST(\u0026#34;http://httpbin.org/post\u0026#34;, body = \u0026#34;A simple text string\u0026#34;) %\u0026gt;% time(count=3) #\u0026gt; 10.144 kb - http://httpbin.org/post code:200 time(ms):267.574 #\u0026gt; 10.144 kb - http://httpbin.org/post code:200 time(ms):113.309 #\u0026gt; 10.144 kb - http://httpbin.org/post code:200 time(ms):99.938 #\u0026gt; \u0026lt;http time\u0026gt; #\u0026gt; Avg. min (ms): 99.938 #\u0026gt; Avg. max (ms): 267.574 #\u0026gt; Avg. mean (ms): 160.2737 The return object is a list with slots for all the httr response objects, the times for each request, and the average times. The number of requests, and the delay between requests are included as attributes.\nres \u0026lt;- GET(\u0026#34;http://google.com\u0026#34;) %\u0026gt;% time(count=3) #\u0026gt; 29.392 kb - http://www.google.com/ code:200 time(ms):82.086 #\u0026gt; 29.392 kb - http://www.google.com/ code:200 time(ms):78.15 #\u0026gt; 29.392 kb - http://www.google.com/ code:200 time(ms):79.763 attributes(res) #\u0026gt; $names #\u0026gt; [1] \u0026#34;times\u0026#34; \u0026#34;averages\u0026#34; \u0026#34;request\u0026#34; #\u0026gt; #\u0026gt; $count #\u0026gt; [1] 3 #\u0026gt; #\u0026gt; $delay #\u0026gt; [1] 0.5 #\u0026gt; #\u0026gt; $class #\u0026gt; [1] \u0026#34;http_time\u0026#34; Or print a summary of a response, gives more detail\nres %\u0026gt;% summary() #\u0026gt; \u0026lt;http time, averages (min max mean)\u0026gt; #\u0026gt; Total (s): 78.15 82.086 79.99967 #\u0026gt; Tedirect (s): 26.695 34.319 29.80633 #\u0026gt; Namelookup time (s): 0.025 0.03 0.028 #\u0026gt; Connect (s): 0.028 0.034 0.032 #\u0026gt; Pretransfer (s): 0.069 0.081 0.07633333 #\u0026gt; Starttransfer (s): 45.44 49.326 47.95867 Messages are printed using cat, so you can suppress those using verbose=FALSE, like\nGET(\u0026#34;http://google.com\u0026#34;) %\u0026gt;% time(count=3, verbose = FALSE) #\u0026gt; \u0026lt;http time\u0026gt; #\u0026gt; Avg. min (ms): 86.12 #\u0026gt; Avg. max (ms): 94.035 #\u0026gt; Avg. mean (ms): 89.12467 Ping an endpoint The idea with ping() is to simply return the http status code along with a message describing what that code means. That\u0026rsquo;s it.\nThis function is a bit different, accepts a url as first parameter, but can accept any args passed on to httr verb functions, like GET, POST, etc.\n\u0026#34;http://google.com\u0026#34; %\u0026gt;% ping() #\u0026gt; \u0026lt;http ping\u0026gt; 200 #\u0026gt; Message: OK #\u0026gt; Description: Request fulfilled, document follows Or pass in additional arguments to modify request\n\u0026#34;http://google.com\u0026#34; %\u0026gt;% ping(config=verbose()) #\u0026gt; -\u0026gt; GET / HTTP/1.1 #\u0026gt; -\u0026gt; User-Agent: curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1 #\u0026gt; -\u0026gt; Host: google.com #\u0026gt; -\u0026gt; Accept-Encoding: gzip ...cutoff Even simpler verbs httr is already easy, but Get():\nAllows use of an intuitive chaining workflow Parses data for you using httr built in format guesser, which should work in most cases A simple GET request\n\u0026#34;http://httpbin.org/get\u0026#34; %\u0026gt;% Get() #\u0026gt; $args #\u0026gt; named list() #\u0026gt; #\u0026gt; $headers #\u0026gt; $headers$Accept #\u0026gt; [1] \u0026#34;application/json, text/xml, application/xml, */*\u0026#34; #\u0026gt; #\u0026gt; $headers$`Accept-Encoding` #\u0026gt; [1] \u0026#34;gzip\u0026#34; #\u0026gt; #\u0026gt; $headers$Host #\u0026gt; [1] \u0026#34;httpbin.org\u0026#34; #\u0026gt; #\u0026gt; $headers$`User-Agent` #\u0026gt; [1] \u0026#34;curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $origin #\u0026gt; [1] \u0026#34;24.21.209.71\u0026#34; #\u0026gt; #\u0026gt; $url #\u0026gt; [1] \u0026#34;http://httpbin.org/get\u0026#34; You can buid up options by calling functions\n\u0026#34;http://httpbin.org/get\u0026#34; %\u0026gt;% Progress() %\u0026gt;% Verbose() #\u0026gt; \u0026lt;http request\u0026gt; #\u0026gt; url: http://httpbin.org/get #\u0026gt; config: #\u0026gt; Config: #\u0026gt; List of 4 #\u0026gt; $ noprogress :FALSE #\u0026gt; $ progressfunction:function (...) #\u0026gt; $ debugfunction :function (...) #\u0026gt; $ verbose :TRUE Then eventually execute the GET request\n\u0026#34;http://httpbin.org/get\u0026#34; %\u0026gt;% Verbose() %\u0026gt;% Progress() %\u0026gt;% Get() #\u0026gt; -\u0026gt; GET /get HTTP/1.1 #\u0026gt; -\u0026gt; User-Agent: curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1 #\u0026gt; -\u0026gt; Host: httpbin.org #\u0026gt; -\u0026gt; Accept-Encoding: gzip #\u0026gt; -\u0026gt; Accept: application/json, text/xml, application/xml, */* #\u0026gt; -\u0026gt; #\u0026gt; \u0026lt;- HTTP/1.1 200 OK #\u0026gt; \u0026lt;- Server: nginx #\u0026gt; \u0026lt;- Date: Fri, 30 Jan 2015 17:38:58 GMT #\u0026gt; \u0026lt;- Content-Type: application/json #\u0026gt; \u0026lt;- Content-Length: 288 #\u0026gt; \u0026lt;- Connection: keep-alive #\u0026gt; \u0026lt;- Access-Control-Allow-Origin: * #\u0026gt; \u0026lt;- Access-Control-Allow-Credentials: true #\u0026gt; \u0026lt;- #\u0026gt; |=======================================| 100% #\u0026gt; #\u0026gt; $args #\u0026gt; named list() #\u0026gt; #\u0026gt; $headers #\u0026gt; $headers$Accept #\u0026gt; [1] \u0026#34;application/json, text/xml, application/xml, */*\u0026#34; #\u0026gt; #\u0026gt; $headers$`Accept-Encoding` #\u0026gt; [1] \u0026#34;gzip\u0026#34; #\u0026gt; #\u0026gt; $headers$Host #\u0026gt; [1] \u0026#34;httpbin.org\u0026#34; #\u0026gt; #\u0026gt; $headers$`User-Agent` #\u0026gt; [1] \u0026#34;curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $origin #\u0026gt; [1] \u0026#34;24.21.209.71\u0026#34; #\u0026gt; #\u0026gt; $url #\u0026gt; [1] \u0026#34;http://httpbin.org/get\u0026#34; #\u0026gt; ","permalink":"http://localhost:1313/2015/01/httping/","summary":"\u003cp\u003eI\u0026rsquo;ve been working on a little thing called \u003ccode\u003ehttping\u003c/code\u003e - a small R package that started as a pkg to Ping urls and time requests. It\u0026rsquo;s a port of the Ruby gem \u003ca href=\"https://github.com/jpignata/httping\"\u003ehttping\u003c/a\u003e. The \u003ccode\u003ehttr\u003c/code\u003e package is in \u003ccode\u003eDepends\u003c/code\u003e in this package, so its functions can be called directly, without having to load \u003ccode\u003ehttr\u003c/code\u003e explicitly yourself.\u003c/p\u003e\n\u003cp\u003eIn addition to timing requests, I\u0026rsquo;ve been tinkering with how to make http requests, with curl options accepting and returning the same object so they can be chained together, and then that object passed to a http verb like \u003ccode\u003eGET\u003c/code\u003e. Maybe this is a bad idea, but maybe not.\u003c/p\u003e","title":"httping - ping and time http requests"},{"content":"We\u0026rsquo;ve (ropensci) been working on an R client for interacting with Elasticsearch for a while now, first commit was November 2013.\nElasticsearch is a document database built on the JVM. elastic interacts with the Elasticsearch HTTP API, and includes functions for setting connection details to Elasticsearch instances, loading bulk data, searching for documents with both HTTP query variables and JSON based body requests. In addition, elastic provides functions for interacting with APIs for indices, documents, nodes, clusters, an interface to the cat API, and more.\nHere\u0026rsquo;s a few examples of what you can do.\nNote: elastic was just pushed to CRAN. It just got accepted, so binaries may not be available, try again soon, or install from Github, or install from source from CRAN like install.packages(\u0026quot;https://cran.r-project.org/src/contrib/elastic_0.3.0.tar.gz\u0026quot;, repos=NULL, type=\u0026quot;source\u0026quot;).\nInstallation install.packages(\u0026#34;elastic\u0026#34;) Or install development version:\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/elastic\u0026#34;) Then load package\nlibrary(\u0026#34;elastic\u0026#34;) Install Elasticsearch Elasticsearch installation help Unix (linux/osx)\nReplace 1.4.1 with the version you are working with.\nDownload zip or tar file from Elasticsearch see here for download, e.g., curl -L -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.1.tar.gz Uncompress it: tar -xvf elasticsearch-1.4.1.tar.gz Move it: sudo mv /path/to/elasticsearch-1.4.1 /usr/local Navigate to /usr/local: cd /usr/local Add shortcut: sudo ln -s elasticsearch-1.4.1 elasticsearch On OSX, you can install via Homebrew: brew install elasticsearch\nWindows\nWindows users can follow the above, but unzip the zip file instead of uncompressing the tar file.\nStart Elasticsearch Navigate to elasticsearch: cd /usr/local/elasticsearch Start elasticsearch: bin/elasticsearch I create a little bash shortcut called es that does both of the above commands in one step (cd /usr/local/elasticsearch \u0026amp;\u0026amp; bin/elasticsearch).\nNote: Windows users should run the elasticsearch.bat file\nInitialize connection The function connect() is used before doing anything else to set the connection details to your remote or local elasticsearch store. The details created by connect() are written to your options for the current session, and are used by elastic functions.\nconnect() On package load, your base url and port are set to https://127.0.0.1 and 9200, respectively. You can of course override these settings per session or for all sessions.\nGet data Elasticsearch has a bulk load API to load data in fast. The format is pretty weird though. It\u0026rsquo;s sort of JSON, but would pass no JSON linter. I include a few data sets in elastic so it\u0026rsquo;s easy to get up and running, and so when you run examples in this package they\u0026rsquo;ll actually run the same way (hopefully).\nShakespeare data Elasticsearch provides some data on Shakespeare plays. I\u0026rsquo;ve provided a subset of this data in this package. Get the path for the file specific to your machine:\nshakespeare \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;shakespeare_data.json\u0026#34;, package = \u0026#34;elastic\u0026#34;) Then load the data into Elasticsearch:\ndocs_bulk(shakespeare) Public Library of Science (PLOS) data A dataset inluded in the elastic package is metadata for PLOS scholarly articles.\nplosdat \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;plos_data.json\u0026#34;, package = \u0026#34;elastic\u0026#34;) docs_bulk(plosdat) Global Biodiversity Information Facility (GBIF) data A dataset inluded in the elastic package is data for GBIF species occurrence records. Get the file path, then load:\ngbifdat \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;gbif_data.json\u0026#34;, package = \u0026#34;elastic\u0026#34;) docs_bulk(gbifdat) GBIF geo data with a coordinates element to allow geo_shape queries\ngbifgeo \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;gbif_geo.json\u0026#34;, package = \u0026#34;elastic\u0026#34;) docs_bulk(gbifgeo) The Search function The main interface to searching documents in your Elasticsearch store is the function Search(). I nearly always develop R software using all lowercase, but R has a function called search(), and I wanted to avoid collision with that function.\nSearch() is an interface to both the HTTP search API (in which queries are passed in the URI of the request, meaning queries have to be relatively simple), as well as the POST API, or the Query DSL, in which queries are passed in the body of the request (so can be much more complex).\nThere are a huge amount of ways you can search Elasticsearch documents - this tutorial covers some of them, and highlights the ways in which you interact with the R outputs.\nSearch an index out \u0026lt;- Search(index=\u0026#34;shakespeare\u0026#34;) out$hits$total #\u0026gt; [1] 5000 out$hits$hits[[1]] #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;4\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_score` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source` #\u0026gt; $`_source`$line_id #\u0026gt; [1] 5 #\u0026gt; #\u0026gt; $`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speech_number #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source`$line_number #\u0026gt; [1] \u0026#34;1.1.2\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speaker #\u0026gt; [1] \u0026#34;KING HENRY IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$text_entry #\u0026gt; [1] \u0026#34;Find we a time for frighted peace to pant,\u0026#34; Search an index by type Search(index=\u0026#34;shakespeare\u0026#34;, type=\u0026#34;act\u0026#34;)$hits$hits[[1]] #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;act\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;2227\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_score` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source` #\u0026gt; $`_source`$line_id #\u0026gt; [1] 2228 #\u0026gt; #\u0026gt; $`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speech_number #\u0026gt; [1] 81 #\u0026gt; #\u0026gt; $`_source`$line_number #\u0026gt; [1] \u0026#34;\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speaker #\u0026gt; [1] \u0026#34;FALSTAFF\u0026#34; #\u0026gt; #\u0026gt; $`_source`$text_entry #\u0026gt; [1] \u0026#34;ACT IV\u0026#34; Return certain fields Search(index=\u0026#34;shakespeare\u0026#34;, fields=c(\u0026#39;play_name\u0026#39;,\u0026#39;speaker\u0026#39;))$hits$hits[[1]] #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;4\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_score` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $fields #\u0026gt; $fields$speaker #\u0026gt; $fields$speaker[[1]] #\u0026gt; [1] \u0026#34;KING HENRY IV\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $fields$play_name #\u0026gt; $fields$play_name[[1]] #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; Sorting Search(index=\u0026#34;shakespeare\u0026#34;, type=\u0026#34;act\u0026#34;, sort=\u0026#34;text_entry\u0026#34;)$hits$hits[1:2] #\u0026gt; [[1]] #\u0026gt; [[1]]$`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; [[1]]$`_type` #\u0026gt; [1] \u0026#34;act\u0026#34; #\u0026gt; #\u0026gt; [[1]]$`_id` #\u0026gt; [1] \u0026#34;2227\u0026#34; #\u0026gt; #\u0026gt; [[1]]$`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; [[1]]$`_score` #\u0026gt; NULL #\u0026gt; #\u0026gt; [[1]]$`_source` #\u0026gt; [[1]]$`_source`$line_id #\u0026gt; [1] 2228 #\u0026gt; #\u0026gt; [[1]]$`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; [[1]]$`_source`$speech_number #\u0026gt; [1] 81 #\u0026gt; #\u0026gt; [[1]]$`_source`$line_number #\u0026gt; [1] \u0026#34;\u0026#34; #\u0026gt; #\u0026gt; [[1]]$`_source`$speaker #\u0026gt; [1] \u0026#34;FALSTAFF\u0026#34; #\u0026gt; #\u0026gt; [[1]]$`_source`$text_entry #\u0026gt; [1] \u0026#34;ACT IV\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; [[1]]$sort #\u0026gt; [[1]]$sort[[1]] #\u0026gt; [1] \u0026#34;act\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; #\u0026gt; [[2]] #\u0026gt; [[2]]$`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; [[2]]$`_type` #\u0026gt; [1] \u0026#34;act\u0026#34; #\u0026gt; #\u0026gt; [[2]]$`_id` #\u0026gt; [1] \u0026#34;2633\u0026#34; #\u0026gt; #\u0026gt; [[2]]$`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; [[2]]$`_score` #\u0026gt; NULL #\u0026gt; #\u0026gt; [[2]]$`_source` #\u0026gt; [[2]]$`_source`$line_id #\u0026gt; [1] 2634 #\u0026gt; #\u0026gt; [[2]]$`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; [[2]]$`_source`$speech_number #\u0026gt; [1] 9 #\u0026gt; #\u0026gt; [[2]]$`_source`$line_number #\u0026gt; [1] \u0026#34;\u0026#34; #\u0026gt; #\u0026gt; [[2]]$`_source`$speaker #\u0026gt; [1] \u0026#34;ARCHBISHOP OF YORK\u0026#34; #\u0026gt; #\u0026gt; [[2]]$`_source`$text_entry #\u0026gt; [1] \u0026#34;ACT V\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; [[2]]$sort #\u0026gt; [[2]]$sort[[1]] #\u0026gt; [1] \u0026#34;act\u0026#34; Paging Search(index=\u0026#34;shakespeare\u0026#34;, size=1, from=1, fields=\u0026#39;text_entry\u0026#39;)$hits #\u0026gt; $total #\u0026gt; [1] 5000 #\u0026gt; #\u0026gt; $max_score #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $hits #\u0026gt; $hits[[1]] #\u0026gt; $hits[[1]]$`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $hits[[1]]$`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $hits[[1]]$`_id` #\u0026gt; [1] \u0026#34;9\u0026#34; #\u0026gt; #\u0026gt; $hits[[1]]$`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $hits[[1]]$`_score` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $hits[[1]]$fields #\u0026gt; $hits[[1]]$fields$text_entry #\u0026gt; $hits[[1]]$fields$text_entry[[1]] #\u0026gt; [1] \u0026#34;Nor more shall trenching war channel her fields,\u0026#34; Queries Using the q parameter you can pass in a query, which gets passed in the URI of the query. This type of query is less powerful than the below query passed in the body of the request, using the body parameter.\nSearch(index=\u0026#34;shakespeare\u0026#34;, type=\u0026#34;act\u0026#34;, q=\u0026#34;speaker:KING HENRY IV\u0026#34;)$hits$total #\u0026gt; [1] 9 Query DSL searches - queries sent in the body of the request Pass in as an R list\naggs \u0026lt;- list(aggs = list(stats = list(terms = list(field = \u0026#34;text_entry\u0026#34;)))) Search(index=\u0026#34;shakespeare\u0026#34;, body=aggs)$hits$hits[[1]] #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;4\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_score` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source` #\u0026gt; $`_source`$line_id #\u0026gt; [1] 5 #\u0026gt; #\u0026gt; $`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speech_number #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source`$line_number #\u0026gt; [1] \u0026#34;1.1.2\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speaker #\u0026gt; [1] \u0026#34;KING HENRY IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$text_entry #\u0026gt; [1] \u0026#34;Find we a time for frighted peace to pant,\u0026#34; Or pass in as json query with newlines, easy to read\naggs \u0026lt;- \u0026#39;{ \u0026#34;aggs\u0026#34;: { \u0026#34;stats\u0026#34; : { \u0026#34;terms\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;text_entry\u0026#34; } } } }\u0026#39; Search(index=\u0026#34;shakespeare\u0026#34;, body=aggs)$hits$hits[[1]] #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;4\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_score` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source` #\u0026gt; $`_source`$line_id #\u0026gt; [1] 5 #\u0026gt; #\u0026gt; $`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speech_number #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source`$line_number #\u0026gt; [1] \u0026#34;1.1.2\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speaker #\u0026gt; [1] \u0026#34;KING HENRY IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$text_entry #\u0026gt; [1] \u0026#34;Find we a time for frighted peace to pant,\u0026#34; Or pass in collapsed json string\naggs \u0026lt;- \u0026#39;{\u0026#34;aggs\u0026#34;:{\u0026#34;stats\u0026#34;:{\u0026#34;terms\u0026#34;:{\u0026#34;field\u0026#34;:\u0026#34;text_entry\u0026#34;}}}}\u0026#39; Search(index=\u0026#34;shakespeare\u0026#34;, body=aggs)$hits$hits[[1]] #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;4\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_score` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source` #\u0026gt; $`_source`$line_id #\u0026gt; [1] 5 #\u0026gt; #\u0026gt; $`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speech_number #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source`$line_number #\u0026gt; [1] \u0026#34;1.1.2\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speaker #\u0026gt; [1] \u0026#34;KING HENRY IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$text_entry #\u0026gt; [1] \u0026#34;Find we a time for frighted peace to pant,\u0026#34; Fuzzy query Fuzzy query on numerics\nfuzzy \u0026lt;- list(query = list(fuzzy = list(speech_number = 7))) Search(index=\u0026#34;shakespeare\u0026#34;, body=fuzzy)$hits$total #\u0026gt; [1] 523 fuzzy \u0026lt;- list(query = list(fuzzy = list(speech_number = list(value = 7, fuzziness = 4)))) Search(index=\u0026#34;shakespeare\u0026#34;, body=fuzzy)$hits$total #\u0026gt; [1] 1499 Range query With numeric\nbody \u0026lt;- list(query=list(range=list(decimalLongitude=list(gte=1, lte=3)))) Search(\u0026#39;gbif\u0026#39;, body=body)$hits$total #\u0026gt; [1] 24 body \u0026lt;- list(query=list(range=list(decimalLongitude=list(gte=2.9, lte=10)))) Search(\u0026#39;gbif\u0026#39;, body=body)$hits$total #\u0026gt; [1] 166 With dates\nbody \u0026lt;- list(query=list(range=list(eventDate=list(gte=\u0026#34;2012-01-01\u0026#34;, lte=\u0026#34;now\u0026#34;)))) Search(\u0026#39;gbif\u0026#39;, body=body)$hits$total #\u0026gt; [1] 899 body \u0026lt;- list(query=list(range=list(eventDate=list(gte=\u0026#34;2014-01-01\u0026#34;, lte=\u0026#34;now\u0026#34;)))) Search(\u0026#39;gbif\u0026#39;, body=body)$hits$total #\u0026gt; [1] 685 Highlighting body \u0026lt;- \u0026#39;{ \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34; : \u0026#34;cell\u0026#34; } }, \u0026#34;highlight\u0026#34;: { \u0026#34;fields\u0026#34;: { \u0026#34;title\u0026#34;: {\u0026#34;number_of_fragments\u0026#34;: 2} } } }\u0026#39; out \u0026lt;- Search(\u0026#39;plos\u0026#39;, \u0026#39;article\u0026#39;, body=body) out$hits$total #\u0026gt; [1] 57 sapply(out$hits$hits, function(x) x$highlight$title[[1]])[8:10] #\u0026gt; [1] \u0026#34;c-FLIP Protects Eosinophils from TNF-α-Mediated \u0026lt;em\u0026gt;Cell\u0026lt;/em\u0026gt; Death In Vivo\u0026#34; #\u0026gt; [2] \u0026#34;DUSP1 Is a Novel Target for Enhancing Pancreatic Cancer \u0026lt;em\u0026gt;Cell\u0026lt;/em\u0026gt; Sensitivity to Gemcitabine\u0026#34; #\u0026gt; [3] \u0026#34;Carbon Ion Radiation Inhibits Glioma and Endothelial \u0026lt;em\u0026gt;Cell\u0026lt;/em\u0026gt; Migration Induced by Secreted VEGF\u0026#34; Scrolling search - instead of paging Search(\u0026#39;shakespeare\u0026#39;, q=\u0026#34;a*\u0026#34;)$hits$total #\u0026gt; [1] 2747 res \u0026lt;- Search(index = \u0026#39;shakespeare\u0026#39;, q=\u0026#34;a*\u0026#34;, scroll=\u0026#34;1m\u0026#34;) res \u0026lt;- Search(index = \u0026#39;shakespeare\u0026#39;, q=\u0026#34;a*\u0026#34;, scroll=\u0026#34;1m\u0026#34;, search_type = \u0026#34;scan\u0026#34;) length(scroll(scroll_id = res$`_scroll_id`)$hits$hits) #\u0026gt; [1] 50 res \u0026lt;- Search(index = \u0026#39;shakespeare\u0026#39;, q=\u0026#34;a*\u0026#34;, scroll=\u0026#34;5m\u0026#34;, search_type = \u0026#34;scan\u0026#34;) out \u0026lt;- list() hits \u0026lt;- 1 while(hits != 0){ res \u0026lt;- scroll(scroll_id = res$`_scroll_id`) hits \u0026lt;- length(res$hits$hits) if(hits \u0026gt; 0) out \u0026lt;- c(out, res$hits$hits) } length(out) #\u0026gt; [1] 2747 Woohoo! Collected all 2747 documents in very little time.\nThe cat API List cat methods\ncat_() #\u0026gt; =^.^= #\u0026gt; /_cat/allocation #\u0026gt; /_cat/shards #\u0026gt; /_cat/shards/{index} #\u0026gt; /_cat/master #\u0026gt; /_cat/nodes #\u0026gt; /_cat/indices #\u0026gt; /_cat/indices/{index} #\u0026gt; /_cat/segments #\u0026gt; /_cat/segments/{index} #\u0026gt; /_cat/count #\u0026gt; /_cat/count/{index} #\u0026gt; /_cat/recovery #\u0026gt; /_cat/recovery/{index} #\u0026gt; /_cat/health #\u0026gt; /_cat/pending_tasks #\u0026gt; /_cat/aliases #\u0026gt; /_cat/aliases/{alias} #\u0026gt; /_cat/thread_pool #\u0026gt; /_cat/plugins #\u0026gt; /_cat/fielddata #\u0026gt; /_cat/fielddata/{fields} Get aliases\ncat_aliases() #\u0026gt; things plos - - - #\u0026gt; stuff plos - - - Get indices\ncat_indices() #\u0026gt; yellow open plosmore 5 1 1000 0 3.5mb 3.5mb #\u0026gt; yellow open leotheadfadf 5 1 0 0 575b 575b #\u0026gt; red open alsothat 3 2 #\u0026gt; yellow open gbif 5 1 899 0 1mb 1mb #\u0026gt; yellow open gbifgeopoint 5 1 0 0 575b 575b #\u0026gt; yellow open gbifnewgeo 5 1 2 0 5.8kb 5.8kb #\u0026gt; yellow open plos 5 1 1202 39 14.2mb 14.2mb #\u0026gt; yellow open leothedog 5 1 0 0 575b 575b #\u0026gt; yellow open shakespeare 5 1 5000 0 1mb 1mb #\u0026gt; yellow open gbifgeo 5 1 600 0 861.9kb 861.9kb #\u0026gt; yellow open plosbigdata 5 1 20000 0 53.6mb 53.6mb #\u0026gt; yellow open mapuris 5 1 31 0 34.4kb 34.4kb #\u0026gt; yellow open leothelion 5 1 0 0 575b 575b Get nodes\ncat_nodes() #\u0026gt; Scotts-MacBook-Pro.local 192.168.1.104 6 79 3.44 d * Hellfire Work with indices out \u0026lt;- index_get(index=\u0026#39;shakespeare\u0026#39;) names(out$shakespeare$mappings) #\u0026gt; [1] \u0026#34;line\u0026#34; \u0026#34;scene\u0026#34; \u0026#34;act\u0026#34; Check for index existence\nindex_exists(index=\u0026#39;shakespeare\u0026#39;) #\u0026gt; [1] TRUE Delete an index\nindex_delete(index=\u0026#39;plos\u0026#39;) #\u0026gt; $acknowledged #\u0026gt; [1] TRUE Create an index\nindex_create(index=\u0026#39;twitter\u0026#39;) #\u0026gt; $acknowledged #\u0026gt; [1] TRUE Work with documents Get a document\ndocs_get(index=\u0026#39;shakespeare\u0026#39;, type=\u0026#39;line\u0026#39;, id=10) #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;10\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $found #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $`_source` #\u0026gt; $`_source`$line_id #\u0026gt; [1] 11 #\u0026gt; #\u0026gt; $`_source`$play_name #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speech_number #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $`_source`$line_number #\u0026gt; [1] \u0026#34;1.1.8\u0026#34; #\u0026gt; #\u0026gt; $`_source`$speaker #\u0026gt; [1] \u0026#34;KING HENRY IV\u0026#34; #\u0026gt; #\u0026gt; $`_source`$text_entry #\u0026gt; [1] \u0026#34;Nor bruise her flowerets with the armed hoofs\u0026#34; Get certain fields\ndocs_get(index=\u0026#39;shakespeare\u0026#39;, type=\u0026#39;line\u0026#39;, id=10, fields=c(\u0026#39;play_name\u0026#39;,\u0026#39;speaker\u0026#39;)) #\u0026gt; $`_index` #\u0026gt; [1] \u0026#34;shakespeare\u0026#34; #\u0026gt; #\u0026gt; $`_type` #\u0026gt; [1] \u0026#34;line\u0026#34; #\u0026gt; #\u0026gt; $`_id` #\u0026gt; [1] \u0026#34;10\u0026#34; #\u0026gt; #\u0026gt; $`_version` #\u0026gt; [1] 1 #\u0026gt; #\u0026gt; $found #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $fields #\u0026gt; $fields$play_name #\u0026gt; $fields$play_name[[1]] #\u0026gt; [1] \u0026#34;Henry IV\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $fields$speaker #\u0026gt; $fields$speaker[[1]] #\u0026gt; [1] \u0026#34;KING HENRY IV\u0026#34; Test for existence of the document\ndocs_get(index=\u0026#39;plos\u0026#39;, type=\u0026#39;article\u0026#39;, id=1, exists=TRUE) #\u0026gt; [1] FALSE docs_get(index=\u0026#39;plos\u0026#39;, type=\u0026#39;article\u0026#39;, id=123456, exists=TRUE) #\u0026gt; [1] FALSE Thats it Let us know if you have any feedback!\n","permalink":"http://localhost:1313/2015/01/elasticsearch/","summary":"\u003cp\u003eWe\u0026rsquo;ve (ropensci) been working on an R client for interacting with \u003ca href=\"https://www.elasticsearch.org/\"\u003eElasticsearch\u003c/a\u003e for a while now, first commit was November 2013.\u003c/p\u003e\n\u003cp\u003eElasticsearch is a document database built on the JVM. \u003ccode\u003eelastic\u003c/code\u003e interacts with the Elasticsearch HTTP API, and includes functions for setting connection details to Elasticsearch instances, loading bulk data, searching for documents with both HTTP query variables and JSON based body requests. In addition, \u003ccode\u003eelastic\u003c/code\u003e provides functions for interacting with APIs for indices, documents, nodes, clusters, an interface to the cat API, and more.\u003c/p\u003e","title":"elastic - Elasticsearch from R"},{"content":"I maintain, along with other awesome people, the taxize R package - a taxonomic toolbelt for R, for interacting with taxonomic data sources on the web.\nTaxonomy data is not standardized, but there are a lot of common elements, and there is a finite list of taxonomic ranks, and finite number of major taxonomic data sets. Thus, I\u0026rsquo;ve been interested in attempting to define a pseudo standard for expressing taxonomic data in R. The conversation started a while back in a GitHub issue, and hasn\u0026rsquo;t moved very far.\nI decided to start playing with this more, which is easier to do in a separate pacakge. Thus: binomen. It\u0026rsquo;s an attempt to define a set of taxonomic classes/objects in R, along with a suite of functions to help construct and parse these objects.\nWould love any/all feedback.\nHere\u0026rsquo;s some examples:\nInstall Install binomen\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/binomen\u0026#34;) library(\u0026#34;binomen\u0026#34;) Make a taxon Make a taxon object\n(obj \u0026lt;- make_taxon(genus=\u0026#34;Poa\u0026#34;, epithet=\u0026#34;annua\u0026#34;, authority=\u0026#34;L.\u0026#34;, family=\u0026#39;Poaceae\u0026#39;, clazz=\u0026#39;Poales\u0026#39;, kingdom=\u0026#39;Plantae\u0026#39;, variety=\u0026#39;annua\u0026#39;)) #\u0026gt; \u0026lt;taxon\u0026gt; #\u0026gt; binomial: Poa annua #\u0026gt; classification: #\u0026gt; kingdom: Plantae #\u0026gt; clazz: Poales #\u0026gt; family: Poaceae #\u0026gt; genus: Poa #\u0026gt; species: Poa annua #\u0026gt; variety: annua Index to various parts of the object\nThe binomial\nobj$binomial #\u0026gt; \u0026lt;binomial\u0026gt; #\u0026gt; genus: Poa #\u0026gt; epithet: annua #\u0026gt; canonical: Poa annua #\u0026gt; species: Poa annua L. #\u0026gt; authority: L. The authority\nobj$binomial$authority #\u0026gt; [1] \u0026#34;L.\u0026#34; The classification\nobj$classification #\u0026gt; \u0026lt;classification\u0026gt; #\u0026gt; kingdom: Plantae #\u0026gt; clazz: Poales #\u0026gt; family: Poaceae #\u0026gt; genus: Poa #\u0026gt; species: Poa annua #\u0026gt; variety: annua The family\nobj$classification$family #\u0026gt; \u0026lt;taxonref\u0026gt; #\u0026gt; rank: family #\u0026gt; name: Poaceae #\u0026gt; id: none #\u0026gt; uri: none Subset taxon objects Get a single rank\nobj %\u0026gt;% select(family) #\u0026gt; \u0026lt;taxonref\u0026gt; #\u0026gt; rank: family #\u0026gt; name: Poaceae #\u0026gt; id: none #\u0026gt; uri: none Get a range of ranks\nobj %\u0026gt;% range(kingdom, family) #\u0026gt; $kingdom #\u0026gt; \u0026lt;taxonref\u0026gt; #\u0026gt; rank: kingdom #\u0026gt; name: Plantae #\u0026gt; id: none #\u0026gt; uri: none #\u0026gt; #\u0026gt; $clazz #\u0026gt; \u0026lt;taxonref\u0026gt; #\u0026gt; rank: clazz #\u0026gt; name: Poales #\u0026gt; id: none #\u0026gt; uri: none #\u0026gt; #\u0026gt; $family #\u0026gt; \u0026lt;taxonref\u0026gt; #\u0026gt; rank: family #\u0026gt; name: Poaceae #\u0026gt; id: none #\u0026gt; uri: none Extract classification as a data.frame\ngethier(obj) #\u0026gt; rank name #\u0026gt; 1 kingdom Plantae #\u0026gt; 2 clazz Poales #\u0026gt; 3 family Poaceae #\u0026gt; 4 genus Poa #\u0026gt; 5 species Poa annua #\u0026gt; 6 variety annua Taxonomic data.frame\u0026rsquo;s Make one\ndf \u0026lt;- data.frame( order=c(\u0026#39;Asterales\u0026#39;,\u0026#39;Asterales\u0026#39;,\u0026#39;Fagales\u0026#39;,\u0026#39;Poales\u0026#39;,\u0026#39;Poales\u0026#39;,\u0026#39;Poales\u0026#39;), family=c(\u0026#39;Asteraceae\u0026#39;,\u0026#39;Asteraceae\u0026#39;,\u0026#39;Fagaceae\u0026#39;,\u0026#39;Poaceae\u0026#39;,\u0026#39;Poaceae\u0026#39;,\u0026#39;Poaceae\u0026#39;), genus=c(\u0026#39;Helianthus\u0026#39;,\u0026#39;Helianthus\u0026#39;,\u0026#39;Quercus\u0026#39;,\u0026#39;Poa\u0026#39;,\u0026#39;Festuca\u0026#39;,\u0026#39;Holodiscus\u0026#39;), stringsAsFactors = FALSE) (df2 \u0026lt;- taxon_df(df)) #\u0026gt; order family genus #\u0026gt; 1 Asterales Asteraceae Helianthus #\u0026gt; 2 Asterales Asteraceae Helianthus #\u0026gt; 3 Fagales Fagaceae Quercus #\u0026gt; 4 Poales Poaceae Poa #\u0026gt; 5 Poales Poaceae Festuca #\u0026gt; 6 Poales Poaceae Holodiscus Parse - get rank order matching Fagales\ndf2 %\u0026gt;% select(order, Fagales) #\u0026gt; order family genus #\u0026gt; 3 Fagales Fagaceae Quercus get rank family matching Asteraceae\ndf2 %\u0026gt;% select(family, Asteraceae) #\u0026gt; order family genus #\u0026gt; 1 Asterales Asteraceae Helianthus #\u0026gt; 2 Asterales Asteraceae Helianthus get rank genus matching Poa\ndf2 %\u0026gt;% select(genus, Poa) #\u0026gt; order family genus #\u0026gt; 4 Poales Poaceae Poa ","permalink":"http://localhost:1313/2015/01/binomen/","summary":"\u003cp\u003eI maintain, along with other \u003ca href=\"https://github.com/ropensci/taxize/graphs/contributors\"\u003eawesome people\u003c/a\u003e, the \u003ca href=\"https://github.com/ropensci/taxize\"\u003etaxize\u003c/a\u003e R package - a taxonomic toolbelt for R, for interacting with taxonomic data sources on the web.\u003c/p\u003e\n\u003cp\u003eTaxonomy data is not standardized, but there are a lot of common elements, and there is a finite list of taxonomic ranks, and finite number of major taxonomic data sets. Thus, I\u0026rsquo;ve been interested in attempting to define a pseudo standard for expressing taxonomic data in R. The conversation \u003ca href=\"https://github.com/ropensci/taxize/issues/261\"\u003estarted a while back\u003c/a\u003e in a GitHub issue, and hasn\u0026rsquo;t moved very far.\u003c/p\u003e","title":"binomen - taxonomic classes and parsing"},{"content":"Discourse is a great discussion forum application. It\u0026rsquo;s another thing from Jeff Atwood, the co-founder of Stackoverflow/Stackexchange. The installation is epecially easy with their dockerized installation setup on DigitalOcean ([instructions here][https://www.digitalocean.com/community/tutorials/how-to-install-discourse-on-ubuntu-14-04]).\nIn rOpenSci, we\u0026rsquo;ve been using a Google Groups mailing list, which is sufficient I guess, but doesn\u0026rsquo;t support Markdown, and we all know Google can kill products any day, so it makes sense to use something with which we have more control. We\u0026rsquo;ve set up our own Discourse installation to have rOpenSci discussions - find it at discuss.ropensci.org. Check it out if you want to discuss anything rOpenSci related, or general open science, open source software, etc. You can login with email, Mozilla Persona, Twitter, or GitHub.\nDiscourse does have a RESTful API, which I found through the Ruby gem. Why not interact with the API via R?\nInstall Install discgolf\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;sckott/discgolf\u0026#34;) library(\u0026#34;discgolf\u0026#34;) Authentication The Discourse API is based on using a specific installation of Discourse (as far as I know), which requires your username and an API key for that installation. Get those (I found mine in the admin panel), and you can pass them in to each function call, or set as option variables in .Rprofile (use discourse_api_key and discourse_username) or environment variables in .Renviron (use DISCOURSE_API_KEY and DISCOURSE_USERNAME).\nExamples Get the latest topics (abbreviated content for blog post brevity)\nlatest_topics() #\u0026gt; id title fancy_title #\u0026gt; 1 8 Welcome to rOpenSci Discuss Welcome to rOpenSci Discuss #\u0026gt; 2 92 Feedback on geojsonio package? Feedback on geojsonio package? #\u0026gt; 3 102 Astronomy research Astronomy research #\u0026gt; 4 99 Rgbif argument question Rgbif argument question #\u0026gt; 5 93 Feedback on rnoaa ghcnd functions Feedback on rnoaa ghcnd functions #\u0026gt; slug #\u0026gt; 1 welcome-to-ropensci-discuss #\u0026gt; 2 feedback-on-geojsonio-package #\u0026gt; 3 astronomy-research #\u0026gt; 4 rgbif-argument-question #\u0026gt; 5 feedback-on-rnoaa-ghcnd-functions Get new topics\nnew_topics() #\u0026gt; $topic_list #\u0026gt; $topic_list$can_create_topic #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $topic_list$draft #\u0026gt; NULL #\u0026gt; #\u0026gt; $topic_list$draft_key #\u0026gt; [1] \u0026#34;new_topic\u0026#34; #\u0026gt; #\u0026gt; $topic_list$draft_sequence #\u0026gt; [1] 15 #\u0026gt; #\u0026gt; $topic_list$per_page #\u0026gt; [1] 30 #\u0026gt; #\u0026gt; $topic_list$topics #\u0026gt; list() Get topics by a specific user\ntopics_by(\u0026#34;cboettig\u0026#34;) #\u0026gt; $users #\u0026gt; id username uploaded_avatar_id #\u0026gt; 1 3 cboettig 4 #\u0026gt; 2 1 sckott 2 #\u0026gt; 3 35 noamross 57 #\u0026gt; 4 2 karthik 3 #\u0026gt; avatar_template #\u0026gt; 1 /user_avatar/discuss.ropensci.org/cboettig/{size}/4.png #\u0026gt; 2 /user_avatar/discuss.ropensci.org/sckott/{size}/2.png #\u0026gt; 3 /user_avatar/discuss.ropensci.org/noamross/{size}/57.png #\u0026gt; 4 /user_avatar/discuss.ropensci.org/karthik/{size}/3.png #\u0026gt; #\u0026gt; $topic_list #\u0026gt; $topic_list$can_create_topic #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $topic_list$draft #\u0026gt; NULL #\u0026gt; #\u0026gt; $topic_list$draft_key #\u0026gt; [1] \u0026#34;new_topic\u0026#34; #\u0026gt; #\u0026gt; $topic_list$draft_sequence #\u0026gt; [1] 15 #\u0026gt; #\u0026gt; $topic_list$per_page #\u0026gt; [1] 30 #\u0026gt; #\u0026gt; $topic_list$topics #\u0026gt; id title #\u0026gt; 1 15 Using Discourse for blog comments as well? #\u0026gt; 2 16 Reply by email? #\u0026gt; fancy_title #\u0026gt; 1 Using Discourse for blog comments as well? #\u0026gt; 2 Reply by email? #\u0026gt; slug posts_count reply_count #\u0026gt; 1 using-discourse-for-blog-comments-as-well 8 4 #\u0026gt; 2 reply-by-email 6 2 #\u0026gt; highest_post_number image_url created_at #\u0026gt; 1 8 NA 2014-12-15T19:33:11.879Z #\u0026gt; 2 6 NA 2014-12-15T20:10:36.414Z #\u0026gt; last_posted_at bumped bumped_at unseen #\u0026gt; 1 2015-01-02T19:47:42.403Z TRUE 2015-01-02T19:47:42.403Z FALSE #\u0026gt; 2 2014-12-17T00:18:31.427Z TRUE 2014-12-17T00:18:31.427Z FALSE #\u0026gt; last_read_post_number unread new_posts pinned unpinned visible closed #\u0026gt; 1 8 0 0 FALSE NA TRUE FALSE #\u0026gt; 2 6 0 0 FALSE NA TRUE FALSE #\u0026gt; archived notification_level bookmarked liked views like_count #\u0026gt; 1 FALSE 2 TRUE FALSE 71 0 #\u0026gt; 2 FALSE 3 TRUE FALSE 54 0 #\u0026gt; has_summary archetype last_poster_username category_id pinned_globally #\u0026gt; 1 FALSE regular cboettig 3 FALSE #\u0026gt; 2 FALSE regular sckott 1 FALSE #\u0026gt; bookmarked_post_numbers #\u0026gt; 1 1 #\u0026gt; 2 1 #\u0026gt; posters #\u0026gt; 1 latest, NA, NA, NA, Original Poster, Most Recent Poster, Frequent Poster, Frequent Poster, Frequent Poster, 3, 1, 35, 2 #\u0026gt; 2 NA, latest, Original Poster, Most Recent Poster, 3, 1 Get a single topic by id number\ntopic(8) #\u0026gt; id name username #\u0026gt; 1 11 system system #\u0026gt; 2 14 Scott Chamberlain sckott #\u0026gt; 3 51 Scott Chamberlain sckott #\u0026gt; avatar_template uploaded_avatar_id #\u0026gt; 1 /user_avatar/discuss.ropensci.org/system/{size}/1.png 1 #\u0026gt; 2 /user_avatar/discuss.ropensci.org/sckott/{size}/2.png 2 #\u0026gt; 3 /user_avatar/discuss.ropensci.org/sckott/{size}/2.png 2 Create topic\ntext \u0026lt;- \u0026#39; print(\u0026#34;hello world\u0026#34;) #\u0026gt; [1] \u0026#34;hello world\u0026#34; head(mtcars) #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb #\u0026gt; Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #\u0026gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #\u0026gt; Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #\u0026gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #\u0026gt; Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #\u0026gt; Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 \u0026#39; create_topic(title=\u0026#34;testing from discgolf - 2\u0026#34;, text=text) Wrapup There are more functions I didn\u0026rsquo;t highlight, and there are many methods that haven\u0026rsquo;t been implemented yet\u0026hellip; in good time it will be done.\n","permalink":"http://localhost:1313/2015/01/discourse-in-r/","summary":"\u003cp\u003e\u003ca href=\"https://www.discourse.org/\"\u003eDiscourse\u003c/a\u003e is a great discussion forum application. It\u0026rsquo;s another thing from \u003ca href=\"https://en.wikipedia.org/wiki/Jeff_Atwood\"\u003eJeff Atwood\u003c/a\u003e, the co-founder of \u003ca href=\"https://stackoverflow.com/\"\u003eStackoverflow/Stackexchange\u003c/a\u003e. The installation is epecially easy with their dockerized installation setup on DigitalOcean ([instructions here][https://www.digitalocean.com/community/tutorials/how-to-install-discourse-on-ubuntu-14-04]).\u003c/p\u003e\n\u003cp\u003eIn \u003ca href=\"https://ropensci.org/\"\u003erOpenSci\u003c/a\u003e, we\u0026rsquo;ve been using a Google Groups mailing list, which is sufficient I guess, but doesn\u0026rsquo;t support Markdown, and we all know \u003ca href=\"https://www.slate.com/articles/technology/technology/2013/03/google_reader_why_did_everyone_s_favorite_rss_program_die_what_free_web.html\"\u003eGoogle can kill products any day\u003c/a\u003e, so it makes sense to use something with which we have more control. We\u0026rsquo;ve set up our own Discourse installation to have rOpenSci discussions - find it at \u003ca href=\"https://meta.discourse.org/\"\u003ediscuss.ropensci.org\u003c/a\u003e. Check it out if you want to discuss anything rOpenSci related, or general open science, open source software, etc. You can login with email, Mozilla Persona, Twitter, or GitHub.\u003c/p\u003e","title":"discgolf - Dicourse from R"},{"content":"At rOpenSci we\u0026rsquo;ve been working on an R package (geojsonio) to make converting R data in various formats to geoJSON and topoJSON, and vice versa. We hope to do this one job very well, and handle all reasonable use cases.\nFunctions in this package are organized first around what you\u0026rsquo;re working with or want to get, geojson or topojson, then convert to or read from various formats:\ngeojson_list()/topojson_list() - convert to geojson/topojson as R list format geojson_json()/topojson_json() - convert to geojson/topojson as json geojson_read()``topojson_read() - read a geojson/topojson file from file path or URL geojson_write()/topojson_write() - write a geojson/topojson file locally Each of the above functions have methods for various objects/classes, including numeric, data.frame, list, SpatialPolygons, SpatialPolygonsDataFrame, SpatialLines, SpatialLinesDataFrame, SpatialPoints, SpatialPointsDataFrame.\nInstall Install rgdal - in case you can\u0026rsquo;t get it installed from binary , here\u0026rsquo;s what works on a Mac.\ninstall.packages(\u0026#34;https://cran.r-project.org/src/contrib/rgdal_0.9-1.tar.gz\u0026#34;, repos = NULL, type=\u0026#34;source\u0026#34;, configure.args = \u0026#34;--with-gdal-config=/Library/Frameworks/GDAL.framework/Versions/1.10/unix/bin/gdal-config --with-proj-include=/Library/Frameworks/PROJ.framework/unix/include --with-proj-lib=/Library/Frameworks/PROJ.framework/unix/lib\u0026#34;) Install geojsonio\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/geojsonio\u0026#34;) library(\u0026#34;geojsonio\u0026#34;) GeoJSON Convert various formats to geojson From a numeric vector of length 2, as json or list\ngeojson_json(c(32.45,-99.74)) #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[32.45,-99.74]},\u0026#34;properties\u0026#34;:{}} geojson_list(c(32.45,-99.74)) #\u0026gt; $type #\u0026gt; [1] \u0026#34;Point\u0026#34; #\u0026gt; #\u0026gt; $geometry #\u0026gt; $geometry$type #\u0026gt; [1] \u0026#34;Point\u0026#34; #\u0026gt; #\u0026gt; $geometry$coordinates #\u0026gt; [1] 32.45 -99.74 #\u0026gt; #\u0026gt; #\u0026gt; $properties #\u0026gt; NULL #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;geo_list\u0026#34; #\u0026gt; attr(,\u0026#34;from\u0026#34;) #\u0026gt; [1] \u0026#34;numeric\u0026#34; From a data.frame\nlibrary(\u0026#39;maps\u0026#39;) data(us.cities) geojson_json(us.cities[1:2,], lat=\u0026#39;lat\u0026#39;, lon=\u0026#39;long\u0026#39;) #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;FeatureCollection\u0026#34;,\u0026#34;features\u0026#34;:[{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[-99.74,32.45]},\u0026#34;properties\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Abilene TX\u0026#34;,\u0026#34;country.etc\u0026#34;:\u0026#34;TX\u0026#34;,\u0026#34;pop\u0026#34;:\u0026#34;113888\u0026#34;,\u0026#34;capital\u0026#34;:\u0026#34;0\u0026#34;}},{\u0026#34;type\u0026#34;:\u0026#34;Feature\u0026#34;,\u0026#34;geometry\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;Point\u0026#34;,\u0026#34;coordinates\u0026#34;:[-81.52,41.08]},\u0026#34;properties\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;Akron OH\u0026#34;,\u0026#34;country.etc\u0026#34;:\u0026#34;OH\u0026#34;,\u0026#34;pop\u0026#34;:\u0026#34;206634\u0026#34;,\u0026#34;capital\u0026#34;:\u0026#34;0\u0026#34;}}]} geojson_list(us.cities[1:2,], lat=\u0026#39;lat\u0026#39;, lon=\u0026#39;long\u0026#39;) #\u0026gt; $type #\u0026gt; [1] \u0026#34;FeatureCollection\u0026#34; #\u0026gt; #\u0026gt; $features #\u0026gt; $features[[1]] #\u0026gt; $features[[1]]$type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$geometry #\u0026gt; $features[[1]]$geometry$type #\u0026gt; [1] \u0026#34;Point\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$geometry$coordinates #\u0026gt; [1] -99.74 32.45 #\u0026gt; #\u0026gt; #\u0026gt; $features[[1]]$properties #\u0026gt; $features[[1]]$properties$name #\u0026gt; [1] \u0026#34;Abilene TX\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$properties$country.etc #\u0026gt; [1] \u0026#34;TX\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$properties$pop #\u0026gt; [1] \u0026#34;113888\u0026#34; #\u0026gt; #\u0026gt; $features[[1]]$properties$capital #\u0026gt; [1] \u0026#34;0\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; #\u0026gt; $features[[2]] #\u0026gt; $features[[2]]$type #\u0026gt; [1] \u0026#34;Feature\u0026#34; #\u0026gt; #\u0026gt; $features[[2]]$geometry #\u0026gt; $features[[2]]$geometry$type #\u0026gt; [1] \u0026#34;Point\u0026#34; #\u0026gt; #\u0026gt; $features[[2]]$geometry$coordinates #\u0026gt; [1] -81.52 41.08 #\u0026gt; #\u0026gt; #\u0026gt; $features[[2]]$properties #\u0026gt; $features[[2]]$properties$name #\u0026gt; [1] \u0026#34;Akron OH\u0026#34; #\u0026gt; #\u0026gt; $features[[2]]$properties$country.etc #\u0026gt; [1] \u0026#34;OH\u0026#34; #\u0026gt; #\u0026gt; $features[[2]]$properties$pop #\u0026gt; [1] \u0026#34;206634\u0026#34; #\u0026gt; #\u0026gt; $features[[2]]$properties$capital #\u0026gt; [1] \u0026#34;0\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;geo_list\u0026#34; #\u0026gt; attr(,\u0026#34;from\u0026#34;) #\u0026gt; [1] \u0026#34;data.frame\u0026#34; From SpatialPolygons class\nlibrary(\u0026#39;sp\u0026#39;) poly1 \u0026lt;- Polygons(list(Polygon(cbind(c(-100,-90,-85,-100), c(40,50,45,40)))), \u0026#34;1\u0026#34;) poly2 \u0026lt;- Polygons(list(Polygon(cbind(c(-90,-80,-75,-90), c(30,40,35,30)))), \u0026#34;2\u0026#34;) sp_poly \u0026lt;- SpatialPolygons(list(poly1, poly2), 1:2) to json\ngeojson_json(sp_poly) #\u0026gt; {\u0026#34;type\u0026#34;:\u0026#34;Polygon\u0026#34;,\u0026#34;bbox\u0026#34;:[-100,-75,30,50],\u0026#34;coordinates\u0026#34;:[[[-100,40],[-90,50],[-85,45],[-100,40]],[[-90,30],[-80,40],[-75,35],[-90,30]]],\u0026#34;properties\u0026#34;:{}} to list\ngeojson_list(sp_poly)$coordinates[[1]] #\u0026gt; [[1]] #\u0026gt; [[1]][[1]] #\u0026gt; [1] -100 #\u0026gt; #\u0026gt; [[1]][[2]] #\u0026gt; [1] 40 #\u0026gt; #\u0026gt; #\u0026gt; [[2]] #\u0026gt; [[2]][[1]] #\u0026gt; [1] -90 #\u0026gt; #\u0026gt; [[2]][[2]] #\u0026gt; [1] 50 #\u0026gt; #\u0026gt; #\u0026gt; [[3]] #\u0026gt; [[3]][[1]] #\u0026gt; [1] -85 #\u0026gt; #\u0026gt; [[3]][[2]] #\u0026gt; [1] 45 #\u0026gt; #\u0026gt; #\u0026gt; [[4]] #\u0026gt; [[4]][[1]] #\u0026gt; [1] -100 #\u0026gt; #\u0026gt; [[4]][[2]] #\u0026gt; [1] 40 Write geojson library(\u0026#39;maps\u0026#39;) data(us.cities) geojson_write(us.cities[1:2,], lat=\u0026#39;lat\u0026#39;, lon=\u0026#39;long\u0026#39;) #\u0026gt; \u0026lt;geojson\u0026gt; #\u0026gt; Path: myfile.geojson #\u0026gt; From class: data.frame Read geojson file \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;california.geojson\u0026#34;, package = \u0026#34;geojsonio\u0026#34;) out \u0026lt;- geojson_read(file) #\u0026gt; OGR data source with driver: GeoJSON #\u0026gt; Source: \u0026#34;/Users/sacmac/Library/R/3.1/library/geojsonio/examples/california.geojson\u0026#34;, layer: \u0026#34;OGRGeoJSON\u0026#34; #\u0026gt; with 1 features and 11 fields #\u0026gt; Feature type: wkbMultiPolygon with 2 dimensions plot(out) TopoJSON Convert to TopoJSON with Node topojson client For topojson you will need Mike Bostock\u0026rsquo;s command line client. Install it by doing\nsudo npm install -g topojson Download a zipped shape fileset, this one for distribution of Quercus wislizeni. Unzip the zip file to a folder. Then do (changing the path to your path)\ntopojson_write(shppath=\u0026#39;~/Downloads/querwisl\u0026#39;, path = \u0026#34;~/Downloads\u0026#34;, projection=\u0026#39;albers\u0026#39;, projargs=list(rotate=\u0026#39;[60, -35, 0]\u0026#39;)) #\u0026gt; OGR data source with driver: ESRI Shapefile #\u0026gt; Source: \u0026#34;/Users/sacmac/Downloads/querwisl\u0026#34;, layer: \u0026#34;querwisl\u0026#34; #\u0026gt; with 35 features and 5 fields #\u0026gt; Feature type: wkbPolygon with 2 dimensions Which prints progress on the conversion of the shape file. And prints the topojson CLI call, including the location of the output file, here /Users/sacmac/querwisl.json\nOGR data source with driver: ESRI Shapefile Source: \u0026#34;/Users/sacmac/Downloads/querwisl\u0026#34;, layer: \u0026#34;querwisl\u0026#34; with 35 features and 5 fields Feature type: wkbPolygon with 2 dimensions topojson -o /Users/sacmac/querwisl.json -q 1e4 -s 0 --shapefile-encoding utf8 --projection \u0026#39;d3.geo.albers().rotate([60, -35, 0])\u0026#39; -- /var/folders/gs/4khph0xs0436gmd2gdnwsg080000gn/T//Rtmp49APW7/querwisl.shp bounds: -403.2554825867553 401.27189387582916 -295.798050380061 585.4214768677039 (cartesian) pre-quantization: 0.010746817902459677 0.018416799979185387 topology: 35 arcs, 2492 points prune: retained 35 / 35 arcs (100%) You can then use this topojson file wherever. We\u0026rsquo;ll add a function soon to automagically throw this file up as a Github gist to get an interactive map.\nRead topojson TopoJSON\nurl \u0026lt;- \u0026#34;https://raw.githubusercontent.com/shawnbot/d3-cartogram/master/data/us-states.topojson\u0026#34; out \u0026lt;- topojson_read(url) #\u0026gt; OGR data source with driver: GeoJSON #\u0026gt; Source: \u0026#34;https://raw.githubusercontent.com/shawnbot/d3-cartogram/master/data/us-states.topojson\u0026#34;, layer: \u0026#34;states\u0026#34; #\u0026gt; with 51 features and 2 fields #\u0026gt; Feature type: wkbPolygon with 2 dimensions plot(out) Use case: Play with US states Using data from https://github.com/glynnbird/usstatesgeojson\nGet some geojson\nlibrary(\u0026#39;httr\u0026#39;) res \u0026lt;- GET(\u0026#39;https://api.github.com/repos/glynnbird/usstatesgeojson/contents\u0026#39;) st_names \u0026lt;- Filter(function(x) grepl(\u0026#34;\\\\.geojson\u0026#34;, x), sapply(content(res), \u0026#34;[[\u0026#34;, \u0026#34;name\u0026#34;)) base \u0026lt;- \u0026#39;https://raw.githubusercontent.com/glynnbird/usstatesgeojson/master/\u0026#39; st_files \u0026lt;- paste0(base, st_names) Make a faceted plot\nlibrary(\u0026#39;ggplot2\u0026#39;) library(\u0026#39;plyr\u0026#39;) st_use \u0026lt;- st_files[7:13] geo \u0026lt;- lapply(st_use, geojson_read, verbose = FALSE) df \u0026lt;- ldply(setNames(lapply(geo, fortify), gsub(\u0026#34;\\\\.geojson\u0026#34;, \u0026#34;\u0026#34;, st_names[7:13]))) ggplot(df, aes(long, lat, group = group)) + geom_polygon() + facet_wrap(~ .id, scales = \u0026#34;free\u0026#34;) Maps We\u0026rsquo;re also working on map_gist() - to push up a geojson or topojson file as a GitHub gist (renders as an interactive map). It still needs some work\u0026hellip;\nValidate geojson geojsonlint.com has a nice web interface you can use to make sure you\u0026rsquo;re geoJSON is in the right format. In addition, it provides a RESTful endpoint at geojsonlint.com/validate to validate geoJSON. This was just added today in this package, and can be used from many data types.\nGood geoJSON\nvalidate(\u0026#39;{\u0026#34;type\u0026#34;: \u0026#34;Point\u0026#34;, \u0026#34;coordinates\u0026#34;: [-100, 80]}\u0026#39;) #\u0026gt; $status #\u0026gt; [1] \u0026#34;ok\u0026#34; Bad geoJSON\nvalidate(\u0026#39;{\u0026#34;type\u0026#34;: \u0026#34;Rhombus\u0026#34;, \u0026#34;coordinates\u0026#34;: [[1, 2], [3, 4], [5, 6]]}\u0026#39;) #\u0026gt; $message #\u0026gt; [1] \u0026#34;\\\u0026#34;Rhombus\\\u0026#34; is not a valid GeoJSON type.\u0026#34; #\u0026gt; #\u0026gt; $status #\u0026gt; [1] \u0026#34;error\u0026#34; A file\nfile \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;zillow_or.geojson\u0026#34;, package = \u0026#34;togeojson\u0026#34;) validate(x = as.location(file)) #\u0026gt; $status #\u0026gt; [1] \u0026#34;ok\u0026#34; A URL\nurl \u0026lt;- \u0026#34;https://raw.githubusercontent.com/glynnbird/usstatesgeojson/master/california.geojson\u0026#34; validate(as.location(url)) #\u0026gt; $status #\u0026gt; [1] \u0026#34;ok\u0026#34; From the output of geojson_list()\nlibrary(\u0026#34;maps\u0026#34;) data(us.cities) x \u0026lt;- geojson_list(us.cities[1:2,], lat=\u0026#39;lat\u0026#39;, lon=\u0026#39;long\u0026#39;) validate(x) #\u0026gt; $status #\u0026gt; [1] \u0026#34;ok\u0026#34; From SpatialPoints class\nlibrary(\u0026#34;sp\u0026#34;) a \u0026lt;- c(1,2,3,4,5) b \u0026lt;- c(3,2,5,1,4) x \u0026lt;- SpatialPoints(cbind(a,b)) validate(x) #\u0026gt; $status #\u0026gt; [1] \u0026#34;ok\u0026#34; ","permalink":"http://localhost:1313/2015/01/geojson-topojson-io/","summary":"\u003cp\u003eAt rOpenSci we\u0026rsquo;ve been working on an R package (\u003ccode\u003egeojsonio\u003c/code\u003e) to make converting R data in various formats to \u003ca href=\"https://geojson.org/geojson-spec.html\"\u003egeoJSON\u003c/a\u003e and \u003ca href=\"https://github.com/topojson/topojson-specification/blob/master/README.md\"\u003etopoJSON\u003c/a\u003e, and vice versa. We hope to do this one job very well, and handle all reasonable use cases.\u003c/p\u003e\n\u003cp\u003eFunctions in this package are organized first around what you\u0026rsquo;re working with or want to get, \u003ccode\u003egeojson\u003c/code\u003e or \u003ccode\u003etopojson\u003c/code\u003e, then convert to or read from various formats:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egeojson_list()\u003c/code\u003e/\u003ccode\u003etopojson_list()\u003c/code\u003e - convert to geojson/topojson as R list format\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003egeojson_json()\u003c/code\u003e/\u003ccode\u003etopojson_json()\u003c/code\u003e - convert to geojson/topojson as json\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003egeojson_read()``topojson_read()\u003c/code\u003e - read a geojson/topojson file from file path or URL\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003egeojson_write()\u003c/code\u003e/\u003ccode\u003etopojson_write()\u003c/code\u003e - write a geojson/topojson file locally\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of the above functions have methods for various objects/classes, including \u003ccode\u003enumeric\u003c/code\u003e, \u003ccode\u003edata.frame\u003c/code\u003e, \u003ccode\u003elist\u003c/code\u003e, \u003ccode\u003eSpatialPolygons\u003c/code\u003e, \u003ccode\u003eSpatialPolygonsDataFrame\u003c/code\u003e, \u003ccode\u003eSpatialLines\u003c/code\u003e, \u003ccode\u003eSpatialLinesDataFrame\u003c/code\u003e, \u003ccode\u003eSpatialPoints\u003c/code\u003e, \u003ccode\u003eSpatialPointsDataFrame\u003c/code\u003e.\u003c/p\u003e","title":"R I/O for geojson and topojson"},{"content":"GitHub has this site https://gist.github.com/ in which we can share code, text, images, maps, plots, etc super easily, without having to open up a repo, etc. GitHub gists are a great way to throw up an example use case to show someone, or show code that\u0026rsquo;s throwing errors to a support person, etc. In addition, there\u0026rsquo;s API access, which means we can interact with Gists not just from their web interface, but from the command line, or any programming language. There are clients for Node.js, Ruby, Python, and on and on. But AFAIK there wasn\u0026rsquo;t one for R. Along with Ramnath and others, we\u0026rsquo;ve been working on an R client for gists. v0.1 is now on CRAN. Below is an overview.\nOne useful thing about this package in terms of other R packages is that you can now depend on gistr to easily share results as Gists. For example, you can share maps as gists (via geojson, rendered as interactive map), or code, or plots, etc. That is, you don\u0026rsquo;t have to re-write an interface to Github Gists yourself. We plan on using gistr in a few rOpenSci packages.\nInstallation Installation Install from CRAN\ninstall.packages(\u0026#34;gistr\u0026#34;) Or the development version from GitHub\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/gistr\u0026#34;) library(\u0026#34;gistr\u0026#34;) Authentication There are two ways to authorise gistr to work with your GitHub account:\nGenerate a personal access token (PAT) at https://help.github.com/articles/creating-an-access-token-for-command-line-use and record it in the GITHUB_PAT envar. Interactively login into your GitHub account and authorise with OAuth. Using the PAT is recommended.\nUsing the gist_auth() function you can authenticate seperately first, or if you\u0026rsquo;re not authenticated, this function will run internally with each functionn call. If you have a PAT, that will be used, if not, OAuth will be used.\ngist_auth() Workflow In gistr you can use pipes to pass outputs from one function to another. If you have used dplyr with pipes you can see the difference, and perhaps the utility, of this workflow over the traditional workflow in R. You can use a non-piping or a piping workflow with gistr. Examples below use a mix of both workflows. Here is an example of a piping wofklow (with some explanation):\ngists(what = \u0026#34;minepublic\u0026#34;)[[6]] %\u0026gt;% # List my public gists, and index 1st add_files(\u0026#34;~/alm_othersources.md\u0026#34;) %\u0026gt;% # Add new file to that gist update() # update sends a PATCH command to Gists API to add file to your gist And a non-piping workflow that does the same exact thing:\ng \u0026lt;- gists(what = \u0026#34;minepublic\u0026#34;)[[1]] g \u0026lt;- add_files(g, \u0026#34;~/alm_othersources.md\u0026#34;) update(g) Or you could string them all together in one line (but it\u0026rsquo;s rather difficult to follow what\u0026rsquo;s going on because you have to read from the inside out)\nupdate(add_files(gists(what = \u0026#34;minepublic\u0026#34;)[[1]], \u0026#34;~/alm_othersources.md\u0026#34;)) Rate limit information rate_limit() #\u0026gt; Rate limit: 5000 #\u0026gt; Remaining: 4948 #\u0026gt; Resets in: 46 minutes List gists Limiting to a few results here to keep it brief\ngists(per_page = 2) #\u0026gt; [[1]] #\u0026gt; \u0026lt;gist\u0026gt;575fd0342ae15f89645f #\u0026gt; URL: https://gist.github.com/575fd0342ae15f89645f #\u0026gt; Description: Script to disable the mouse acceleration on Ubuntu (and probably also other distributions). Read the \u0026#39;usage\u0026#39; section. #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:41:49Z / 2015-01-04T17:41:50Z #\u0026gt; Files: disable-mouse-accel.sh #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;gist\u0026gt;1ebff1af906f214f98cd #\u0026gt; URL: https://gist.github.com/1ebff1af906f214f98cd #\u0026gt; Description: Ladda #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:41:45Z / 2015-01-04T17:41:45Z #\u0026gt; Files: Ladda.markdown, index.html, script.js, style.css Since a certain date/time\ngists(since=\u0026#39;2014-05-26T00:00:00Z\u0026#39;, per_page = 2) #\u0026gt; [[1]] #\u0026gt; \u0026lt;gist\u0026gt;575fd0342ae15f89645f #\u0026gt; URL: https://gist.github.com/575fd0342ae15f89645f #\u0026gt; Description: Script to disable the mouse acceleration on Ubuntu (and probably also other distributions). Read the \u0026#39;usage\u0026#39; section. #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:41:49Z / 2015-01-04T17:41:50Z #\u0026gt; Files: disable-mouse-accel.sh #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;gist\u0026gt;1ebff1af906f214f98cd #\u0026gt; URL: https://gist.github.com/1ebff1af906f214f98cd #\u0026gt; Description: Ladda #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:41:45Z / 2015-01-04T17:41:45Z #\u0026gt; Files: Ladda.markdown, index.html, script.js, style.css Request different types of gists, one of public, minepublic, mineall, or starred.\ngists(\u0026#39;minepublic\u0026#39;, per_page = 2) #\u0026gt; [[1]] #\u0026gt; \u0026lt;gist\u0026gt;588921d4111e00551bcf #\u0026gt; URL: https://gist.github.com/588921d4111e00551bcf #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:33:47Z / 2015-01-04T17:33:56Z #\u0026gt; Files: code.R #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;gist\u0026gt;54c0195ee8753c7aaf9f #\u0026gt; URL: https://gist.github.com/54c0195ee8753c7aaf9f #\u0026gt; Description: a new cool gist #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:33:47Z / 2015-01-04T17:33:47Z #\u0026gt; Files: stuff.md List a single gist gist(id = \u0026#39;f1403260eb92f5dfa7e1\u0026#39;) #\u0026gt; \u0026lt;gist\u0026gt;f1403260eb92f5dfa7e1 #\u0026gt; URL: https://gist.github.com/f1403260eb92f5dfa7e1 #\u0026gt; Description: Querying bitly from R #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2014-10-15T20:40:12Z / 2014-10-15T21:54:29Z #\u0026gt; Files: bitly_r.md Create gist You can pass in files\nFirst, get a file to work with\nstuffpath \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;stuff.md\u0026#34;, package = \u0026#34;gistr\u0026#34;) gist_create(files=stuffpath, description=\u0026#39;a new cool gist\u0026#39;) gist_create(files=stuffpath, description=\u0026#39;a new cool gist\u0026#39;, browse = FALSE) #\u0026gt; \u0026lt;gist\u0026gt;aa391404638f2f368b99 #\u0026gt; URL: https://gist.github.com/aa391404638f2f368b99 #\u0026gt; Description: a new cool gist #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:42:37Z / 2015-01-04T17:42:37Z #\u0026gt; Files: stuff.md Or, wrap gist_create() around some code in your R session/IDE, like so, with just the function name, and a {' at the start and a }' at the end.\ngist_create(code={\u0026#39; x \u0026lt;- letters numbers \u0026lt;- runif(8) numbers [1] 0.3229318 0.5933054 0.7778408 0.3898947 0.1309717 0.7501378 0.3206379 0.3379005 \u0026#39;}, browse=FALSE) #\u0026gt; \u0026lt;gist\u0026gt;85158117880f197df422 #\u0026gt; URL: https://gist.github.com/85158117880f197df422 #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:42:37Z / 2015-01-04T17:42:37Z #\u0026gt; Files: code.R You can also knit an input file before posting as a gist:\nfile \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;stuff.Rmd\u0026#34;, package = \u0026#34;gistr\u0026#34;) gist_create(file, description=\u0026#39;a new cool gist\u0026#39;, knit=TRUE) #\u0026gt; \u0026lt;gist\u0026gt;4162b9c53479fbc298db #\u0026gt; URL: https://gist.github.com/4162b9c53479fbc298db #\u0026gt; Description: a new cool gist #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2014-10-27T16:07:31Z / 2014-10-27T16:07:31Z #\u0026gt; Files: stuff.md Or code blocks before (note that code blocks without knitr block demarcations will result in unexecuted code):\ngist_create(code={\u0026#39; x \u0026lt;- letters (numbers \u0026lt;- runif(8)) \u0026#39;}, knit=TRUE) #\u0026gt; \u0026lt;gist\u0026gt;ec45c396dee4aa492139 #\u0026gt; URL: https://gist.github.com/ec45c396dee4aa492139 #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2014-10-27T16:09:09Z / 2014-10-27T16:09:09Z #\u0026gt; Files: file81720d1ceff.md knit code from file path, code block, or gist file knit a local file\nfile \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;stuff.Rmd\u0026#34;, package = \u0026#34;gistr\u0026#34;) run(file, knitopts = list(quiet=TRUE)) %\u0026gt;% gist_create(browse = FALSE) #\u0026gt; \u0026lt;gist\u0026gt;d5d86e11964c36cb4f1e #\u0026gt; URL: https://gist.github.com/d5d86e11964c36cb4f1e #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:42:38Z / 2015-01-04T17:42:38Z #\u0026gt; Files: stuff.md knit a code block (knitr code block notation missing, do add that in) (result not shown)\nrun({\u0026#39; x \u0026lt;- letters (numbers \u0026lt;- runif(8)) \u0026#39;}) %\u0026gt;% gist_create knit a file from a gist, has to get file first (result not shown)\ngists(\u0026#39;minepublic\u0026#39;)[[1]] %\u0026gt;% run() %\u0026gt;% update() List commits on a gist gists()[[1]] %\u0026gt;% commits() #\u0026gt; [[1]] #\u0026gt; \u0026lt;commit\u0026gt; #\u0026gt; Version: faa7a40f4b55aa7914be7685c75d5c80731971bb #\u0026gt; User: sckott #\u0026gt; Commited: 2015-01-04T17:42:37Z #\u0026gt; Commits [total, additions, deletions]: [5,5,0] Star a gist Star\ngist(\u0026#39;7ddb9810fc99c84c65ec\u0026#39;) %\u0026gt;% star() #\u0026gt; \u0026lt;gist\u0026gt;7ddb9810fc99c84c65ec #\u0026gt; URL: https://gist.github.com/7ddb9810fc99c84c65ec #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2014-06-27T17:50:37Z / 2014-06-27T17:50:37Z #\u0026gt; Files: code.R, manifest.yml, rrt_manifest.yml Unstar\ngist(\u0026#39;7ddb9810fc99c84c65ec\u0026#39;) %\u0026gt;% unstar() #\u0026gt; \u0026lt;gist\u0026gt;7ddb9810fc99c84c65ec #\u0026gt; URL: https://gist.github.com/7ddb9810fc99c84c65ec #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2014-06-27T17:50:37Z / 2014-06-27T17:50:37Z #\u0026gt; Files: code.R, manifest.yml, rrt_manifest.yml Update a gist Add files\nFirst, path to file\nfile \u0026lt;- system.file(\u0026#34;examples\u0026#34;, \u0026#34;alm_othersources.md\u0026#34;, package = \u0026#34;gistr\u0026#34;) gists(what = \u0026#34;minepublic\u0026#34;)[[1]] %\u0026gt;% add_files(file) %\u0026gt;% update() #\u0026gt; \u0026lt;gist\u0026gt;85158117880f197df422 #\u0026gt; URL: https://gist.github.com/85158117880f197df422 #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:42:37Z / 2015-01-04T17:42:42Z #\u0026gt; Files: alm_othersources.md, code.R Delete files\ngists(what = \u0026#34;minepublic\u0026#34;)[[1]] %\u0026gt;% delete_files(file) %\u0026gt;% update() #\u0026gt; \u0026lt;gist\u0026gt;85158117880f197df422 #\u0026gt; URL: https://gist.github.com/85158117880f197df422 #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:42:37Z / 2015-01-04T17:42:42Z #\u0026gt; Files: code.R Open a gist in your default browser gists()[[1]] %\u0026gt;% browse() Opens the gist in your default browser\nGet embed script gists()[[1]] %\u0026gt;% embed() #\u0026gt; [1] \u0026#34;\u0026lt;script src=\\\u0026#34;https://gist.github.com/sckott/85158117880f197df422.js\\\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026#34; List forks Returns a list of gist objects, just like gists()\ngist(id=\u0026#39;1642874\u0026#39;) %\u0026gt;% forks(per_page=2) #\u0026gt; [[1]] #\u0026gt; \u0026lt;gist\u0026gt;1642989 #\u0026gt; URL: https://gist.github.com/1642989 #\u0026gt; Description: Spline Transition #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2012-01-19T21:45:20Z / 2014-12-10T03:25:19Z #\u0026gt; Files: #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;gist\u0026gt;1643051 #\u0026gt; URL: https://gist.github.com/1643051 #\u0026gt; Description: Line Transition (Broken) #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2012-01-19T21:51:30Z / 2014-04-09T03:11:36Z #\u0026gt; Files: Fork a gist Returns a gist object\ng \u0026lt;- gists() (forked \u0026lt;- g[[ sample(seq_along(g), 1) ]] %\u0026gt;% fork()) #\u0026gt; \u0026lt;gist\u0026gt;62b05dd4ebc2d29add15 #\u0026gt; URL: https://gist.github.com/62b05dd4ebc2d29add15 #\u0026gt; Description: Solution to level 1 in Untrusted: https://alex.nisnevich.com/untrusted/ #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2015-01-04T17:42:43Z / 2015-01-04T17:42:43Z #\u0026gt; Files: untrusted-lvl1-solution.js Example use case Working with the Mapzen Pelias geocoding API\nThe API is described at https://github.com/pelias/pelias, and is still in alpha they say. The steps: get data, make a gist. The data is returned from Mapzen as geojson, so all we have to do is literally push it up to GitHub gists and we\u0026rsquo;re done b/c GitHub renders the map.\nlibrary(\u0026#39;httr\u0026#39;) base \u0026lt;- \u0026#34;https://pelias.mapzen.com/search\u0026#34; res \u0026lt;- GET(base, query = list(input = \u0026#39;coffee shop\u0026#39;, lat = 45.5, lon = -122.6)) json \u0026lt;- content(res, as = \u0026#34;text\u0026#34;) gist_create(code = json, filename = \u0026#34;pelias_test.geojson\u0026#34;) #\u0026gt; \u0026lt;gist\u0026gt;017214637bcfeb198070 #\u0026gt; URL: https://gist.github.com/017214637bcfeb198070 #\u0026gt; Description: #\u0026gt; Public: TRUE #\u0026gt; Created/Edited: 2014-10-28T14:42:36Z / 2014-10-28T14:42:36Z #\u0026gt; Files: pelias_test.geojson And here\u0026rsquo;s that gist: https://gist.github.com/sckott/017214637bcfeb198070\n","permalink":"http://localhost:1313/2015/01/gistr-github-gists/","summary":"\u003cp\u003eGitHub has this site \u003ca href=\"https://gist.github.com/\"\u003ehttps://gist.github.com/\u003c/a\u003e in which we can share code, text, images, maps, plots, etc super easily, without having to open up a repo, etc. GitHub gists are a great way to throw up an example use case to show someone, or show code that\u0026rsquo;s throwing errors to a support person, etc. In addition, there\u0026rsquo;s API access, which means we can interact with Gists not just from their web interface, but from the command line, or any programming language. There are clients for \u003ca href=\"https://github.com/mbostock/gistup\"\u003eNode.js\u003c/a\u003e, \u003ca href=\"https://rubygems.org/gems/gist\"\u003eRuby\u003c/a\u003e, \u003ca href=\"https://pypi.python.org/pypi/gists/0.4.6\"\u003ePython\u003c/a\u003e, and on and on. But AFAIK there wasn\u0026rsquo;t one for R. Along with \u003ca href=\"https://github.com/ramnathv\"\u003eRamnath\u003c/a\u003e and others, we\u0026rsquo;ve been working on an R client for gists. \u003ccode\u003ev0.1\u003c/code\u003e is \u003ca href=\"https://cran.r-project.org/web/packages/gistr/index.html\"\u003enow on CRAN\u003c/a\u003e. Below is an overview.\u003c/p\u003e","title":"gistr - R client for GitHub gists"},{"content":"I\u0026rsquo;ve been working on a Python port of the R package taxize that I maintain. It\u0026rsquo;s still early days with this Python library, I\u0026rsquo;d love to know what people think. For example, I\u0026rsquo;m giving back Pandas DataFrame\u0026rsquo;s from most functions. Does this make sense?\nInstallation sudo pip install git+git://github.com/sckott/pytaxize.git#egg=pytaxize Or git clone the repo down, and python setup.py build \u0026amp;\u0026amp; python setup.py install\nLoad library import pytaxize ITIS ping pytaxize.itis_ping() \u0026#39;This is the ITIS Web Service, providing access to the data behind www.itis.gov. The database contains 665,266 scientific names (501,207 of them valid/accepted) and 122,735 common names.\u0026#39; Get hierarchy down from tsn pytaxize.gethierarchydownfromtsn(tsn = 161030) tsn rankName taxonName parentName parentTsn 0 161048 Class Sarcopterygii Osteichthyes 161030 1 161061 Class Actinopterygii Osteichthyes 161030 Get hierarchy up from tsn pytaxize.gethierarchyupfromtsn(tsn = 37906) author parentName parentTsn rankName taxonName tsn 0 Gaertn. ex Schreb. Asteraceae 35420 Genus Liatris 37906 Get rank names pytaxize.getranknames() kingdomname rankid rankname 0 Bacteria 10 Kingdom 1 Bacteria 20 Subkingdom 2 Bacteria 30 Phylum 3 Bacteria 40 Subphylum 4 Bacteria 50 Superclass 5 Bacteria 60 Class 6 Bacteria 70 Subclass 7 Bacteria 80 Infraclass 8 Bacteria 90 Superorder 9 Bacteria 100 Order 10 Bacteria 110 Suborder 11 Bacteria 120 Infraorder 12 Bacteria 130 Superfamily 13 Bacteria 140 Family 14 Bacteria 150 Subfamily 15 Bacteria 160 Tribe 16 Bacteria 170 Subtribe 17 Bacteria 180 Genus 18 Bacteria 190 Subgenus 19 Bacteria 220 Species 20 Bacteria 230 Subspecies 21 Protozoa 10 Kingdom 22 Protozoa 20 Subkingdom 23 Protozoa 25 Infrakingdom 24 Protozoa 30 Phylum 25 Protozoa 40 Subphylum 26 Protozoa 45 Infraphylum 27 Protozoa 47 Parvphylum 28 Protozoa 50 Superclass 29 Protozoa 60 Class .. ... ... ... 150 Chromista 190 Subgenus 151 Chromista 200 Section 152 Chromista 210 Subsection 153 Chromista 220 Species 154 Chromista 230 Subspecies 155 Chromista 240 Variety 156 Chromista 250 Subvariety 157 Chromista 260 Form 158 Chromista 270 Subform 159 Archaea 10 Kingdom 160 Archaea 20 Subkingdom 161 Archaea 30 Phylum 162 Archaea 40 Subphylum 163 Archaea 50 Superclass 164 Archaea 60 Class 165 Archaea 70 Subclass 166 Archaea 80 Infraclass 167 Archaea 90 Superorder 168 Archaea 100 Order 169 Archaea 110 Suborder 170 Archaea 120 Infraorder 171 Archaea 130 Superfamily 172 Archaea 140 Family 173 Archaea 150 Subfamily 174 Archaea 160 Tribe 175 Archaea 170 Subtribe 176 Archaea 180 Genus 177 Archaea 190 Subgenus 178 Archaea 220 Species 179 Archaea 230 Subspecies Search by scientific name pytaxize.searchbyscientificname(x=\u0026#34;Tardigrada\u0026#34;) combinedname tsn 0 Rotaria tardigrada 58274 1 Notommata tardigrada 58898 2 Pilargis tardigrada 65562 3 Tardigrada 155166 4 Heterotardigrada 155167 5 Arthrotardigrada 155168 6 Mesotardigrada 155358 7 Eutardigrada 155362 8 Scytodes tardigrada 866744 Get accepted names from tsn pytaxize.getacceptednamesfromtsn(\u0026#39;208527\u0026#39;) If accepted, returns the same id\n\u0026#39;208527\u0026#39; More For the other functions see https://github.com/sckott/pytaxize/blob/master/pytaxize/itis.py\n","permalink":"http://localhost:1313/2014/12/pytaxize-itis/","summary":"\u003cp\u003eI\u0026rsquo;ve been working on a Python port of the R package \u003ccode\u003etaxize\u003c/code\u003e that I maintain. It\u0026rsquo;s still early days with this Python library, I\u0026rsquo;d love to know what people think. For example, I\u0026rsquo;m giving back Pandas DataFrame\u0026rsquo;s from most functions. Does this make sense?\u003c/p\u003e\n\u003ch2 id=\"installation\"\u003eInstallation\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo pip install git+git://github.com/sckott/pytaxize.git#egg=pytaxize\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOr \u003ccode\u003egit clone\u003c/code\u003e the repo down, and \u003ccode\u003epython setup.py build \u0026amp;\u0026amp; python setup.py install\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"load-library\"\u003eLoad library\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pytaxize\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"itis-ping\"\u003eITIS ping\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epytaxize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitis_ping()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;This is the ITIS Web Service, providing access to the data behind www.itis.gov. The database contains 665,266 scientific names (501,207 of them valid/accepted) and 122,735 common names.\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"get-hierarchy-down-from-tsn\"\u003eGet hierarchy down from tsn\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epytaxize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egethierarchydownfromtsn(tsn \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e161030\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      tsn rankName       taxonName    parentName parentTsn\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e161048\u003c/span\u003e    Class   Sarcopterygii  Osteichthyes    \u003cspan style=\"color:#ae81ff\"\u003e161030\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e161061\u003c/span\u003e    Class  Actinopterygii  Osteichthyes    \u003cspan style=\"color:#ae81ff\"\u003e161030\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"get-hierarchy-up-from-tsn\"\u003eGet hierarchy up from tsn\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epytaxize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egethierarchyupfromtsn(tsn \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e37906\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e               author  parentName parentTsn rankName taxonName    tsn\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e  Gaertn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e ex Schreb\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e  Asteraceae     \u003cspan style=\"color:#ae81ff\"\u003e35420\u003c/span\u003e    Genus   Liatris  \u003cspan style=\"color:#ae81ff\"\u003e37906\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"get-rank-names\"\u003eGet rank names\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epytaxize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetranknames()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    kingdomname rankid      rankname\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e       Kingdom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e    Subkingdom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e30\u003c/span\u003e        Phylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e40\u003c/span\u003e     Subphylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e    Superclass\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e60\u003c/span\u003e         Class\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e70\u003c/span\u003e      Subclass\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e80\u003c/span\u003e    Infraclass\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e      Bacteria     \u003cspan style=\"color:#ae81ff\"\u003e90\u003c/span\u003e    Superorder\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e      Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e         Order\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e110\u003c/span\u003e      Suborder\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e120\u003c/span\u003e    Infraorder\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e130\u003c/span\u003e   Superfamily\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e13\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e140\u003c/span\u003e        Family\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e14\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e150\u003c/span\u003e     Subfamily\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e160\u003c/span\u003e         Tribe\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e170\u003c/span\u003e      Subtribe\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e17\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e180\u003c/span\u003e         Genus\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e190\u003c/span\u003e      Subgenus\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e19\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e220\u003c/span\u003e       Species\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e     Bacteria    \u003cspan style=\"color:#ae81ff\"\u003e230\u003c/span\u003e    Subspecies\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e21\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e       Kingdom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e22\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e    Subkingdom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e23\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e25\u003c/span\u003e  Infrakingdom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e24\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e30\u003c/span\u003e        Phylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e25\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e40\u003c/span\u003e     Subphylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e26\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e45\u003c/span\u003e   Infraphylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e27\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e47\u003c/span\u003e    Parvphylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e28\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e    Superclass\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e29\u003c/span\u003e     Protozoa     \u003cspan style=\"color:#ae81ff\"\u003e60\u003c/span\u003e         Class\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e..\u003c/span\u003e          \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e    \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e           \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e150\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e190\u003c/span\u003e      Subgenus\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e151\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e200\u003c/span\u003e       Section\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e152\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e210\u003c/span\u003e    Subsection\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e153\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e220\u003c/span\u003e       Species\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e154\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e230\u003c/span\u003e    Subspecies\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e155\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e240\u003c/span\u003e       Variety\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e156\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e250\u003c/span\u003e    Subvariety\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e157\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e260\u003c/span\u003e          Form\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e158\u003c/span\u003e   Chromista    \u003cspan style=\"color:#ae81ff\"\u003e270\u003c/span\u003e       Subform\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e159\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e       Kingdom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e160\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e    Subkingdom\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e161\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e30\u003c/span\u003e        Phylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e162\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e40\u003c/span\u003e     Subphylum\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e163\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e    Superclass\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e164\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e60\u003c/span\u003e         Class\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e165\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e70\u003c/span\u003e      Subclass\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e166\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e80\u003c/span\u003e    Infraclass\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e167\u003c/span\u003e     Archaea     \u003cspan style=\"color:#ae81ff\"\u003e90\u003c/span\u003e    Superorder\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e168\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e         Order\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e169\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e110\u003c/span\u003e      Suborder\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e170\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e120\u003c/span\u003e    Infraorder\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e171\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e130\u003c/span\u003e   Superfamily\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e172\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e140\u003c/span\u003e        Family\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e173\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e150\u003c/span\u003e     Subfamily\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e174\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e160\u003c/span\u003e         Tribe\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e175\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e170\u003c/span\u003e      Subtribe\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e176\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e180\u003c/span\u003e         Genus\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e177\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e190\u003c/span\u003e      Subgenus\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e178\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e220\u003c/span\u003e       Species\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e179\u003c/span\u003e     Archaea    \u003cspan style=\"color:#ae81ff\"\u003e230\u003c/span\u003e    Subspecies\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"search-by-scientific-name\"\u003eSearch by scientific name\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epytaxize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esearchbyscientificname(x\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Tardigrada\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e           combinedname     tsn\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e    Rotaria tardigrada   \u003cspan style=\"color:#ae81ff\"\u003e58274\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e  Notommata tardigrada   \u003cspan style=\"color:#ae81ff\"\u003e58898\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e   Pilargis tardigrada   \u003cspan style=\"color:#ae81ff\"\u003e65562\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e            Tardigrada  \u003cspan style=\"color:#ae81ff\"\u003e155166\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e      Heterotardigrada  \u003cspan style=\"color:#ae81ff\"\u003e155167\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e      Arthrotardigrada  \u003cspan style=\"color:#ae81ff\"\u003e155168\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e        Mesotardigrada  \u003cspan style=\"color:#ae81ff\"\u003e155358\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e          Eutardigrada  \u003cspan style=\"color:#ae81ff\"\u003e155362\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e   Scytodes tardigrada  \u003cspan style=\"color:#ae81ff\"\u003e866744\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"get-accepted-names-from-tsn\"\u003eGet accepted names from tsn\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epytaxize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetacceptednamesfromtsn(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;208527\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIf accepted, returns the same id\u003c/p\u003e","title":"pytaxize - low level ITIS functions"},{"content":"I was in San Francisco last week for an altmetrics conference at PLOS. While there, I visited the Asian Art Museum, just the Roads of Arabia exhibition.\nIt was a great exhibit. While I was looking at the pieces, I read many labels, and thought, \u0026ldquo;hey, what if someone wants this metadata\u0026rdquo;?\nSince we have an R package in development for scraping museum metadata (called musemeta), I just started some scraping code for this museum. Unfortunately, I don\u0026rsquo;t think the pieces from the Roads of Arabia exhibit are on their site, so no metadata to get. But they do have their main collection searchable online at https://www.asianart.org/collections/collection. Examples follow.\nInstallation install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/musemeta\u0026#34;) library(\u0026#34;musemeta\u0026#34;) Get metadata for a single object You have to get the ID for the piece from their website, e.g., 11462 from the url https://searchcollection.asianart.org/view/objects/asitem/nid/11462. Once you have an ID you can pass it in ot the aam() function.\n(out \u0026lt;- aam(11462)) #\u0026gt; \u0026lt;AAM metadata\u0026gt; Molded plaque (tsha tsha) #\u0026gt; Object id: 1992.96 #\u0026gt; Object name: Votive plaque #\u0026gt; Date: approx. 1992 #\u0026gt; Artist: #\u0026gt; Medium: Plaster mixed with resin and pigment #\u0026gt; Credit line: Gift of Robert Tevis #\u0026gt; On display?: no #\u0026gt; Collection: Decorative Arts #\u0026gt; Department: Himalayan Art #\u0026gt; Dimensions: #\u0026gt; Label: Molded plaques (tsha tshas) are small sacred images, flat or #\u0026gt; three-dimensional, shaped out of clay in metal molds. The #\u0026gt; images are usually unbaked, and sometimes seeds, paper, or #\u0026gt; human ashes were mixed with the clay. Making tsha tshas is #\u0026gt; a meritorious act, and monasteries give them away to #\u0026gt; pilgrims. Some Tibetans carry tsha tshas inside the amulet #\u0026gt; boxes they wear or stuff them into larger images as part of #\u0026gt; the consecration of those images. In Bhutan tsha tshas are #\u0026gt; found in mani walls (a wall of stones carved with prayers) #\u0026gt; or piled up in caves.The practice of making such plaques #\u0026gt; began in India, and from there it spread to other countries #\u0026gt; in Asia with the introduction of Buddhism. Authentic tsha #\u0026gt; tshas are cast from clay. Modern examples , such as those #\u0026gt; made for the tourist trade in Tibet, are made of plaster #\u0026gt; and cast from ancient (1100-1200) molds and hand colored to #\u0026gt; give them the appearance of age. The output is printed for clarity, but you can dig into each element, like\nout$label #\u0026gt; [1] \u0026#34;Molded plaques (tsha tshas) are small sacred images, flat or three-dimensional, shaped out of clay in metal molds. The images are usually unbaked, and sometimes seeds, paper, or human ashes were mixed with the clay. Making tsha tshas is a meritorious act, and monasteries give them away to pilgrims. Some Tibetans carry tsha tshas inside the amulet boxes they wear or stuff them into larger images as part of the consecration of those images. In Bhutan tsha tshas are found in mani walls (a wall of stones carved with prayers) or piled up in caves.The practice of making such plaques began in India, and from there it spread to other countries in Asia with the introduction of Buddhism. Authentic tsha tshas are cast from clay. Modern examples , such as those made for the tourist trade in Tibet, are made of plaster and cast from ancient (1100-1200) molds and hand colored to give them the appearance of age.\u0026#34; Get metadata for many objects The aam() function is not vectorized, but you can easily get data for many IDs via lapply type functions, etc.\nlapply(c(17150,17140,17144), aam) #\u0026gt; [[1]] #\u0026gt; \u0026lt;AAM metadata\u0026gt; Boys sumo wrestling #\u0026gt; Object id: 2005.100.35 #\u0026gt; Object name: Woodblock print #\u0026gt; Date: approx. 1769 #\u0026gt; Artist: Suzuki HarunobuJapanese, 1724 - 1770 #\u0026gt; Medium: Ink and colors on paper #\u0026gt; Credit line: Gift of the Grabhorn Ukiyo-e Collection #\u0026gt; On display?: no #\u0026gt; Collection: Prints And Drawings #\u0026gt; Department: Japanese Art #\u0026gt; Dimensions: H. 12 5/8 in x W. 5 3/4 in, H. 32.1 cm x W. 14.6 cm #\u0026gt; Label: 40 é木Ø春t信M 相\u0026#39;撲oVびÑSuzuki Harunobu, 1725?1770Boys sumo wrestling ( Sumō #\u0026gt; ?)c. 1769Woodblock print ( nishiki-e) Hosoban #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;AAM metadata\u0026gt; Autumn Moon of Matsukaze #\u0026gt; Object id: 2005.100.25 #\u0026gt; Object name: Woodblock print #\u0026gt; Date: 1768-1769 #\u0026gt; Artist: Suzuki HarunobuJapanese, 1724 - 1770 #\u0026gt; Medium: Ink and colors on paper #\u0026gt; Credit line: Gift of the Grabhorn Ukiyo-e Collection #\u0026gt; On display?: no #\u0026gt; Collection: Prints And Drawings #\u0026gt; Department: Japanese Art #\u0026gt; Dimensions: H. 12 1/2 in x W. 5 3/4 in, H. 31.7 cm x W. 14.6 cm #\u0026gt; Label: 30 é木Ø春t信M 『w流¬æ八\u0026#34;ª景i』x 「u松¼のÌ秋H月」vSuzuki Harunobu, 1725?1770\u0026#34;Autumn Moon of #\u0026gt; Matsukaze\u0026#34; (Matsukaze no shū ?)From Fashionable Eight Views #\u0026gt; of Noh Chants (Fū ?ū ?17681769Woodblock print #\u0026gt; (nishiki-e)Hosoban #\u0026gt; #\u0026gt; [[3]] #\u0026gt; \u0026lt;AAM metadata\u0026gt; Hunting for fireflies #\u0026gt; Object id: 2005.100.29 #\u0026gt; Object name: Woodblock print #\u0026gt; Date: 1767-1768 #\u0026gt; Artist: Suzuki HarunobuJapanese, 1724 - 1770 #\u0026gt; Medium: Ink and colors on paper #\u0026gt; Credit line: Gift of the Grabhorn Ukiyo-e Collection #\u0026gt; On display?: no #\u0026gt; Collection: Prints And Drawings #\u0026gt; Department: Japanese Art #\u0026gt; Dimensions: H. 10 1/2 in x W. 8 in, H. 26.7 cm x W. 20.3 cm #\u0026gt; Label: 34 é木Ø春t信M u狩ëりèSuzuki Harunobu, 1725?1770Hunting for #\u0026gt; fireflies17671768Woodblock print ( nishiki-e) Chū ? No search, boo Note that there is no search functionality yet for this source. Maybe someone can add that via pull requests :)\nLike the others The others sources in musemeta mostly work the same way as the above.\n","permalink":"http://localhost:1313/2014/12/museum-aamsf/","summary":"\u003cp\u003eI was in San Francisco last week for an altmetrics conference at PLOS. While there, I visited the \u003ca href=\"https://www.asianart.org/\"\u003eAsian Art Museum\u003c/a\u003e, just the \u003ca href=\"https://www.asianart.org/exhibitions_index/roads-of-arabia\"\u003eRoads of Arabia exhibition\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIt was a great exhibit. While I was looking at the pieces, I read many labels, and thought, \u0026ldquo;hey, what if someone wants this metadata\u0026rdquo;?\u003c/p\u003e\n\u003cp\u003eSince we have an R package in development for scraping museum metadata (called \u003ca href=\"https://github.com/ropensci/musemeta\"\u003emusemeta\u003c/a\u003e), I just started some scraping code for this museum. Unfortunately, I don\u0026rsquo;t think the pieces from the Roads of Arabia exhibit are on their site, so no metadata to get. But they do have their main collection searchable online at \u003ca href=\"https://www.asianart.org/collections/collection\"\u003ehttps://www.asianart.org/collections/collection\u003c/a\u003e. Examples follow.\u003c/p\u003e","title":"Museum metadata - the Asian Art Museum of San Francisco"},{"content":"The Lagotto application is a Rails app that collects and serves up via RESTful API article level metrics data for research objects. So far, this application has only been applied to scholarly articles, but will see action on datasets soon.\nMartin Fenner has lead the development of Lagotto. He recently set up a discussion site if you want to chat about it.\nThe application has a nice GUI interface, and a quite nice RESTful API.\nLagotto is open source! Because of this, and the quality of the software, other publishers have started using it to gather and deliver publicly article level metrics data, including:\neLife Public Knowledge Project (PKP) Copernicus Crossref Pensoft The PLOS instance at https://alm.plos.org/ is always the most up to date with the Lagotto software, but Crossref has the largest number of articles.\nI\u0026rsquo;ve been working on three clients for the Lagotto REST API, including for a while now on R, recently on Python, and just last week on Ruby.\nPlease do try the clients, report bugs, request features - you know the open source drill\u0026hellip;\nI\u0026rsquo;d say the R client is the most mature, while Python is less so, end the Ruby gem the least mature.\nInstallation R\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/alm\u0026#34;) Python\ngit clone https://github.com/cameronneylon/pyalm.git cd pyalm git checkout scott python setup.py install Ruby\ngem install httparty json rake git clone https://github.com/sckott/alm.git cd alm make # which runs build and install tasks If you don\u0026rsquo;t have make, then just run gem build alm.gemspec and gem install alm-0.1.0.gem seperately.\nExample In this example, we\u0026rsquo;ll get altmetrics data for two DOIs: 10.1371/journal.pone.0029797, and 10.1371/journal.pone.0029798 (click on links to go to paper).\nR library(\u0026#39;alm\u0026#39;) ids \u0026lt;- c(\u0026#34;10.1371/journal.pone.0029797\u0026#34;,\u0026#34;10.1371/journal.pone.0029798\u0026#34;) alm_ids(ids, info=\u0026#34;summary\u0026#34;) #\u0026gt; $meta #\u0026gt; total total_pages page error #\u0026gt; 1 2 1 1 NA #\u0026gt; #\u0026gt; $data #\u0026gt; $data$`10.1371/journal.pone.0029798` #\u0026gt; $data$`10.1371/journal.pone.0029798`$info #\u0026gt; doi #\u0026gt; 1 10.1371/journal.pone.0029798 #\u0026gt; title #\u0026gt; 1 Mitochondrial Electron Transport Is the Cellular Target of the Oncology Drug Elesclomol #\u0026gt; canonical_url #\u0026gt; 1 https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029798 #\u0026gt; pmid pmcid mendeley_uuid #\u0026gt; 1 22253786 3256171 b08cc99e-b526-3f0c-adaa-d5ee6d0d978a #\u0026gt; update_date issued #\u0026gt; 1 2014-12-09T02:52:47Z 2012-01-11 #\u0026gt; #\u0026gt; $data$`10.1371/journal.pone.0029798`$signposts #\u0026gt; doi viewed saved discussed cited #\u0026gt; 1 10.1371/journal.pone.0029798 4346 14 2 26 #\u0026gt; #\u0026gt; #\u0026gt; $data$`10.1371/journal.pone.0029797` #\u0026gt; $data$`10.1371/journal.pone.0029797`$info #\u0026gt; doi #\u0026gt; 1 10.1371/journal.pone.0029797 #\u0026gt; title #\u0026gt; 1 Ecological Guild Evolution and the Discovery of the World\u0026#39;s Smallest Vertebrate #\u0026gt; canonical_url #\u0026gt; 1 https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029797 #\u0026gt; pmid pmcid mendeley_uuid #\u0026gt; 1 22253785 3256195 897fbbd6-5a23-3552-8077-97251b82c1e1 #\u0026gt; update_date issued #\u0026gt; 1 2014-12-09T02:52:46Z 2012-01-11 #\u0026gt; #\u0026gt; $data$`10.1371/journal.pone.0029797`$signposts #\u0026gt; doi viewed saved discussed cited #\u0026gt; 1 10.1371/journal.pone.0029797 34282 81 244 8 Python import pyalm ids = [\u0026#34;10.1371/journal.pone.0029797\u0026#34;,\u0026#34;10.1371/journal.pone.0029798\u0026#34;] pyalm.get_alm(ids, info=\u0026#34;summary\u0026#34;) #\u0026gt; {\u0026#39;articles\u0026#39;: [\u0026lt;ArticleALM Mitochondrial Electron Transport Is the Cellular Target of the Oncology Drug Elesclomol, #\u0026gt; DOI 10.1371/journal.pone.0029798\u0026gt;, #\u0026gt; \u0026lt;ArticleALM Ecological Guild Evolution and the Discovery of the World\u0026#39;s Smallest Vertebrate, #\u0026gt; DOI 10.1371/journal.pone.0029797\u0026gt;], #\u0026gt; \u0026#39;meta\u0026#39;: {u\u0026#39;error\u0026#39;: None, u\u0026#39;page\u0026#39;: 1, u\u0026#39;total\u0026#39;: 2, u\u0026#39;total_pages\u0026#39;: 1}} Ruby require \u0026#39;alm\u0026#39; Alm.alm(ids: [\u0026#34;10.1371/journal.pone.0029797\u0026#34;,\u0026#34;10.1371/journal.pone.0029798\u0026#34;], key: ENV[\u0026#39;PLOS_API_KEY\u0026#39;]) #\u0026gt; =\u0026gt; {\u0026#34;total\u0026#34;=\u0026gt;2, #\u0026gt; \u0026#34;total_pages\u0026#34;=\u0026gt;1, #\u0026gt; \u0026#34;page\u0026#34;=\u0026gt;1, #\u0026gt; \u0026#34;error\u0026#34;=\u0026gt;nil, #\u0026gt; \u0026#34;data\u0026#34;=\u0026gt; #\u0026gt; [{\u0026#34;doi\u0026#34;=\u0026gt;\u0026#34;10.1371/journal.pone.0029798\u0026#34;, #\u0026gt; \u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Mitochondrial Electron Transport Is the Cellular Target of the Oncology Drug Elesclomol\u0026#34;, #\u0026gt; \u0026#34;issued\u0026#34;=\u0026gt;{\u0026#34;date-parts\u0026#34;=\u0026gt;[[2012, 1, 11]]}, #\u0026gt; \u0026#34;canonical_url\u0026#34;=\u0026gt;\u0026#34;https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029798\u0026#34;, #\u0026gt; \u0026#34;pmid\u0026#34;=\u0026gt;\u0026#34;22253786\u0026#34;, #\u0026gt; \u0026#34;pmcid\u0026#34;=\u0026gt;\u0026#34;3256171\u0026#34;, #\u0026gt; \u0026#34;mendeley_uuid\u0026#34;=\u0026gt;\u0026#34;b08cc99e-b526-3f0c-adaa-d5ee6d0d978a\u0026#34;, #\u0026gt; \u0026#34;viewed\u0026#34;=\u0026gt;4346, #\u0026gt; \u0026#34;saved\u0026#34;=\u0026gt;14, #\u0026gt; \u0026#34;discussed\u0026#34;=\u0026gt;2, #\u0026gt; \u0026#34;cited\u0026#34;=\u0026gt;26, #\u0026gt; \u0026#34;update_date\u0026#34;=\u0026gt;\u0026#34;2014-12-09T02:52:47Z\u0026#34;}, #\u0026gt; {\u0026#34;doi\u0026#34;=\u0026gt;\u0026#34;10.1371/journal.pone.0029797\u0026#34;, #\u0026gt; \u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Ecological Guild Evolution and the Discovery of the World\u0026#39;s Smallest Vertebrate\u0026#34;, #\u0026gt; \u0026#34;issued\u0026#34;=\u0026gt;{\u0026#34;date-parts\u0026#34;=\u0026gt;[[2012, 1, 11]]}, #\u0026gt; \u0026#34;canonical_url\u0026#34;=\u0026gt;\u0026#34;https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029797\u0026#34;, #\u0026gt; \u0026#34;pmid\u0026#34;=\u0026gt;\u0026#34;22253785\u0026#34;, #\u0026gt; \u0026#34;pmcid\u0026#34;=\u0026gt;\u0026#34;3256195\u0026#34;, #\u0026gt; \u0026#34;mendeley_uuid\u0026#34;=\u0026gt;\u0026#34;897fbbd6-5a23-3552-8077-97251b82c1e1\u0026#34;, #\u0026gt; \u0026#34;viewed\u0026#34;=\u0026gt;34282, #\u0026gt; \u0026#34;saved\u0026#34;=\u0026gt;81, #\u0026gt; \u0026#34;discussed\u0026#34;=\u0026gt;244, #\u0026gt; \u0026#34;cited\u0026#34;=\u0026gt;8, #\u0026gt; \u0026#34;update_date\u0026#34;=\u0026gt;\u0026#34;2014-12-09T02:52:46Z\u0026#34;}]} ","permalink":"http://localhost:1313/2014/12/icanhaz-altmetrics/","summary":"\u003cp\u003eThe Lagotto application is a Rails app that collects and serves up via RESTful API article level metrics data for research objects. So far, this application has only been applied to scholarly articles, but will \u003ca href=\"https://articlemetrics.github.io/MDC/\"\u003esee action on datasets soon\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blog.martinfenner.org/\"\u003eMartin Fenner\u003c/a\u003e has lead the development of Lagotto. He recently set up \u003ca href=\"https://discuss.lagotto.io/\"\u003ea discussion site\u003c/a\u003e if you want to chat about it.\u003c/p\u003e\n\u003cp\u003eThe application has a \u003ca href=\"https://alm.plos.org/\"\u003enice GUI interface\u003c/a\u003e, and a quite nice \u003ca href=\"https://alm.plos.org/docs/api\"\u003eRESTful API\u003c/a\u003e.\u003c/p\u003e","title":"icanhaz altmetrics"},{"content":"The Lagotto application is a Rails app that collects and serves up via RESTful API article level metrics data for research objects. So far, this application has only been applied to scholarly articles, but will see action on datasets soon.\nMartin Fenner has lead the development of Lagotto. He recently set up a discussion site if you want to chat about it.\nThe application has a nice GUI interface, and a quite nice RESTful API.\nLagotto is open source! Because of this, and the quality of the software, other publishers have started using it to gather and deliver publicly article level metrics data, including:\neLife Public Knowledge Project (PKP) Copernicus Crossref Pensoft The PLOS instance at https://alm.plos.org/ is always the most up to date with the Lagotto software, but Crossref has the largest number of articles.\nI\u0026rsquo;ve been working on three clients for the Lagotto REST API, including for a while now on R, recently on Python, and just last week on Ruby.\nPlease do try the clients, report bugs, request features - you know the open source drill\u0026hellip;\nI\u0026rsquo;d say the R client is the most mature, while Python is less so, end the Ruby gem the least mature.\nInstallation R\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/alm\u0026#34;) Python\ngit clone https://github.com/cameronneylon/pyalm.git cd pyalm git checkout scott python setup.py install Ruby\ngem install httparty json rake git clone https://github.com/sckott/alm.git cd alm make # which runs build and install tasks If you don\u0026rsquo;t have make, then just run gem build alm.gemspec and gem install alm-0.1.0.gem seperately.\nExample In this example, we\u0026rsquo;ll get altmetrics data for two DOIs: 10.1371/journal.pone.0029797, and 10.1371/journal.pone.0029798 (click on links to go to paper).\nR library(\u0026#39;alm\u0026#39;) ids \u0026lt;- c(\u0026#34;10.1371/journal.pone.0029797\u0026#34;,\u0026#34;10.1371/journal.pone.0029798\u0026#34;) alm_ids(ids, info=\u0026#34;summary\u0026#34;) #\u0026gt; $meta #\u0026gt; total total_pages page error #\u0026gt; 1 2 1 1 NA #\u0026gt; #\u0026gt; $data #\u0026gt; $data$`10.1371/journal.pone.0029798` #\u0026gt; $data$`10.1371/journal.pone.0029798`$info #\u0026gt; doi #\u0026gt; 1 10.1371/journal.pone.0029798 #\u0026gt; title #\u0026gt; 1 Mitochondrial Electron Transport Is the Cellular Target of the Oncology Drug Elesclomol #\u0026gt; canonical_url #\u0026gt; 1 https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029798 #\u0026gt; pmid pmcid mendeley_uuid #\u0026gt; 1 22253786 3256171 b08cc99e-b526-3f0c-adaa-d5ee6d0d978a #\u0026gt; update_date issued #\u0026gt; 1 2014-12-09T02:52:47Z 2012-01-11 #\u0026gt; #\u0026gt; $data$`10.1371/journal.pone.0029798`$signposts #\u0026gt; doi viewed saved discussed cited #\u0026gt; 1 10.1371/journal.pone.0029798 4346 14 2 26 #\u0026gt; #\u0026gt; #\u0026gt; $data$`10.1371/journal.pone.0029797` #\u0026gt; $data$`10.1371/journal.pone.0029797`$info #\u0026gt; doi #\u0026gt; 1 10.1371/journal.pone.0029797 #\u0026gt; title #\u0026gt; 1 Ecological Guild Evolution and the Discovery of the World\u0026#39;s Smallest Vertebrate #\u0026gt; canonical_url #\u0026gt; 1 https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029797 #\u0026gt; pmid pmcid mendeley_uuid #\u0026gt; 1 22253785 3256195 897fbbd6-5a23-3552-8077-97251b82c1e1 #\u0026gt; update_date issued #\u0026gt; 1 2014-12-09T02:52:46Z 2012-01-11 #\u0026gt; #\u0026gt; $data$`10.1371/journal.pone.0029797`$signposts #\u0026gt; doi viewed saved discussed cited #\u0026gt; 1 10.1371/journal.pone.0029797 34282 81 244 8 Python import pyalm ids = [\u0026#34;10.1371/journal.pone.0029797\u0026#34;,\u0026#34;10.1371/journal.pone.0029798\u0026#34;] pyalm.get_alm(ids, info=\u0026#34;summary\u0026#34;) #\u0026gt; {\u0026#39;articles\u0026#39;: [\u0026lt;ArticleALM Mitochondrial Electron Transport Is the Cellular Target of the Oncology Drug Elesclomol, #\u0026gt; DOI 10.1371/journal.pone.0029798\u0026gt;, #\u0026gt; \u0026lt;ArticleALM Ecological Guild Evolution and the Discovery of the World\u0026#39;s Smallest Vertebrate, #\u0026gt; DOI 10.1371/journal.pone.0029797\u0026gt;], #\u0026gt; \u0026#39;meta\u0026#39;: {u\u0026#39;error\u0026#39;: None, u\u0026#39;page\u0026#39;: 1, u\u0026#39;total\u0026#39;: 2, u\u0026#39;total_pages\u0026#39;: 1}} Ruby require \u0026#39;alm\u0026#39; Alm.alm(ids: [\u0026#34;10.1371/journal.pone.0029797\u0026#34;,\u0026#34;10.1371/journal.pone.0029798\u0026#34;], key: ENV[\u0026#39;PLOS_API_KEY\u0026#39;]) #\u0026gt; =\u0026gt; {\u0026#34;total\u0026#34;=\u0026gt;2, #\u0026gt; \u0026#34;total_pages\u0026#34;=\u0026gt;1, #\u0026gt; \u0026#34;page\u0026#34;=\u0026gt;1, #\u0026gt; \u0026#34;error\u0026#34;=\u0026gt;nil, #\u0026gt; \u0026#34;data\u0026#34;=\u0026gt; #\u0026gt; [{\u0026#34;doi\u0026#34;=\u0026gt;\u0026#34;10.1371/journal.pone.0029798\u0026#34;, #\u0026gt; \u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Mitochondrial Electron Transport Is the Cellular Target of the Oncology Drug Elesclomol\u0026#34;, #\u0026gt; \u0026#34;issued\u0026#34;=\u0026gt;{\u0026#34;date-parts\u0026#34;=\u0026gt;[[2012, 1, 11]]}, #\u0026gt; \u0026#34;canonical_url\u0026#34;=\u0026gt;\u0026#34;https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029798\u0026#34;, #\u0026gt; \u0026#34;pmid\u0026#34;=\u0026gt;\u0026#34;22253786\u0026#34;, #\u0026gt; \u0026#34;pmcid\u0026#34;=\u0026gt;\u0026#34;3256171\u0026#34;, #\u0026gt; \u0026#34;mendeley_uuid\u0026#34;=\u0026gt;\u0026#34;b08cc99e-b526-3f0c-adaa-d5ee6d0d978a\u0026#34;, #\u0026gt; \u0026#34;viewed\u0026#34;=\u0026gt;4346, #\u0026gt; \u0026#34;saved\u0026#34;=\u0026gt;14, #\u0026gt; \u0026#34;discussed\u0026#34;=\u0026gt;2, #\u0026gt; \u0026#34;cited\u0026#34;=\u0026gt;26, #\u0026gt; \u0026#34;update_date\u0026#34;=\u0026gt;\u0026#34;2014-12-09T02:52:47Z\u0026#34;}, #\u0026gt; {\u0026#34;doi\u0026#34;=\u0026gt;\u0026#34;10.1371/journal.pone.0029797\u0026#34;, #\u0026gt; \u0026#34;title\u0026#34;=\u0026gt;\u0026#34;Ecological Guild Evolution and the Discovery of the World\u0026#39;s Smallest Vertebrate\u0026#34;, #\u0026gt; \u0026#34;issued\u0026#34;=\u0026gt;{\u0026#34;date-parts\u0026#34;=\u0026gt;[[2012, 1, 11]]}, #\u0026gt; \u0026#34;canonical_url\u0026#34;=\u0026gt;\u0026#34;https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029797\u0026#34;, #\u0026gt; \u0026#34;pmid\u0026#34;=\u0026gt;\u0026#34;22253785\u0026#34;, #\u0026gt; \u0026#34;pmcid\u0026#34;=\u0026gt;\u0026#34;3256195\u0026#34;, #\u0026gt; \u0026#34;mendeley_uuid\u0026#34;=\u0026gt;\u0026#34;897fbbd6-5a23-3552-8077-97251b82c1e1\u0026#34;, #\u0026gt; \u0026#34;viewed\u0026#34;=\u0026gt;34282, #\u0026gt; \u0026#34;saved\u0026#34;=\u0026gt;81, #\u0026gt; \u0026#34;discussed\u0026#34;=\u0026gt;244, #\u0026gt; \u0026#34;cited\u0026#34;=\u0026gt;8, #\u0026gt; \u0026#34;update_date\u0026#34;=\u0026gt;\u0026#34;2014-12-09T02:52:46Z\u0026#34;}]} ","permalink":"http://localhost:1313/2014/12/altmetrics-anywhere/","summary":"\u003cp\u003eThe Lagotto application is a Rails app that collects and serves up via RESTful API article level metrics data for research objects. So far, this application has only been applied to scholarly articles, but will \u003ca href=\"https://articlemetrics.github.io/MDC/\"\u003esee action on datasets soon\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://blog.martinfenner.org/\"\u003eMartin Fenner\u003c/a\u003e has lead the development of Lagotto. He recently set up \u003ca href=\"https://discuss.lagotto.io/\"\u003ea discussion site\u003c/a\u003e if you want to chat about it.\u003c/p\u003e\n\u003cp\u003eThe application has a \u003ca href=\"https://alm.plos.org/\"\u003enice GUI interface\u003c/a\u003e, and a quite nice \u003ca href=\"https://alm.plos.org/docs/api\"\u003eRESTful API\u003c/a\u003e.\u003c/p\u003e","title":"Altmetrics from anywhere"},{"content":"At rOpenSci we occasssionally hear from our users that they run into an error like:\nError in function (type, msg, asError = TRUE) : easy handled already used in multi handle This error occurs in the httr package that we use to do http requests to sources of data on the web. It happens when e.g., you make a lot of requests to a resource, then it gets interrupted somehow - then you make another call, and you get the error above. Let\u0026rsquo;s try it with the an version of httr (v0.5):\nlibrary(\u0026#34;httr\u0026#34;) # run, then esc to cause multi handle error replicate(50, GET(\u0026#34;http://google.com/\u0026#34;)) # then retry single call, which trows multi handle error GET(\u0026#34;http://google.com/\u0026#34;) #\u0026gt; Error in function (type, msg, asError = TRUE) : #\u0026gt; easy handled already used in multi handle There are any number of reasons why your session may get interrupted, including an internet outage, the web service you are requesesting data from times out, etc. There hasn\u0026rsquo;t been a straight-forward way to handle this, until recently.\nIn httr version 0.6, there are two new functions handle_find() and handle_reset() to help deal with this error.\nFirst, install newest httr from Github\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;hadley/httr\u0026#34;) library(\u0026#34;httr\u0026#34;) Make a bunch of requests to google, interrupting part way through\nreplicate(50, HEAD(\u0026#34;http://google.com/\u0026#34;)) Then retry single call, which trows multi handle error\nHEAD(\u0026#34;http://google.com/\u0026#34;) #\u0026gt; Error in function (type, msg, asError = TRUE) : #\u0026gt; easy handled already used in multi handle Find handle\nhandle_find(\u0026#34;http://google.com/\u0026#34;) #\u0026gt; Host: http://google.com/ \u0026lt;0x10f3d1600\u0026gt; Reset handle\nhandle_reset(\u0026#34;http://google.com/\u0026#34;) Try call again, this time it should work\nHEAD(\u0026#34;http://google.com/\u0026#34;) #\u0026gt; Response [http://www.google.com/] #\u0026gt; Date: 2014-12-08 13:37 #\u0026gt; Status: 200 #\u0026gt; Content-Type: text/html; charset=ISO-8859-1 #\u0026gt; \u0026lt;EMPTY BODY\u0026gt; Usage in ropensci packages We have more work to do yet to integrate this into our packages. It\u0026rsquo;s great you can reset a handle as above, but to reset the handle you need to search for the URL used in the request, which our users would have to dig into the code for the function they are using. That is easy-ish to do, but perhaps not everyone knows they can get to the code easily. So, we may try seting a parameter in functions that would let reset the handle to clear this error.\nNote Note that Hadley is planning on eliminating RCurl dependency (https://github.com/hadley/httr/issues/172), so there may be a different solution in the future.\n","permalink":"http://localhost:1313/2014/12/multi-handle/","summary":"\u003cp\u003eAt rOpenSci we occasssionally hear from our users that they run into an error like:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eError \u003cspan style=\"color:#66d9ef\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e (type, msg, asError \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTRUE\u003c/span\u003e)  \u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  easy handled already used \u003cspan style=\"color:#66d9ef\"\u003ein\u003c/span\u003e multi handle\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis error occurs in the \u003ccode\u003ehttr\u003c/code\u003e package that we use to do http requests to sources of data on the web. It happens when e.g., you make a lot of requests to a resource, then it gets interrupted somehow - then you make another call, and you get the error above. Let\u0026rsquo;s try it with the an version of \u003ccode\u003ehttr\u003c/code\u003e (\u003ccode\u003ev0.5\u003c/code\u003e):\u003c/p\u003e","title":"Dealing with multi handle errors"},{"content":"I just missed another chat on the rOpenSci website:\nI want to know the number of publications by people from a certain country, but I dont know how to achieve this\u0026hellip;\nFun! Let\u0026rsquo;s do that. It\u0026rsquo;s a bit complicated because there is no field like geography of the authors. But there are affiliation fields, from which we can collect data we need.\nInstallation You\u0026rsquo;ll need the GitHub version for the coutry names data, or just use the CRAN version, and get country names elsewhere.\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/rplos\u0026#34;) library(\u0026#34;rplos\u0026#34;) Get the data articles \u0026lt;- searchplos(q=\u0026#39;*:*\u0026#39;, limit = 5, fl=c(\u0026#34;id\u0026#34;,\u0026#34;author_affiliate\u0026#34;), fq=list(\u0026#39;article_type:\u0026#34;Research Article\u0026#34;\u0026#39;, \u0026#34;doc_type:full\u0026#34;)) Search for country names in affilitation field (countries \u0026lt;- lapply(articles$data$author_affiliate, function(x){ out \u0026lt;- sapply(isocodes$name, function(z) grepl(z, x)) isocodes$name[out] })) #\u0026gt; [[1]] #\u0026gt; character(0) #\u0026gt; #\u0026gt; [[2]] #\u0026gt; [1] \u0026#34;Jersey\u0026#34; \u0026#34;United States\u0026#34; #\u0026gt; #\u0026gt; [[3]] #\u0026gt; [1] \u0026#34;China\u0026#34; \u0026#34;Germany\u0026#34; #\u0026gt; #\u0026gt; [[4]] #\u0026gt; character(0) #\u0026gt; #\u0026gt; [[5]] #\u0026gt; [1] \u0026#34;Argentina\u0026#34; \u0026#34;United Kingdom\u0026#34; You can combine this data with the previously collected data:\n# Helper function splitem \u0026lt;- function(x){ if(length(x) == 0) { NA } else { if(length(x) \u0026gt; 1) paste0(x, collapse = \u0026#34;, \u0026#34;) else x } } articles$data$countries \u0026lt;- sapply(countries, splitem) head(articles$data) #\u0026gt; id #\u0026gt; 1 10.1371/journal.pone.0095870 #\u0026gt; 2 10.1371/journal.pone.0110535 #\u0026gt; 3 10.1371/journal.pone.0110991 #\u0026gt; 4 10.1371/journal.pone.0111234 #\u0026gt; 5 10.1371/journal.pone.0111388 #\u0026gt; author_affiliate #\u0026gt; 1 Institute of Epidemiology and Preventive Medicine, College of Public Health, National Taiwan University, Taipei, Taiwan; Department of Clinical Laboratory Sciences and Medical Biotechnology, College of Medicine, National Taiwan University, Taipei, Taiwan; Department of Gastroenterology, Ren-Ai Branch, Taipei City Hospital, Taipei, Taiwan; Division of Gastroenterology, Department of Internal Medicine, National Taiwan University Hospital and National Taiwan University College of Medicine, Taipei, Taiwan; Liver Research Unit, Chang Gung Memorial Hospital, Chang Gung University College of Medicine, Taipei, Taiwan; Division of Gastroenterology, Department of Medicine, Taipei Veterans General Hospital, Taipei, Taiwan; Cheng Hsin General Hospital, Taipei, Taiwan #\u0026gt; 2 Durham Nephrology Associates, Durham, North Carolina, United States of America; Scientific Activities Department, The National Kidney Foundation, Inc., New York, New York, United States of America; Covance Inc., Princeton, New Jersey, United States of America; Departments of Medicine and Population Health Sciences, University of Wisconsin School of Medicine and Public Health, Madison, Wisconsin, United States of America; Department of Family Medicine, University at Buffalo, Buffalo, New York, United States of America; Baylor Health Care System, Baylor Heart and Vascular Institute, Dallas, Texas, United States of America; Department of Medicine, Division of Nephrology, Icahn School of Medicine at Mount Sinai, New York, New York, United States of America #\u0026gt; 3 State Key Laboratory of Electronic Thin Films and Integrated Devices, School of Microelectronics and Solid-State electronics, University of Electronic Science and Technology of China, Sichuan, China; Electrical and Computer Engineering, Kaiserslautern University of Technology, Kaiserslautern German Gottlieb-Daimler-Strabe, Kaiserslautern, Germany #\u0026gt; 4 SB RAS Institute of Chemical Biology and Fundamental Medicine, Novosibirsk, Russia; Pacific Institute of Bioorganic Chemistry, Far East Division, Russian Academy of Sciences, Vladivostok, Russia; Novosibirsk State University, Novosibirsk, Russia #\u0026gt; 5 CONICET, Consejo Nacional de Investigaciones Científicas y Técnicas, Ciudad Autónoma de Buenos Aires, Buenos Aires, Argentina; INGEO, Instituto de Geología, Facultad de Ciencias Exactas, Físicas y Naturales, Universidad Nacional de San Juan, San Juan, San Juan, Argentina; School of Geography, Earth and Environmental Sciences, University of Birmingham, Birmingham, West Midlands, United Kingdom #\u0026gt; countries #\u0026gt; 1 \u0026lt;NA\u0026gt; #\u0026gt; 2 Jersey, United States #\u0026gt; 3 China, Germany #\u0026gt; 4 \u0026lt;NA\u0026gt; #\u0026gt; 5 Argentina, United Kingdom Bigger data set Okay, cool, lets do it on a bigger data set, and this time, we\u0026rsquo;ll get another variable counter_total_all, which is the combination of page views/pdf downloads for each article. This will allow us to ask Is number of countries included in the authors related to page views?. I have no idea if this question makes sense, but nonetheless, it is a question :)\narticles \u0026lt;- searchplos(q=\u0026#39;*:*\u0026#39;, limit = 1000, fl=c(\u0026#34;id\u0026#34;,\u0026#34;counter_total_all\u0026#34;,\u0026#34;author_affiliate\u0026#34;), fq=list(\u0026#39;article_type:\u0026#34;Research Article\u0026#34;\u0026#39;, \u0026#34;doc_type:full\u0026#34;)) #\u0026gt; 1 #\u0026gt; 2 Get countries\ncountries \u0026lt;- lapply(articles$data$author_affiliate, function(x){ out \u0026lt;- sapply(isocodes$name, function(z) grepl(z, x)) isocodes$name[out] }) df \u0026lt;- articles$data df$countries \u0026lt;- sapply(countries, splitem) Let\u0026rsquo;s remove those rows with 0 countries, since the authors must be from somewhere, so the country name matching must have errored.\ndf$n_countries \u0026lt;- sapply(countries, length) df \u0026lt;- df[ df$n_countries \u0026gt; 0, ] Plot data\nlibrary(\u0026#34;ggplot2\u0026#34;) ggplot(df, aes(n_countries, as.numeric(counter_total_all))) + geom_point() + labs(y=\u0026#34;total page views\u0026#34;) + theme_grey(base_size = 16) Conclusion: meh, maybe, maybe not\nInto rplos We\u0026rsquo;ll probably add a function like this into rplos, as a convenient way to handle this use case.\n","permalink":"http://localhost:1313/2014/12/rplos-pubs-country/","summary":"\u003cp\u003eI just missed another chat on the rOpenSci website:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI want to know the number of publications by people from a certain country, but I dont know how to achieve this\u0026hellip;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFun! Let\u0026rsquo;s do that. It\u0026rsquo;s a bit complicated because there is no field like geography of the authors. But there are affiliation fields, from which we can collect data we need.\u003c/p\u003e\n\u003ch2 id=\"installation\"\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eYou\u0026rsquo;ll need the GitHub version for the coutry names data, or just use the CRAN version, and get country names elsewhere.\u003c/p\u003e","title":"Publications by author country"},{"content":"Recently noticed a little Python library called httpcode that does a simple thing: gives information on http codes in the CLI. I thought this could maybe potentially be useful for R. So I made an R version.\nInstallation devtools::install_github(\u0026#34;sckott/httpcode\u0026#34;) library(\u0026#34;httpcode\u0026#34;) Search by http code http_code(100) #\u0026gt; \u0026lt;Status code: 100\u0026gt; #\u0026gt; Message: Continue #\u0026gt; Explanation: Request received, please continue http_code(400) #\u0026gt; \u0026lt;Status code: 400\u0026gt; #\u0026gt; Message: Bad Request #\u0026gt; Explanation: Bad request syntax or unsupported method http_code(503) #\u0026gt; \u0026lt;Status code: 503\u0026gt; #\u0026gt; Message: Service Unavailable #\u0026gt; Explanation: The server cannot process the request due to a high load http_code(999) #\u0026gt; Error: No description found for code: 999 Fuzzy code search http_code(\u0026#39;1xx\u0026#39;) #\u0026gt; [[1]] #\u0026gt; \u0026lt;Status code: 100\u0026gt; #\u0026gt; Message: Continue #\u0026gt; Explanation: Request received, please continue #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;Status code: 101\u0026gt; #\u0026gt; Message: Switching Protocols #\u0026gt; Explanation: Switching to new protocol; obey Upgrade header #\u0026gt; #\u0026gt; [[3]] #\u0026gt; \u0026lt;Status code: 102\u0026gt; #\u0026gt; Message: Processing #\u0026gt; Explanation: WebDAV; RFC 2518 http_code(\u0026#39;3xx\u0026#39;) #\u0026gt; [[1]] #\u0026gt; \u0026lt;Status code: 300\u0026gt; #\u0026gt; Message: Multiple Choices #\u0026gt; Explanation: Object has several resources -- see URI list #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;Status code: 301\u0026gt; #\u0026gt; Message: Moved Permanently #\u0026gt; Explanation: Object moved permanently -- see URI list #\u0026gt; #\u0026gt; [[3]] #\u0026gt; \u0026lt;Status code: 302\u0026gt; #\u0026gt; Message: Found #\u0026gt; Explanation: Object moved temporarily -- see URI list #\u0026gt; #\u0026gt; [[4]] #\u0026gt; \u0026lt;Status code: 303\u0026gt; #\u0026gt; Message: See Other #\u0026gt; Explanation: Object moved -- see Method and URL list #\u0026gt; #\u0026gt; [[5]] #\u0026gt; \u0026lt;Status code: 304\u0026gt; #\u0026gt; Message: Not Modified #\u0026gt; Explanation: Document has not changed since given time #\u0026gt; #\u0026gt; [[6]] #\u0026gt; \u0026lt;Status code: 305\u0026gt; #\u0026gt; Message: Use Proxy #\u0026gt; Explanation: You must use proxy specified in Location to access this resource. #\u0026gt; #\u0026gt; [[7]] #\u0026gt; \u0026lt;Status code: 306\u0026gt; #\u0026gt; Message: Switch Proxy #\u0026gt; Explanation: Subsequent requests should use the specified proxy #\u0026gt; #\u0026gt; [[8]] #\u0026gt; \u0026lt;Status code: 307\u0026gt; #\u0026gt; Message: Temporary Redirect #\u0026gt; Explanation: Object moved temporarily -- see URI list #\u0026gt; #\u0026gt; [[9]] #\u0026gt; \u0026lt;Status code: 308\u0026gt; #\u0026gt; Message: Permanent Redirect #\u0026gt; Explanation: Object moved permanently http_code(\u0026#39;30[12]\u0026#39;) #\u0026gt; [[1]] #\u0026gt; \u0026lt;Status code: 301\u0026gt; #\u0026gt; Message: Moved Permanently #\u0026gt; Explanation: Object moved permanently -- see URI list #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;Status code: 302\u0026gt; #\u0026gt; Message: Found #\u0026gt; Explanation: Object moved temporarily -- see URI list http_code(\u0026#39;30[34]\u0026#39;) #\u0026gt; [[1]] #\u0026gt; \u0026lt;Status code: 303\u0026gt; #\u0026gt; Message: See Other #\u0026gt; Explanation: Object moved -- see Method and URL list #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;Status code: 304\u0026gt; #\u0026gt; Message: Not Modified #\u0026gt; Explanation: Document has not changed since given time Search by text message http_search(\u0026#34;request\u0026#34;) #\u0026gt; [[1]] #\u0026gt; \u0026lt;Status code: 100\u0026gt; #\u0026gt; Message: Continue #\u0026gt; Explanation: Request received, please continue #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;Status code: 200\u0026gt; #\u0026gt; Message: OK #\u0026gt; Explanation: Request fulfilled, document follows #\u0026gt; #\u0026gt; [[3]] #\u0026gt; \u0026lt;Status code: 202\u0026gt; #\u0026gt; Message: Accepted #\u0026gt; Explanation: Request accepted, processing continues off-line #\u0026gt; #\u0026gt; [[4]] #\u0026gt; \u0026lt;Status code: 203\u0026gt; #\u0026gt; Message: Non-Authoritative Information #\u0026gt; Explanation: Request fulfilled from cache #\u0026gt; #\u0026gt; [[5]] #\u0026gt; \u0026lt;Status code: 204\u0026gt; #\u0026gt; Message: No Content #\u0026gt; Explanation: Request fulfilled, nothing follows #\u0026gt; #\u0026gt; [[6]] #\u0026gt; \u0026lt;Status code: 306\u0026gt; #\u0026gt; Message: Switch Proxy #\u0026gt; Explanation: Subsequent requests should use the specified proxy #\u0026gt; #\u0026gt; [[7]] #\u0026gt; \u0026lt;Status code: 400\u0026gt; #\u0026gt; Message: Bad Request #\u0026gt; Explanation: Bad request syntax or unsupported method #\u0026gt; #\u0026gt; [[8]] #\u0026gt; \u0026lt;Status code: 403\u0026gt; #\u0026gt; Message: Forbidden #\u0026gt; Explanation: Request forbidden -- authorization will not help #\u0026gt; #\u0026gt; [[9]] #\u0026gt; \u0026lt;Status code: 408\u0026gt; #\u0026gt; Message: Request Timeout #\u0026gt; Explanation: Request timed out; try again later. #\u0026gt; #\u0026gt; [[10]] #\u0026gt; \u0026lt;Status code: 409\u0026gt; #\u0026gt; Message: Conflict #\u0026gt; Explanation: Request conflict. #\u0026gt; #\u0026gt; [[11]] #\u0026gt; \u0026lt;Status code: 413\u0026gt; #\u0026gt; Message: Request Entity Too Large #\u0026gt; Explanation: Entity is too large. #\u0026gt; #\u0026gt; [[12]] #\u0026gt; \u0026lt;Status code: 414\u0026gt; #\u0026gt; Message: Request-URI Too Long #\u0026gt; Explanation: URI is too long. #\u0026gt; #\u0026gt; [[13]] #\u0026gt; \u0026lt;Status code: 416\u0026gt; #\u0026gt; Message: Requested Range Not Satisfiable #\u0026gt; Explanation: Cannot satisfy request range. #\u0026gt; #\u0026gt; [[14]] #\u0026gt; \u0026lt;Status code: 503\u0026gt; #\u0026gt; Message: Service Unavailable #\u0026gt; Explanation: The server cannot process the request due to a high load #\u0026gt; #\u0026gt; [[15]] #\u0026gt; \u0026lt;Status code: 505\u0026gt; #\u0026gt; Message: HTTP Version Not Supported #\u0026gt; Explanation: Cannot fulfill request. http_search(\u0026#34;forbidden\u0026#34;) #\u0026gt; [[1]] #\u0026gt; \u0026lt;Status code: 403\u0026gt; #\u0026gt; Message: Forbidden #\u0026gt; Explanation: Request forbidden -- authorization will not help http_search(\u0026#34;too\u0026#34;) #\u0026gt; [[1]] #\u0026gt; \u0026lt;Status code: 413\u0026gt; #\u0026gt; Message: Request Entity Too Large #\u0026gt; Explanation: Entity is too large. #\u0026gt; #\u0026gt; [[2]] #\u0026gt; \u0026lt;Status code: 414\u0026gt; #\u0026gt; Message: Request-URI Too Long #\u0026gt; Explanation: URI is too long. http_search(\u0026#34;birds\u0026#34;) #\u0026gt; Error: No status code found for search: : birds ","permalink":"http://localhost:1313/2014/12/http-codes/","summary":"\u003cp\u003eRecently noticed a little Python library called \u003ca href=\"https://github.com/rspivak/httpcode\"\u003ehttpcode\u003c/a\u003e that does a simple thing: gives information on http codes in the CLI. I thought this could maybe potentially be useful for R. So I made an R version.\u003c/p\u003e\n\u003ch2 id=\"installation\"\u003eInstallation\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevtools\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall_github\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sckott/httpcode\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;httpcode\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"search-by-http-code\"\u003eSearch by http code\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 100\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Continue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request received, please continue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e400\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 400\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Bad Request\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Bad request syntax or unsupported method\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e503\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 503\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Service Unavailable\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: The server cannot process the request due to a high load\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e999\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; Error: No description found for code: 999\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"fuzzy-code-search\"\u003eFuzzy code search\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1xx\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[1]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 100\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Continue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request received, please continue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[2]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 101\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Switching Protocols\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Switching to new protocol; obey Upgrade header\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[3]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 102\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Processing\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: WebDAV; RFC 2518\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;3xx\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[1]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 300\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Multiple Choices\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object has several resources -- see URI list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[2]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 301\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Moved Permanently\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved permanently -- see URI list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[3]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 302\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Found\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved temporarily -- see URI list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[4]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 303\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: See Other\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved -- see Method and URL list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[5]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 304\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Not Modified\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Document has not changed since given time\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[6]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 305\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Use Proxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: You must use proxy specified in Location to access this resource.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[7]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 306\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Switch Proxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Subsequent requests should use the specified proxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[8]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 307\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Temporary Redirect\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved temporarily -- see URI list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[9]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 308\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Permanent Redirect\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved permanently\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30[12]\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[1]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 301\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Moved Permanently\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved permanently -- see URI list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[2]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 302\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Found\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved temporarily -- see URI list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_code\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30[34]\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[1]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 303\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: See Other\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Object moved -- see Method and URL list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[2]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 304\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Not Modified\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Document has not changed since given time\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"search-by-text-message\"\u003eSearch by text message\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_search\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;request\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[1]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 100\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Continue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request received, please continue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[2]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 200\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: OK\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request fulfilled, document follows\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[3]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 202\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Accepted\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request accepted, processing continues off-line\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[4]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 203\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Non-Authoritative Information\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request fulfilled from cache\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[5]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 204\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: No Content\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request fulfilled, nothing follows\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[6]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 306\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Switch Proxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Subsequent requests should use the specified proxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[7]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 400\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Bad Request\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Bad request syntax or unsupported method\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[8]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 403\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Forbidden\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request forbidden -- authorization will not help\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[9]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 408\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Request Timeout\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request timed out; try again later.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[10]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 409\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Conflict\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request conflict.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[11]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 413\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Request Entity Too Large\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Entity is too large.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[12]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 414\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Request-URI Too Long\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: URI is too long.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[13]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 416\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Requested Range Not Satisfiable\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Cannot satisfy request range.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[14]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 503\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Service Unavailable\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: The server cannot process the request due to a high load\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[15]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 505\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: HTTP Version Not Supported\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Cannot fulfill request.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_search\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;forbidden\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[1]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 403\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Forbidden\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Request forbidden -- authorization will not help\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_search\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;too\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[1]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 413\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Request Entity Too Large\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: Entity is too large.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; [[2]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; \u0026lt;Status code: 414\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Message: Request-URI Too Long\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   Explanation: URI is too long.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehttp_search\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;birds\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; Error: No status code found for search: : birds\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"http codes"},{"content":"A missed chat on the rOpenSci website the other day asked:\nHi there, i am trying to use the taxize package and have a .csv file of species names to run through taxize updating them. What would be the code i would need to run to achieve this?\nOne way to answer this is to talk about the basic approach to importing data, doing stuff to the data, then recombining data. There are many ways to do this, but I\u0026rsquo;ll go over a few of them.\nInstall taxize install.packages(\u0026#34;taxize\u0026#34;) install.packages(\u0026#34;downloader\u0026#34;) library(\u0026#34;taxize\u0026#34;) Import data We\u0026rsquo;ll use Winston Chang\u0026rsquo;s new downloader package to avoid problems with https, and get a dataset from our ropensci datasets repo https://github.com/ropensci/datasets\ndownloader::download(\u0026#34;https://raw.githubusercontent.com/ropensci/datasets/master/planttraits/morphological.csv\u0026#34;, \u0026#34;morphological.csv\u0026#34;) dat \u0026lt;- read.csv(\u0026#34;morphological.csv\u0026#34;, stringsAsFactors = FALSE) head(dat) #\u0026gt; species log_SLA leaf_water_content log_wood_density #\u0026gt; 1 Abies concolor 3.46 0.51 -0.52 #\u0026gt; 2 Abies grandis 3.58 0.49 -0.51 #\u0026gt; 3 Abies magnifica 3.87 0.62 -0.53 #\u0026gt; 4 Acacia farnesiana NA NA NA #\u0026gt; 5 Acer glabrum 5.07 0.69 -0.54 #\u0026gt; 6 Adenostoma fasciculata 3.56 0.46 -0.31 #\u0026gt; log_ht log_N #\u0026gt; 1 7.72 0.02 #\u0026gt; 2 7.51 -0.31 #\u0026gt; 3 7.58 -0.14 #\u0026gt; 4 5.70 NA #\u0026gt; 5 3.25 1.02 #\u0026gt; 6 5.33 0.29 After importing data, there are a variety of approaches you could take:\nVector: Take species names as vector from your data.frame, cleaning them, then re-attching to the data.frame later, or In-Place: Use for loops or lapply family functions to iterate over each name while simultaneously re-inserting into the data.frame 1. Vector Make a vector of names\nsplist \u0026lt;- dat$species Then proceed to do name cleaning, e.g, we can use the tnrs function to see if any names are potentially not spelled correctly.\ntnrs_out \u0026lt;- tnrs(splist, source = \u0026#34;iPlant_TNRS\u0026#34;) head(tnrs_out) #\u0026gt; submittedname acceptedname sourceid score #\u0026gt; 1 Ceanothus prostratus Ceanothus prostratus iPlant_TNRS 1 #\u0026gt; 2 Abies magnifica Abies magnifica iPlant_TNRS 1 #\u0026gt; 3 Arctostaphylos canescens Arctostaphylos canescens iPlant_TNRS 1 #\u0026gt; 4 Berberis nervosa Berberis nervosa iPlant_TNRS 1 #\u0026gt; 5 Arbutus menziesii Arbutus menziesii iPlant_TNRS 1 #\u0026gt; 6 Calocedrus decurrens Calocedrus decurrens iPlant_TNRS 1 #\u0026gt; matchedname authority #\u0026gt; 1 Ceanothus prostratus Benth. #\u0026gt; 2 Abies magnifica A. Murray bis #\u0026gt; 3 Arctostaphylos canescens Eastw. #\u0026gt; 4 Berberis nervosa Pursh #\u0026gt; 5 Arbutus menziesii Pursh #\u0026gt; 6 Calocedrus decurrens (Torr.) Florin #\u0026gt; uri #\u0026gt; 1 http://www.tropicos.org/Name/27500276 #\u0026gt; 2 http://www.tropicos.org/Name/24900142 #\u0026gt; 3 http://www.tropicos.org/Name/12302547 #\u0026gt; 4 http://www.tropicos.org/Name/3500175 #\u0026gt; 5 http://www.tropicos.org/Name/12302436 #\u0026gt; 6 http://www.tropicos.org/Name/9400069 Those with score of less than 1 may have misspellings\ntnrs_out[ tnrs_out$score \u0026lt; 1, ] #\u0026gt; submittedname acceptedname sourceid score #\u0026gt; 23 Adenostoma fasciculata Adenostoma fasciculatum iPlant_TNRS 0.97 #\u0026gt; 24 Arctostaphylos glandulosus Arctostaphylos glandulosa iPlant_TNRS 0.97 #\u0026gt; 36 Chamaebatia foliosa Chamaebatia foliolosa iPlant_TNRS 0.95 #\u0026gt; 38 Juniperus californicus Juniperus californica iPlant_TNRS 0.97 #\u0026gt; 77 Prunus illicifolia Prunus ilicifolia iPlant_TNRS 0.99 #\u0026gt; 78 Prunus subcordatus Prunus subcordata iPlant_TNRS 0.97 #\u0026gt; matchedname authority #\u0026gt; 23 Adenostoma fasciculatum Hook. \u0026amp; Arn. #\u0026gt; 24 Arctostaphylos glandulosa Eastw. #\u0026gt; 36 Chamaebatia foliolosa Benth. #\u0026gt; 38 Juniperus californica Carrière #\u0026gt; 77 Prunus ilicifolia (Nutt. ex Hook. \u0026amp; Arn.) D. Dietr. #\u0026gt; 78 Prunus subcordata Benth. #\u0026gt; uri #\u0026gt; 23 http://www.tropicos.org/Name/27801458 #\u0026gt; 24 http://www.tropicos.org/Name/12300542 #\u0026gt; 36 http://www.tropicos.org/Name/27801486 #\u0026gt; 38 http://www.tropicos.org/Name/9400374 #\u0026gt; 77 http://www.tropicos.org/Name/27801102 #\u0026gt; 78 http://www.tropicos.org/Name/27801124 So let\u0026rsquo;s take the acceptedname column as a the new names and assign to a new vector\ncleaned_names \u0026lt;- tnrs_out$acceptedname Then join names back, replacing them, or adding as a new column\nReplace\ndat$species \u0026lt;- cleaned_names head(dat) #\u0026gt; species log_SLA leaf_water_content log_wood_density #\u0026gt; 1 Ceanothus prostratus 3.46 0.51 -0.52 #\u0026gt; 2 Abies magnifica 3.58 0.49 -0.51 #\u0026gt; 3 Arctostaphylos canescens 3.87 0.62 -0.53 #\u0026gt; 4 Berberis nervosa NA NA NA #\u0026gt; 5 Arbutus menziesii 5.07 0.69 -0.54 #\u0026gt; 6 Calocedrus decurrens 3.56 0.46 -0.31 #\u0026gt; log_ht log_N #\u0026gt; 1 7.72 0.02 #\u0026gt; 2 7.51 -0.31 #\u0026gt; 3 7.58 -0.14 #\u0026gt; 4 5.70 NA #\u0026gt; 5 3.25 1.02 #\u0026gt; 6 5.33 0.29 New column\ndat$species_cleaned \u0026lt;- cleaned_names head(dat) #\u0026gt; species log_SLA leaf_water_content log_wood_density #\u0026gt; 1 Ceanothus prostratus 3.46 0.51 -0.52 #\u0026gt; 2 Abies magnifica 3.58 0.49 -0.51 #\u0026gt; 3 Arctostaphylos canescens 3.87 0.62 -0.53 #\u0026gt; 4 Berberis nervosa NA NA NA #\u0026gt; 5 Arbutus menziesii 5.07 0.69 -0.54 #\u0026gt; 6 Calocedrus decurrens 3.56 0.46 -0.31 #\u0026gt; log_ht log_N species_cleaned #\u0026gt; 1 7.72 0.02 Ceanothus prostratus #\u0026gt; 2 7.51 -0.31 Abies magnifica #\u0026gt; 3 7.58 -0.14 Arctostaphylos canescens #\u0026gt; 4 5.70 NA Berberis nervosa #\u0026gt; 5 3.25 1.02 Arbutus menziesii #\u0026gt; 6 5.33 0.29 Calocedrus decurrens 2. In-place You can use functions from the dplyr package to split-apply-combine, where split is split apart your vector for each taxon, apply to apply a function or functions to do name cleaning, then combine to put them back together.\nHere, we\u0026rsquo;ll attach taxonomic ids from the Catalogue of Life to each species (each row) (with just a subset of the data to save time):\nlibrary(\u0026#34;dplyr\u0026#34;) tbl_df(dat)[1:5,] %\u0026gt;% rowwise() %\u0026gt;% mutate(colid = get_colid(species)) %\u0026gt;% select(species, colid) #\u0026gt; Source: local data frame [5 x 2] #\u0026gt; Groups: \u0026lt;by row\u0026gt; #\u0026gt; #\u0026gt; species colid #\u0026gt; 1 Ceanothus prostratus 19544732 #\u0026gt; 2 Abies magnifica 18158318 #\u0026gt; 3 Arctostaphylos canescens 19358934 #\u0026gt; 4 Berberis nervosa 19374077 #\u0026gt; 5 Arbutus menziesii 19358819 Let\u0026rsquo;s do something a bit more complicated. Get common names for each taxon in a new column, if more than 1, concatenate into a single character string for easy inclusion in a data.frame\nsci2comm_concat \u0026lt;- function(x){ temp \u0026lt;- sci2comm(x, db = \u0026#34;eol\u0026#34;) if(length(temp) == 0) NA else paste0(temp[[1]], collapse = \u0026#34;, \u0026#34;) } dat_new \u0026lt;- tbl_df(dat)[1:5,] %\u0026gt;% rowwise() %\u0026gt;% mutate(comm = sci2comm_concat(species)) To see the new column, do\ndat_new %\u0026gt;% select(comm) #\u0026gt; Source: local data frame [5 x 1] #\u0026gt; Groups: \u0026lt;by row\u0026gt; #\u0026gt; #\u0026gt; comm #\u0026gt; 1 Mahala-mat Ceanothus, prostrate ceanothus, squawcarpet #\u0026gt; 2 Prächtige Tanne, Goldtanne (Gold-Tanne), Kalifornische Rot-Tanne, Pracht-Ta #\u0026gt; 3 hoary manzanita, hoary manzanita, Sonoma manzanita #\u0026gt; 4 Longleaf Oregon-grape, Cascade barberry, Dull Oregon grape, Oregon grape-ho #\u0026gt; 5 pacific madrone, Madrona, madrone, Kalifornianmansikkapuu ","permalink":"http://localhost:1313/2014/12/taxize-workflows/","summary":"\u003cp\u003eA missed chat on the rOpenSci website the other day asked:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHi there, i am trying to use the taxize package and have a .csv file of species names to run through taxize updating them. What would be the code i would need to run to achieve this?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eOne way to answer this is to talk about the basic approach to importing data, doing stuff to the data, then recombining data. There are many ways to do this, but I\u0026rsquo;ll go over a few of them.\u003c/p\u003e","title":"taxize workflows"},{"content":"Just today we\u0026rsquo;ve hit 1000 commits on taxize! taxize is an R client to search across lots of taxonomic databases on the web. In honor of the 1000 commit milestone, here\u0026rsquo;s some stats on the project.\nBefore that, lots of people have contributed to taxize, it\u0026rsquo;s a big group effort:\nEduard Szöcs Zachary Foster Carl Boettiger Karthik Ram Jari Oksanen Francis Michonneau Oliver Keyes David LeBauer Ben Marwick Anirvan Chatterjee In addition, we\u0026rsquo;ve had lots of feedback from users, including feature requests and bug reports, making taxize a lot better.\nSetup library(\u0026#34;devtools\u0026#34;) library(\u0026#34;httr\u0026#34;) library(\u0026#34;ggplot2\u0026#34;) library(\u0026#34;stringr\u0026#34;) library(\u0026#34;plyr\u0026#34;) library(\u0026#34;dplyr\u0026#34;) Define functions github_auth \u0026lt;- function(appname = getOption(\u0026#34;gh_appname\u0026#34;), key = getOption(\u0026#34;gh_id\u0026#34;), secret = getOption(\u0026#34;gh_secret\u0026#34;)) { if (is.null(getOption(\u0026#34;gh_token\u0026#34;))) { myapp \u0026lt;- oauth_app(appname, key, secret) token \u0026lt;- oauth2.0_token(oauth_endpoints(\u0026#34;github\u0026#34;), myapp) options(gh_token = token) } else { token \u0026lt;- getOption(\u0026#34;gh_token\u0026#34;) } return(token) } make_url \u0026lt;- function(x, y, z) { sprintf(\u0026#34;https://api.github.com/repos/%s/%s/%s\u0026#34;, x, y, z) } process_result \u0026lt;- function(x) { stop_for_status(x) if (!x$headers$`content-type` == \u0026#34;application/json; charset=utf-8\u0026#34;) stop(\u0026#34;content type mismatch\u0026#34;) tmp \u0026lt;- content(x, as = \u0026#34;text\u0026#34;) jsonlite::fromJSON(tmp, flatten = TRUE) } gh_commits \u0026lt;- function(repo, owner = \u0026#34;ropensci\u0026#34;, ...) { token \u0026lt;- github_auth() outout \u0026lt;- list(); iter \u0026lt;- 0; nexturl \u0026lt;- \u0026#34;dontstop\u0026#34; while(nexturl != \u0026#34;stop\u0026#34;){ iter \u0026lt;- iter + 1 req \u0026lt;- if(grepl(\u0026#34;https:/\u0026#34;, nexturl)) GET(nexturl, config = c(token = token)) else GET(make_url(owner, repo, \u0026#34;commits\u0026#34;), query = list(per_page=100), config = c(token = token)) outout[[iter]] \u0026lt;- process_result(req) link \u0026lt;- req$headers$link nexturl \u0026lt;- if(is.null(link)){ \u0026#34;stop\u0026#34; } else { if(grepl(\u0026#34;next\u0026#34;, link)){ stringr::str_extract(link, \u0026#34;https://[0-9A-Za-z/?=\\\\._\u0026amp;]+\u0026#34;) } else { \u0026#34;stop\u0026#34; } } } outout \u0026lt;- outout[sapply(outout, function(x) !identical(x, list()))] dplyr::rbind_all(outout) } gh_issues \u0026lt;- function(repo, owner = \u0026#34;ropensci\u0026#34;, ...) { token \u0026lt;- github_auth() outout \u0026lt;- list(); iter \u0026lt;- 0; nexturl \u0026lt;- \u0026#34;dontstop\u0026#34; while(nexturl != \u0026#34;stop\u0026#34;){ iter \u0026lt;- iter + 1 req \u0026lt;- if(grepl(\u0026#34;https:/\u0026#34;, nexturl)) GET(nexturl, query=list(state=\u0026#34;all\u0026#34;), config = c(token = token)) else GET(make_url(owner, repo, \u0026#34;issues\u0026#34;), query = list(per_page=100, state=\u0026#34;all\u0026#34;), config = c(token = token)) outout[[iter]] \u0026lt;- process_result(req) link \u0026lt;- req$headers$link nexturl \u0026lt;- if(is.null(link)){ \u0026#34;stop\u0026#34; } else { if(grepl(\u0026#34;next\u0026#34;, link)){ stringr::str_extract(link, \u0026#34;https://[0-9A-Za-z/?=\\\\._\u0026amp;]+\u0026#34;) } else { \u0026#34;stop\u0026#34; } } } outout \u0026lt;- outout[sapply(outout, function(x) !identical(x, list()))] dplyr::rbind_all(outout) } gh_commit \u0026lt;- function(sha, repo, owner = \u0026#34;ropensci\u0026#34;, ...) { token \u0026lt;- github_auth() req \u0026lt;- GET(paste0(make_url(owner, repo, \u0026#34;commits\u0026#34;), \u0026#34;/\u0026#34;, sha), config = c(token = token, ...)) process_result(req) } gh_verb \u0026lt;- function(owner = \u0026#34;ropensci\u0026#34;, repo, verb, args=list(), ...) { token \u0026lt;- github_auth() req \u0026lt;- GET(make_url(owner, repo, verb), query=args, config = c(token = token, ...)) process_result(req) } Commits List of commits\nout \u0026lt;- gh_commits(\u0026#34;taxize\u0026#34;) Get changes for each commit\nchanges \u0026lt;- vapply(out$sha, function(x) gh_commit(x, repo=\u0026#34;taxize\u0026#34;)$stats$total, numeric(1)) changesdf \u0026lt;- data.frame(changes=unname(changes), sha=names(changes)) Combine\nout \u0026lt;- inner_join(out, changesdf) Total changes through time (additions + deletions)\nct \u0026lt;- function(x) as.POSIXct(x, format=\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;, tz=\u0026#34;UTC\u0026#34;) out %\u0026gt;% mutate(commit.committer.date = ct(commit.committer.date)) %\u0026gt;% ggplot(aes(x=commit.committer.date, y=changes)) + geom_area(fill=\u0026#34;#87D2A0\u0026#34;) + theme_grey(base_size = 20) By Authors\nout %\u0026gt;% group_by(author.login) %\u0026gt;% summarise(n = n()) %\u0026gt;% ggplot(aes(author.login, n)) + geom_bar(stat = \u0026#34;identity\u0026#34;, fill=\u0026#34;#87D2A0\u0026#34;) + coord_flip() + theme_grey(base_size = 20) Issues out \u0026lt;- gh_issues(\u0026#34;taxize\u0026#34;) Number of issues\nNROW(out) #\u0026gt; [1] 382 Number of open issues\nout %\u0026gt;% filter(state == \u0026#34;open\u0026#34;) %\u0026gt;% NROW #\u0026gt; [1] 35 Number of pull requests\nout %\u0026gt;% filter(!is.na(pull_request.url)) %\u0026gt;% NROW #\u0026gt; [1] 119 Forks, number of NROW(gh_verb(repo = \u0026#34;taxize\u0026#34;, verb=\u0026#34;forks\u0026#34;)) #\u0026gt; [1] 16 Stars, number of NROW(gh_verb(repo = \u0026#34;taxize\u0026#34;, verb=\u0026#34;stargazers\u0026#34;, args=list(per_page=100))) #\u0026gt; [1] 44 Watchers, number of NROW(gh_verb(repo = \u0026#34;taxize\u0026#34;, verb=\u0026#34;subscribers\u0026#34;, args=list(per_page=100))) #\u0026gt; [1] 12 ","permalink":"http://localhost:1313/2014/11/taxize-1000/","summary":"\u003cp\u003eJust today we\u0026rsquo;ve hit 1000 commits on \u003ccode\u003etaxize\u003c/code\u003e!  \u003ccode\u003etaxize\u003c/code\u003e is an R client to search across lots of taxonomic databases on the web. In honor of the 1000 commit milestone, here\u0026rsquo;s some stats on the project.\u003c/p\u003e\n\u003cp\u003eBefore that, lots of people have contributed to \u003ccode\u003etaxize\u003c/code\u003e, it\u0026rsquo;s a big group effort:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EDiLD\"\u003eEduard Szöcs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/zachary-foster\"\u003eZachary Foster\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/cboettig\"\u003eCarl Boettiger\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/karthik\"\u003eKarthik Ram\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jarioksa\"\u003eJari Oksanen\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/fmichonneau\"\u003eFrancis Michonneau\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Ironholds\"\u003eOliver Keyes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dlebauer\"\u003eDavid LeBauer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/benmarwick\"\u003eBen Marwick\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/anirvan\"\u003eAnirvan Chatterjee\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition, we\u0026rsquo;ve had lots of feedback from users, including feature requests and bug reports, making \u003ccode\u003etaxize\u003c/code\u003e a lot better.\u003c/p\u003e","title":"1000 commits to taxize"},{"content":"Recently I had need to create a client for scraping museum metadata to help out some folks that use that kind of data. It\u0026rsquo;s called musemeta. One of the data sources in that package uses the open source data portal software CKAN, and so we can interact with the CKAN API to get data. Since many groups can use CKAN API/etc infrastucture because it\u0026rsquo;s open source, I thought why not have a general purpose R client for this, since there are other clients for Python, PHP, Ruby, etc.\nHere\u0026rsquo;s a bit of an intro:\nSetup Get/load packages\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;ropensci/ckanr\u0026#34;) library(\u0026#34;ckanr\u0026#34;) Note: the default URL is for https://data.techno-science.ca/. You can change that in the url parameter\nSome package details All API alls are POST requests, and handled through a helper function called ckan_POST(). See ckanr:::ckan_POST to see the function.\nThere are some common parameters across most functions that are worth describing a bit:\noffset (numeric) Where to start getting items from (optional, default: 0) limit (numeric) The maximum number of items to return (optional, default: 31) url Base url to use. Default: https://data.techno-science.ca as (character) One of list (default), table, or json. Parsing with table option uses jsonlite::fromJSON(..., simplifyDataFrame = TRUE), which attempts to parse data to data.frame\u0026rsquo;s when possible, so the result can vary. ... Curl args passed on to httr::POST Changes changes(limit = 2, as = \u0026#34;table\u0026#34;) #\u0026gt; user_id timestamp #\u0026gt; 1 b50449ea-1dcc-4d52-b620-fc95bf56034b 2014-11-06T18:58:08.001743 #\u0026gt; 2 b50449ea-1dcc-4d52-b620-fc95bf56034b 2014-11-06T18:55:55.059527 #\u0026gt; object_id #\u0026gt; 1 cc6a523c-cecf-4a95-836b-295a11ce2bce #\u0026gt; 2 cc6a523c-cecf-4a95-836b-295a11ce2bce #\u0026gt; revision_id data.package.maintainer #\u0026gt; 1 5d11079e-fc05-4121-9fd5-fe086f5e5f33 #\u0026gt; 2 4a591538-0584-487b-8ed1-3260d1d09d77 #\u0026gt; data.package.name data.package.metadata_modified data.package.author #\u0026gt; 1 test 2014-11-06T18:55:54.772675 #\u0026gt; 2 test 2014-11-06T18:55:54.772675 #\u0026gt; data.package.url data.package.notes data.package.owner_org #\u0026gt; 1 fafa260d-e2bf-46cd-9c35-34c1dfa46c57 #\u0026gt; 2 fafa260d-e2bf-46cd-9c35-34c1dfa46c57 #\u0026gt; data.package.private data.package.maintainer_email #\u0026gt; 1 FALSE #\u0026gt; 2 FALSE #\u0026gt; data.package.author_email data.package.state data.package.version #\u0026gt; 1 deleted #\u0026gt; 2 active #\u0026gt; data.package.creator_user_id #\u0026gt; 1 b50449ea-1dcc-4d52-b620-fc95bf56034b #\u0026gt; 2 b50449ea-1dcc-4d52-b620-fc95bf56034b #\u0026gt; data.package.id data.package.title #\u0026gt; 1 cc6a523c-cecf-4a95-836b-295a11ce2bce test #\u0026gt; 2 cc6a523c-cecf-4a95-836b-295a11ce2bce test #\u0026gt; data.package.revision_id data.package.type #\u0026gt; 1 5d11079e-fc05-4121-9fd5-fe086f5e5f33 dataset #\u0026gt; 2 4a591538-0584-487b-8ed1-3260d1d09d77 dataset #\u0026gt; data.package.license_id id #\u0026gt; 1 notspecified 59c308c8-68b2-4b92-bc57-129378d31882 #\u0026gt; 2 notspecified a8577e2c-f742-49c2-bef3-ca3299e58704 #\u0026gt; activity_type #\u0026gt; 1 deleted package #\u0026gt; 2 changed package List datasets datasets(as = \u0026#34;table\u0026#34;) #\u0026gt; [1] \u0026#34;artifact-data-agriculture\u0026#34; #\u0026gt; [2] \u0026#34;artifact-data-aviation\u0026#34; #\u0026gt; [3] \u0026#34;artifact-data-bookbinding\u0026#34; #\u0026gt; [4] \u0026#34;artifact-data-chemistry\u0026#34; #\u0026gt; [5] \u0026#34;artifact-data-communications\u0026#34; #\u0026gt; [6] \u0026#34;artifact-data-computing-technology\u0026#34; #\u0026gt; [7] \u0026#34;artifact-data-domestic-technology\u0026#34; #\u0026gt; [8] \u0026#34;artifact-data-energy-electric\u0026#34; #\u0026gt; [9] \u0026#34;artifact-data-exploration-and-survey\u0026#34; #\u0026gt; [10] \u0026#34;artifact-data-fisheries\u0026#34; #\u0026gt; [11] \u0026#34;artifact-data-forestry\u0026#34; #\u0026gt; [12] \u0026#34;artifact-data-horology\u0026#34; #\u0026gt; [13] \u0026#34;artifact-data-industrial-technology\u0026#34; #\u0026gt; [14] \u0026#34;artifact-data-lighting-technology\u0026#34; #\u0026gt; [15] \u0026#34;artifact-data-location-canada-agriculture-and-food-museum\u0026#34; #\u0026gt; [16] \u0026#34;artifact-data-location-canada-aviation-and-space-museum\u0026#34; #\u0026gt; [17] \u0026#34;artifact-data-location-canada-science-and-technology-museum\u0026#34; #\u0026gt; [18] \u0026#34;artifact-data-marine-transportation\u0026#34; #\u0026gt; [19] \u0026#34;artifact-data-mathematics\u0026#34; #\u0026gt; [20] \u0026#34;artifact-data-medical-technology\u0026#34; #\u0026gt; [21] \u0026#34;artifact-data-meteorology\u0026#34; #\u0026gt; [22] \u0026#34;artifact-data-metrology\u0026#34; #\u0026gt; [23] \u0026#34;artifact-data-mining-and-metallurgy\u0026#34; #\u0026gt; [24] \u0026#34;artifact-data-motorized-ground-transportation\u0026#34; #\u0026gt; [25] \u0026#34;artifact-data-non-motorized-ground-transportation\u0026#34; #\u0026gt; [26] \u0026#34;artifact-data-on-loan\u0026#34; #\u0026gt; [27] \u0026#34;artifact-data-photography\u0026#34; #\u0026gt; [28] \u0026#34;artifact-data-physics\u0026#34; #\u0026gt; [29] \u0026#34;artifact-data-printing\u0026#34; #\u0026gt; [30] \u0026#34;artifact-data-railway-transportation\u0026#34; #\u0026gt; [31] \u0026#34;artifact-dataset-fire-fighting\u0026#34; List tags tag_list(\u0026#39;aviation\u0026#39;, as=\u0026#39;table\u0026#39;) #\u0026gt; vocabulary_id display_name #\u0026gt; 1 NA Aviation #\u0026gt; 2 NA Canada Aviation and Space Museum #\u0026gt; id name #\u0026gt; 1 cc1db2db-b08b-4888-897f-a17eade2461b Aviation #\u0026gt; 2 8d05a650-bc7b-4b89-bcc8-c10177e60119 Canada Aviation and Space Museum Show tags Subset for readme brevity\ntag_show(\u0026#39;Aviation\u0026#39;)$packages[[1]][1:3] #\u0026gt; $owner_org #\u0026gt; [1] \u0026#34;fafa260d-e2bf-46cd-9c35-34c1dfa46c57\u0026#34; #\u0026gt; #\u0026gt; $maintainer #\u0026gt; [1] \u0026#34;\u0026#34; #\u0026gt; #\u0026gt; $relationships_as_object #\u0026gt; list() List groups group_list(as=\u0026#39;table\u0026#39;) #\u0026gt; display_name description #\u0026gt; 1 Communications #\u0026gt; 2 Domestic and Industrial Technology #\u0026gt; 3 Everything #\u0026gt; 4 Location #\u0026gt; 5 Resources #\u0026gt; 6 Scientific Instrumentation #\u0026gt; 7 Transportation #\u0026gt; title #\u0026gt; 1 Communications #\u0026gt; 2 Domestic and Industrial Technology #\u0026gt; 3 Everything #\u0026gt; 4 Location #\u0026gt; 5 Resources #\u0026gt; 6 Scientific Instrumentation #\u0026gt; 7 Transportation #\u0026gt; image_display_url #\u0026gt; 1 http://data.techno-science.ca/uploads/group/20141024-162305.6896412comm.jpg #\u0026gt; 2 http://data.techno-science.ca/uploads/group/20141024-162324.3636615domtech.jpg #\u0026gt; 3 http://data.techno-science.ca/uploads/group/20141024-162448.0656596everything.jpg #\u0026gt; 4 http://data.techno-science.ca/uploads/group/20141024-162528.8786547location.jpg #\u0026gt; 5 http://data.techno-science.ca/uploads/group/20141024-162608.3732604resour.jpg #\u0026gt; 6 http://data.techno-science.ca/uploads/group/20141024-162549.1925831sciinst.jpg #\u0026gt; 7 http://data.techno-science.ca/uploads/group/20141024-162624.1872823transport.jpg #\u0026gt; approval_status is_organization state #\u0026gt; 1 approved FALSE active #\u0026gt; 2 approved FALSE active #\u0026gt; 3 approved FALSE active #\u0026gt; 4 approved FALSE active #\u0026gt; 5 approved FALSE active #\u0026gt; 6 approved FALSE active #\u0026gt; 7 approved FALSE active #\u0026gt; image_url #\u0026gt; 1 20141024-162305.6896412comm.jpg #\u0026gt; 2 20141024-162324.3636615domtech.jpg #\u0026gt; 3 20141024-162448.0656596everything.jpg #\u0026gt; 4 20141024-162528.8786547location.jpg #\u0026gt; 5 20141024-162608.3732604resour.jpg #\u0026gt; 6 20141024-162549.1925831sciinst.jpg #\u0026gt; 7 20141024-162624.1872823transport.jpg #\u0026gt; revision_id packages type #\u0026gt; 1 cc302424-2e68-4fcc-9a3a-6de60748c2e4 5 group #\u0026gt; 2 b7d95b87-5999-45f9-8775-c64094842551 2 group #\u0026gt; 3 c2f0c59a-a543-4d67-a61f-4f387068ba53 1 group #\u0026gt; 4 6816d571-d2bd-4131-b99d-80e7e6797492 4 group #\u0026gt; 5 e37ee30d-577b-4349-8f0e-eaa4543497e8 6 group #\u0026gt; 6 74eba42e-08b3-4400-b40f-3d6159ae6e9d 10 group #\u0026gt; 7 a6cc4aab-eae9-42ba-9ab4-cbf45d5c6a0e 7 group #\u0026gt; id name #\u0026gt; 1 5268ce18-e3b8-4802-b29e-30740b46e52d communications #\u0026gt; 2 5a9a8095-9e0c-485e-84f6-77f577607991 domestic-and-industrial-technology #\u0026gt; 3 d7dd233e-a1cc-43da-8152-f7ed15d26756 everything #\u0026gt; 4 770fc9c0-d4f3-48b0-a4ee-e00c6882df1d location #\u0026gt; 5 f6c205de-cc95-4308-ac9f-5a63f1a5c7ee resources #\u0026gt; 6 b98ff457-2031-48b6-b681-9adb3afc501b scientific-instrumentation #\u0026gt; 7 a73bf7be-310d-472e-83e1-43a3d87602ba transportation Show groups Subset for readme brevity\ngroup_show(\u0026#39;communications\u0026#39;, as=\u0026#39;table\u0026#39;)$users #\u0026gt; openid about capacity name created #\u0026gt; 1 NA \u0026lt;NA\u0026gt; admin marc 2014-10-24T14:44:29.885262 #\u0026gt; 2 NA admin sepandar 2014-10-23T19:40:42.056418 #\u0026gt; email_hash sysadmin #\u0026gt; 1 a32002c960476614370a16e9fb81f436 FALSE #\u0026gt; 2 10b930a228afd1da2647d62e70b71bf8 TRUE #\u0026gt; activity_streams_email_notifications state number_of_edits #\u0026gt; 1 FALSE active 376 #\u0026gt; 2 FALSE active 44 #\u0026gt; number_administered_packages display_name fullname #\u0026gt; 1 39 marc \u0026lt;NA\u0026gt; #\u0026gt; 2 1 sepandar #\u0026gt; id #\u0026gt; 1 27778230-2e90-4818-9f00-bbf778c8fa09 #\u0026gt; 2 b50449ea-1dcc-4d52-b620-fc95bf56034b Show a package package_show(\u0026#39;34d60b13-1fd5-430e-b0ec-c8bc7f4841cf\u0026#39;, as=\u0026#39;table\u0026#39;)$resources #\u0026gt; resource_group_id cache_last_updated #\u0026gt; 1 ea8533d9-cdc6-4e0e-97b9-894e06d50b92 NA #\u0026gt; 2 ea8533d9-cdc6-4e0e-97b9-894e06d50b92 NA #\u0026gt; 3 ea8533d9-cdc6-4e0e-97b9-894e06d50b92 NA #\u0026gt; 4 ea8533d9-cdc6-4e0e-97b9-894e06d50b92 NA #\u0026gt; revision_timestamp webstore_last_updated #\u0026gt; 1 2014-10-28T18:13:22.213530 NA #\u0026gt; 2 2014-11-04T02:59:50.567068 NA #\u0026gt; 3 2014-11-05T21:23:58.533397 NA #\u0026gt; 4 2014-11-05T21:25:16.848423 NA #\u0026gt; id size state hash #\u0026gt; 1 be2b0af8-24a8-4a55-8b30-89f5459b713a NA active #\u0026gt; 2 7d65910e-4bdc-4f06-a213-e24e36762767 NA active #\u0026gt; 3 97622ad7-1507-4f6a-8acb-14e826447389 NA active #\u0026gt; 4 7a72498a-c49c-4e84-8b10-58991de10df6 NA active #\u0026gt; description format #\u0026gt; 1 XML Dataset XML #\u0026gt; 2 Data dictionary for CSTMC artifact datasets. XLS #\u0026gt; 3 Tips for using the artifacts datasets. .php #\u0026gt; 4 Tips for using the artifacts datasets. .php #\u0026gt; tracking_summary.total tracking_summary.recent mimetype_inner url_type #\u0026gt; 1 0 0 NA NA #\u0026gt; 2 0 0 NA NA #\u0026gt; 3 0 0 NA NA #\u0026gt; 4 0 0 NA NA #\u0026gt; mimetype cache_url name #\u0026gt; 1 NA NA Artifact Data - Vacuum Tubes (XML) #\u0026gt; 2 NA NA Data Dictionary #\u0026gt; 3 NA NA Tips (English) #\u0026gt; 4 NA NA Tips (French) #\u0026gt; created #\u0026gt; 1 2014-10-28T18:13:22.240393 #\u0026gt; 2 2014-11-04T02:59:50.643658 #\u0026gt; 3 2014-11-04T18:14:23.952937 #\u0026gt; 4 2014-11-05T21:25:16.887796 #\u0026gt; url #\u0026gt; 1 http://source.techno-science.ca/datasets-donn%C3%A9es/artifacts-artefacts/groups-groupes/vacuum-tubes-tubes-electronique.xml #\u0026gt; 2 http://source.techno-science.ca/datasets-donn%C3%A9es/artifacts-artefacts/cstmc-artifact-data-dictionary-dictionnaire-de-donnees-artefacts-smstc.xls #\u0026gt; 3 http://techno-science.ca/en/open-data/tips-using-artifact-open-data-set.php #\u0026gt; 4 http://techno-science.ca/fr/donnees-ouvertes/conseils-donnees-ouvertes-artefacts.php #\u0026gt; webstore_url last_modified position revision_id #\u0026gt; 1 NA NA 0 9a27d884-f181-4842-ab47-cda35a8bf99a #\u0026gt; 2 NA NA 1 5d27b3e6-7870-4c12-a122-9e9f5adee4a0 #\u0026gt; 3 NA NA 2 40993f16-402b-439c-9288-2f2b177e4b8f #\u0026gt; 4 NA NA 3 57f1488e-a140-4eb6-9329-fc13202a73af #\u0026gt; resource_type #\u0026gt; 1 NA #\u0026gt; 2 NA #\u0026gt; 3 NA #\u0026gt; 4 NA Search for packages out \u0026lt;- package_search(q = \u0026#39;*:*\u0026#39;, rows = 2, as=\u0026#34;table\u0026#34;)$results out[, !names(out) %in% \u0026#39;resources\u0026#39;] #\u0026gt; license_title maintainer relationships_as_object #\u0026gt; 1 Open Government Licence - Canada NULL #\u0026gt; 2 Open Government Licence - Canada NULL #\u0026gt; private maintainer_email revision_timestamp #\u0026gt; 1 FALSE 2014-11-05T23:17:46.220002 #\u0026gt; 2 FALSE 2014-11-05T23:17:04.923594 #\u0026gt; id metadata_created #\u0026gt; 1 35d5484d-38ce-495e-8722-7857c4fd17bf 2014-10-28T20:13:11.572558 #\u0026gt; 2 da65507d-b018-4d3b-bde3-5419cf29d144 2014-10-28T14:59:21.386177 #\u0026gt; metadata_modified author author_email state version #\u0026gt; 1 2014-11-05T23:17:46.220657 active #\u0026gt; 2 2014-11-05T23:17:04.924229 active #\u0026gt; creator_user_id type num_resources #\u0026gt; 1 27778230-2e90-4818-9f00-bbf778c8fa09 dataset 4 #\u0026gt; 2 27778230-2e90-4818-9f00-bbf778c8fa09 dataset 4 #\u0026gt; tags #\u0026gt; 1 NA, Location, Location, 2014-10-28T20:13:11.572558, active, da88c5a2-3766-41ea-a75b-9c87047cc528 #\u0026gt; 2 NA, Computing Technology, Computing Technology, 2014-10-28T14:59:21.386177, active, 5371dc28-9ce8-4f21-9afb-1f155f132bfe #\u0026gt; tracking_summary.total tracking_summary.recent #\u0026gt; 1 35 10 #\u0026gt; 2 24 8 #\u0026gt; groups #\u0026gt; 1 Location, , http://data.techno-science.ca/uploads/group/20141024-162528.8786547location.jpg, Location, 770fc9c0-d4f3-48b0-a4ee-e00c6882df1d, location #\u0026gt; 2 Scientific Instrumentation, , http://data.techno-science.ca/uploads/group/20141024-162549.1925831sciinst.jpg, Scientific Instrumentation, b98ff457-2031-48b6-b681-9adb3afc501b, scientific-instrumentation #\u0026gt; license_id relationships_as_subject num_tags organization.description #\u0026gt; 1 ca-ogl-lgo NULL 1 #\u0026gt; 2 ca-ogl-lgo NULL 1 #\u0026gt; organization.created organization.title organization.name #\u0026gt; 1 2014-10-24T14:49:36.878579 CSTMC cstmc #\u0026gt; 2 2014-10-24T14:49:36.878579 CSTMC cstmc #\u0026gt; organization.revision_timestamp organization.is_organization #\u0026gt; 1 2014-10-24T14:49:36.813670 TRUE #\u0026gt; 2 2014-10-24T14:49:36.813670 TRUE #\u0026gt; organization.state organization.image_url #\u0026gt; 1 active #\u0026gt; 2 active #\u0026gt; organization.revision_id organization.type #\u0026gt; 1 7a325a56-46f1-419c-b7b2-ec7501edb35a organization #\u0026gt; 2 7a325a56-46f1-419c-b7b2-ec7501edb35a organization #\u0026gt; organization.id organization.approval_status #\u0026gt; 1 fafa260d-e2bf-46cd-9c35-34c1dfa46c57 approved #\u0026gt; 2 fafa260d-e2bf-46cd-9c35-34c1dfa46c57 approved #\u0026gt; name isopen url #\u0026gt; 1 artifact-data-location-canada-science-and-technology-museum FALSE #\u0026gt; 2 artifact-data-computing-technology FALSE #\u0026gt; notes #\u0026gt; 1 This dataset includes artifacts in the collection of the Canada Science and Technology Museums Corporation that are currently in the Canada Science and Technology Museum. #\u0026gt; 2 This dataset includes artifacts in the collection of the Canada Science and Technology Museums Corporation related to computing technology. #\u0026gt; owner_org extras #\u0026gt; 1 fafa260d-e2bf-46cd-9c35-34c1dfa46c57 NULL #\u0026gt; 2 fafa260d-e2bf-46cd-9c35-34c1dfa46c57 NULL #\u0026gt; license_url #\u0026gt; 1 http://data.gc.ca/eng/open-government-licence-canada #\u0026gt; 2 http://data.gc.ca/eng/open-government-licence-canada #\u0026gt; title #\u0026gt; 1 Artifact Data - Location - Canada Science and Technology Museum #\u0026gt; 2 Artifact Data - Computing Technology #\u0026gt; revision_id #\u0026gt; 1 694a977a-c238-47a4-8671-caddca4edfca #\u0026gt; 2 858cb240-76a0-406a-800c-e4ae6cc56ab9 Search for resources resource_search(q = \u0026#39;name:data\u0026#39;, limit = 2, as=\u0026#39;table\u0026#39;) #\u0026gt; $count #\u0026gt; [1] 71 #\u0026gt; #\u0026gt; $results #\u0026gt; resource_group_id cache_last_updated #\u0026gt; 1 01a82e52-01bf-4a9c-9b45-c4f9b92529fa NA #\u0026gt; 2 01a82e52-01bf-4a9c-9b45-c4f9b92529fa NA #\u0026gt; webstore_last_updated id size state #\u0026gt; 1 NA e179e910-27fb-44f4-a627-99822af49ffa NA active #\u0026gt; 2 NA ba84e8b7-b388-4d2a-873a-7b107eb7f135 NA active #\u0026gt; last_modified hash description format #\u0026gt; 1 NA XML Dataset XML #\u0026gt; 2 NA Data dictionary for CSTMC artifact datasets. XLS #\u0026gt; mimetype_inner url_type mimetype cache_url #\u0026gt; 1 NA NA NA NA #\u0026gt; 2 NA NA NA NA #\u0026gt; name created #\u0026gt; 1 Artifact Data - Exploration and Survey (XML) 2014-10-28T15:50:35.374303 #\u0026gt; 2 Data Dictionary 2014-11-03T18:01:02.094210 #\u0026gt; url #\u0026gt; 1 http://source.techno-science.ca/datasets-donn%C3%A9es/artifacts-artefacts/groups-groupes/exploration-and-survey-exploration-et-leve.xml #\u0026gt; 2 http://source.techno-science.ca/datasets-donn%C3%A9es/artifacts-artefacts/cstmc-artifact-data-dictionary-dictionnaire-de-donnees-artefacts-smstc.xls #\u0026gt; webstore_url position revision_id resource_type #\u0026gt; 1 NA 0 a22e6741-3e89-4db0-a802-ba594b1c1fad NA #\u0026gt; 2 NA 1 da1f8585-521d-47ef-8ead-7832474a3421 NA Future work There\u0026rsquo;s already an issue to add support for DataStore This client needs to be tested against many other CKAN API instances to make sure it\u0026rsquo;s robust Add a test suite Use cases: would be nice to include in the package documentation use cases Other things? Get in touch on twitter @recology_ or below ","permalink":"http://localhost:1313/2014/11/ckanr-intro/","summary":"\u003cp\u003eRecently I had need to create a client for scraping museum metadata to help out some folks that use that kind of data. It\u0026rsquo;s called \u003ca href=\"https://github.com/ropensci/musemeta\"\u003emusemeta\u003c/a\u003e. One of the data sources in that package uses the open source \u003cem\u003edata portal software\u003c/em\u003e \u003ca href=\"https://ckan.org/\"\u003eCKAN\u003c/a\u003e, and so we can interact with \u003ca href=\"https://docs.ckan.org/en/latest/api/index.html\"\u003ethe CKAN API\u003c/a\u003e to get data. Since many groups can use CKAN API/etc infrastucture because it\u0026rsquo;s open source, I thought why not have a general purpose R client for this, since \u003ca href=\"https://github.com/ckan/ckan/wiki/CKAN-API-Clients\"\u003ethere are other clients\u003c/a\u003e for Python, PHP, Ruby, etc.\u003c/p\u003e","title":"Intro to alpha ckanr - R client for CKAN RESTful API"},{"content":"Recently I\u0026rsquo;ve had fun playing with the GitHub API, and here are some notes to self about this fun having.\nSetup Get/load packages\ninstall.packages(c(\u0026#39;devtools\u0026#39;,\u0026#39;jsonlite\u0026#39;,\u0026#39;httr\u0026#39;,\u0026#39;yaml\u0026#39;)) library(\u0026#34;devtools\u0026#34;) library(\u0026#34;httr\u0026#34;) library(\u0026#34;yaml\u0026#34;) Define a vector of package names pkgs \u0026lt;- c(\u0026#34;alm\u0026#34;, \u0026#34;bmc\u0026#34;, \u0026#34;bold\u0026#34;, \u0026#34;clifro\u0026#34;, \u0026#34;ecoengine\u0026#34;, \u0026#34;elastic\u0026#34;, \u0026#34;fulltext\u0026#34;, \u0026#34;geonames\u0026#34;, \u0026#34;gistr\u0026#34;, \u0026#34;RNeXML\u0026#34;, \u0026#34;rnoaa\u0026#34;, \u0026#34;rnpn\u0026#34;, \u0026#34;traits\u0026#34;, \u0026#34;rplos\u0026#34;, \u0026#34;rsnps\u0026#34;, \u0026#34;rWBclimate\u0026#34;, \u0026#34;solr\u0026#34;, \u0026#34;spocc\u0026#34;, \u0026#34;taxize\u0026#34;, \u0026#34;togeojson\u0026#34;, \u0026#34;treeBASE\u0026#34;) pkgs \u0026lt;- sort(pkgs) Define functions github_auth \u0026lt;- function(appname = getOption(\u0026#34;gh_appname\u0026#34;), key = getOption(\u0026#34;gh_id\u0026#34;), secret = getOption(\u0026#34;gh_secret\u0026#34;)) { if (is.null(getOption(\u0026#34;gh_token\u0026#34;))) { myapp \u0026lt;- oauth_app(appname, key, secret) token \u0026lt;- oauth2.0_token(oauth_endpoints(\u0026#34;github\u0026#34;), myapp) options(gh_token = token) } else { token \u0026lt;- getOption(\u0026#34;gh_token\u0026#34;) } return(token) } make_url \u0026lt;- function(x, y, z) { sprintf(\u0026#34;https://api.github.com/repos/%s/%s/%s\u0026#34;, x, y, z) } process_result \u0026lt;- function(x) { stop_for_status(x) if (!x$headers$`content-type` == \u0026#34;application/json; charset=utf-8\u0026#34;) stop(\u0026#34;content type mismatch\u0026#34;) tmp \u0026lt;- content(x, as = \u0026#34;text\u0026#34;) jsonlite::fromJSON(tmp, flatten = TRUE) } parse_file \u0026lt;- function(x) { tmp \u0026lt;- gsub(\u0026#34;\\n\\\\s+\u0026#34;, \u0026#34;\\n\u0026#34;, paste(vapply(strsplit(x, \u0026#34;\\n\u0026#34;)[[1]], RCurl::base64Decode, character(1), USE.NAMES = FALSE), collapse = \u0026#34; \u0026#34;)) lines \u0026lt;- readLines(textConnection(tmp)) vapply(lines, gsub, character(1), pattern = \u0026#34;\\\\s\u0026#34;, replacement = \u0026#34;\u0026#34;, USE.NAMES = FALSE) } request \u0026lt;- function(owner = \u0026#34;ropensci\u0026#34;, repo, file=\u0026#34;DESCRIPTION\u0026#34;, ...) { req \u0026lt;- GET(make_url(owner, repo, paste0(\u0026#34;contents/\u0026#34;, file)), config = c(token = github_auth(), ...)) if(req$status_code != 200) { NA } else { cts \u0026lt;- process_result(req)$content parse_file(cts) } } has_term \u0026lt;- function(what, ...) any(grepl(what, request(...))) has_file \u0026lt;- function(what, ...) if(all(is.na(request(file = what, ...)))) FALSE else TRUE Do stuff Does a package depend on a particular package? e.g., look for httr in the DESCRIPTION file (which is the default file name in request() above)\nhas_term(\u0026#34;httr\u0026#34;, repo=\u0026#34;taxize\u0026#34;) #\u0026gt; [1] TRUE has_term(\u0026#34;maptools\u0026#34;, repo=\u0026#34;taxize\u0026#34;) #\u0026gt; [1] FALSE Do a series of R packages have a file for how to contribute CONTRIBUTING.md?\nYes\nhas_file(\u0026#34;CONTRIBUTING.md\u0026#34;, repo=\u0026#34;taxize\u0026#34;) #\u0026gt; [1] TRUE Many packages\nvapply(pkgs, function(x) has_file(\u0026#34;CONTRIBUTING.md\u0026#34;, repo=x), logical(1)) #\u0026gt; alm bmc bold clifro ecoengine elastic #\u0026gt; FALSE FALSE FALSE FALSE FALSE FALSE #\u0026gt; fulltext geonames gistr RNeXML rnoaa rnpn #\u0026gt; TRUE FALSE FALSE TRUE TRUE FALSE #\u0026gt; rplos rsnps rWBclimate solr spocc taxize #\u0026gt; FALSE FALSE FALSE FALSE TRUE TRUE #\u0026gt; togeojson traits treeBASE #\u0026gt; FALSE FALSE FALSE Check rate limit Define function\nrate_limit \u0026lt;- function(...) { token \u0026lt;- github_auth() req \u0026lt;- GET(\u0026#34;https://api.github.com/rate_limit\u0026#34;, config = c(token = token, ...)) process_result(req) } Check it\nrate_limit() #\u0026gt; $resources #\u0026gt; $resources$core #\u0026gt; $resources$core$limit #\u0026gt; [1] 5000 #\u0026gt; #\u0026gt; $resources$core$remaining #\u0026gt; [1] 4925 #\u0026gt; #\u0026gt; $resources$core$reset #\u0026gt; [1] 1417031016 #\u0026gt; #\u0026gt; #\u0026gt; $resources$search #\u0026gt; $resources$search$limit #\u0026gt; [1] 30 #\u0026gt; #\u0026gt; $resources$search$remaining #\u0026gt; [1] 30 #\u0026gt; #\u0026gt; $resources$search$reset #\u0026gt; [1] 1417028069 #\u0026gt; #\u0026gt; #\u0026gt; #\u0026gt; $rate #\u0026gt; $rate$limit #\u0026gt; [1] 5000 #\u0026gt; #\u0026gt; $rate$remaining #\u0026gt; [1] 4925 #\u0026gt; #\u0026gt; $rate$reset #\u0026gt; [1] 1417031016 Convert time to reset to human readable form\nas.POSIXct(rate_limit()$rate$reset, origin=\u0026#34;1970-01-01\u0026#34;) #\u0026gt; [1] \u0026#34;2014-11-26 11:43:36 PST\u0026#34; ","permalink":"http://localhost:1313/2014/11/github-fun/","summary":"\u003cp\u003eRecently I\u0026rsquo;ve had fun playing with the GitHub API, and here are some notes to self about this fun having.\u003c/p\u003e\n\u003ch2 id=\"setup\"\u003eSetup\u003c/h2\u003e\n\u003cp\u003eGet/load packages\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall.packages\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;devtools\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;jsonlite\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;httr\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;yaml\u0026#39;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;devtools\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;httr\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yaml\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"define-a-vector-of-package-names\"\u003eDefine a vector of package names\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epkgs \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;alm\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;bmc\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;bold\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;clifro\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ecoengine\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;elastic\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;fulltext\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;geonames\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gistr\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;RNeXML\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rnoaa\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rnpn\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;traits\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rplos\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rsnps\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rWBclimate\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;solr\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;spocc\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;taxize\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;togeojson\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;treeBASE\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epkgs \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esort\u003c/span\u003e(pkgs)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"define-functions\"\u003eDefine functions\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egithub_auth \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(appname \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetOption\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gh_appname\u0026#34;\u003c/span\u003e), key \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetOption\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gh_id\u0026#34;\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        secret \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetOption\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gh_secret\u0026#34;\u003c/span\u003e)) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eis.null\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003egetOption\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gh_token\u0026#34;\u003c/span\u003e))) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    myapp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eoauth_app\u003c/span\u003e(appname, key, secret)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    token \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eoauth2.0_token\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eoauth_endpoints\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;github\u0026#34;\u003c/span\u003e), myapp)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003eoptions\u003c/span\u003e(gh_token \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e token)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    token \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetOption\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gh_token\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e(token)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emake_url \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x, y, z) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003esprintf\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://api.github.com/repos/%s/%s/%s\u0026#34;\u003c/span\u003e, x, y, z)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_result \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003estop_for_status\u003c/span\u003e(x)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#f92672\"\u003e!\u003c/span\u003ex\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eheaders\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003e`content-type` \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;application/json; charset=utf-8\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content type mismatch\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  tmp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econtent\u003c/span\u003e(x, as \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  jsonlite\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003efromJSON\u003c/span\u003e(tmp, flatten \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTRUE\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eparse_file \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  tmp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egsub\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\n\\\\s+\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\n\u0026#34;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e              \u003cspan style=\"color:#a6e22e\"\u003epaste\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003evapply\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003estrsplit\u003c/span\u003e(x, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\n\u0026#34;\u003c/span\u003e)[[1]], RCurl\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ebase64Decode,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           \u003cspan style=\"color:#a6e22e\"\u003echaracter\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e), USE.NAMES \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFALSE\u003c/span\u003e), collapse \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34; \u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  lines \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ereadLines\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003etextConnection\u003c/span\u003e(tmp))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003evapply\u003c/span\u003e(lines, gsub, \u003cspan style=\"color:#a6e22e\"\u003echaracter\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e), pattern \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\\\s\u0026#34;\u003c/span\u003e, replacement \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         USE.NAMES \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFALSE\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erequest \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(owner \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ropensci\u0026#34;\u003c/span\u003e, repo, file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;DESCRIPTION\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#66d9ef\"\u003e...\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  req \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eGET\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003emake_url\u003c/span\u003e(owner, repo, \u003cspan style=\"color:#a6e22e\"\u003epaste0\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;contents/\u0026#34;\u003c/span\u003e, file)), \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e             config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(token \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egithub_auth\u003c/span\u003e(), \u003cspan style=\"color:#66d9ef\"\u003e...\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e(req\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003estatus_code \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e200\u003c/span\u003e) { \u003cspan style=\"color:#66d9ef\"\u003eNA\u003c/span\u003e } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    cts \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eprocess_result\u003c/span\u003e(req)\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003econtent\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003eparse_file\u003c/span\u003e(cts)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehas_term \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(what, \u003cspan style=\"color:#66d9ef\"\u003e...\u003c/span\u003e) \u003cspan style=\"color:#a6e22e\"\u003eany\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003egrepl\u003c/span\u003e(what, \u003cspan style=\"color:#a6e22e\"\u003erequest\u003c/span\u003e(\u003cspan style=\"color:#66d9ef\"\u003e...\u003c/span\u003e)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehas_file \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(what, \u003cspan style=\"color:#66d9ef\"\u003e...\u003c/span\u003e) \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eall\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eis.na\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003erequest\u003c/span\u003e(file \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e what, \u003cspan style=\"color:#66d9ef\"\u003e...\u003c/span\u003e)))) \u003cspan style=\"color:#66d9ef\"\u003eFALSE\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTRUE\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"do-stuff\"\u003eDo stuff\u003c/h2\u003e\n\u003cp\u003eDoes a package depend on a particular package? e.g., look for \u003ccode\u003ehttr\u003c/code\u003e in the \u003ccode\u003eDESCRIPTION\u003c/code\u003e file (which is the default file name in \u003ccode\u003erequest()\u003c/code\u003e above)\u003c/p\u003e","title":"Fun with the GitHub API"},{"content":"I\u0026rsquo;ve reworked sofa recently after someone reported a bug in the package. Since the last post on this package on 2013-06-21, there\u0026rsquo;s a bunch of changes:\nRemoved the sofa_ prefix from all functions as it wasn\u0026rsquo;t really necessary. Replaced rjson/RJSONIO with jsonlite for JSON I/O. New functions: revisions() - to get the revision numbers for a document. uuids() - get any number of UUIDs - e.g., if you want to set document IDs with UUIDs Most functions that deal with documents are prefixed with doc_ Functions that deal with databases are prefixed with db_ Simplified all code, reducing duplication All functions take cushion as the first parameter, for consistency sake. Changed cushion() function so that you can only register one cushion with each function call, and the function takes parameters for each element now, name (name of the cushion, whatever you want), user (user name, if applicable), pwd (password, if applicable), type (one of localhost, cloudant, or iriscouch), and port (if applicable). Changed package license from CC0 to MIT There\u0026rsquo;s still more to do, but I\u0026rsquo;m pretty happy with the recent changes, and I hope at least some find the package useful. Also, would love people to try it out as all bugs are shallow and all that\u0026hellip;\nThe following are a few examples of package use.\nInstall CouchDB Instructions here\nStart CouchDB In your terminal\ncouchdb You can interact with your CouchDB databases as well in your browser. Navigate to http://localhost:5984/_utils\nInstall sofa install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;sckott/sofa\u0026#34;) library(\u0026#39;sofa\u0026#39;) Authenticate - Cushions As an example, here\u0026rsquo;s how I set up details for connecting to my Cloudant couch:\ncushion(name = \u0026#39;cloudant\u0026#39;, user = \u0026#39;\u0026lt;user name\u0026gt;\u0026#39;, pwd = \u0026#39;\u0026lt;password\u0026gt;\u0026#39;, type = \u0026#34;cloudant\u0026#34;) By default there is a built-in cushion for localhost so you don\u0026rsquo;t have to do that, unless you want to change those details, e.g., the port number. Right now cushions aren\u0026rsquo;t preserved across R sessions, but working on that.\nFor example, I\u0026rsquo;ll lay down a cushion for Cloudant, then I can call cushions() to see my cushions:\ncushion(name = \u0026#39;cloudant\u0026#39;, user = \u0026#39;\u0026lt;user name\u0026gt;\u0026#39;, pwd = \u0026#39;\u0026lt;pwd\u0026gt;\u0026#39;, type = \u0026#34;cloudant\u0026#34;) cushions() By default, if you don\u0026rsquo;t provide a cushion name, you are using localhost.\nPing the server ping() #\u0026gt; $couchdb #\u0026gt; [1] \u0026#34;Welcome\u0026#34; #\u0026gt; #\u0026gt; $uuid #\u0026gt; [1] \u0026#34;2c10f0c6d9bd17205b692ae93cd4cf1d\u0026#34; #\u0026gt; #\u0026gt; $version #\u0026gt; [1] \u0026#34;1.6.0\u0026#34; #\u0026gt; #\u0026gt; $vendor #\u0026gt; $vendor$version #\u0026gt; [1] \u0026#34;1.6.0-1\u0026#34; #\u0026gt; #\u0026gt; $vendor$name #\u0026gt; [1] \u0026#34;Homebrew\u0026#34; Nice, it\u0026rsquo;s working.\nCreate a new database, and list available databases db_create(dbname=\u0026#39;sofadb\u0026#39;) #\u0026gt; $ok #\u0026gt; [1] TRUE see if its there now\ndb_list() #\u0026gt; [1] \u0026#34;_replicator\u0026#34; \u0026#34;_users\u0026#34; \u0026#34;alm_couchdb\u0026#34; \u0026#34;cachecall\u0026#34; \u0026#34;hello_earth\u0026#34; #\u0026gt; [6] \u0026#34;leothelion\u0026#34; \u0026#34;mran\u0026#34; \u0026#34;mydb\u0026#34; \u0026#34;newdbs\u0026#34; \u0026#34;sofadb\u0026#34; Create documents Create a document WITH a name (uses PUT)\ndoc1 \u0026lt;- \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;sofa\u0026#34;,\u0026#34;beer\u0026#34;:\u0026#34;IPA\u0026#34;}\u0026#39; doc_create(dbname=\u0026#34;sofadb\u0026#34;, doc=doc1, docid=\u0026#34;a_beer\u0026#34;) #\u0026gt; $ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $id #\u0026gt; [1] \u0026#34;a_beer\u0026#34; #\u0026gt; #\u0026gt; $rev #\u0026gt; [1] \u0026#34;1-a48c98c945bcc05d482bc6f938c89882\u0026#34; Create a document WITHOUT a name (uses POST)\ndoc2 \u0026lt;- \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;sofa\u0026#34;,\u0026#34;icecream\u0026#34;:\u0026#34;rocky road\u0026#34;}\u0026#39; doc_create(dbname=\u0026#34;sofadb\u0026#34;, doc=doc2) #\u0026gt; $ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $id #\u0026gt; [1] \u0026#34;c5c5c332c25cf62cc584647a81006f6d\u0026#34; #\u0026gt; #\u0026gt; $rev #\u0026gt; [1] \u0026#34;1-fd0da7fcb8d3afbfc5757d065c92362c\u0026#34; List documents List them\nalldocs(dbname=\u0026#34;sofadb\u0026#34;) #\u0026gt; id key #\u0026gt; 1 a_beer a_beer #\u0026gt; 2 c5c5c332c25cf62cc584647a81006f6d c5c5c332c25cf62cc584647a81006f6d #\u0026gt; rev #\u0026gt; 1 1-a48c98c945bcc05d482bc6f938c89882 #\u0026gt; 2 1-fd0da7fcb8d3afbfc5757d065c92362c Optionally include the documents, returned as a list by default as it would be hard to parse an endless number of document formats.\nalldocs(dbname=\u0026#34;sofadb\u0026#34;, include_docs = TRUE) #\u0026gt; $total_rows #\u0026gt; [1] 2 #\u0026gt; #\u0026gt; $offset #\u0026gt; [1] 0 #\u0026gt; #\u0026gt; $rows #\u0026gt; $rows[[1]] #\u0026gt; $rows[[1]]$id #\u0026gt; [1] \u0026#34;a_beer\u0026#34; #\u0026gt; #\u0026gt; $rows[[1]]$key #\u0026gt; [1] \u0026#34;a_beer\u0026#34; #\u0026gt; #\u0026gt; $rows[[1]]$value #\u0026gt; $rows[[1]]$value$rev #\u0026gt; [1] \u0026#34;1-a48c98c945bcc05d482bc6f938c89882\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $rows[[1]]$doc #\u0026gt; $rows[[1]]$doc$`_id` #\u0026gt; [1] \u0026#34;a_beer\u0026#34; #\u0026gt; #\u0026gt; $rows[[1]]$doc$`_rev` #\u0026gt; [1] \u0026#34;1-a48c98c945bcc05d482bc6f938c89882\u0026#34; #\u0026gt; #\u0026gt; $rows[[1]]$doc$name #\u0026gt; [1] \u0026#34;sofa\u0026#34; #\u0026gt; #\u0026gt; $rows[[1]]$doc$beer #\u0026gt; [1] \u0026#34;IPA\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; #\u0026gt; $rows[[2]] #\u0026gt; $rows[[2]]$id #\u0026gt; [1] \u0026#34;c5c5c332c25cf62cc584647a81006f6d\u0026#34; #\u0026gt; #\u0026gt; $rows[[2]]$key #\u0026gt; [1] \u0026#34;c5c5c332c25cf62cc584647a81006f6d\u0026#34; #\u0026gt; #\u0026gt; $rows[[2]]$value #\u0026gt; $rows[[2]]$value$rev #\u0026gt; [1] \u0026#34;1-fd0da7fcb8d3afbfc5757d065c92362c\u0026#34; #\u0026gt; #\u0026gt; #\u0026gt; $rows[[2]]$doc #\u0026gt; $rows[[2]]$doc$`_id` #\u0026gt; [1] \u0026#34;c5c5c332c25cf62cc584647a81006f6d\u0026#34; #\u0026gt; #\u0026gt; $rows[[2]]$doc$`_rev` #\u0026gt; [1] \u0026#34;1-fd0da7fcb8d3afbfc5757d065c92362c\u0026#34; #\u0026gt; #\u0026gt; $rows[[2]]$doc$name #\u0026gt; [1] \u0026#34;sofa\u0026#34; #\u0026gt; #\u0026gt; $rows[[2]]$doc$icecream #\u0026gt; [1] \u0026#34;rocky road\u0026#34; Update a document Change IPA (india pale ale) to IPL (india pale lager). We need to get revisions first as we need to include revision number when we update a document.\n(revs \u0026lt;- revisions(dbname = \u0026#34;sofadb\u0026#34;, docid = \u0026#34;a_beer\u0026#34;)) #\u0026gt; [1] \u0026#34;1-a48c98c945bcc05d482bc6f938c89882\u0026#34; newdoc \u0026lt;- \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;sofa\u0026#34;,\u0026#34;beer\u0026#34;:\u0026#34;IPL\u0026#34;}\u0026#39; doc_update(dbname = \u0026#34;sofadb\u0026#34;, doc = newdoc, docid = \u0026#34;a_beer\u0026#34;, rev = revs[1]) #\u0026gt; $ok #\u0026gt; [1] TRUE #\u0026gt; #\u0026gt; $id #\u0026gt; [1] \u0026#34;a_beer\u0026#34; #\u0026gt; #\u0026gt; $rev #\u0026gt; [1] \u0026#34;2-f2390eb18b8f9a870c915c6712a7f65e\u0026#34; Should be two revisions now\nrevisions(dbname = \u0026#34;sofadb\u0026#34;, docid = \u0026#34;a_beer\u0026#34;) #\u0026gt; [1] \u0026#34;2-f2390eb18b8f9a870c915c6712a7f65e\u0026#34; #\u0026gt; [2] \u0026#34;1-a48c98c945bcc05d482bc6f938c89882\u0026#34; Get headers for a document doc_head(dbname = \u0026#34;sofadb\u0026#34;, docid = \u0026#34;a_beer\u0026#34;) #\u0026gt; [[1]] #\u0026gt; [[1]]$status #\u0026gt; [1] 200 #\u0026gt; #\u0026gt; [[1]]$version #\u0026gt; [1] \u0026#34;HTTP/1.1\u0026#34; #\u0026gt; #\u0026gt; [[1]]$headers #\u0026gt; $server #\u0026gt; [1] \u0026#34;CouchDB/1.6.0 (Erlang OTP/17)\u0026#34; #\u0026gt; #\u0026gt; $etag #\u0026gt; [1] \u0026#34;\\\u0026#34;2-f2390eb18b8f9a870c915c6712a7f65e\\\u0026#34;\u0026#34; #\u0026gt; #\u0026gt; $date #\u0026gt; [1] \u0026#34;Tue, 18 Nov 2014 21:19:16 GMT\u0026#34; #\u0026gt; #\u0026gt; $`content-type` #\u0026gt; [1] \u0026#34;application/json\u0026#34; #\u0026gt; #\u0026gt; $`content-length` #\u0026gt; [1] \u0026#34;88\u0026#34; #\u0026gt; #\u0026gt; $`cache-control` #\u0026gt; [1] \u0026#34;must-revalidate\u0026#34; #\u0026gt; #\u0026gt; attr(,\u0026#34;class\u0026#34;) #\u0026gt; [1] \u0026#34;insensitive\u0026#34; \u0026#34;list\u0026#34; JSON vs. list Across all/most functions you can request json or list as output with the as parameter.\ndb_list(as = \u0026#34;list\u0026#34;) #\u0026gt; [1] \u0026#34;_replicator\u0026#34; \u0026#34;_users\u0026#34; \u0026#34;alm_couchdb\u0026#34; \u0026#34;cachecall\u0026#34; \u0026#34;hello_earth\u0026#34; #\u0026gt; [6] \u0026#34;leothelion\u0026#34; \u0026#34;mran\u0026#34; \u0026#34;mydb\u0026#34; \u0026#34;newdbs\u0026#34; \u0026#34;sofadb\u0026#34; db_list(as = \u0026#34;json\u0026#34;) #\u0026gt; [1] \u0026#34;[\\\u0026#34;_replicator\\\u0026#34;,\\\u0026#34;_users\\\u0026#34;,\\\u0026#34;alm_couchdb\\\u0026#34;,\\\u0026#34;cachecall\\\u0026#34;,\\\u0026#34;hello_earth\\\u0026#34;,\\\u0026#34;leothelion\\\u0026#34;,\\\u0026#34;mran\\\u0026#34;,\\\u0026#34;mydb\\\u0026#34;,\\\u0026#34;newdbs\\\u0026#34;,\\\u0026#34;sofadb\\\u0026#34;]\\n\u0026#34; Curl options Across all functions you can pass in curl options. We\u0026rsquo;re using httr internally, so you can use httr helper functions to make some curl options easier. Examples:\nVerbose output\nlibrary(\u0026#34;httr\u0026#34;) db_list(config=verbose()) #\u0026gt; [1] \u0026#34;_replicator\u0026#34; \u0026#34;_users\u0026#34; \u0026#34;alm_couchdb\u0026#34; \u0026#34;cachecall\u0026#34; \u0026#34;hello_earth\u0026#34; #\u0026gt; [6] \u0026#34;leothelion\u0026#34; \u0026#34;mran\u0026#34; \u0026#34;mydb\u0026#34; \u0026#34;newdbs\u0026#34; \u0026#34;sofadb\u0026#34; Progress\ndb_list(config=progress()) #\u0026gt; | | | 0% | |=================================================================| 100% #\u0026gt; [1] \u0026#34;_replicator\u0026#34; \u0026#34;_users\u0026#34; \u0026#34;alm_couchdb\u0026#34; \u0026#34;cachecall\u0026#34; \u0026#34;hello_earth\u0026#34; #\u0026gt; [6] \u0026#34;leothelion\u0026#34; \u0026#34;mran\u0026#34; \u0026#34;mydb\u0026#34; \u0026#34;newdbs\u0026#34; \u0026#34;sofadb\u0026#34; Set a timeout\ndb_list(config=timeout(seconds = 0.001)) #\u0026gt; #\u0026gt; Error in function (type, msg, asError = TRUE) : #\u0026gt; Operation timed out after 3 milliseconds with 0 out of -1 bytes received Full text search I\u0026rsquo;m working on an R client for Elaticsearch called elastic - find it at https://github.com/ropensci/elastic\nThinking about where to include functions to allow elastic and sofa to work together\u0026hellip;if you have any thoughts hit up the issues. I\u0026rsquo;ll probably include helper functions for CouchDB search in the elastic package, interfacing with the CouchDB plugin for Elasticsearch.\n","permalink":"http://localhost:1313/2014/11/sofa/","summary":"\u003cp\u003eI\u0026rsquo;ve reworked \u003ccode\u003esofa\u003c/code\u003e recently after someone reported a bug in the package. Since the last post on this package on 2013-06-21, there\u0026rsquo;s a bunch of changes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRemoved the \u003ccode\u003esofa_\u003c/code\u003e prefix from all functions as it wasn\u0026rsquo;t really necessary.\u003c/li\u003e\n\u003cli\u003eReplaced \u003ccode\u003erjson\u003c/code\u003e/\u003ccode\u003eRJSONIO\u003c/code\u003e with \u003ccode\u003ejsonlite\u003c/code\u003e for JSON I/O.\u003c/li\u003e\n\u003cli\u003eNew functions:\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erevisions()\u003c/code\u003e - to get the revision numbers for a document.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003euuids()\u003c/code\u003e - get any number of UUIDs - e.g., if you want to set document IDs with UUIDs\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMost functions that deal with documents are prefixed with \u003ccode\u003edoc_\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFunctions that deal with databases are prefixed with \u003ccode\u003edb_\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSimplified all code, reducing duplication\u003c/li\u003e\n\u003cli\u003eAll functions take \u003ccode\u003ecushion\u003c/code\u003e as the first parameter, for consistency sake.\u003c/li\u003e\n\u003cli\u003eChanged \u003ccode\u003ecushion()\u003c/code\u003e function so that you can only register one cushion with each function call,\nand the function takes parameters for each element now, \u003ccode\u003ename\u003c/code\u003e (name of the cushion, whatever you want), \u003ccode\u003euser\u003c/code\u003e (user name, if applicable), \u003ccode\u003epwd\u003c/code\u003e (password, if applicable), \u003ccode\u003etype\u003c/code\u003e (one of localhost, cloudant, or iriscouch), and \u003ccode\u003eport\u003c/code\u003e (if applicable).\u003c/li\u003e\n\u003cli\u003eChanged package license from \u003ccode\u003eCC0\u003c/code\u003e to \u003ccode\u003eMIT\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere\u0026rsquo;s still more to do, but I\u0026rsquo;m pretty happy with the recent changes, and I hope at least some find the package useful. Also, would love people to try it out as all bugs are shallow and all that\u0026hellip;\u003c/p\u003e","title":"sofa - reboot"},{"content":"The paper One paper from my graduate work asked most generally ~ \u0026ldquo;How much does the variation in magnitudes and signs of species interaction outcomes vary?\u0026rdquo;. More specifically, we wanted to know if variation differed among species interaction classes (mutualism, competition, predation), and among various \u0026ldquo;gradients\u0026rdquo; (space, time, etc.). To answer this question, we used a meta-analysis approach (rather than e.g., a field experiment). We published the paper recently.\np.s. I really really wish we would have put it in an open access journal\u0026hellip;\nThe data Anyway, I\u0026rsquo;m here to talk about the data. We didn\u0026rsquo;t get the data up with the paper, but it is up on Figshare now. The files there are the following:\nconiditionality.R - script used to process the data from variables_prelim.csv variables_prelim.csv - description of variables in the preliminary data set, matches conditionality_data_prelim.csv variables_used.csv - description of variables in the used data set, matches conditionality_data_used.csv conditionality_data_prelim.csv - preliminary data, the raw data conditionality_data_used.csv - the data used for our paper README.md - the readme paper_selection.csv - the list of papers we went through, with remarks about paper selection Please do play with the data, publish some papers, etc, etc. It took 6 of us about 4 years to collect this data; we skimmed through ~11,000 papers on the first pass (aka. skimming through abstracts in Google Scholar and Web of Science), then decided on nearly 500 papers to get data from, and narrowed down to 247 papers for the publication mentioned above. Now, there was no funding for this, so it was sort of done in between other projects, but still, it was simply A LOT of tables to digitize, and graphs to extract data points from. Anyway, hopefully you will find this data useful :p\nEML I think this dataset would be a great introduction to the potential power of EML (Ecological Metadata Langauge). At rOpenSci, one of our team Carl Boettiger, along with Claas-Thido Pfaff, Duncan Temple Lang, Karthik Ram, and Matt Jones, have created an R client for EML, to parse EML files and to create and publish them.\nWhat is EML?/Why EML? A demonstration is in order\u0026hellip;\nExample using EML with this dataset Install EML library(\u0026#34;devtools\u0026#34;) install.packages(\u0026#34;RHTMLForms\u0026#34;, repos = \u0026#34;http://www.omegahat.org/R/\u0026#34;, type=\u0026#34;source\u0026#34;) install_github(\u0026#34;ropensci/EML\u0026#34;, build=FALSE, dependencies=c(\u0026#34;DEPENDS\u0026#34;, \u0026#34;IMPORTS\u0026#34;)) Load EML\nlibrary(\u0026#39;EML\u0026#39;) Prepare metadata # dataset prelim_dat \u0026lt;- read.csv(\u0026#34;conditionality_data_prelim.csv\u0026#34;) # variable descriptions for each column prelim_vars \u0026lt;- read.csv(\u0026#34;variables_prelim.csv\u0026#34;, stringsAsFactors = FALSE) Get column definitions in a vector\ncol_defs \u0026lt;- prelim_vars$description Create unit definitions for each column\nunit_defs \u0026lt;- list( c(unit = \u0026#34;number\u0026#34;, bounds = c(0, Inf)), c(unit = \u0026#34;number\u0026#34;, bounds = c(0, Inf)), \u0026#34;independent replicates\u0026#34;, c(unit = \u0026#34;number\u0026#34;, bounds = c(0, Inf)), ... \u0026lt;CUTOFF\u0026gt; ) Write an EML file eml_write(prelim_dat, unit.defs = unit_defs, col.defs = col_defs, creator = \u0026#34;Scott Chamberlain\u0026#34;, contact = \u0026#34;myrmecocystus@gmail.com\u0026#34;, file = \u0026#34;conditionality_data_prelim_eml.xml\u0026#34;) ## [1] \u0026#34;conditionality_data_prelim_eml.xml\u0026#34; Validate the EML file eml_validate(\u0026#34;conditionality_data_prelim_eml.xml\u0026#34;) ## EML specific tests XML specific tests ## TRUE TRUE Read data and metadata gg \u0026lt;- eml_read(\u0026#34;conditionality_data_prelim_eml.xml\u0026#34;) eml_get(gg, \u0026#34;contact\u0026#34;) ## [1] \u0026#34;myrmecocystus@gmail.com\u0026#34; eml_get(gg, \u0026#34;citation_info\u0026#34;) ## Chamberlain S (2014-10-06). _metadata_. dat \u0026lt;- eml_get(gg, \u0026#34;data.frame\u0026#34;) head(dat[,c(1:10)]) ## order i indrep avg author_last finit_1 finit_2 finit_abv co_author ## 1 1 1 a 1 Devall margaret s ms Thein ## 2 2 1 a 2 Devall margaret s ms Thein ## 3 3 1 a 3 Devall margaret s ms Thein ## 4 4 1 a 4 Devall margaret s ms Thein ## 5 5 1 a 5 Devall margaret s ms Thein ## 6 6 1 a 6 Devall margaret s ms Thein ## sinit_1 ## 1 leonard ## 2 leonard ## 3 leonard ## 4 leonard ## 5 leonard ## 6 leonard Publish We can also use the EML package to publish the data, here to Figshare.\nFirst, install rfigshare\ninstall.packages(\u0026#34;rfigshare\u0026#34;) library(\u0026#39;rfigshare\u0026#39;) Then publish using eml_publish()\nfigid \u0026lt;- eml_publish( file = \u0026#34;conditionality_data_prelim_eml.xml\u0026#34;, description = \u0026#34;EML file for Chamberlain, S.A., J.A. Rudgers, and J.L. Bronstein. 2014. How context-dependent are species interactions. Ecology Letters\u0026#34;, categories = \u0026#34;Ecology\u0026#34;, tags = \u0026#34;EML\u0026#34;, destination = \u0026#34;figshare\u0026#34;, visibility = \u0026#34;public\u0026#34;, title = \u0026#34;condionality data, EML\u0026#34;) fs_make_public(figid) ","permalink":"http://localhost:1313/2014/10/conditionality-meta-analysis/","summary":"\u003ch2 id=\"the-paper\"\u003eThe paper\u003c/h2\u003e\n\u003cp\u003eOne paper from my graduate work asked most generally ~ \u0026ldquo;How much does the variation in magnitudes and signs of species interaction outcomes vary?\u0026rdquo;. More specifically, we wanted to know if variation differed among species interaction classes (mutualism, competition, predation), and among various \u0026ldquo;gradients\u0026rdquo; (space, time, etc.). To answer this question, we used a meta-analysis approach (rather than e.g., a field experiment). We \u003ca href=\"https://scottchamberlain.info/publications/\"\u003epublished the paper\u003c/a\u003e recently.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ep.s. I really really wish we would have put it in an open access journal\u0026hellip;\u003c/p\u003e","title":"Conditionality meta-analysis data"},{"content":"My last blog post on this package was so long ago the package wrapped both New York Times APIs and Sunlight Labs APIs and the package was called govdat. I split that package up into rsunlight for Sunlight Labs APIs and rtimes for some New York Times APIs. rtimes is in development at Github.\nWe\u0026rsquo;ve updated the package to include four sets of functions, one set for each of four Sunlight Labs APIs (with a separate prefix for each API):\nCongress API (cg_) Open States API (os_) Capitol Words API (cw_) Influence Explorer API (ie_) Then there are many methods for each API.\nrsunlight intro Installation First, installation\ndevtools::install_github(\u0026#34;ropengov/rsunlight\u0026#34;) Load the library\nlibrary(\u0026#34;rsunlight\u0026#34;) Congress API Search for Fed level bills that include the term health care in them.\nres \u0026lt;- cg_bills(query=\u0026#39;health care\u0026#39;) head(res$results[,1:4]) ## nicknames congress last_version_on sponsor_id ## 1 obamacare 111 2010-08-25 S000749 ## 2 obamacare, ppaca 111 2010-08-25 R000053 ## 3 NULL 113 2013-10-09 K000220 ## 4 NULL 111 2009-01-06 I000056 ## 5 NULL 112 2011-01-05 I000056 ## 6 NULL 111 2009-05-05 D000197 Search for bills that have the two terms transparency and accountability within 5 words of each other in the bill.\nres \u0026lt;- cg_bills(query=\u0026#39;transparency accountability\u0026#39;~5) head(res$results[,1:4]) ## congress last_version_on sponsor_id ## 1 111 2009-01-15 R000435 ## 2 113 2013-07-17 R000595 ## 3 112 2011-12-08 R000435 ## 4 113 2013-09-19 R000435 ## 5 112 2011-11-10 R000595 ## 6 113 2013-07-23 C000560 ## urls.govtrack ## 1 http://www.govtrack.us/congress/bills/111/hr557 ## 2 https://www.govtrack.us/congress/bills/113/s1313 ## 3 http://www.govtrack.us/congress/bills/112/hr2829 ## 4 https://www.govtrack.us/congress/bills/113/hr3155 ## 5 http://www.govtrack.us/congress/bills/112/s1848 ## 6 https://www.govtrack.us/congress/bills/113/s1347 ## urls.opencongress ## 1 http://www.opencongress.org/bill/111-h557/show ## 2 http://www.opencongress.org/bill/s1313-113 ## 3 http://www.opencongress.org/bill/112-h2829/show ## 4 http://www.opencongress.org/bill/hr3155-113 ## 5 http://www.opencongress.org/bill/112-s1848/show ## 6 http://www.opencongress.org/bill/s1347-113 ## urls.congress ## 1 http://beta.congress.gov/bill/111th/house-bill/557 ## 2 http://beta.congress.gov/bill/113th/senate-bill/1313 ## 3 http://beta.congress.gov/bill/112th/house-bill/2829 ## 4 http://beta.congress.gov/bill/113th/house-bill/3155 ## 5 http://beta.congress.gov/bill/112th/senate-bill/1848 ## 6 http://beta.congress.gov/bill/113th/senate-bill/1347 Open States API Search State Bills, in this case search for the term agriculture in Texas.\nres \u0026lt;- os_billsearch(terms = \u0026#39;agriculture\u0026#39;, state = \u0026#39;tx\u0026#39;) head(res) ## title ## 1 Relating to authorizing the issuance of revenue bonds to fund capital projects at public institutions of higher education; making an appropriation. ## 2 Relating to authorizing the issuance of revenue bonds to fund capital projects at public institutions of higher education. ## 3 Relating to authorizing the issuance of revenue bonds to fund capital projects at public institutions of higher education. ## 4 Relating to authorizing the issuance of revenue bonds to fund capital projects at public institutions of higher education. ## 5 Relating to authorizing the issuance of revenue bonds to fund capital projects at public institutions of higher education; making an appropriation. ## 6 Relating to access to certain facilities by search and rescue dogs and their handlers; providing a criminal penalty. ## created_at updated_at id chamber state ## 1 2013-08-01 03:33:40 2013-08-07 03:10:10 TXB00034894 upper tx ## 2 2013-08-01 03:33:38 2013-08-02 03:20:14 TXB00034893 upper tx ## 3 2013-07-21 03:03:53 2013-07-28 03:28:30 TXB00034814 upper tx ## 4 2013-07-03 02:44:03 2013-07-14 03:00:31 TXB00034514 upper tx ## 5 2013-06-16 03:48:13 2013-06-23 04:02:49 TXB00033988 upper tx ## 6 2013-03-03 04:47:26 2013-07-01 21:25:36 TXB00027556 upper tx ## session type ## 1 833 bill ## 2 833 bill ## 3 832 bill ## 4 832 bill ## 5 831 bill ## 6 83 bill ## subjects ## 1 Commerce, Education, Budget, Spending, and Taxes ## 2 Commerce, Education, Budget, Spending, and Taxes ## 3 Commerce, Education, Budget, Spending, and Taxes ## 4 Commerce, Education, Budget, Spending, and Taxes ## 5 Commerce, Education, Budget, Spending, and Taxes ## 6 Commerce, Business and Consumers, Animal Rights and Wildlife Issues, Health, Crime ## bill_id ## 1 SB 3 ## 2 SB 10 ## 3 SB 40 ## 4 SB 6 ## 5 SB 44 ## 6 SB 1010 Search for legislators in California (ca) and in the democratic party\nres \u0026lt;- os_legislatorsearch(state = \u0026#39;ca\u0026#39;, party = \u0026#39;democratic\u0026#39;, fields = c(\u0026#39;full_name\u0026#39;,\u0026#39;+capitol_office.phone\u0026#39;)) head(res) ## phone id full_name ## 1 (916) 319-2014 CAL000058 Nancy Skinner ## 2 (916) 319-2015 CAL000059 Joan Buchanan ## 3 (916) 319-2022 CAL000084 Paul Fong ## 4 (916) 319-2046 CAL000089 John Pérez ## 5 (916) 319-2080 CAL000098 V. Manuel Pérez ## 6 (916) 319-2001 CAL000101 Wesley Chesbro Now you can call each representative, yay!\nCapitol Words API Search for phrase climate change used by politicians between September 5th and 16th, 2011:\nhead(cw_text(phrase=\u0026#39;climate change\u0026#39;, start_date=\u0026#39;2011-09-05\u0026#39;, end_date=\u0026#39;2011-09-16\u0026#39;, party=\u0026#39;D\u0026#39;)[,c(\u0026#39;speaker_last\u0026#39;,\u0026#39;origin_url\u0026#39;)]) ## speaker_last ## 1 Tsongas ## 2 Inslee ## 3 Costa ## 4 Boxer ## 5 Durbin ## 6 Boxer ## origin_url ## 1 http://origin.www.gpo.gov/fdsys/pkg/CREC-2011-09-14/html/CREC-2011-09-14-pt1-PgH6149-5.htm ## 2 http://origin.www.gpo.gov/fdsys/pkg/CREC-2011-09-15/html/CREC-2011-09-15-pt1-PgH6186.htm ## 3 http://origin.www.gpo.gov/fdsys/pkg/CREC-2011-09-13/html/CREC-2011-09-13-pt1-PgE1609-2.htm ## 4 http://origin.www.gpo.gov/fdsys/pkg/CREC-2011-09-15/html/CREC-2011-09-15-pt1-PgS5650.htm ## 5 http://origin.www.gpo.gov/fdsys/pkg/CREC-2011-09-13/html/CREC-2011-09-13-pt1-PgS5510.htm ## 6 http://origin.www.gpo.gov/fdsys/pkg/CREC-2011-09-13/html/CREC-2011-09-13-pt1-PgS5513-2.htm Plot mentions of the term climate change over time for Democrats vs. Republicans\nlibrary(\u0026#39;ggplot2\u0026#39;) dat_d \u0026lt;- cw_timeseries(phrase=\u0026#39;climate change\u0026#39;, party=\u0026#34;D\u0026#34;) dat_d$party \u0026lt;- rep(\u0026#34;D\u0026#34;, nrow(dat_d)) dat_r \u0026lt;- cw_timeseries(phrase=\u0026#39;climate change\u0026#39;, party=\u0026#34;R\u0026#34;) dat_r$party \u0026lt;- rep(\u0026#34;R\u0026#34;, nrow(dat_r)) dat_both \u0026lt;- rbind(dat_d, dat_r) ggplot(dat_both, aes(day, count, colour=party)) + geom_line() + theme_grey(base_size=20) + scale_colour_manual(values=c(\u0026#34;blue\u0026#34;,\u0026#34;red\u0026#34;)) Influence Explorer API Search for contributions of equal to or more than $20,000,000.\nie_contr(amount=\u0026#39;\u0026gt;|20000000\u0026#39;)[,c(\u0026#39;amount\u0026#39;,\u0026#39;recipient_name\u0026#39;,\u0026#39;contributor_name\u0026#39;)] ## amount ## 1 25177212.00 ## 2 20000000.00 ## 3 20000000.00 ## 4 20000000.00 ## 5 20000000.00 ## 6 20000000.00 ## 7 50000000.00 ## 8 34000000.00 ## 9 28000000.00 ## 10 20000000.00 ## recipient_name ## 1 Republican National Cmte ## 2 CALIFORNIANS TO CLOSE THE OUT-OF-STATE CORPORATE TAX LOOPHOLE ## 3 WHITMAN, MEG ## 4 WHITMAN, MEG ## 5 WHITMAN, MEG ## 6 WHITMAN, MEG ## 7 GOLISANO, B THOMAS (G) ## 8 GOLISANO, B THOMAS (G) ## 9 GOLISANO, B THOMAS (G) ## 10 GOLISANO, B THOMAS (G) ## contributor_name ## 1 Romney Victory ## 2 STEYER, THOMAS F ## 3 WHITMAN, MARGARET (MEG) ## 4 WHITMAN, MARGARET (MEG) ## 5 WHITMAN, MARGARET (MEG) ## 6 WHITMAN, MARGARET (MEG) ## 7 GOLISANO, B THOMAS ## 8 GOLISANO, B THOMAS ## 9 GOLISANO, B THOMAS ## 10 GOLISANO, B THOMAS Top industries, by contributions given. UNKOWN is a very influential industry. Of course law firms are high up there, as well as real estate. I\u0026rsquo;m sure oil and gas is embarrased that they\u0026rsquo;re contributing less than pulic sector unions.\n(res \u0026lt;- ie_industries(method=\u0026#39;top_ind\u0026#39;, limit=10)) ## count amount id ## 1 14919818 3825359507.21 cdb3f500a3f74179bb4a5eb8b2932fa6 ## 2 3600761 2787678962.95 f50cf984a2e3477c8167d32e2b14e052 ## 3 329906 1717649914.58 9cac88377c3b400e89c2d6762e3f28f6 ## 4 1386613 1707457092.04 7500030dffe24844aa467a75f7aedfd1 ## 5 774496 1563637586.57 0af3f418f426497e8bbf916bfc074ebc ## 6 546367 1389220855.35 52e5d4c6c0fa47c3bdb199a28f96d434 ## 7 2134350 1384221307.53 a05a0d06f6814b31bece35a81fcb40c7 ## 8 1003850 986588892.83 8ada0fc2d6994f2ab06c7e025dff2284 ## 9 567082 775241387.17 52766c4910a846f2813a1dda212b7027 ## 10 151006 706747646.35 13718be68388456d9b6e8db753f06e72 ## should_show_entity name ## 1 TRUE UNKNOWN ## 2 TRUE LAWYERS/LAW FIRMS ## 3 TRUE CANDIDATE SELF-FINANCE ## 4 TRUE REAL ESTATE ## 5 TRUE SECURITIES \u0026amp; INVESTMENT ## 6 TRUE PUBLIC SECTOR UNIONS ## 7 TRUE HEALTH PROFESSIONALS ## 8 TRUE INSURANCE ## 9 TRUE OIL \u0026amp; GAS ## 10 TRUE CASINOS/GAMBLING res$amount \u0026lt;- as.numeric(res$amount) ggplot(res, aes(reorder(name, amount), amount)) + geom_bar(stat = \u0026#34;identity\u0026#34;) + coord_flip() + scale_y_continuous(labels=dollar) + theme_grey(base_size = 14) Feedback Please do use rsunlight, and let us know what you want fixed, new features, etc.\nStill to come: Functions to visualize data from each API. You can do this yourself, but a few functions will be created to help those that are new to R. Vectorize functions so that you can give many inputs to a function instead of a single input. test suite: embarrasingly, there is no test suite yet, boo me. I plan to push rsunlight to CRAN soon as v0.3 ","permalink":"http://localhost:1313/2014/08/rsunlight/","summary":"\u003cp\u003eMy \u003ca href=\"http://recology.info/2014/05/rsunlight/\"\u003elast blog post on this package\u003c/a\u003e was so long ago the package wrapped both New York Times APIs and Sunlight Labs APIs and the package was called \u003ccode\u003egovdat\u003c/code\u003e. I split that package up into \u003ccode\u003ersunlight\u003c/code\u003e for Sunlight Labs APIs and \u003ccode\u003ertimes\u003c/code\u003e for some New York Times APIs. \u003ccode\u003ertimes\u003c/code\u003e is \u003ca href=\"https://github.com/ropengov/rtimes\"\u003ein development at Github\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ve updated the package to include four sets of functions, one set for each of four Sunlight Labs APIs (with a separate prefix for each API):\u003c/p\u003e","title":"rsunlight - R client for Sunlight Labs APIs"},{"content":"My last blog post introduced the R package I\u0026rsquo;m working on analogsea, an R client for the Digital Ocean API.\nThings have changed a bit, including fillig out more functions for all API endpoints, and incorparting feedback from Hadley and Karthik. The package is as v0.1 now, so I thought I\u0026rsquo;d say a few things about how it works.\nNote that Digital Ocean\u0026rsquo;s v2 API is in beta stage now, so the current version of analogsea at v0.1 works with their v1 API. The v2 branch of analogsea is being developed for their v2 API.\nIf you sign up for an account with Digital Ocean use this referral link: https://www.digitalocean.com/?refcode=0740f5169634 so I can earn some credits. thx :)\nFirst, installation\nNote: I did try to submit to CRAN, but Ripley complained about the package name so I\u0026rsquo;d rather not waste my time esp since people using this likely will already know about install_github().\ndevtools::install_github(\u0026#34;sckott/analogsea\u0026#34;) Load the library\nlibrary(\u0026#34;analogsea\u0026#34;) ## Loading required package: magrittr Authenticate has changed a bit. Whereas auth details were stored as environment variables before, I\u0026rsquo;m just using R\u0026rsquo;s options. do_auth() will ask for your Digital Ocean details. You can enter them each R session, or store them in your .Rprofile file. After successful authentication, each function simply looks for your auth details with getOption(). You don\u0026rsquo;t have to use this function first, though if you don\u0026rsquo;t your first call to another function will ask for auth details.\ndo_auth() sizes, images, and keys functions have changed a bit, by default outputting a data.frame now.\nList available regions\nregions() ## id name slug ## 1 3 San Francisco 1 sfo1 ## 2 4 New York 2 nyc2 ## 3 5 Amsterdam 2 ams2 ## 4 6 Singapore 1 sgp1 List available sizes\nsizes() ## id name slug memory cpu disk cost_per_hour cost_per_month ## 1 66 512MB 512mb 512 1 20 0.00744 5.0 ## 2 63 1GB 1gb 1024 1 30 0.01488 10.0 ## 3 62 2GB 2gb 2048 2 40 0.02976 20.0 ## 4 64 4GB 4gb 4096 2 60 0.05952 40.0 ## 5 65 8GB 8gb 8192 4 80 0.11905 80.0 ## 6 61 16GB 16gb 16384 8 160 0.23810 160.0 ## 7 60 32GB 32gb 32768 12 320 0.47619 320.0 ## 8 70 48GB 48gb 49152 16 480 0.71429 480.0 ## 9 69 64GB 64gb 65536 20 640 0.95238 640.0 List available images\nhead(images()) ## id name slug distribution public sfo1 ## 1 3209452 rstudioserverssh_snap \u0026lt;NA\u0026gt; Ubuntu FALSE 1 ## 2 1601 CentOS 5.8 x64 centos-5-8-x64 CentOS TRUE 1 ## 3 1602 CentOS 5.8 x32 centos-5-8-x32 CentOS TRUE 1 ## 4 12573 Debian 6.0 x64 debian-6-0-x64 Debian TRUE 1 ## 5 12575 Debian 6.0 x32 debian-6-0-x32 Debian TRUE 1 ## 6 14097 Ubuntu 10.04 x64 ubuntu-10-04-x64 Ubuntu TRUE 1 ## nyc1 ams1 nyc2 ams2 sgp1 ## 1 NA NA NA NA NA ## 2 1 1 1 1 1 ## 3 1 1 1 1 1 ## 4 1 1 1 1 1 ## 5 1 1 1 1 1 ## 6 1 1 1 1 1 List ssh keys\nkeys() ## $ssh_keys ## $ssh_keys[[1]] ## $ssh_keys[[1]]$id ## [1] 89103 ## ## $ssh_keys[[1]]$name ## [1] \u0026#34;Scott Chamberlain\u0026#34; One change that\u0026rsquo;s of interest is that most of the various droplets_*() functions take in the outputs of other droplets_*() functions. This means that we can pipe outputs of one droplets_*() function to another, including non-droplet_* functions (see examples).\nLet\u0026rsquo;s create a droplet:\n(res \u0026lt;- droplets_new(name=\u0026#34;foo\u0026#34;, size_slug = \u0026#39;512mb\u0026#39;, image_slug = \u0026#39;ubuntu-14-04-x64\u0026#39;, region_slug = \u0026#39;sfo1\u0026#39;, ssh_key_ids = 89103)) $droplet $droplet$id [1] 1880805 $droplet$name [1] \u0026#34;foo\u0026#34; $droplet$image_id [1] 3240036 $droplet$size_id [1] 66 $droplet$event_id [1] 26711810 List my droplets\nThis function used to be do_droplets_get()\ndroplets() ## $droplet_ids ## [1] 1880805 ## ## $droplets ## $droplets[[1]] ## $droplets[[1]]$id ## [1] 1880805 ## ## $droplets[[1]]$name ## [1] \u0026#34;foo\u0026#34; ## ## $droplets[[1]]$image_id ## [1] 3240036 ## ## $droplets[[1]]$size_id ## [1] 66 ## ## $droplets[[1]]$region_id ## [1] 3 ## ## $droplets[[1]]$backups_active ## [1] FALSE ## ## $droplets[[1]]$ip_address ## [1] \u0026#34;162.243.152.56\u0026#34; ## ## $droplets[[1]]$private_ip_address ## NULL ## ## $droplets[[1]]$locked ## [1] FALSE ## ## $droplets[[1]]$status ## [1] \u0026#34;active\u0026#34; ## ## $droplets[[1]]$created_at ## [1] \u0026#34;2014-06-18T14:15:35Z\u0026#34; ## ## ## ## $event_id ## NULL As mentioned above we can now pipe output of droplet*() functions to other droplet*() functions.\nHere, pipe output of lising droplets droplets() to the events() function\ndroplets() %\u0026gt;% events() ## Error: No event id found In this case there were no event ids to get event data on.\nHere, we\u0026rsquo;ll get details for the droplet we just created, then pipe that to droplets_power_off()\ndroplets(1880805) %\u0026gt;% droplets_power_off ## $droplet_ids ## [1] 1880805 ## ## $droplets ## $droplets$droplet_ids ## [1] 1880805 ## ## $droplets$droplets ## $droplets$droplets$id ## [1] 1880805 ## ## $droplets$droplets$name ## [1] \u0026#34;foo\u0026#34; ## ## $droplets$droplets$image_id ## [1] 3240036 ## ## $droplets$droplets$size_id ## [1] 66 ## ## $droplets$droplets$region_id ## [1] 3 ## ## $droplets$droplets$backups_active ## [1] FALSE ## ## $droplets$droplets$ip_address ## [1] \u0026#34;162.243.152.56\u0026#34; ## ## $droplets$droplets$private_ip_address ## NULL ## ## $droplets$droplets$locked ## [1] FALSE ## ## $droplets$droplets$status ## [1] \u0026#34;active\u0026#34; ## ## $droplets$droplets$created_at ## [1] \u0026#34;2014-06-18T14:15:35Z\u0026#34; ## ## $droplets$droplets$backups ## list() ## ## $droplets$droplets$snapshots ## list() ## ## ## $droplets$event_id ## NULL ## ## ## $event_id ## [1] 26714109 Then pipe it again to droplets_power_on()\ndroplets(1880805) %\u0026gt;% droplets_power_on ## $droplet_ids ## [1] 1880805 ## ## $droplets ## $droplets$droplet_ids ## [1] 1880805 ## ## $droplets$droplets ## $droplets$droplets$id ## [1] 1880805 ## ## $droplets$droplets$name ## [1] \u0026#34;foo\u0026#34; ## ## $droplets$droplets$image_id ## [1] 3240036 ## ## $droplets$droplets$size_id ## [1] 66 ## ## $droplets$droplets$region_id ## [1] 3 ## ## $droplets$droplets$backups_active ## [1] FALSE ## ## $droplets$droplets$ip_address ## [1] \u0026#34;162.243.152.56\u0026#34; ## ## $droplets$droplets$private_ip_address ## NULL ## ## $droplets$droplets$locked ## [1] FALSE ## ## $droplets$droplets$status ## [1] \u0026#34;off\u0026#34; ## ## $droplets$droplets$created_at ## [1] \u0026#34;2014-06-18T14:15:35Z\u0026#34; ## ## $droplets$droplets$backups ## list() ## ## $droplets$droplets$snapshots ## list() ## ## ## $droplets$event_id ## NULL ## ## ## $event_id ## [1] 26714152 Sys.sleep(6) droplets(1880805)$droplets$status ## [1] \u0026#34;off\u0026#34; Why not use more pipes?\ndroplets(1880805) %\u0026gt;% droplets_power_off %\u0026gt;% droplets_power_on %\u0026gt;% events Last time I talked about installing R, RStudio, etc. on a droplet. I\u0026rsquo;m still working out bugs in that stuff, but do test out so it can get better faster. See do_install().\n","permalink":"http://localhost:1313/2014/06/analogsea-v01/","summary":"\u003cp\u003eMy \u003ca href=\"http://recology.info/2014/05/analogsea/\"\u003elast blog \u003c/a\u003e post introduced the R package I\u0026rsquo;m working on \u003ccode\u003eanalogsea\u003c/code\u003e, an R client for the Digital Ocean API.\u003c/p\u003e\n\u003cp\u003eThings have changed a bit, including fillig out more functions for all API endpoints, and incorparting feedback from Hadley and Karthik. The package is as \u003ccode\u003ev0.1\u003c/code\u003e now, so I thought I\u0026rsquo;d say a few things about how it works.\u003c/p\u003e\n\u003cp\u003eNote that Digital Ocean\u0026rsquo;s v2 API is in beta stage now, so the current version of \u003ccode\u003eanalogsea\u003c/code\u003e at \u003ccode\u003ev0.1\u003c/code\u003e works with their v1 API. The \u003ca href=\"https://github.com/sckott/analogsea/tree/v2\"\u003ev2 branch of analogsea\u003c/a\u003e is being developed for their v2 API.\u003c/p\u003e","title":"analogsea - v0.1 notes"},{"content":"I think this package name is my best yet. Maybe it doesn\u0026rsquo;t make sense though? At least it did at the time\u0026hellip;\nAnyway, the main motivation for this package was to be able to automate spinning up Linux boxes to do cloud R/RStudio work. Of course if you are a command line native this is all easy for you, but if you are afraid of the command line and/or just don\u0026rsquo;t want to deal with it, this tool will hopefully help.\nMost of the functions in this package wrap the Digital Ocean API. So you can do things like create a new droplet, get information on your droplets, destroy droplets, get information on available images, make snapshots, etc. Basically everything you can do from their website you can do here. Note that all functions are prefixed with do_ (for Digital Ocean).\nThe droplet creation part is what we can leverage to spin up a cloud machine to then install R on, and optionally RStudio server, and even RStudio Shiny server. This allows you to stay within R entirely, not having to go to ssh into the Linux machine itself or go to the Digital Ocean website (after initial setup of course).\nIf you try this, I recommend using this on R on the command line as you can more easily kill the R session if something goes wrong, and quickly open a new tab/window to ssh into the Linux machine if you want.\nFirst, installation\ndevtools::install_github(\u0026#34;sckott/analogsea\u0026#34;) Load the library\nlibrary(\u0026#34;analogsea\u0026#34;) Firt, authenticate. This will ask for your Digital Ocean details. You can enter them each R session, or store them in your .Renviron file. After successful authentication, each function simply looks for your auth details with Sys.getenv().\ndo_auth() List available regions\nsapply(do_regions()$regions, \u0026#34;[[\u0026#34;, \u0026#34;name\u0026#34;) ## [1] \u0026#34;San Francisco 1\u0026#34; \u0026#34;New York 2\u0026#34; \u0026#34;Amsterdam 2\u0026#34; \u0026#34;Singapore 1\u0026#34; List available images\nsapply(do_images()$images, \u0026#34;[[\u0026#34;, \u0026#34;name\u0026#34;) ## [1] \u0026#34;rstudioserverssh_snap\u0026#34; ## [2] \u0026#34;CentOS 5.8 x64\u0026#34; ## [3] \u0026#34;CentOS 5.8 x32\u0026#34; ## [4] \u0026#34;Debian 6.0 x64\u0026#34; ## [5] \u0026#34;Debian 6.0 x32\u0026#34; ## [6] \u0026#34;Ubuntu 10.04 x64\u0026#34; ## [7] \u0026#34;Ubuntu 10.04 x32\u0026#34; ## [8] \u0026#34;Arch Linux 2013.05 x64\u0026#34; ## [9] \u0026#34;Arch Linux 2013.05 x32\u0026#34; ## [10] \u0026#34;CentOS 6.4 x32\u0026#34; ## [11] \u0026#34;CentOS 6.4 x64\u0026#34; ## [12] \u0026#34;Ubuntu 12.04.4 x32\u0026#34; ## [13] \u0026#34;Ubuntu 12.04.4 x64\u0026#34; ## [14] \u0026#34;Ubuntu 13.10 x32\u0026#34; ## [15] \u0026#34;Ubuntu 13.10 x64\u0026#34; ## [16] \u0026#34;Fedora 19 x32\u0026#34; ## [17] \u0026#34;Fedora 19 x64\u0026#34; ## [18] \u0026#34;MEAN on Ubuntu 12.04.4\u0026#34; ## [19] \u0026#34;Ghost 0.4.2 on Ubuntu 12.04\u0026#34; ## [20] \u0026#34;Wordpress on Ubuntu 13.10\u0026#34; ## [21] \u0026#34;Ruby on Rails on Ubuntu 12.10 (Nginx + Unicorn)\u0026#34; ## [22] \u0026#34;Redmine on Ubuntu 12.04\u0026#34; ## [23] \u0026#34;Ubuntu 14.04 x32\u0026#34; ## [24] \u0026#34;Ubuntu 14.04 x64\u0026#34; ## [25] \u0026#34;Fedora 20 x32\u0026#34; ## [26] \u0026#34;Fedora 20 x64\u0026#34; ## [27] \u0026#34;Dokku v0.2.3 on Ubuntu 14.04\u0026#34; ## [28] \u0026#34;Debian 7.0 x64\u0026#34; ## [29] \u0026#34;Debian 7.0 x32\u0026#34; ## [30] \u0026#34;CentOS 6.5 x64\u0026#34; ## [31] \u0026#34;CentOS 6.5 x32\u0026#34; ## [32] \u0026#34;Docker 0.11.1 on Ubuntu 13.10 x64\u0026#34; ## [33] \u0026#34;Django on Ubuntu 14.04\u0026#34; ## [34] \u0026#34;LAMP on Ubuntu 14.04\u0026#34; ## [35] \u0026#34;node-v0.10.28 on Ubuntu 14.04\u0026#34; ## [36] \u0026#34;GitLab 6.9.0 CE\u0026#34; List available sizes\ndo.call(rbind, do_sizes()$sizes) ## id name slug memory cpu disk cost_per_hour cost_per_month ## [1,] 66 \u0026#34;512MB\u0026#34; \u0026#34;512mb\u0026#34; 512 1 20 0.00744 \u0026#34;5.0\u0026#34; ## [2,] 63 \u0026#34;1GB\u0026#34; \u0026#34;1gb\u0026#34; 1024 1 30 0.01488 \u0026#34;10.0\u0026#34; ## [3,] 62 \u0026#34;2GB\u0026#34; \u0026#34;2gb\u0026#34; 2048 2 40 0.02976 \u0026#34;20.0\u0026#34; ## [4,] 64 \u0026#34;4GB\u0026#34; \u0026#34;4gb\u0026#34; 4096 2 60 0.05952 \u0026#34;40.0\u0026#34; ## [5,] 65 \u0026#34;8GB\u0026#34; \u0026#34;8gb\u0026#34; 8192 4 80 0.1191 \u0026#34;80.0\u0026#34; ## [6,] 61 \u0026#34;16GB\u0026#34; \u0026#34;16gb\u0026#34; 16384 8 160 0.2381 \u0026#34;160.0\u0026#34; ## [7,] 60 \u0026#34;32GB\u0026#34; \u0026#34;32gb\u0026#34; 32768 12 320 0.4762 \u0026#34;320.0\u0026#34; ## [8,] 70 \u0026#34;48GB\u0026#34; \u0026#34;48gb\u0026#34; 49152 16 480 0.7143 \u0026#34;480.0\u0026#34; ## [9,] 69 \u0026#34;64GB\u0026#34; \u0026#34;64gb\u0026#34; 65536 20 640 0.9524 \u0026#34;640.0\u0026#34; Let\u0026rsquo;s create a droplet:\n(res \u0026lt;- do_droplets_new(name=\u0026#34;foo\u0026#34;, size_slug = \u0026#39;512mb\u0026#39;, image_slug = \u0026#39;ubuntu-14-04-x64\u0026#39;, region_slug = \u0026#39;sfo1\u0026#39;, ssh_key_ids = 89103)) $status [1] \u0026#34;OK\u0026#34; $droplet $droplet$id [1] 1733336 $droplet$name [1] \u0026#34;foo\u0026#34; $droplet$image_id [1] 3240036 $droplet$size_id [1] 66 $droplet$event_id [1] 25278892 attr(,\u0026#34;class\u0026#34;) [1] \u0026#34;dodroplet\u0026#34; List my droplets\ndo_droplets_get() ## $status ## [1] \u0026#34;OK\u0026#34; ## ## $droplets ## $droplets[[1]] ## $droplets[[1]]$id ## [1] 1733336 ## ## $droplets[[1]]$name ## [1] \u0026#34;foo\u0026#34; ## ## $droplets[[1]]$image_id ## [1] 3240036 ## ## $droplets[[1]]$size_id ## [1] 66 ## ## $droplets[[1]]$region_id ## [1] 3 ## ## $droplets[[1]]$backups_active ## [1] FALSE ## ## $droplets[[1]]$ip_address ## [1] \u0026#34;107.170.211.252\u0026#34; ## ## $droplets[[1]]$private_ip_address ## NULL ## ## $droplets[[1]]$locked ## [1] FALSE ## ## $droplets[[1]]$status ## [1] \u0026#34;active\u0026#34; ## ## $droplets[[1]]$created_at ## [1] \u0026#34;2014-05-28T05:59:22Z\u0026#34; Cool, we have a new Linux box with 512 mb RAM, running Ubuntu 14.04 in the SF region. Notice that I\u0026rsquo;m using my SSH key here. If you don\u0026rsquo;t use your SSH key, Digital Ocean will email you a password, which you then use. We just have to wait a bit (sometimes 20 seconds, sometimes a few minutes) for it to spin up.\nNow we can install stuff. Here, I\u0026rsquo;ll install R, and RStudio Server. This step prints out the progress as you would see if you did ssh into the box itself outside of R. The RStudio Server instance will pop up in your default browser when this operation is done.\ndo_install(res$droplet$id, what=\u0026#39;rstudio\u0026#39;, usr=\u0026#39;hey\u0026#39;, pwd=\u0026#39;there\u0026#39;) You can install some things like the libcurl and libxml libraries too with the deps parameter.\nWhen you\u0026rsquo;re done, you can destroy your droplet from R too\ndo_droplets_destroy(res$droplet$id) ## $status ## [1] \u0026#34;OK\u0026#34; ## ## $event_id ## [1] 25279124 Let me know if you have any thoughts :)\n","permalink":"http://localhost:1313/2014/05/analogsea/","summary":"\u003cp\u003eI think this package name is my best yet. Maybe it doesn\u0026rsquo;t make sense though? At least it did at the time\u0026hellip;\u003c/p\u003e\n\u003cp\u003eAnyway, the main motivation for this package was to be able to automate spinning up Linux boxes to do cloud R/RStudio work. Of course if you are a command line native this is all easy for you, but if you are afraid of the command line and/or just don\u0026rsquo;t want to deal with it, this tool will hopefully help.\u003c/p\u003e","title":"analogsea - an R client for the Digital Ocean API"},{"content":"Someone asked about plotting something like this today\nI wrote a few functions previously to do something like this. However, since then ggplot2 has changed, and one of the functions no longer works.\nHence, I fixed opts() to theme(), theme_blank() to element_blank(), and panel.background = element_blank() to plot.background = element_blank() to get the histograms to show up with the line plot and not cover it.\nThe new functions:\nloghistplot \u0026lt;- function(data) { names(data) \u0026lt;- c(\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;) # rename columns # get min and max axis values min_x \u0026lt;- min(data$x) max_x \u0026lt;- max(data$x) min_y \u0026lt;- min(data$y) max_y \u0026lt;- max(data$y) # get bin numbers bin_no \u0026lt;- max(hist(data$x, plot = FALSE)$counts) + 5 # create plots a \u0026lt;- ggplot(data, aes(x = x, y = y)) + theme_bw(base_size=16) + geom_smooth(method = \u0026#34;glm\u0026#34;, family = \u0026#34;binomial\u0026#34;, se = TRUE, colour=\u0026#39;black\u0026#39;, size=1.5, alpha = 0.3) + scale_x_continuous(limits=c(min_x,max_x)) + theme(panel.grid.major = element_blank(), panel.grid.minor=element_blank(), panel.background = element_blank(), plot.background = element_blank()) + labs(y = \u0026#34;Probability\\n\u0026#34;, x = \u0026#34;\\nYour X Variable\u0026#34;) theme_loghist \u0026lt;- list( theme(panel.grid.major = element_blank(), panel.grid.minor=element_blank(), axis.text.y = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.background = element_blank(), plot.background = element_blank()) ) b \u0026lt;- ggplot(data[data$y == unique(data$y)[1], ], aes(x = x)) + theme_bw(base_size=16) + geom_histogram(fill = \u0026#34;grey\u0026#34;) + scale_y_continuous(limits=c(0,bin_no)) + scale_x_continuous(limits=c(min_x,max_x)) + theme_loghist + labs(y=\u0026#39;\\n\u0026#39;, x=\u0026#39;\\n\u0026#39;) c \u0026lt;- ggplot(data[data$y == unique(data$y)[2], ], aes(x = x)) + theme_bw(base_size=16) + geom_histogram(fill = \u0026#34;grey\u0026#34;) + scale_y_continuous(trans=\u0026#39;reverse\u0026#39;, limits=c(bin_no,0)) + scale_x_continuous(limits=c(min_x,max_x)) + theme_loghist + labs(y=\u0026#39;\\n\u0026#39;, x=\u0026#39;\\n\u0026#39;) grid.newpage() pushViewport(viewport(layout = grid.layout(1,1))) vpa_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) vpb_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) vpc_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) print(b, vp = vpb_) print(c, vp = vpc_) print(a, vp = vpa_) } logpointplot \u0026lt;- function(data) { names(data) \u0026lt;- c(\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;) # rename columns # get min and max axis values min_x \u0026lt;- min(data$x) max_x \u0026lt;- max(data$x) min_y \u0026lt;- min(data$y) max_y \u0026lt;- max(data$y) # create plots ggplot(data, aes(x = x, y = y)) + theme_bw(base_size=16) + geom_point(size = 3, alpha = 0.5, position = position_jitter(w=0, h=0.02)) + geom_smooth(method = \u0026#34;glm\u0026#34;, family = \u0026#34;binomial\u0026#34;, se = TRUE, colour=\u0026#39;black\u0026#39;, size=1.5, alpha = 0.3) + scale_x_continuous(limits=c(min_x,max_x)) + theme(panel.grid.major = element_blank(), panel.grid.minor=element_blank(), panel.background = element_blank()) + labs(y = \u0026#34;Probability\\n\u0026#34;, x = \u0026#34;\\nYour X Variable\u0026#34;) } Install ggplot2 and gridExtra if you don\u0026rsquo;t have them:\ninstall.packages(c(\u0026#34;ggplot2\u0026#34;,\u0026#34;gridExtra\u0026#34;), repos = \u0026#34;http://cran.rstudio.com\u0026#34;) And their use:\nLogistic histogram plots\nloghistplot(data=mtcars[,c(\u0026#34;mpg\u0026#34;,\u0026#34;vs\u0026#34;)]) loghistplot(movies[,c(\u0026#34;rating\u0026#34;,\u0026#34;Action\u0026#34;)]) Logistic point plots\nloghistplot(data=mtcars[,c(\u0026#34;mpg\u0026#34;,\u0026#34;vs\u0026#34;)]) loghistplot(movies[,c(\u0026#34;rating\u0026#34;,\u0026#34;Action\u0026#34;)]) ","permalink":"http://localhost:1313/2014/05/logplotreboot/","summary":"\u003cp\u003eSomeone asked about plotting something like this today\u003c/p\u003e\n\u003cp\u003eI \u003ca href=\"/posts/2012-01-10-logistic-regression-barplot-fig/\"\u003ewrote a few functions previously\u003c/a\u003e to do something like this. However, since then \u003ccode\u003eggplot2\u003c/code\u003e has changed, and one of the functions no longer works.\u003c/p\u003e\n\u003cp\u003eHence, I fixed \u003ccode\u003eopts()\u003c/code\u003e to \u003ccode\u003etheme()\u003c/code\u003e, \u003ccode\u003etheme_blank()\u003c/code\u003e to \u003ccode\u003eelement_blank()\u003c/code\u003e, and \u003ccode\u003epanel.background = element_blank()\u003c/code\u003e to \u003ccode\u003eplot.background = element_blank()\u003c/code\u003e to get the histograms to show up with the line plot and not cover it.\u003c/p\u003e\n\u003cp\u003eThe new functions:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eloghistplot  \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(data) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003enames\u003c/span\u003e(data) \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;x\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;y\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# rename columns\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#75715e\"\u003e# get min and max axis values\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  min_x \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emin\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ex)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  max_x \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emax\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ex)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  min_y \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emin\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  max_y \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emax\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#75715e\"\u003e# get bin numbers\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  bin_no \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emax\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ehist\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ex, plot \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFALSE\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ecounts) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#75715e\"\u003e# create plots\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  a \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(data, \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e x, y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e y)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003etheme_bw\u003c/span\u003e(base_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003egeom_smooth\u003c/span\u003e(method \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;glm\u0026#34;\u003c/span\u003e, family \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;binomial\u0026#34;\u003c/span\u003e, se \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTRUE\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                colour\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e, alpha \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003escale_x_continuous\u003c/span\u003e(limits\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(min_x,max_x)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003etheme\u003c/span\u003e(panel.grid.major \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          panel.grid.minor\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          panel.background \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          plot.background \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e()) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003elabs\u003c/span\u003e(y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Probability\\n\u0026#34;\u003c/span\u003e, x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\nYour X Variable\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  theme_loghist \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003elist\u003c/span\u003e(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003etheme\u003c/span\u003e(panel.grid.major \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          panel.grid.minor\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          axis.text.y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          axis.text.x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          axis.ticks \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          panel.border \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          panel.background \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          plot.background \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  b \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(data[data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey)[1], ], \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e x)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003etheme_bw\u003c/span\u003e(base_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003egeom_histogram\u003c/span\u003e(fill \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;grey\u0026#34;\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003escale_y_continuous\u003c/span\u003e(limits\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e,bin_no)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003escale_x_continuous\u003c/span\u003e(limits\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(min_x,max_x)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    theme_loghist \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003elabs\u003c/span\u003e(y\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\\n\u0026#39;\u003c/span\u003e, x\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\\n\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  c \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(data[data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey)[2], ], \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e x)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003etheme_bw\u003c/span\u003e(base_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003egeom_histogram\u003c/span\u003e(fill \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;grey\u0026#34;\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003escale_y_continuous\u003c/span\u003e(trans\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;reverse\u0026#39;\u003c/span\u003e, limits\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(bin_no,\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003escale_x_continuous\u003c/span\u003e(limits\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(min_x,max_x)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    theme_loghist \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003elabs\u003c/span\u003e(y\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\\n\u0026#39;\u003c/span\u003e, x\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\\n\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003egrid.newpage\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003epushViewport\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eviewport\u003c/span\u003e(layout \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egrid.layout\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  vpa_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eviewport\u003c/span\u003e(width \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, height \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e, y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  vpb_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eviewport\u003c/span\u003e(width \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, height \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e, y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  vpc_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eviewport\u003c/span\u003e(width \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, height \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e, y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eprint\u003c/span\u003e(b, vp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vpb_)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eprint\u003c/span\u003e(c, vp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vpc_)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eprint\u003c/span\u003e(a, vp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vpa_)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003elogpointplot  \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(data) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003enames\u003c/span\u003e(data) \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;x\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;y\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# rename columns\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#75715e\"\u003e# get min and max axis values\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  min_x \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emin\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ex)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  max_x \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emax\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ex)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  min_y \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emin\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  max_y \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emax\u003c/span\u003e(data\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ey)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#75715e\"\u003e# create plots\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(data, \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e x, y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e y)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003etheme_bw\u003c/span\u003e(base_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003egeom_point\u003c/span\u003e(size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e, alpha \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e, position \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eposition_jitter\u003c/span\u003e(w\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, h\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.02\u003c/span\u003e)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003egeom_smooth\u003c/span\u003e(method \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;glm\u0026#34;\u003c/span\u003e, family \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;binomial\u0026#34;\u003c/span\u003e, se \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTRUE\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                colour\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e, alpha \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003escale_x_continuous\u003c/span\u003e(limits\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(min_x,max_x)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003etheme\u003c/span\u003e(panel.grid.major \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          panel.grid.minor\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          panel.background \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eelement_blank\u003c/span\u003e()) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003elabs\u003c/span\u003e(y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Probability\\n\u0026#34;\u003c/span\u003e, x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\nYour X Variable\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eInstall \u003ccode\u003eggplot2\u003c/code\u003e and \u003ccode\u003egridExtra\u003c/code\u003e if you don\u0026rsquo;t have them:\u003c/p\u003e","title":"Logistic plot reboot"},{"content":"The history Cowsay is a terminal program that generates ascii pictures of a cow saying what you tell the cow to say in a bubble. See the Wikipedia page for more information: https://en.wikipedia.org/wiki/Cowsay - Install cowsay to use in your terminal (on OSX):\nbrew update brew install cowsay Type cowsay hello world!, and you get:\n______________ \u0026lt; hello world! \u0026gt; -------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Optionally, you can install fortune to get pseudorandom messages from a database of quotations. On OSX do brew install fortune, then you can pipe a fortune quote to cowsay:\nfortune | cowsay And get something like:\n______________________________________ / \u0026#34;To take a significant step forward, \\ | you must make a series of finite | | improvements.\u0026#34; -- Donald J. Atwood, | \\ General Motors / -------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || You can also get different animals. Try cowsay -f tux \u0026lt;yourmessage\u0026gt;\nCowsay in R Why cowsay for R? Why not. You never know what you will learn in fun side projects. Basically, this cowsay R package we are making prints messages that you pass to the function say. There are three arguments to the say function:\nwhat: What do you want to say? You can pass it a custom message, anything you want, like what\u0026rsquo;s up, or howdy!. You can also get R\u0026rsquo;s version of fortunes, quotes about R. Just pass the exact term forture. If you want a fact about cats from the Cat Facts API, pass in catfact. Last, you can get the current time by passing time to this parameter. by: Type of animal, one of cow, chicken, poop, cat, ant, pumpkin, ghost, spider, rabbit, pig, snowman, or frog. If you want more animals, send a pull request, or ask and at some point it will be added. type: One of message (default), warning, or string (returns string). You could use string to pass into other functions, etc., instead of printing a warning or message. There are three other contributors so far (a big thanks to them):\nTyler Rinker Thomas Leeper Noam Ross Installation library(devtools) install_github(\u0026#34;cowsay\u0026#34;, \u0026#34;sckott\u0026#34;) library(cowsay) p.s. or install_github(\u0026quot;sckott/cowsay\u0026quot;) if you have a newer version of devtools\nGet time say(\u0026#34;time\u0026#34;) ----- 2014-02-20 14:15:35 ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || say(\u0026#34;time\u0026#34;, \u0026#34;chicken\u0026#34;) ----- 2014-02-20 14:15:35 ------ \\ \\ _ _/ } `\u0026gt;\u0026#39; \\ `| \\ | /\u0026#39;-. .-. \\\u0026#39; \u0026#39;;`--\u0026#39; .\u0026#39; \\\u0026#39;. `\u0026#39;-./ \u0026#39;.`-..-;` `;-..\u0026#39; _| _| /` /` Vary type of output, default calls message say(\u0026#34;hello world\u0026#34;) ----- hello world ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || say(\u0026#34;hello world\u0026#34;, type = \u0026#34;warning\u0026#34;) Warning: ----- hello world ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || say(\u0026#34;hello world\u0026#34;, type = \u0026#34;string\u0026#34;) [1] \u0026#34;\\n ----- \\n hello world \\n ------ \\n \\\\ ^__^ \\n \\\\ (oo)\\\\ ________ \\n (__)\\\\ )\\\\ /\\\\ \\n ||------w|\\n || ||\u0026#34; Catfacts!!!! From the catfacts API, we can get random cat facts. If you put in catfact you by default get a cat saying it.\nsay(\u0026#34;catfact\u0026#34;, \u0026#34;cat\u0026#34;) ----- Neutering a cat extends its life span by two or three years. ------ \\ \\ \\`*-. ) _`-. . : `. . : _ \u0026#39; ; *` _. `*-._ `-.-\u0026#39; `-. ; ` `. :. . \\ .\\ . : .-\u0026#39; . \u0026#39; `+.; ; \u0026#39; : : \u0026#39; | ; ;-. ; \u0026#39; : :`-: _.`* ; .*\u0026#39; / .*\u0026#39; ; .*`- +\u0026#39; `*\u0026#39; `*-* `*-* `*-*\u0026#39; R fortunes say(\u0026#34;fortune\u0026#34;) ----- If I were to be treated by a cure created by stepwise regression, I would prefer voodoo. Dieter Menne in a thread about regressions with many variables R-help October 2009 ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || say(\u0026#34;fortune\u0026#34;, \u0026#34;pig\u0026#34;) ----- Cross posting is sociopathic. Roger Koenker NA R-help November 2008 ------ \\ \\ _//| .-~~~-. _/oo } }-@ (\u0026#39;\u0026#39;)_ } | `--\u0026#39;| { }--{ } //_/ /_/ Incorporate into a function Define a function\nfoo \u0026lt;- function(x) { if (x \u0026lt; 5) say(\u0026#34;woops, x should be 5 or greater\u0026#34;) } Call the function, with an error on purpose\nfoo(3) ----- woops, x should be 5 or greater ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || Or capture a warning or message and pass to the say function\nfoo2 \u0026lt;- function(x) { err \u0026lt;- tryCatch(x^2, error = function(e) e) say(err$message, \u0026#34;frog\u0026#34;) } Then call the function\nfoo2(\u0026#34;hello\u0026#34;) ----- non-numeric argument to binary operator ------ \\ \\ (.)_(.) _ ( _ ) _ / \\/`-----\u0026#39;\\/ \\ __\\ ( ( ) ) /__ ) /\\ \\._./ /\\ ( )_/ /|\\ /|\\ \\_( Awesome. Much better to have an error message from a frog than just the harsh console alone :)\n","permalink":"http://localhost:1313/2014/02/cowsay/","summary":"\u003ch2 id=\"the-history\"\u003eThe history\u003c/h2\u003e\n\u003cp\u003eCowsay is a terminal program that generates ascii pictures of a cow saying what you tell the cow to say in a bubble. See the Wikipedia page for more information: \u003ca href=\"https://en.wikipedia.org/wiki/Cowsay\"\u003ehttps://en.wikipedia.org/wiki/Cowsay\u003c/a\u003e - Install cowsay to use in your terminal (on OSX):\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ebrew update\nbrew install cowsay\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eType \u003ccode\u003ecowsay hello world!\u003c/code\u003e, and you get:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e ______________\n\u0026lt; hello world! \u0026gt;\n --------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOptionally, you can install \u003ca href=\"https://en.wikipedia.org/wiki/Fortune_(Unix)\"\u003efortune\u003c/a\u003e to get pseudorandom messages from a database of quotations. On OSX do \u003ccode\u003ebrew install fortune\u003c/code\u003e, then you can pipe a fortune quote to \u003ccode\u003ecowsay\u003c/code\u003e:\u003c/p\u003e","title":"cowsay - ascii messages and warnings for R"},{"content":"I\u0026rsquo;ve been learning Ruby, and decided to scratch an itch: getting citations for papers to put in a bibtex file or my Zotero library. This usually requires two parts: 1) searching for an article with keywords, and then 2) getting the citation once the paper is found. Since I am lazy, I would prefer to do this from the command line instead of opening up a browser. Thus =\u0026gt; cites. (Note, I\u0026rsquo;m sure someone has created something better - the point is I\u0026rsquo;m learnin\u0026rsquo; me some Ruby) cites does two things:\nSearch for a paper. Uses the CrossRef Metadata Search API, which allows POST requests of free form text. Get a citation from a DOI. Uses CrossRef citation formatting service to search for citation information. Each of the two above tasks are functions that you can use within Ruby, and are available from the command line/terminal so that you don\u0026rsquo;t have to spin up Ruby. During a typical writing workflow (in which you are using bibtex formatted references) one can want a citation for their paper, and instead of opening up a browser and using Google Scholar or Web of Science, etc., you can quickly search in your terminal by doing e.g., thor cite:search 'keywords that will help find the paper, including author, year, etc.'. Which if matches will give you a DOI. Then you can do thor cite:getcite DOI/string | pbcopy and you get the bibtex reference in your clipboard. Then just paste into your bibtex file or references manager. See more examples below. First, we need to install dependencies\ngem install httparty bibtex-ruby launchy sudo gem install thor Then clone the repo down. The Makefile in the repo builds the gem, and installs the Thor module so you have access to it from anywhere. If you don\u0026rsquo;t want the Thor commands, just do make install and just the gem will be installed.\ngit clone git@github.com:sckott/cites.git cd cites make From the command line: Thor I decided to use Thor to make functions within cites available on the cli. Thor is cool. For example, you can list the commands available like\nthor list cite ----- thor cite:getcite # Get a citation from a DOI thor cite:launch paper # Open a paper from a given DOI in your default browser thor cite:search STRING # Get a DOI from a search string Get help for a particular method\nthor help cite:getcite Usage: thor cite:getcite Options: [--format=FORMAT] # Default: text [--style=STYLE] # Default: apa [--locale=LOCALE] # Default: en-US Get a citation from a DOI This is what\u0026rsquo;s associated with cites from the cli using Thor.\nOther commands are available, just type thor on the cli, and press enter. Search for a paper From the CLI\nthor cite:search \u0026#39;Piwowar sharing data increases citation PLOS\u0026#39; {\u0026#34;match\u0026#34;=\u0026gt;true, \u0026#34;doi\u0026#34;=\u0026gt;\u0026#34;10.1371/journal.pone.0000308\u0026#34;, \u0026#34;text\u0026#34;=\u0026gt;\u0026#34;Piwowar sharing data increases citation PLOS\u0026#34;} And you can do many searches, separated with commas, like\nthor cite:search \u0026#39;Piwowar sharing data increases citation PLOS,boettiger Modeling stabilizing selection\u0026#39; Search within Ruby\nrequire \u0026#39;cites\u0026#39; Cites.search(\u0026#39;Piwowar sharing data increases citation PLOS\u0026#39;) =\u0026gt; [{\u0026#34;match\u0026#34;=\u0026gt;true, \u0026#34;doi\u0026#34;=\u0026gt;\u0026#34;10.1371/journal.pone.0000308\u0026#34;, \u0026#34;text\u0026#34;=\u0026gt;\u0026#34;Piwowar sharing data increases citation PLOS\u0026#34;}] Get a reference from a DOI From the CLI, default output is text format, apa style, locale en-US\nthor cite:getcite \u0026#39;10.1186/1471-2105-14-16\u0026#39; Boyle, B., Hopkins, N., Lu, Z., Raygoza Garay, J. A., Mozzherin, D., Rees, T., Matasci, N., et al. (2013). The taxonomic name resolution service: an online tool for automated standardization of plant names. BMC Bioinformatics, 14(1), 16. Springer (Biomed Central Ltd.). doi:10.1186/1471-2105-14-16 Because we\u0026rsquo;re using thor you can pass in options to the call on the cli, like here choose ris for the format\nthor cite:getcite \u0026#39;10.1371/journal.pone.0000308\u0026#39; --format=ris TY - JOUR T2 - PLoS ONE AU - Piwowar, Heather A. AU - Day, Roger S. AU - Fridsma, Douglas B. SN - 1932-6203 TI - Sharing Detailed Research Data Is Associated with Increased Citation Rate SP - e308 VL - 2 PB - Public Library of Science DO - 10.1371/journal.pone.0000308 PY - 2007 UR - http://dx.doi.org/10.1371/journal.pone.0000308 ER - And here bibtex for the format\nthor cite:getcite \u0026#39;10.1371/journal.pone.0000308\u0026#39; --format=bibtex @article{Piwowar_Day_Fridsma_2007, title = {Sharing Detailed Research Data Is Associated with Increased Citation Rate}, volume = {2}, url = {http://dx.doi.org/10.1371/journal.pone.0000308}, doi = {10.1371/journal.pone.0000308}, number = {3}, journal = {PLoS ONE}, publisher = {Public Library of Science}, author = {Piwowar, Heather A. and Day, Roger S. and Fridsma, Douglas B.}, editor = {Ioannidis, JohnEditor}, year = {2007}, month = {mar}, pages = {e308} } Two more options, style and locale are only available with text format, like\nthor cite:getcite \u0026#39;10.1371/journal.pone.0000308\u0026#39; --format=text --style=mla --locale=fr-FR Piwowar, Heather A., Roger S. Day, et Douglas B. Fridsma. « Sharing Detailed Research Data Is Associated with Increased Citation Rate ». éd par. John Ioannidis. PLoS ONE 2.3 (2007): e308. Within Ruby\nrequire \u0026#39;cites\u0026#39; Cites.doi2cit(\u0026#39;10.1371/journal.pone.0000308\u0026#39;) =\u0026gt; [\u0026#34;Piwowar, H. A., Day, R. S., \u0026amp; Fridsma, D. B. (2007). Sharing Detailed Research Data Is Associated with Increased Citation Rate. (J. Ioannidis, Ed.)PLoS ONE, 2(3), e308. Public Library of Science. doi:10.1371/journal.pone.0000308\u0026#34;] Open paper in browser Uses Macrodocs. The default, using Macrodocs, only works for open access (#OA) articles. You can set the option oa to be false.\nthor cite:launch \u0026#39;10.1371/journal.pone.0000308\u0026#39; It\u0026rsquo;s super simple, it just concatenates your DOI onto http://macrodocs.org/?doi= to give in this case http://macrodocs.org/?doi=10.1371/journal.pone.0000308 for what you will get from that command.\nWhen you don\u0026rsquo;t have an open access article, set the oa option flag to false, like --oa=false\nthor cite:launch \u0026#39;10.1111/1365-2745.12157\u0026#39; --oa=false Setting --oa=false simply concatenates your doi onto http://dx.doi.org/, which then attempts to resolve to likely the publishers page for the article.\n","permalink":"http://localhost:1313/2014/01/cites/","summary":"\u003cp\u003eI\u0026rsquo;ve been learning Ruby, and decided to scratch an itch: getting citations for papers to put in a bibtex file or my Zotero library. This usually requires two parts: 1) searching for an article with keywords, and then 2) getting the citation once the paper is found.  Since I am lazy, I would prefer to do this from the command line instead of opening up a browser.  Thus =\u0026gt; \u003ccode\u003ecites\u003c/code\u003e. (Note, I\u0026rsquo;m sure someone has created something better - the point is I\u0026rsquo;m learnin\u0026rsquo; me some Ruby)\n\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\n\u003cstrong\u003ecites does two things:\u003c/strong\u003e\u003c/p\u003e","title":"cites - citation stuff from the command line"},{"content":"Gaug.es is a really nice looking analytics platform as an alternative to Google Analytics. It is a paid service, but not that expensive really.\nWe\u0026rsquo;ve made an R package to interact with the Gaug.es API called rgauges. Find it on Github and on CRAN.\nAlthough working with the Gaug.es API is nice and easy, they don\u0026rsquo;t keep hourly visit stats and provide those via the API, so that you have to continually collect them yourself if you want them. That\u0026rsquo;s what I have done for my own website.\nIt took a few steps to get this data:\nI wrote a little Ruby script using Twelve gem to collect data from the Gaug.es API every day at the same time, which just gets the patst 24 hours of data. This script makes a call to the Gaug.es API and sends the data to a CouchDB database hosted on Cloudant. In reality, the script is is embeded in a rack app as I don\u0026rsquo;t think you can throw up a standalone script to Heroku. Here\u0026rsquo;s the script: class MyApp require \u0026#39;couchrest\u0026#39; require \u0026#39;twelve\u0026#39; require \u0026#39;date\u0026#39; require \u0026#39;time\u0026#39; def self.getgaugesdata_scott bfg = Twelve.new(\u0026#39;\u0026lt;gaugeskey\u0026gt;\u0026#39;) out = bfg.gauges(\u0026#39;\u0026lt;gaugeskey\u0026gt;\u0026#39;)[\u0026#39;recent_hours\u0026#39;] yip = { \u0026#34;from_url\u0026#34;=\u0026gt; \u0026#34;http://sckott.github.io/\u0026#34;, \u0026#34;coll_date\u0026#34;=\u0026gt; Date.today.to_s, \u0026#34;coll_time\u0026#34;=\u0026gt; Time.now.utc.localtime.to_s, \u0026#34;recent_hours\u0026#34;=\u0026gt; out } @db = CouchRest.database!(\u0026#34;https://app16517180.heroku:\u0026lt;key\u0026gt;@app16517180.heroku.cloudant.com/gaugesdb_scott\u0026#34;) @db.save_doc(yip) def call env [200, {\u0026#34;Content-Type\u0026#34; =\u0026gt; \u0026#34;text/html\u0026#34;}, [\u0026#34;no output printed here\u0026#34;]] end end One little catch though: I run the Ruby script on Heroku, so I don\u0026rsquo;t have to do it locally, but Heroku free instance goes down unless it\u0026rsquo;s doing something. So I used a little service called UptimeRobot to ping the Heroku app every 5 minutes. UptimeRobot also is giving you uptime stats too on your app, which I don\u0026rsquo;t really need, but is a cool feature.\nAnd that\u0026rsquo;s it. Now the data is stored from each day\u0026rsquo;s collection of visitor stats to a free Cloudant CouchDB database.\nRegular Gaug.es data First, let\u0026rsquo;s look at what you can do with data that Gaug.es does give to you, using the rgauges R package.\nInstall rgauges install.packages(\u0026#34;devtools\u0026#34;) library(devtools) install_github(\u0026#34;rgauges\u0026#34;, \u0026#34;ropensci\u0026#34;) Load rgauges and other dependency libraries library(rgauges) library(ggplot2) Your info gs_me() ## $user ## $user$name ## [1] \u0026#34;Scott Chamberlain\u0026#34; ## ## $user$email ## [1] \u0026#34;myrmecocystus@gmail.com\u0026#34; ## ## $user$id ## [1] \u0026#34;4eddbafb613f5d5139000001\u0026#34; ## ## $user$last_name ## [1] \u0026#34;Chamberlain\u0026#34; ## ## $user$urls ## $user$urls$self ## [1] \u0026#34;https://secure.gaug.es/me\u0026#34; ## ## $user$urls$clients ## [1] \u0026#34;https://secure.gaug.es/clients\u0026#34; ## ## $user$urls$gauges ## [1] \u0026#34;https://secure.gaug.es/gauges\u0026#34; ## ## ## $user$first_name ## [1] \u0026#34;Scott\u0026#34; Traffic gs_traffic(id = \u0026#34;4efd83a6f5a1f5158a000004\u0026#34;) ## $metadata ## $metadata$views ## [1] 386 ## ## $metadata$urls ## $metadata$urls$older ## [1] \u0026#34;https://secure.gaug.es/gauges/4efd83a6f5a1f5158a000004/traffic?date=2013-12-01\u0026#34; ## ## $metadata$urls$newer ## NULL ## ## ## $metadata$date ## [1] \u0026#34;2014-01-17\u0026#34; ## ## $metadata$people ## [1] 208 ## ## ## $data ## views date people ## 1 7 2014-01-01 5 ## 2 17 2014-01-02 7 ## 3 39 2014-01-03 17 ## 4 15 2014-01-04 9 ## 5 14 2014-01-05 7 ## 6 33 2014-01-06 22 ## 7 19 2014-01-07 11 ## 8 15 2014-01-08 11 ## 9 53 2014-01-09 24 ## 10 53 2014-01-10 13 ## 11 14 2014-01-11 9 ## 12 11 2014-01-12 10 ## 13 14 2014-01-13 14 ## 14 11 2014-01-14 9 ## 15 23 2014-01-15 16 ## 16 16 2014-01-16 14 ## 17 32 2014-01-17 25 Screen/browser information gs_reso(id = \u0026#34;4efd83a6f5a1f5158a000004\u0026#34;) ## $browser_height ## views title ## 1 190 600 ## 2 77 768 ## 3 53 900 ## 4 38 480 ## 5 28 1024 ## ## $browser_width ## views title ## 1 153 1280 ## 2 91 1024 ## 3 58 1600 ## 4 36 800 ## 5 30 1440 ## 6 6 2000 ## 7 6 320 ## 8 6 480 ## ## $screen_width ## views title ## 1 176 1280 ## 2 90 1600 ## 3 55 1440 ## 4 35 1024 ## 5 14 2000 ## 6 6 320 ## 7 6 480 ## 8 4 800 Visualize traffic data You\u0026rsquo;ll need to load ggplot2\nlibrary(ggplot2) out \u0026lt;- gs_gauge_detail(id = \u0026#34;4efd83a6f5a1f5158a000004\u0026#34;) vis_gauge(out) ## Using hour, time as id variables ## Using date as id variables ## Using date as id variables ## NULL Historic hourly Gaug.es data Now let\u0026rsquo;s play with the hourly data. To do that we aren\u0026rsquo;t going to use rgauges, but rather call the Cloudant API. CouchDB provides a RESTful API out of the box, so we can do a call like https://app16517180.heroku.cloudant.com/gaugesdb_scott/_all_docs?limit=20 to get metadata (or other calls to get the documents themselves). (note: that url won\u0026rsquo;t work for you since you don\u0026rsquo;t have my login info)\nGet some data library(devtools) install_github(\u0026#34;sckott/sofa\u0026#34;) # or install_github(\u0026#39;sofa\u0026#39;, \u0026#39;sckott\u0026#39;) library(sofa) cloudant_name \u0026lt;- \u0026#34;app16517180.heroku\u0026#34; cloudant_pwd \u0026lt;- getOption(\u0026#34;sofa_cloudant_heroku\u0026#34;)[[2]] cushion(sofa_cloudant = c(cloudant_name, cloudant_pwd)) dat \u0026lt;- sofa_alldocs(cushion = \u0026#34;sofa_cloudant\u0026#34;, dbname = \u0026#34;gaugesdb_scott\u0026#34;, include_docs = \u0026#34;true\u0026#34;) Manipulate and visualize library(plyr) dates \u0026lt;- ldply(dat$rows, function(x) x$doc$coll_date) min(dates$V1) ## [1] \u0026#34;2013-06-26\u0026#34; max(dates$V1) ## [1] \u0026#34;2014-01-16\u0026#34; length(dates$V1) ## [1] 198 So we\u0026rsquo;ve got 198 days of data, first collected near end of June, and most recent yesterday. Now get actual visits data\ndf \u0026lt;- ldply(dat$rows, function(x) { y \u0026lt;- do.call(rbind, lapply(x$doc$recent_hours, data.frame)) data.frame(date = x$doc$coll_date, y) }) df$date \u0026lt;- as.Date(df$date) df$hour \u0026lt;- as.numeric(df$hour) library(reshape2) df_melt \u0026lt;- melt(df, id.vars = c(\u0026#34;date\u0026#34;, \u0026#34;hour\u0026#34;)) head(df_melt) ## date hour variable value ## 1 2013-09-18 1 views 2 ## 2 2013-09-18 2 views 3 ## 3 2013-09-18 3 views 2 ## 4 2013-09-18 4 views 0 ## 5 2013-09-18 5 views 1 ## 6 2013-09-18 6 views 10 We need to combine the date and hour in to one date time string:\ndf_melt \u0026lt;- transform(df_melt, datetime = as.POSIXct(paste(date, sprintf(\u0026#34;%s:00:00\u0026#34;, hour)))) head(df_melt) ## date hour variable value datetime ## 1 2013-09-18 1 views 2 2013-09-18 01:00:00 ## 2 2013-09-18 2 views 3 2013-09-18 02:00:00 ## 3 2013-09-18 3 views 2 2013-09-18 03:00:00 ## 4 2013-09-18 4 views 0 2013-09-18 04:00:00 ## 5 2013-09-18 5 views 1 2013-09-18 05:00:00 ## 6 2013-09-18 6 views 10 2013-09-18 06:00:00 Now plot all data\nlibrary(ggplot2); library(scales) gauge_theme \u0026lt;- function(){ list(theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = c(0.85,0.85), legend.key = element_blank())) } ggplot(df_melt, aes(datetime, value, group=variable, colour=variable)) + theme_bw(base_size=18) + geom_line(size=2) + scale_color_brewer(name=\u0026#34;\u0026#34;, palette=3) + labs(x=\u0026#34;\u0026#34;, y=\u0026#34;\u0026#34;) + gauge_theme() And just one day\noneday \u0026lt;- df_melt[ as.character(df_melt$date) %in% \u0026#34;2013-11-12\u0026#34;, ] ggplot(oneday, aes(datetime, value, group=variable, colour=variable)) + theme_bw(base_size=18) + geom_line(size=2) + scale_color_brewer(name=\u0026#34;\u0026#34;, palette=3) + labs(x=\u0026#34;\u0026#34;, y=\u0026#34;\u0026#34;) + gauge_theme() ","permalink":"http://localhost:1313/2014/01/rgauges-hourly/","summary":"\u003cp\u003e\u003ca href=\"http://get.gaug.es/\"\u003eGaug.es\u003c/a\u003e is a really nice looking analytics platform as an alternative to Google Analytics. It is a paid service, but not that expensive really.\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ve made an R package to interact with the Gaug.es API called \u003ccode\u003ergauges\u003c/code\u003e. Find it \u003ca href=\"https://github.com/ropensci/rgauges\"\u003eon Github\u003c/a\u003e and \u003ca href=\"http://cran.r-project.org/web/packages/rgauges/index.html\"\u003eon CRAN\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAlthough working with the Gaug.es API is nice and easy, they don\u0026rsquo;t keep hourly visit stats and provide those via the API, so that you have to continually collect them yourself if you want them. That\u0026rsquo;s what I have done for my own website.\u003c/p\u003e","title":"rgauges - fun with hourly web site analytics"},{"content":"I started using Jekyll when I didn\u0026rsquo;t really know HTML, CSS, or Ruby - so I\u0026rsquo;ve had to learn a lot - but using Jekyll has been a great learning experience for all those languages.\nI\u0026rsquo;ve tried to boil down steps to building a Jekyll site or blog to the minimal steps:\nInstall Jekyll Mac/Linux/Unix: Install dependencies: Ruby RubyGems Install Jekyll using RubyGems gem install jekyll (you may need to do sudo...) If you\u0026rsquo;re having trouble installing, see the troubleshooting page. Windows: Jekyll doesn\u0026rsquo;t officially support installation on Windows - follow these steps for a Windows install. Make a site The easiest way to get started is by using the command jekyll new SITENAME - let\u0026rsquo;s replace SITENAME with foobar for this example.\nSo we run jekyll new foobar, which gives us:\nNew jekyll site installed in /Users/scottmac2/foobar. Go into that directory, and run\ncd foobar jekyll serve Which gives you the files and directories:\n| --|- _config.yml |- _posts |- css |- _layouts |- _site |- index.html Then point your browser to http://localhost:4000/. And you should see the following:\nWrite a new blog post We\u0026rsquo;ll add a new file to the _posts folder.\n--- layout: post title: My second post date: 2013-11-20 categories: jekyll programming R --- My second blog post! Paste this in to a new file in the _posts folder, save as today\u0026rsquo;s date ({{ page.date | date: \u0026ldquo;%Y-%m-%d\u0026rdquo; }}) plus the post name, which gives us {{ page.date | date: \u0026ldquo;%Y-%m-%d\u0026rdquo; }}-second-post.md.\nDeploying An obvious option given that Jekyll was built by Github, is to put it up on Github. Github has some instructions here. Here is my attempt at instructions:\nIf you don\u0026rsquo;t have a Github account already, create one - it\u0026rsquo;s free. Set up Git. Github\u0026rsquo;s help for this: https://help.github.com/articles/set-up-git Creat a new repo on Github, with the same name as your repo on your machine, in this case foobar. Make your new blog directory foobar a git repo by doing git init within the repo. Add you files to be tracked by git via git add --all Commit your changes by git commit -am 'new blog files added' Make a gh-pages branch by doing git branch gh-pages. Add link for your repo on Github: git remote add origin https://github.com/\u0026lt;yourgithubusername\u0026gt;/foobar.git Push to Github using git push -u origin master Github gives you one repo that you can name \u0026lt;yourgithubusername\u0026gt;.github.io that will be viewable at the URL http://\u0026lt;yourgithubusername\u0026gt;.github.io. You can have your blog/website on the master branch, and you don\u0026rsquo;t need to create a gh-pages branch. But if you have your site in any other named repo, you will need the gh-pages branch. If you don\u0026rsquo;t use a \u0026lt;yourgithubusername\u0026gt;.github.io repo, your site will be viewable at \u0026lt;yourgithubusername\u0026gt;.github.io/\u0026lt;reponame\u0026gt;, in this case \u0026lt;yourgithubusername\u0026gt;.github.io/foobar.\nBeginners take note: Instead of the command line, you could use a Git GUI, from Github (OSX, Windows), or others, e.g., GitBox.\nOther info That\u0026rsquo;s the basics of how to get started. Inevitably, you\u0026rsquo;ll run into problems with various dependencies. The Jekyll site has a lot of documntation now, so go there for help - and see a roundup of links below.\nFor inspiration, here are many examples of sites that use Jekyll: http://jekyllrb.com/docs/sites/. If you want to build off someone else\u0026rsquo;s work, find one that provides their code.\nA roundup of links for building static sites with jekyll\nhttp://net.tutsplus.com/tutorials/other/building-static-sites-with-jekyll/ http://www.andrewmunsell.com/tutorials/jekyll-by-example/ Jekyll Bootstrap Jekyll thoughts by Carl Boettiger: http://carlboettiger.info/2012/12/30/learning-jekyll.html http://danielmcgraw.com/2011/04/14/The-Ultimate-Guide-To-Getting-Started-With-Jekyll-Part-1/ A book on building sites with Jekyll http://yeswejekyll.com/ http://hellarobots.com/2012/01/06/blogging-with-jekyll-quickstart.html http://www.sitepoint.com/zero-to-jekyll-in-20-minutes/ ","permalink":"http://localhost:1313/2013/11/jekyll-intro/","summary":"\u003cp\u003eI started using Jekyll when I didn\u0026rsquo;t really know HTML, CSS, or Ruby - so I\u0026rsquo;ve had to learn a lot - but using Jekyll has been a great learning experience for all those languages.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve tried to boil down steps to building a Jekyll site or blog to the minimal steps:\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\n\u003ch3 id=\"install-jekyll\"\u003eInstall Jekyll\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMac/Linux/Unix:\n\u003cul\u003e\n\u003cli\u003eInstall dependencies:\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.ruby-lang.org/en/downloads/\"\u003eRuby\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://rubygems.org/pages/download\"\u003eRubyGems\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eInstall Jekyll using RubyGems \u003ccode\u003egem install jekyll\u003c/code\u003e (you may need to do \u003ccode\u003esudo...\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eIf you\u0026rsquo;re having trouble installing, see the \u003ca href=\"http://jekyllrb.com/docs/troubleshooting/\"\u003etroubleshooting page\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWindows: Jekyll doesn\u0026rsquo;t officially support installation on Windows - follow \u003ca href=\"http://www.madhur.co.in/blog/2011/09/01/runningjekyllwindows.html\"\u003ethese steps\u003c/a\u003e for a Windows install.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e","title":"Jekyll - an intro"},{"content":"Code in journals, that is, code you would type to do some programmatic operation in say R or Python, is kind of a mess to say the least. Okay, so you can SEE code in papers, but code is not formatted in a way that facilites reuse. If an author in a paper writes out some code for software they create, or an analysis they do in the paper, wouldn\u0026rsquo;t it be nice for a reader to be able to copy and paste that code directly into whatever environment that code should execute in, and actually work. Of course there is dependencies, etc. for that software to worry about, but here I am just concerned with the code formatting in articles. Code is displayed as an image in some cases (gasp!). Additionally, there\u0026rsquo;s this thing called the internet, and we can use color, so let\u0026rsquo;s highlight code already. At least in one of our recent rOpenSci papers in F1000 Research, they do use syntax highlighting - w00t!\nCarl Boettiger (@cboettig) and I disccused how frustrated we are with the state of code in papers, and started a Github gist, listing publishers/journals and how they display code. It lives here: https://gist.github.com/sckott/6787278.\nWe have a start, but would like your help in filling this list out more. What are the code presentation practices for various publishers and journals? With a list of what currently happens, perhaps we can start to convince publishers to display code more appropriately, partly by pointing out that \u0026ldquo;XYZ publisher does it really well, why can\u0026rsquo;t you?\u0026rdquo;. I tried to record info in a standardized way across publishers\u0026hellip;\n","permalink":"http://localhost:1313/2013/10/codeinpapers/","summary":"\u003cp\u003eCode in journals, that is, code you would type to do some programmatic operation in say R or Python, is kind of a mess to say the least. Okay, so you can \u003cstrong\u003eSEE\u003c/strong\u003e code in papers, but code is not formatted in a way that facilites reuse. If an author in a paper writes out some code for software they create, or an analysis they do in the paper, wouldn\u0026rsquo;t it be nice for a reader to be able to copy and paste that code directly into whatever environment that code should execute in, and actually work. Of course there is dependencies, etc. for that software to worry about, but here I am just concerned with the code formatting in articles. Code is displayed as an image in some cases (gasp!). Additionally, there\u0026rsquo;s this thing called the internet, and we can use color, so let\u0026rsquo;s highlight code already. At least in one of our recent \u003ca href=\"http://ropensci.org/\"\u003erOpenSci\u003c/a\u003e papers in F1000 Research, \u003ca href=\"http://f1000research.com/articles/2-191/v1\"\u003ethey do use syntax highlighting\u003c/a\u003e - w00t!\u003c/p\u003e","title":"Code display in scholarly journals"},{"content":"Note: This is cross-posted from the rOpenSci blog, which will update with this post when our technical snafu is fixed.\nWith the US government shut down, many of the federal government provided data APIs are down. We write R packages to interact with many of these APIs. We have been tweeting about what APIs that are down related to R pacakges we make, but we thought we would write up a proper blog post on the issue.\nNCBI services are still up! NCBI is within NIH, which is within the Department of Health and Human Services. Here is the message on the NCBI page:\nThe information on this web site remains accessible; but, due to the lapse in government funding, the information may not be up to date, and the agency may not be able to respond to inquiries until appropriations are enacted. For updates regarding government operating status see USA.gov.\nMost USGS services are down. Some of the message on the USGS page (ITIS is under USGS, which is under the Department of the Interior):\nDue to the Federal government shutdown, usgs.gov and most associated web sites are unavailable. Only web sites necessary to protect lives and property will be maintained\u0026hellip;\nHowever, the USGS BISON service is still up for some reason - perhaps a different pot of money than other USGS projects?\nSome of the shutdown message from NOAA, under the Department of Commerce:\nDue to the Federal government shutdown, NOAA.gov and most associated web sites are unavailable. Specific NOAA web sites necessary to protect lives and property are operational and will be maintained.\nHere\u0026rsquo;s a table of APIs we interact with, the related R package, and any notes:\nAPI provider API still up? rOpenSci R package USGS BISON (Biodiversity Information Serving our Nation) link Yep! rbison NOAA Climate data link No :( rnoaa USGS ITIS (Integrated Taxonomic Information Service) link No :( taxize NCBI Entrez link No :( taxize,rentrez PubMed link No :( rpubmed For those wanting to get NOAA climate data, perhaps check out the RNCEP package.\nFor those using taxize, you can grab taxonomic IDs from NCBI using get_uid() rather than the ITIS version get_tsn(). With a UID from NCBI, you can do things like get a taxonomic classification using the function classification(). There are many non-government taxonomic sources in taxize, so you should be able to find what you need without ITIS. Other functions that use ITIS, and that you should avoid until the shutdown is over, are:\nA long list carried over from the itis package that is now within taxize: getacceptednamesfromtsn(), getanymatchcount(), getcommentdetailfromtsn(), getcommonnamesfromtsn(), getcoremetadatafromtsn(), getcoveragefromtsn(), getcredibilityratingfromtsn(), getcredibilityratings(), getcurrencyfromtsn(), getdatedatafromtsn(), getdescription(), getexpertsfromtsn(), getfullhierarchyfromtsn(), getfullrecordfromlsid(), getfullrecordfromtsn(), getgeographicdivisionsfromtsn(), getgeographicvalues(), getglobalspeciescompletenessfromtsn(), gethierarchydownfromtsn(), gethierarchyupfromtsn(), getitistermsfromcommonname(), getitistermsfromscientificname(), getjurisdictionaloriginfromtsn(), getjurisdictionoriginvalues(), getjurisdictionvalues(), getkingdomnamefromtsn(), getkingdomnames(), getlastchangedate(), getlsidfromtsn(), getothersourcesfromtsn(), getparenttsnfromtsn(), getpublicationsfromtsn(), getranknames(), getrecordfromlsid(), getreviewyearfromtsn(), getscientificnamefromtsn(), getsynonymnamesfromtsn(), gettaxonauthorshipfromtsn(), gettaxonomicranknamefromtsn(), gettaxonomicusagefromtsn(), gettsnbyvernacularlanguage(), gettsnfromlsid(), getunacceptabilityreasonfromtsn(), getvernacularlanguages(), searchbycommonname(), searchbycommonnamebeginswith(), searchbycommonnameendswith(), searchbyscientificname(), searchforanymatch(), searchforanymatchpaged() itis_acceptname() itis_downstream() itis_name() itis_taxrank() In tax_agg(), only use db=\u0026ldquo;ncbi\u0026rdquo; In tax_name(), only use db=\u0026ldquo;ncbi\u0026rdquo; In tax_rank(), only use db=\u0026ldquo;ncbi\u0026rdquo; Let us know if you have any questions or comments.\n","permalink":"http://localhost:1313/2013/10/shutdown/","summary":"\u003cp\u003e\u003cem\u003eNote: This is cross-posted from the \u003ca href=\"http://ropensci.org/blog\"\u003erOpenSci blog\u003c/a\u003e, which will update with this post when our technical snafu is fixed.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWith the US government shut down, many of the federal government provided data APIs are down. We write R packages to interact with many of these APIs. We have been tweeting about what APIs that are down related to R pacakges we make, but we thought we would write up a proper blog post on the issue.\u003c/p\u003e","title":"Guide to using rOpenSci packages during the US Gov't shutdown"},{"content":"Eduard Szöcs and I started developing a taxonomic toolbelt for the R language a while back , which lets you interact with a multitude of taxonomic databases on the web. We have a paper in F1000Research if you want to find out more (see here).\nI thought it would be fun to rewrite some of taxize in other languages to learn more languages. Ruby and Python made the most sense to try. I did try others (Julia, Node), but gave up on those for now. The goal here isn\u0026rsquo;t to port taxize to Python and Ruby right now - it\u0026rsquo;s for me to learn myself some coding.\nAnyway, here\u0026rsquo;s use of the same function in three languages: R, Ruby, and Python. The function searches the Global Names Index, but is named slightly differently in R (gni_search) vs. Ruby/Python (gniSearch). (yes, I realize the package names aren\u0026rsquo;t consistent)\nNote that there are only a few functions available in the Ruby and Python versions:\nitisPing gnrResolve gniParse gniSearch gniDetails colChildren (Python, not Ruby) And the behavior of these functions does not necessarily match that in the R version.\nOne thing I have learned is that packaging in R is much harder than in Python or Ruby. devtools does make R packaging easier, but still\u0026hellip;\nR Code here\ninstall.packages(\u0026#34;taxize\u0026#34;) library(taxize) Then search for a taxonomic name\nout \u0026lt;- gni_search(\u0026#39;Helianthus annuus\u0026#39;) out[1,] name id 1 Helianthus annuus 3329657 lsid 1 urn:lsid:globalnames.org:index:18f9c244-a450-535e-adcd-2bfaf85c9b2e uuid_hex resource_url 1 18f9c244-a450-535e-adcd-2bfaf85c9b2e none Ruby Code here\ngit clone https://github.com/sckott/tacksize.git cd tacksize gem build tacksize.gemspec gem install ./tacksize-0.0.1.gem In a Ruby repl, like irb, search for a taxonomic name\nrequire \u0026#39;tacksize\u0026#39; out = Tacksize.gniSearch(:search_term =\u0026gt; \u0026#39;Helianthus annuus\u0026#39;) out[0] =\u0026gt; {\u0026#34;uuid_hex\u0026#34;=\u0026gt;\u0026#34;18f9c244-a450-535e-adcd-2bfaf85c9b2e\u0026#34;, \u0026#34;name\u0026#34;=\u0026gt;\u0026#34;Helianthus annuus\u0026#34;, \u0026#34;lsid\u0026#34;=\u0026gt;\u0026#34;urn:lsid:globalnames.org:index:18f9c244-a450-535e-adcd-2bfaf85c9b2e\u0026#34;, \u0026#34;resource_uri\u0026#34;=\u0026gt;\u0026#34;http://gni.globalnames.org/name_strings/3329657.xml\u0026#34;, \u0026#34;id\u0026#34;=\u0026gt;3329657} Python Code here\ngit clone https://github.com/sckott/pytaxize.git cd pytaxize python setup.py install In a Python repl, like ipython, search for a taxonomic name\nimport pytaxize out = pytaxize.gniSearch(name = \u0026#39;Helianthus annuus\u0026#39;) out[\u0026#39;name_strings\u0026#39;][0] {u\u0026#39;id\u0026#39;: 3329657, u\u0026#39;lsid\u0026#39;: u\u0026#39;urn:lsid:globalnames.org:index:18f9c244-a450-535e-adcd-2bfaf85c9b2e\u0026#39;, u\u0026#39;name\u0026#39;: u\u0026#39;Helianthus annuus\u0026#39;, u\u0026#39;resource_uri\u0026#39;: u\u0026#39;http://gni.globalnames.org/name_strings/3329657.xml\u0026#39;, u\u0026#39;uuid_hex\u0026#39;: u\u0026#39;18f9c244-a450-535e-adcd-2bfaf85c9b2e\u0026#39;} ","permalink":"http://localhost:1313/2013/09/taxonomy-in-three-acts/","summary":"\u003cp\u003eEduard Szöcs and I started developing a taxonomic toolbelt for the R language a while back , which lets you interact with a multitude of taxonomic databases on the web. We have a paper in F1000Research if you want to find out more (see \u003ca href=\"http://f1000research.com/articles/2-191/v1\"\u003ehere\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eI thought it would be fun to rewrite some of taxize in other languages to learn more languages. Ruby and Python made the most sense to try. I did try others (Julia, Node), but gave up on those for now. The goal here isn\u0026rsquo;t to port taxize to Python and Ruby right now - it\u0026rsquo;s for me to learn myself some coding.\u003c/p\u003e","title":"Taxonomy data from the web in three languages"},{"content":"I am on my way out of academia, so I want to share what I won\u0026rsquo;t ever get around to finishing. I started a paper many years ago examining the prevalence of natural enemy pressure on pollinators, and patterns of occurrence of pollinator natural enemies in relation to plant attributes.\nAnyway, Figshare seemed like the perfect place to put this. I licensed the materials under CC0, so feel free to do whatever you want with it. Check it out at https://figshare.com/articles/dataset/Pollinator_niche_breadth_and_natural_enemies/803123\n","permalink":"http://localhost:1313/2013/09/natenemies/","summary":"\u003cp\u003eI am on my way out of academia, so I want to share what I won\u0026rsquo;t ever get around to finishing. I started a paper many years ago examining the prevalence of natural enemy pressure on pollinators, and patterns of occurrence of pollinator natural enemies in relation to plant attributes.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/beeenemy.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAnyway, Figshare seemed like the perfect place to put this. I licensed the materials under CC0, so feel free to do whatever you want with it.  Check it out at \u003ca href=\"https://figshare.com/articles/dataset/Pollinator_niche_breadth_and_natural_enemies/803123\"\u003ehttps://figshare.com/articles/dataset/Pollinator_niche_breadth_and_natural_enemies/803123\u003c/a\u003e\u003c/p\u003e","title":"Pollinator niche breadth and natural enemies"},{"content":"I started an R package a while back, and a few people have shown interest, so I thought it was time to revist the code. govdat is an interface to various APIs for government data: currently the Sunlight Labs APIs, and the New York Times congress API. Returned objects from functions are simple lists. In future versions of govdat, I may change how data is returned. The following are examples (which is also the package vignette) of using the Sunlight Labs API. I will add examples of using the New York Times Congress API once their site is up again; I\u0026rsquo;m doing this on 2013-08-28, just after the takedown of their site.\nI show just a bit of each data object returned for brevity. And yes, I realize this is not related at all to ecology.\nYou will need an API key to use both Sunlight Labs APIs and the New York Times APIs. Get your API key at Sunlight Labs here and NYT here. You can pass in your key within each function or you can put the key in your .Rprofile file on your machine (which is read from the default R working directory) and the key will be read in automatically inside the function. I recommend the latter option.\nDo let me know of bugs or feature requests over at the Github issues page here.\nInstall govdat install.packages(\u0026#34;devtools\u0026#34;) library(devtools) install_github(\u0026#34;govdat\u0026#34;, \u0026#34;sckott\u0026#34;) Load govdat library(govdat) Gets details (subcommittees + membership) for a committee by id. key \u0026lt;- getOption(\u0026#34;SunlightLabsKey\u0026#34;) out \u0026lt;- sll_cg_getcommittees(id = \u0026#34;JSPR\u0026#34;) out$response$committee$members[[1]]$legislator[1:5] $website [1] \u0026#34;http://www.alexander.senate.gov\u0026#34; $fax [1] \u0026#34;202-228-3398\u0026#34; $govtrack_id [1] \u0026#34;300002\u0026#34; $firstname [1] \u0026#34;Lamar\u0026#34; $chamber [1] \u0026#34;senate\u0026#34; Gets a list of all committees that a member serves on, including subcommittes. out \u0026lt;- sll_cg_getcommitteesallleg(bioguide_id = \u0026#34;S000148\u0026#34;) out$response$committees[[1]] $committee $committee$chamber [1] \u0026#34;Senate\u0026#34; $committee$id [1] \u0026#34;SSRA\u0026#34; $committee$name [1] \u0026#34;Senate Committee on Rules and Administration\u0026#34; Get districts for a latitude/longitude. out \u0026lt;- sll_cg_getdistrictlatlong(latitude = 35.778788, longitude = -78.787805) out$response$districts [[1]] [[1]]$district [[1]]$district$state [1] \u0026#34;NC\u0026#34; [[1]]$district$number [1] \u0026#34;2\u0026#34; Get districts that overlap for a certain zip code. out \u0026lt;- sll_cg_getdistrictzip(zip = 27511) out$response$districts [[1]] [[1]]$district [[1]]$district$state [1] \u0026#34;NC\u0026#34; [[1]]$district$number [1] \u0026#34;2\u0026#34; [[2]] [[2]]$district [[2]]$district$state [1] \u0026#34;NC\u0026#34; [[2]]$district$number [1] \u0026#34;4\u0026#34; [[3]] [[3]]$district [[3]]$district$state [1] \u0026#34;NC\u0026#34; [[3]]$district$number [1] \u0026#34;13\u0026#34; Search congress people and senate members. out \u0026lt;- sll_cg_getlegislatorsearch(name = \u0026#34;Reed\u0026#34;) out$response$results[[1]]$result$legislator[1:5] $website [1] \u0026#34;http://www.reed.senate.gov\u0026#34; $fax [1] \u0026#34;202-224-4680\u0026#34; $govtrack_id [1] \u0026#34;300081\u0026#34; $firstname [1] \u0026#34;John\u0026#34; $chamber [1] \u0026#34;senate\u0026#34; Search congress people and senate members for a zip code. out \u0026lt;- sll_cg_legislatorsallforzip(zip = 77006) library(plyr) ldply(out$response$legislators, function(x) data.frame(x$legislator[c(\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;)])) firstname lastname 1 Sheila Jackson Lee 2 Ted Cruz 3 John Cornyn 4 Ted Poe Find the popularity of a phrase over a period of time. Get a list of how many times the phrase \u0026ldquo;united states\u0026rdquo; appears in the Congressional Record in each month between January and June, 2010: sll_cw_timeseries(phrase = \u0026#34;united states\u0026#34;, start_date = \u0026#34;2009-01-01\u0026#34;, end_date = \u0026#34;2009-04-30\u0026#34;, granularity = \u0026#34;month\u0026#34;) 4 records returned count month 1 3805 2009-01-01 2 3512 2009-02-01 3 6018 2009-03-01 4 2967 2009-04-01 Plot data library(ggplot2) dat \u0026lt;- sll_cw_timeseries(phrase = \u0026#34;climate change\u0026#34;) 1354 records returned ggplot(dat, aes(day, count)) + geom_line() + theme_grey(base_size = 20) Plot more data dat_d \u0026lt;- sll_cw_timeseries(phrase = \u0026#34;climate change\u0026#34;, party = \u0026#34;D\u0026#34;) 908 records returned dat_d$party \u0026lt;- rep(\u0026#34;D\u0026#34;, nrow(dat_d)) dat_r \u0026lt;- sll_cw_timeseries(phrase = \u0026#34;climate change\u0026#34;, party = \u0026#34;R\u0026#34;) 623 records returned dat_r$party \u0026lt;- rep(\u0026#34;R\u0026#34;, nrow(dat_r)) dat_both \u0026lt;- rbind(dat_d, dat_r) ggplot(dat_both, aes(day, count, colour = party)) + geom_line() + theme_grey(base_size = 20) + scale_colour_manual(values = c(\u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;)) Search OpenStates bills. out \u0026lt;- sll_os_billsearch(terms = \u0026#34;agriculture\u0026#34;, state = \u0026#34;tx\u0026#34;, chamber = \u0026#34;upper\u0026#34;) lapply(out, \u0026#34;[[\u0026#34;, \u0026#34;title\u0026#34;)[100:110] [[1]] [1] \u0026#34;Relating to the sale by the Brazos River Authority of certain property at Possum Kingdom Lake.\u0026#34; [[2]] [1] \u0026#34;Proposing a constitutional amendment providing immediate additional revenue for the state budget by creating the Texas Gaming Commission, and authorizing and regulating the operation of casino games and slot machines by a limited number of licensed operators and certain Indian tribes.\u0026#34; [[3]] [1] \u0026#34;Relating to production requirements for holders of winery permits.\u0026#34; [[4]] [1] \u0026#34;Relating to the use of human remains in the training of search and rescue animals.\u0026#34; [[5]] [1] \u0026#34;Relating to end-of-course assessment instruments administered to public high school students and other measures of secondary-level performance.\u0026#34; [[6]] [1] \u0026#34;Relating to public high school graduation, including curriculum and assessment requirements for graduation and funding in support of certain curriculum authorized for graduation.\u0026#34; [[7]] [1] \u0026#34;Relating to certain residential and other structures and mitigation of loss to those structures resulting from natural catastrophes; providing a criminal penalty.\u0026#34; [[8]] [1] \u0026#34;Recognizing March 28, 2013, as Texas Water Conservation Day at the State Capitol.\u0026#34; [[9]] [1] \u0026#34;Recognizing March 26, 2013, as Lubbock Day at the State Capitol.\u0026#34; [[10]] [1] \u0026#34;In memory of Steve Jones.\u0026#34; [[11]] [1] \u0026#34;Relating to the regulation of dangerous wild animals.\u0026#34; Search Legislators on OpenStates. out \u0026lt;- sll_os_legislatorsearch(state = \u0026#34;tx\u0026#34;, party = \u0026#34;democratic\u0026#34;, active = TRUE) out[[1]][1:5] $last_name [1] \u0026#34;Naishtat\u0026#34; $updated_at [1] \u0026#34;2013-08-29 03:03:22\u0026#34; $nimsp_candidate_id [1] \u0026#34;112047\u0026#34; $full_name [1] \u0026#34;Elliott Naishtat\u0026#34; $`+district_address` [1] \u0026#34; P.O. Box 2910\\nAustin, TX 78768\\n(512) 463-0668\u0026#34; Search for entities - that is, politicians, individuals, or organizations with the given name out \u0026lt;- sll_ts_aggregatesearch(\u0026#34;Nancy Pelosi\u0026#34;) out \u0026lt;- lapply(out, function(x) { x[sapply(x, is.null)] \u0026lt;- \u0026#34;none\u0026#34; x }) ldply(out, data.frame) name count_given firm_income count_lobbied seat 1 Nancy Pelosi (D) 0 0 0 federal:house 2 Nancy Pelosi for Congress 7 0 0 none total_received state lobbying_firm count_received party total_given type 1 14173534 CA none 10054 D 0 politician 2 0 none \u0026lt;NA\u0026gt; 0 none 7250 organization id non_firm_spending is_superpac 1 85ab2e74589a414495d18cc7a9233981 0 none 2 afb432ec90454c8a83a3113061e7be27 0 \u0026lt;NA\u0026gt; Return the top contributoring organizations, ranked by total dollars given. An organization\u0026rsquo;s giving is broken down into money given directly (by the organization\u0026rsquo;s PAC) versus money given by individuals employed by or associated with the organization. out \u0026lt;- sll_ts_aggregatetopcontribs(id = \u0026#34;85ab2e74589a414495d18cc7a9233981\u0026#34;) ldply(out, data.frame) employee_amount total_amount total_count name 1 64000.00 101300.00 79 Akin, Gump et al 2 3500.00 90000.00 29 American Fedn of St/Cnty/Munic Employees 3 0 86600.00 48 National Assn of Realtors 4 0 85000.00 32 United Auto Workers 5 0 82500.00 37 National Education Assn 6 0 77000.00 19 Teamsters Union 7 0 76000.00 36 National Assn of Letter Carriers 8 0 72500.00 18 Intl Brotherhood of Electrical Workers 9 0 72500.00 21 Sheet Metal Workers Union 10 8000.00 72000.00 31 Wells Fargo direct_count employee_count id direct_amount 1 16 63 105dcfc8c9384875a099af230dad9917 37300.00 2 25 4 fb702029157e4c7c887172eba71c66c5 86500.00 3 48 0 bb98402bd4d3471cad392a671ecd733a 86600.00 4 32 0 4d3167b97c9f48deb433aad57bb0ee44 85000.00 5 37 0 1b8fea7e453d4e75841eac48ff9df550 82500.00 6 19 0 f89c8e3ab2b44f72971f91b764868231 77000.00 7 36 0 390767dc6b4b491ca775b1bdf8a36eea 76000.00 8 18 0 b53b4ad137d743a996f4d7467700fc88 72500.00 9 21 0 425be85642b24cc2bc3d8a0bb3c7bc92 72500.00 10 20 11 793070ae7f5e42c2a76a58663a588f3d 64000.00 ","permalink":"http://localhost:1313/2013/08/govdat-vignette/","summary":"\u003cp\u003eI started an R package a while back, and a few people have shown interest, so I thought it was time to revist the code. govdat is an interface to various APIs for government data: currently the Sunlight Labs APIs, and the New York Times congress API. Returned objects from functions are simple lists. In future versions of govdat, I may change how data is returned. The following are examples (which is also the package vignette) of using the Sunlight Labs API. I will add examples of using the New York Times Congress API once their site is up again; I\u0026rsquo;m doing this on 2013-08-28, just after the takedown of their site.\u003c/p\u003e","title":"govdat - SunlightLabs and New York Times Congress data via R"},{"content":"ScienceOnline Climate I recently attended ScienceOnline Climate, a conference in Washington, D.C. at AAAS offices. You may have heard of the ScienceOnline annual meeting in North Carolina - this was one of their topical meetings focused on Climate Change. Another one is coming up in October, ScienceOnline Oceans. Search Twitter for #scioClimate (or the entire list of hashtags here) for tweets from the conference.\nOne of the sessions I attended was focused on how to democratize climate change knowledge, moderated by a fellow from the Union of Concerned Scientists. Search Twitter for #sciodemocracy to see the conversation from that session. There was a lot of very interesting discussion.\nCan we reach the public with phenology data? During the #sciodemocracy session, I had a thought but couldn\u0026rsquo;t articulate it at the time. So here goes. People that are not inolved in climate change discussions may not think about climate change in the framework of changing sea level, melting ice, and altered severity of extreme events. However, many people observe birds, butterflies, and trees outside their apartment windows, cars/trains/buses, or on walks or hikes. When you live in a one place for many years changes in the timing of when birds, butterflies, and trees do certain things are easily noticed. Many of us, including myself, don\u0026rsquo;t necessarily record these changes, but some do! In fact, there are many web sites with databases of observations of birds, butterflies, and more that anyone, not just scientists, can submit observations to. Some examples are the USA National Phenology Network and iNaturalist. And of course there are other databases that are focused on observations of organisms collected by scientists, like GBIF and VertNet.\nSo what? What about it?\nWhen enough of these observations are collected on any one species in one location (e.g., let\u0026rsquo;s say we have 1000 observations of a species in Seattle over 20 years) we can simply ask how has the first date of record of the species in Seattle changed through time? If there is a change in timing of first appearance in the spring through the years, we can hypothesize that this may be due to climate change, and look at the data to see if there is a correlation, etc.\nNon-scientists along with scientists are collecting vast amounts of data on observations of species. This data can be used to make people think about climate change. That is, why don\u0026rsquo;t we not only facilitate the public\u0026rsquo;s ability to collect data, but also to analyze the data - to do their own science, ask their own questions. In this way, people can link a bird appearing for the first time in spring a bit later than the previous year, or a tree flowering a bit early, to variables associated with climate change, like temperature, precipitation, etc.\nEmpowering the general public to do their own science may bring the vague notion of climate change into stark relief - thereby movivating some to take action with their elected representatives, or to at least get curious to find out more.\n","permalink":"http://localhost:1313/2013/08/phenology/","summary":"\u003ch2 id=\"scienceonline-climate\"\u003eScienceOnline Climate\u003c/h2\u003e\n\u003cp\u003eI recently attended ScienceOnline Climate, a conference in Washington, D.C. at AAAS offices. You may have heard of the ScienceOnline annual meeting in North Carolina - this was one of their topical meetings focused on Climate Change. Another one is coming up in October, ScienceOnline Oceans. Search Twitter for #scioClimate (or the entire list of hashtags \u003ca href=\"https://gist.github.com/sckott/6213308\"\u003ehere\u003c/a\u003e) for tweets from the conference.\u003c/p\u003e\n\u003cp\u003eOne of the sessions I attended was focused on how to democratize climate change knowledge, moderated by a fellow from the Union of Concerned Scientists. Search Twitter for #sciodemocracy to see the conversation from that session. There was a lot of very interesting discussion.\u003c/p\u003e","title":"Engaging the public on climate change through phenology data"},{"content":"I recently attended ScienceOnline Climate, a conference in Washington, D.C. at AAAS. You may have heard of the ScienceOnline annual meeting in North Carolina - this was one of their topical meetings focused on Climate Change. I moderated a session on working with data from the web in R, focusing on climate data. Search Twitter for #scioClimate for tweets from the conference, and #sciordata for tweets from the session I ran. The following is an abbreviated demo of what I did in the workshop showing some of what you can do with climate data in R using our packages.\nBefore digging in, why would you want to get climate data programatically vs. via pushing buttons in a browser? Learning a programming language can take some time - we all already know how to use browsers. So why?! First, getting data programatically, especially in R (or Python), allows you to then easily do other stuff, like manipulate data, visualize, and analyze data. Second, if you do your work programatically, you and others can reproduce, and extend, the work you did with little extra effort. Third, programatically getting data makes tasks that are repetitive and slow, fast and easy - you can\u0026rsquo;t easily automate button clicks in a browser. Fourth, you can combine code with writing to make your entire workflow reproducible, whether it\u0026rsquo;s notes, a blog post, or even a research article.\nInteractive visualizations in R Let\u0026rsquo;s start off with something shiny. The majority of time I make static visualizations, which are great for me to look at during analyses, and for publications of research findings in PDFs. However, static visualizations don\u0026rsquo;t take advantage of the interactive nature of the web. Ramnath Vaidyanathan has developed an R package, rCharts, to generate dynamic Javascript visualizations directly from R that can be used interactively in a browser. Here is an example visualizing a dataset that comes with R.\nlibrary(devtools) install_github(\u0026#34;rCharts\u0026#34;, \u0026#34;ramnathv\u0026#34;) library(rCharts) # Load a data set hair_eye_male \u0026lt;- subset(as.data.frame(HairEyeColor), Sex == \u0026#34;Male\u0026#34;) # Make a javascript plot object n1 \u0026lt;- nPlot(Freq ~ Hair, group = \u0026#34;Eye\u0026#34;, data = hair_eye_male, type = \u0026#34;multiBarChart\u0026#34;) # Visualize n1$show(cdn = TRUE) Check out the output here. If you like you can take the source code from the visualization (right click on select View Page Source) and put it in your html files, and you\u0026rsquo;re good to go (as long as you have dependencies, etc.) - quicker than learning d3 and company from scratch, eh. This is a super simple example, but you can imagine the possibilities.\nThe data itself First, install some packages - these are all just on Github, so you need to have devtools installed library(devtools) install_github(\u0026#34;govdat\u0026#34;, \u0026#34;sckott\u0026#34;) install_github(\u0026#34;rnoaa\u0026#34;, \u0026#34;ropensci\u0026#34;) install_github(\u0026#34;rWBclimate\u0026#34;, \u0026#34;ropensci\u0026#34;) install_github(\u0026#34;rnpn\u0026#34;, \u0026#34;ropensci\u0026#34;) Politicians talk - Sunlight Foundation listens Look at mentions of the phrase \u0026ldquo;climate change\u0026rdquo; in congress, using the govdat package library(govdat) library(ggplot2) # Get mentions of climate change from Democrats dat_d \u0026lt;- sll_cw_timeseries(phrase = \u0026#34;climate change\u0026#34;, party = \u0026#34;D\u0026#34;) # Add a column that says this is data from deomcrats dat_d$party \u0026lt;- rep(\u0026#34;D\u0026#34;, nrow(dat_d)) # Get mentions of climate change from Democrats dat_r \u0026lt;- sll_cw_timeseries(phrase = \u0026#34;climate change\u0026#34;, party = \u0026#34;R\u0026#34;) # Add a column that says this is data from republicans dat_r$party \u0026lt;- rep(\u0026#34;R\u0026#34;, nrow(dat_r)) # Put two tables together dat_both \u0026lt;- rbind(dat_d, dat_r) # Plot data ggplot(dat_both, aes(day, count, colour = party)) + theme_grey(base_size = 20) + geom_line() + scale_colour_manual(values = c(\u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;)) NOAA climate data, using the rnoaa package Map sea ice for 12 years, for April only, for the North pole library(rnoaa) library(scales) library(ggplot2) library(doMC) library(plyr) # Get URLs for data urls \u0026lt;- seaiceeurls(mo = \u0026#34;Apr\u0026#34;, pole = \u0026#34;N\u0026#34;)[1:12] # Download sea ice data registerDoMC(cores = 4) out \u0026lt;- llply(urls, noaa_seaice, storepath = \u0026#34;~/seaicedata\u0026#34;, .parallel = TRUE) # Name elements of list names(out) \u0026lt;- seq(1979, 1990, 1) # Make a data.frame df \u0026lt;- ldply(out) # Plot data ggplot(df, aes(long, lat, group = group)) + geom_polygon(fill = \u0026#34;steelblue\u0026#34;) + theme_ice() + facet_wrap(~.id) World Bank climate data, using the rWBclimate package Plotting annual data for different countries Data can be extracted from countries or basins submitted as vectors. Here we will plot the expected temperature anomaly for each 20 year period over a baseline control period of 1961-2000. These countries chosen span the north to south pole. It\u0026rsquo;s clear from the plot that the northern most countries (US and Canada) have the biggest anomaly, and Belize, the most equatorial country, has the smallest anomaly.\nlibrary(rWBclimate) # Search for data country.list \u0026lt;- c(\u0026#34;CAN\u0026#34;, \u0026#34;USA\u0026#34;, \u0026#34;MEX\u0026#34;, \u0026#34;BLZ\u0026#34;, \u0026#34;ARG\u0026#34;) country.dat \u0026lt;- get_model_temp(country.list, \u0026#34;annualanom\u0026#34;, 2010, 2100) # Subset data to one specific model country.dat.bcc \u0026lt;- country.dat[country.dat$gcm == \u0026#34;bccr_bcm2_0\u0026#34;, ] # Exclude A2 scenario country.dat.bcc \u0026lt;- subset(country.dat.bcc, country.dat.bcc$scenario != \u0026#34;a2\u0026#34;) # Plot data ggplot(country.dat.bcc, aes(x = fromYear, y = data, group = locator, colour = locator)) + geom_point() + geom_path() + ylab(\u0026#34;Temperature anomaly over baseline\u0026#34;) + theme_bw(base_size = 20) Phenology data from the USA National Phenology Network, using rnpn library(rnpn) # Lookup names temp \u0026lt;- lookup_names(name = \u0026#34;bird\u0026#34;, type = \u0026#34;common\u0026#34;) comnames \u0026lt;- temp[temp$species_id %in% c(357, 359, 1108), \u0026#34;common_name\u0026#34;] # Get some data out \u0026lt;- getobsspbyday(speciesid = c(357, 359, 1108), startdate = \u0026#34;2010-04-01\u0026#34;, enddate = \u0026#34;2013-09-31\u0026#34;) names(out) \u0026lt;- comnames df \u0026lt;- ldply(out) df$date \u0026lt;- as.Date(df$date) # Visualize data library(ggplot2) ggplot(df, aes(date, count)) + geom_line() + theme_grey(base_size = 20) + facet_grid(.id ~ .) Feedback and new climate data sources Do use the above pacakges (govdat, rnoaa, rWBclimate, and rnpn) to get climate data, and get in touch with bug reports, and feature requests.\nSurely there are other sources of climate data out there that you want to use in R, right? Let us know what else you want to use. Better yet, if you can sling some R code, start writing your own package to interact with a source of climate data on the web - we can lend a hand.\n","permalink":"http://localhost:1313/2013/08/sciordata/","summary":"\u003cp\u003eI recently attended \u003ca href=\"http://scioclimate.wikispaces.com\"\u003eScienceOnline Climate\u003c/a\u003e, a conference in Washington, D.C. at AAAS. You may have heard of the \u003ca href=\"https://twitter.com/#sciox\"\u003eScienceOnline annual meeting in North Carolina\u003c/a\u003e - this was one of their topical meetings focused on Climate Change. I moderated a session on \u003ca href=\"http://scioclimate.wikispaces.com/3W.+Working+With+Science+Data+From+Around+The+Web\"\u003eworking with data from the web in R\u003c/a\u003e, focusing on climate data. Search Twitter for #scioClimate for tweets from the conference, and #sciordata for tweets from the session I ran. The following is an abbreviated demo of what I did in the workshop showing some of what you can do with climate data in R using our packages.\u003c/p\u003e","title":"Working with climate data from the web in R"},{"content":"After my presentation yesterday to a group of grad students on R resources, I did a presentation today on intro to R data manipulation, visualizations, and analyses/visualizations of biparite networks and community level analyses (diversity, rarefaction, ordination, etc.). As I said yesterday I\u0026rsquo;ve been playing with two ways to make reproducible presentations in R: RStudio\u0026rsquo;s presentations built in to RStudio IDE, and Slidify. Yesterday I went with RStudio\u0026rsquo;s product - today I used Slidify. See the Markdown file for the presentation here.\nCheck out the presentation slides here, and if you want, fork the code on Github, change it, submit changes back to me, etc. (click on the image to go to slides)\nHow I actually ran the 2 hr workshop was to present a few slides, then live demo writing the code out with students following along, with a number of times where they do something on their own.\n","permalink":"http://localhost:1313/2013/07/r-ecology-workshop/","summary":"\u003cp\u003eAfter \u003ca href=\"http://sckott.github.io/2013/07/r-resources/\"\u003emy presentation yesterday\u003c/a\u003e to a group of grad students on R resources, I did a presentation today on intro to R data manipulation, visualizations, and analyses/visualizations of biparite networks and community level analyses (diversity, rarefaction, ordination, etc.). As I said \u003ca href=\"http://sckott.github.io/2013/07/r-resources/\"\u003eyesterday\u003c/a\u003e I\u0026rsquo;ve been playing with two ways to make reproducible presentations in R: \u003ca href=\"http://www.rstudio.com/ide/docs/presentations/overview\"\u003eRStudio\u0026rsquo;s presentations\u003c/a\u003e built in to RStudio IDE, and \u003ca href=\"http://slidify.org/\"\u003eSlidify\u003c/a\u003e. Yesterday I went with RStudio\u0026rsquo;s product - today I used Slidify. See the Markdown file for the presentation \u003ca href=\"https://github.com/sckott/posterstalks/blob/gh-pages/sfu/resources/r_resources.Rpres\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"R ecology workshop"},{"content":"I\u0026rsquo;m doing a presentation today to grad students on R resources. I have been writing HTML presentations recently, but some great tools are now available to convert text that is easy to read and write to presentations.\nRStudio has something called R presentations, that is basically Markdown. This tool is built in to RStudio. See some docs here. A cool feature of RStudio\u0026rsquo;s presentations is that the preview of the presentation live updates on each save - nice Another option is the slidify package, made by Ramnath Vaidyanathan. The canonical url for slidify is here. Slidify gives you more options and flexibity than RStudio presentations. For this presentation I went with RStudio\u0026rsquo;s product. See the Markdown file for the presentation here.\nCheck out the presentation slides here, and if you want, fork it on Github, change it, submit changes back to me, etc. (click to go to slides)\n","permalink":"http://localhost:1313/2013/07/r-resources/","summary":"\u003cp\u003eI\u0026rsquo;m doing a presentation today to grad students on R resources. I have been writing HTML presentations recently, but some great tools are now available to convert text that is easy to read and write to presentations.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRStudio has something called \u003ccode\u003eR presentations\u003c/code\u003e, that is basically Markdown. This tool is built in to RStudio. See some docs \u003ca href=\"http://www.rstudio.com/ide/docs/presentations/overview\"\u003ehere\u003c/a\u003e. A cool feature of RStudio\u0026rsquo;s presentations is that the preview of the presentation live updates on each save - nice \u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003eAnother option is the slidify package, made by \u003ca href=\"https://github.com/ramnathv\"\u003eRamnath Vaidyanathan\u003c/a\u003e. The canonical url for slidify is \u003ca href=\"http://slidify.org/\"\u003ehere\u003c/a\u003e. Slidify gives you more options and flexibity than RStudio presentations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor this presentation I went with RStudio\u0026rsquo;s product. See the Markdown file for the presentation \u003ca href=\"https://github.com/sckott/posterstalks/blob/gh-pages/sfu/resources/r_resources.Rpres\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"R resources"},{"content":"As ecologists, we often start graduate school worshiping the ivory tower of academia with its freedom to pursue important ecological questions. However, studies have shown that most of us do not end up in academia. Greater numbers of ecology graduates are leaving the ivory tower for non-academic career paths. But for many graduates, moving from an academic environment to a non-academic job may be difficult. In graduate school we are trained to work in a particular way, often with loose deadlines and unlimited intellectual freedom (within reason of course). The culture and expectations of the non-academic world may be quite different. What are the skills that you need in a government job, or in science journalism? How do you market yourself for a non-academic position? This is a timely topic because funding to academic ecologists is being cut, leaving fewer opportunities in the academic arena. In fact, an ESA Student Section survey found that an ESA 2013 session on non-academic career paths in ecology was the topic of greatest interest.\nSandra Chung and I organized an ESA lunchtime session on Tuesday the 6th with panelists from an array of non-academic careers to offer advice and share their experiences. Each panelist will speak briefly, introducing themselves and a bit about what they do. About half of the time will be reserved for an open discussion in which you all attending the session help decide what to talk about.\nYou can find the description of the session at the ESA site here.\nThe details:\nWhen: Tuesday, August 6, 2013: 11:30 AM-1:15 PM Where: 101B, Minneapolis Convention Center Who (the panelists): Virginia Gewin: Independent science journalist (w/ work in Science/Nature/etc.) Liz Neeley: Science communication/journalism at COMPASS (a non-profit org.) Joe Simonis: Research scientist at the Lincon Park Zoo (a non-profit org.) Ted Hart: Soon to be statistician at NEON Lael Goodman: Analyst w/ the Union of Concerned Scientists Get involved\nAs a placeholder on the web for things related to Beyond Academia, and a place to find out more about the session at ESA, we started a wiki. Check it out here. Please do visit the wiki, and contribute your ideas for topics to discuss. In addition, we have a Resources page on the wiki here to collect resources related to moving beyond academia.\n","permalink":"http://localhost:1313/2013/07/beyond-academia/","summary":"\u003cp\u003eAs ecologists, we often start graduate school worshiping the ivory tower of academia with its freedom to pursue important ecological questions. However, studies have shown that most of us do not end up in academia. Greater numbers of ecology graduates are leaving the ivory tower for non-academic career paths. But for many graduates, moving from an academic environment to a non-academic job may be difficult. In graduate school we are trained to work in a particular way, often with loose deadlines and unlimited intellectual freedom (within reason of course). The culture and expectations of the non-academic world may be quite different. What are the skills that you need in a government job, or in science journalism? How do you market yourself for a non-academic position? This is a timely topic because funding to academic ecologists is being cut, leaving fewer opportunities in the academic arena. In fact, an ESA Student Section survey found that an ESA 2013 session on non-academic career paths in ecology was the topic of greatest interest.\u003c/p\u003e","title":"Beyond academia"},{"content":"I recently engaged with a number of tweeps in response to my tweet:\nRule number 1 wrt science code: DO NOT post your code on your personal website\nThat tweet wasn\u0026rsquo;t super clear, and it\u0026rsquo;s difficult to convey my thoughts in a tweet. What I should have said was do post your code - ideally on Github/Bitbucket/etc. Here goes with a much longer version to explain what I meant. The tweet was just about where to host code, whereas the following is about more than that, but related.\nCode writing during analyses, etc. When you write code to do simulations, analyses, data manipulation, visualization - whatever it is - it helps to version your code. That is, not naming files like myfile_v1.r, myfile_v2.r, etc., but with versioning using version control systems (VCS) like git, svn, mercurial, etc. Although git will give you headaches during the learning process, it takes care of versioning your code for you, finding differences in different versions, helps you manage conflicts from different contributors, and allows you to restore that old code you accidentally deleted.\nAnd you don\u0026rsquo;t have to use git or svn on a code hosting site - you can use git or svn locally on your own machine. However, there are many benefits to putting your code up on the interwebs.\nCollaborating on code Whenever you collaborate on code writing you have the extreme joy of dealing with conflicts. Perhaps you use Dropbox for collaborating on some code writing. Crap, now there is a line of code that messes up the analysis, and you don\u0026rsquo;t know who put it there, and why it\u0026rsquo;s there. Wouldn\u0026rsquo;t it be nice to have a place to collect bugs in the code.\nAll of these things become easy if you host code on a service such as Github. If you are already versioning your code with git you are in luck - all you need to do is create an account on github/bitbucket and push your code up. If not, you should definitley learn git.\nHosting your code on Github (or Bitbucket, etc.) allows each collaborator to work separately on the code simultaneously, then merge their code together, while git helps you take care of merging. An awesome feature of git (and other VCS\u0026rsquo;s) is branching. What the heck is that? Basically, you can create a complete copy of your git project, do any changes you want, then throw it away or merge it back in to your main branch. Pretty sweet.\nSharing your code Whether sharing your code with a collaborator, or with the world, if you put code on a website created specifically for hosting code, I would argue your life would be easier. Groups like Github and Bitbucket have solved a lot of problems around versioning code, displaying it, etc., whereas your website (whether it be Google sites, Wordpress, Tumblr, etc.) can not say the same.\nIt is becoming clear to many that open science has many benefits. For the sake of transparency and contributing to the public science good, I would argue that sharing your code is the right thing to do, especially given that most of us are publicly funded. However, even if you don\u0026rsquo;t want to share your code publicly, you can get free private hosting with an academic discount on Github, and Bitbucket gives you private hosting for free.\nContributing to the software you use Much of the software you and I use in R, Python, etc. is likely hosted on a code hosting platform such as Github, Bitbucket, R-Forge, etc. Code gets better faster if its users report bugs and request features to the software authors. By creating an account on Github, for example, to host your own code, you can easily report bugs or request features where others are developing software you use. This is better than email as only those two people get the benefit of learning from the conversation - while engaging where the software is created, or on a related mailing list, helps everyone.\nOn long-term availability of code Where is the best place to host your code in the long-term. Some may trust their own website over a company - a company can go out of business, be sold to another company and then be shut down, etc. However, code on personal websites can also be lost if a person moves institutions, etc. If you use a VCS, and host your code on Bitbucket/Github/Etc., even if they shut down, you will always have the same code that was up on their site, and you can host it on the newer awesome code hosting site. In addition, even if a company shuts down and you have to move your code, you are getting all the benefits as stated above.\nAnyway\u0026hellip; My point is this: do post your code somewhere, even if on your own site, but I think you\u0026rsquo;ll find that you and others can get the most out of your code if you host it on Bitbucket, Github, etc. Do tell me if you think I\u0026rsquo;m wrong and why.\nA few resources if you\u0026rsquo;re so inclined Push, Pull, Fork: GitHub for Academics Carl Boettiger has some interesting posts on research workflow and github issues as a research to do list Do have a look at Karthik Ram\u0026rsquo;s paper on how git can facilitate greater reproducibility and transparency in science here. Github is posting a bunch of videos on Youtube that are quite helpful for learning how to use git and Github here Git GUIs make using git easier: SourceTree GitBox Github\u0026rsquo;s git GUI ","permalink":"http://localhost:1313/2013/07/code/","summary":"\u003cp\u003eI recently engaged with a number of tweeps in response to my tweet:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRule number 1 wrt science code: DO NOT post your code on your personal website\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThat tweet wasn\u0026rsquo;t super clear, and it\u0026rsquo;s difficult to convey my thoughts in a tweet. What I should have said was do post your code - ideally on Github/Bitbucket/etc. Here goes with a much longer version to explain what I meant. The tweet was just about where to host code, whereas the following is about more than that, but related.\u003c/p\u003e","title":"On writing, sharing, collaborating, and hosting code for science"},{"content":"UPDATE As you can see in Patrick\u0026rsquo;s comment below you can convert to GeoJSON format files with rgdal as an alternative to calling the Ogre web API described below. See here for example code for converting to GeoJSON with rgdal.\nGitHub recently introduced the ability to render GeoJSON files on their site as maps here, and recently introduced here support for TopoJSON, an extension of GeoJSON can be up to 80% smaller than GeoJSON, support for other file extensions (.topojson and .json), and you can embed the maps on other sites (so awesome). The underlying maps used on GitHub are Openstreet Maps.\nA recent blog post showed how to convert .shp or .kml files to GeoJSON to then upload to GitHub here. The approach used Ruby on the command line locally to convert the geospatial files to GeoJSON.\nCan we do this in R? Perhaps others have already done this, but there\u0026rsquo;s more than one way to do anything, no?\nI\u0026rsquo;m not aware of a converter to GeoJSON within R, but there is a web service that can do this, called Ogre. The service lets you POST a file, which then converts to GeoJSON and gives it back to you. Ogre accepts many different file formats: BNA, CSV, DGN, DXF, zipped shapefiles, GeoConcept, GeoJSON, GeoRSS, GML, GMT, KML, MapInfo, and VRT.\nWe can use the Ogre API to upload a local geospatial file of various formats and get the GeoJSON back, then put it up on GitHub, and they render the map for you. Sweetness.\nSo here\u0026rsquo;s the protocol.\n1. Load httr. What is httr? For those not in the know it is a simpler wrapper around RCurl, a curl interface for R. # install.packages(\u0026#39;httr\u0026#39;) library(httr) 2. Here is a function to convert your geospatial files to GeoJSON (with roxygen docs). togeojson \u0026lt;- function(file, writepath = \u0026#34;~\u0026#34;) { url \u0026lt;- \u0026#34;http://ogre.adc4gis.com/convert\u0026#34; tt \u0026lt;- POST(url, body = list(upload = upload_file(file))) out \u0026lt;- content(tt, as = \u0026#34;text\u0026#34;) fileConn \u0026lt;- file(writepath) writeLines(out, fileConn) close(fileConn) } 3. Convert a file to GeoJSON KML\nIn the first line I specify the location of the file on my machine. In the second line the function togeojson reads in the file, sends the file to the API endpoint http://ogre.adc4gis.com/convert, collects the returned GeoJSON object, and saves the GeoJSON to a file that you specify. Here we are converting a KML file with point occurrences (data collected from USGS\u0026rsquo;s BISON service).\nfile \u0026lt;- \u0026#34;~/github/sac/rgeojson/acer_spicatum.kml\u0026#34; togeojson(file, \u0026#34;~/github/sac/rgeojson/acer_spicatum.geojson\u0026#34;) Shapefiles\nHere, we are converting a zip file containing shape files for Pinus contorta (data collected from the USGS here.\nfile \u0026lt;- \u0026#34;~/github/sac/rgeojson/pinucont.zip\u0026#34; togeojson(file, \u0026#34;~/github/sac/rgeojson/pinus.geojson\u0026#34;) 4. Then commit and push to GitHub. And this is what they look like on GitHub Acer spicatum distribution (points)\nPinus contorta distribution (polygons)\nIf you want, you can clone a repo from my account. Then do the below. (of course, you must have git installed, and have a GitHub account\u0026hellip;)\nFirst, fork my rgeojson repo here to your GitHub account.\nSecond, in your terminal/command line\u0026hellip;\ngit clone https://github.com/\u0026lt;yourgithubusername\u0026gt;/rgeojson.git cd rgeojson Third, in R specify the location of either the KML file or the zipped shape files, then call togeojson function to convert the KML file to a GeoJSON file (which should output a file acer_spicatum.geojson)\nfile \u0026lt;- \u0026#34;/path/to/acer_spicatum.kml\u0026#34; togeojson(file, \u0026#34;~/path/to/write/to/acer_spicatum.geojson\u0026#34;) Fourth, back in the terminal\u0026hellip;\ngit add acer_spicatum.geojson git commit -a -m \u0026#39;some cool commit message\u0026#39; git push Fifth, go to your rgeojson repo on GitHub and click on the acer_spicatum.geojson file, and the map should render.\nLook for this functionality to come to the rbison and rgbif R packages soon. Why is that cool? Think of the workflow: Query for species occurrence data in the BISON or GBIF databases, convert the results to a GeoJSON file, push to GitHub, and you have an awesome interactive map on the web. Not too bad eh.\n","permalink":"http://localhost:1313/2013/06/geojson/","summary":"\u003cp\u003e\u003cstrong\u003eUPDATE\u003c/strong\u003e As you can see in Patrick\u0026rsquo;s comment below you can convert to GeoJSON format files with rgdal as an alternative to calling the Ogre web API described below. See \u003ca href=\"https://github.com/patperu/write2geojson/blob/master/write-geojson.R\"\u003ehere\u003c/a\u003e for example code for converting to GeoJSON with rgdal.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eGitHub recently introduced the ability to render \u003ca href=\"http://en.wikipedia.org/wiki/GeoJSON\"\u003eGeoJSON\u003c/a\u003e files on their site as maps \u003ca href=\"https://github.com/blog/1528-there-s-a-map-for-that\"\u003ehere\u003c/a\u003e, and recently introduced \u003ca href=\"https://github.com/blog/1541-geojson-rendering-improvements\"\u003ehere\u003c/a\u003e support for \u003ca href=\"https://github.com/mbostock/topojson\"\u003eTopoJSON\u003c/a\u003e, an extension of GeoJSON can be up to 80% smaller than GeoJSON, support for other file extensions (\u003ccode\u003e.topojson\u003c/code\u003e and \u003ccode\u003e.json\u003c/code\u003e), and you can embed the maps on other sites (so awesome). The underlying maps used on GitHub are \u003ca href=\"http://www.openstreetmap.org/\"\u003eOpenstreet Maps\u003c/a\u003e.\u003c/p\u003e","title":"R to GeoJSON"},{"content":"Have you heard of DataONE? It stands for the Data Observation Network for Earth, and I am involved in the Community Education and Engagement working group at DataONE. We try to communicate about data, data management, and similar things to scientists and other DataONE stakeholders.\nAt our last meeting, we decided to start a blog aggregator to pull in to one place blog posts about data, data management, and related topics. Those reading this blog have likely heard of R-Bloggers - and there are many more aggregator blogs. We are calling this blog aggregator Coffeehouse - as it\u0026rsquo;s sort of a place to gather to talk/read about ideas. Check it out here. If you blog about data management think about adding your blog to Coffeehouse - go to the Add your blog page to do so. A screenshot:\nThe blogs already added to Coffeehouse:\nData Conservancy Data Pub DataCite Research Remix The Signal: Digital Preservation The tech/styling details:\nAs is obvious we are using Wordpress.org, with the Magazine Basic theme. We don\u0026rsquo;t accept comments - when someone clicks on the comments button it sends them back to the original post. This is on purpose so that the authors of the post get the comments on their own site. On the top of each post there is an alert to tell you the post is syndicated, and gives a link to the original post. You can close this alert if it\u0026rsquo;s annoying to you. Style - we have strived to use clean and simple styling to make for a nice reading experience. A cluttered website makes reading painful. And using the Twitter Bootstrap WP plugin Icons: done using the FontAwesome Wordpress Plugin. Aggregating posts is done using the FeedWordPress plugin. The add your blog form: using the Nina forms plugin Analytics: using the Gauges WP plugin That\u0026rsquo;s it. Let us know if you have any thoughts/comments.\n","permalink":"http://localhost:1313/2013/06/coffeehouse/","summary":"\u003cp\u003eHave you heard of \u003ca href=\"http://www.dataone.org/\"\u003eDataONE\u003c/a\u003e? It stands for the Data Observation Network for Earth, and I am involved in the \u003ca href=\"http://www.dataone.org/working_groups/community-education-and-engagement\"\u003eCommunity Education and Engagement working group\u003c/a\u003e at DataONE. We try to communicate about data, data management, and similar things to scientists and other DataONE \u003cem\u003estakeholders\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eAt our last meeting, we decided to start a blog aggregator to pull in to one place blog posts about data, data management, and related topics. Those reading this blog have likely heard of \u003ca href=\"http://www.r-bloggers.com/\"\u003eR-Bloggers\u003c/a\u003e - and there are many more aggregator blogs. We are calling this blog aggregator \u003cstrong\u003eCoffeehouse\u003c/strong\u003e - as it\u0026rsquo;s sort of a place to gather to talk/read about ideas. Check it out \u003ca href=\"https://coffeehouse.dataone.org/\"\u003ehere\u003c/a\u003e. If you blog about data management think about adding your blog to Coffeehouse - go to the \u003ca href=\"https://coffeehouse.dataone.org/add-your-blog/\"\u003e\u003cem\u003eAdd your blog\u003c/em\u003e\u003c/a\u003e page to do so. A screenshot:\u003c/p\u003e","title":"Coffeehouse - an aggregator for blog posts about data, data management, etc."},{"content":"It is getting easier to get data directly into R from the web. Often R packages that retrieve data from the web return useful R data structures to users like a data.frame. This is a good thing of course to make things user friendly.\nHowever, what if you want to drill down into the data that\u0026rsquo;s returned from a query to a database in R? What if you want to get that nice data.frame in R, but you think you may want to look at the raw data later? The raw data from web queries are often JSON or XML data. This type of data, especially JSON, can be easily stored in schemaless so-called NoSQL databases, and queried later.\nA brief aside: What are JSON and XML? This is what JSON looks like (ps if you ever wonder if your JSON is correct, go here):\n{ \u0026#34;name\u0026#34;: \u0026#34;joe\u0026#34;, \u0026#34;hobby\u0026#34;: \u0026#34;codemonkey\u0026#34;, \u0026#34;lives\u0026#34;: [ { \u0026#34;city\u0026#34;: \u0026#34;San Jose\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34; } ] } This is what XML looks like:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;name\u0026gt;joe\u0026lt;/name\u0026gt; \u0026lt;hobby\u0026gt;codemonkey\u0026lt;/hobby\u0026gt; \u0026lt;lives\u0026gt; \u0026lt;city\u0026gt;San Jose\u0026lt;/city\u0026gt; \u0026lt;state\u0026gt;CA\u0026lt;/state\u0026gt; \u0026lt;/lives\u0026gt; But don\u0026rsquo;t worry if it looks complicated - the project I talk about below, sofa, tries to make the interface to JSON and XML easy. Web APIs almost always return either JSON or XML, so this is the raw data.\nSo here\u0026rsquo;s the use case I imagine, or workflow:\nQuery a database on the web, and choose to write the raw data to a local database. Do whatever you want with the output R object - analyze, visualize, etc. Now you want to go back and search through some of the raw data. But, that query took an hour. Since you wrote it to a local database, you can search the data. If you hadn\u0026rsquo;t written it locally, you would have to make a new web call. Note that if you are doing calls to web APIs that get small amounts of data you don\u0026rsquo;t need to worry too much as you can easily just do the call again.\nI\u0026rsquo;ve started an R package to interact with the NoSQL database CouchDB. CouchDB is a schemaless database that speaks JSON, so you can store JSON and get back JSON (don\u0026rsquo;t worry XML, we got you covered - we can just wrap the XML in JSON before putting it in CouchDB). What\u0026rsquo;s especially cool is you can interact with CouchDB via a RESTful API. CouchDB doesn\u0026rsquo;t have full text search built in (though you can build map-reduce Views, basically preset queries on the database), so I added functions (and docs to help) to interact with the CouchDB River plugin for Elasticsearch, which provides powerful full text search via an API interface. But nevermind the tech details - all this just means you can search on the full text of your stored data.\nThere are plenty of databases you can interact with from R, so why CouchDB? For one, it makes a lot of sense to write to a NoSQL database since this blog post is dealing with a use case writing JSON, which isn\u0026rsquo;t a good fit for databases like MySQL, SQLite, PostgreSQL, etc. (though postgres allows you to write JSON). It didn\u0026rsquo;t have to be CouchDB, but at least to me it seems relatively easy to install, you can interact with it via an HTTP API (if you\u0026rsquo;re into that, which I am), and it has a nice web interface (navigate to http://localhost:5984/_utils/ after starting couchdb).\nIs this for the casual R user? Probably not. But, I imagine there are R users out there that want some more flexibility when it comes to interacting with web data in R. It is nice and tidy to get back an R data.frame from a web call, but having the raw data at your fingertips could be super powerful. I\u0026rsquo;ll describe using an R package to interact with a web database with sofa baked in, and discuss a bit about the functions within sofa.\nFirst start CouchDB in your terminal You can do this from anywhere in your directory. See here for instructions on how to install CouchDB.\ncouchdb Then start elasticsearch in your terminal See here for instructions on how to install Elasticsearch and the River CouchDB plugin.\ncd /usr/local/elasticsearch bin/elasticsearch -f Install sofa # Uncomment these lines if you don\u0026#39;t have these packages installed # install.packages(\u0026#39;devtools\u0026#39;) library(devtools) install_github(\u0026#39;sofa\u0026#39;, # \u0026#39;schamberlain\u0026#39;) install_github(\u0026#39;alm\u0026#39;, \u0026#39;ropensci\u0026#39;, ref=\u0026#39;couch\u0026#39;) library(sofa) library(alm) Simultaneously write data to CouchDB along with API calls using the alm package to get altmetrics data on PLoS papers. Ping to make sure CouchDB is on sofa_ping() couchdb version \u0026#34;Welcome\u0026#34; \u0026#34;1.2.1\u0026#34; Create a new database sofa_createdb(dbname = \u0026#34;alm_db\u0026#34;) ok TRUE Write couchdb database name to options options(couch_db_name = \u0026#34;alm_db\u0026#34;) List the databases sofa_listdbs() [1] \u0026#34;_replicator\u0026#34; \u0026#34;_users\u0026#34; [3] \u0026#34;alm_couchdb\u0026#34; \u0026#34;alm_db\u0026#34; [5] \u0026#34;dudedb\u0026#34; \u0026#34;example\u0026#34; [7] \u0026#34;poop\u0026#34; \u0026#34;rplos_db\u0026#34; [9] \u0026#34;shit\u0026#34; \u0026#34;shitty\u0026#34; [11] \u0026#34;shitty2\u0026#34; \u0026#34;test_suite_db\u0026#34; [13] \u0026#34;test_suite_db/with_slashes\u0026#34; \u0026#34;test_suite_reports\u0026#34; [15] \u0026#34;testr2couch\u0026#34; \u0026#34;twitter_db\u0026#34; Search for altmetrics normally, w/o writing to a database head(alm(doi = \u0026#34;10.1371/journal.pone.0029797\u0026#34;)) .id pdf html shares groups comments likes citations total 1 bloglines NA NA NA NA NA NA 0 0 2 citeulike NA NA 1 NA NA NA NA 1 3 connotea NA NA NA NA NA NA 0 0 4 crossref NA NA NA NA NA NA 6 6 5 nature NA NA NA NA NA NA 4 4 6 postgenomic NA NA NA NA NA NA 0 0 Search for altmetrics normally, while writing to a database head(alm(doi = \u0026#34;10.1371/journal.pone.0029797\u0026#34;, write2couch = TRUE)) .id pdf html shares groups comments likes citations total 1 bloglines NA NA NA NA NA NA 0 0 2 citeulike NA NA 1 NA NA NA NA 1 3 connotea NA NA NA NA NA NA 0 0 4 crossref NA NA NA NA NA NA 6 6 5 nature NA NA NA NA NA NA 4 4 6 postgenomic NA NA NA NA NA NA 0 0 Make lots of calls, and write them simultaneously # install_github(\u0026#39;rplos\u0026#39;, \u0026#39;ropensci\u0026#39;) library(rplos) dois \u0026lt;- searchplos(terms = \u0026#34;evolution\u0026#34;, fields = \u0026#34;id\u0026#34;, limit = 100) out \u0026lt;- alm(doi = as.character(dois[, 1]), write2couch = TRUE) lapply(out[1:2], head) $`01` .id pdf html shares groups comments likes citations total 1 bloglines NA NA NA NA NA NA 0 0 2 citeulike NA NA 0 NA NA NA NA 0 3 connotea NA NA NA NA NA NA 0 0 4 crossref NA NA NA NA NA NA 0 0 5 nature NA NA NA NA NA NA 0 0 6 postgenomic NA NA NA NA NA NA 0 0 $`02` .id pdf html shares groups comments likes citations total 1 bloglines NA NA NA NA NA NA 0 0 2 citeulike NA NA 1 NA NA NA NA 1 3 connotea NA NA NA NA NA NA 0 0 4 crossref NA NA NA NA NA NA 2 2 5 nature NA NA NA NA NA NA 0 0 6 postgenomic NA NA NA NA NA NA 0 0 Writing data to CouchDB does take a bit longer system.time(alm(doi = as.character(dois[, 1])[1:60], write2couch = FALSE)) user system elapsed 1.739 0.016 4.554 system.time(alm(doi = as.character(dois[, 1])[1:60], write2couch = TRUE)) user system elapsed 3.579 0.062 6.460 Search using elasticsearch tell elasticsearch to start indexing your database elastic_start(dbname = \u0026#34;alm_db\u0026#34;) $ok [1] TRUE Search your database out \u0026lt;- elastic_search(dbname = \u0026#34;alm_db\u0026#34;, q = \u0026#34;twitter\u0026#34;, parse_ = TRUE) out$hits$total [1] 679 Using views Write a view - here letting key be the default of null sofa_view_put(dbname = \u0026#34;alm_db\u0026#34;, design_name = \u0026#34;myview\u0026#34;, value = \u0026#34;doc.baseurl\u0026#34;) $ok [1] TRUE $id [1] \u0026#34;_design/myview\u0026#34; $rev [1] \u0026#34;1-e7c17cff1b96e4595c3781da53e16ad8\u0026#34; Get info on your new view sofa_view_get(dbname = \u0026#34;alm_db\u0026#34;, design_name = \u0026#34;myview\u0026#34;) $`_id` [1] \u0026#34;_design/myview\u0026#34; $`_rev` [1] \u0026#34;1-e7c17cff1b96e4595c3781da53e16ad8\u0026#34; $views $views$foo map \u0026#34;function(doc){emit(null,doc.baseurl)}\u0026#34; Get data using a view out \u0026lt;- sofa_view_search(dbname = \u0026#34;alm_db\u0026#34;, design_name = \u0026#34;myview\u0026#34;) length(out$rows) # 160 results [1] 161 sapply(out$rows, function(x) x$value)[1:5] # the values, just the API call URLs [1] \u0026#34;http://alm.plos.org/api/v3/articles\u0026#34; [2] \u0026#34;http://alm.plos.org/api/v3/articles\u0026#34; [3] \u0026#34;http://alm.plos.org/api/v3/articles\u0026#34; [4] \u0026#34;http://alm.plos.org/api/v3/articles\u0026#34; [5] \u0026#34;http://alm.plos.org/api/v3/articles\u0026#34; Delete the view sofa_view_del(dbname = \u0026#34;alm_db\u0026#34;, design_name = \u0026#34;myview\u0026#34;) [1] \u0026#34;\u0026#34; What now? Well, if no one uses this, then probably nothing. Though, if people think this could be useful:\nIt would be cool to make easy hooks into any package making web calls to allow users to write data to CouchDB if they choose to, sort of like the example above with rplos. Perhaps automate some of the setup for CouchDB for users, making system calls so they don\u0026rsquo;t have to. Performance: As shown above, simultaneously writing data to CouchDB takes longer than not doing so - removing this time difference will make writing to couch more palatable. What do you think? What is your reaction to this post? Do you have a need for this sort of tool? Do you have similar use cases that could be addressed with sofa?\n","permalink":"http://localhost:1313/2013/06/couch/","summary":"\u003cp\u003eIt is getting easier to get data directly into R from the web. Often R packages that retrieve data from the web return useful R data structures to users like a data.frame. This is a good thing of course to make things user friendly.\u003c/p\u003e\n\u003cp\u003eHowever, what if you want to drill down into the data that\u0026rsquo;s returned from a query to a database in R?  What if you want to get that nice data.frame in R, but you think you may want to look at the raw data later? The raw data from web queries are often JSON or XML data. This type of data, especially JSON, can be easily stored in schemaless so-called NoSQL databases, and queried later.\u003c/p\u003e","title":"Stashing and playing with raw data locally from the web"},{"content":"What is PhyloPic? PhyloPic is an awesome new service - I\u0026rsquo;ll let the creator, Mike Keesey, explain what it is (paraphrasing here):\nPhyloPic stores silhouette images of organisms, and each image is associated with taxonomic names, and stores the taxonomy of all taxa, allowing searching by taxonomic names. Anyone can submit silhouettes to PhyloPic.\nWhat is a silhouette? It\u0026rsquo;s like this:\nby Gareth Monger\nWhat makes PhyloPic not just awesome, but super awesome? All or most images are licensed under Creative Commons licenses. This means you can use the silhouettes without having to ask or pay - just attribute.\nWhat is fylopic? The idea behind Fylopic is to create modular bits and pieces (i.e., functions) to allow you to add silhouettes to not only ggplot2 plots, but base plots as well. That is, you can simply load fylopic in your R session, and add some silhouettes to your phylogeny, or your barchart, etc. - that is, fylopic is meant to be a helper in your workflow to add in silhouettes to visualizations.\nSome people prefer base plots while others prefer ggplot2 plots (me!), so it would be nice to have both options. Phylogenies at the moment render faster in base plots. I don\u0026rsquo;t yet have implementations for base plots, but will come soon, or you can send a pull request to add it.\nOne interesting use case could be to be able to get a set of silhouettes, then get a phylogeny for taxa associatd with the silhouettess using the NCBI taxonomy, but it\u0026rsquo;s not easily available yet (though I may be able to use Ben Morris\u0026rsquo; phylocommons soon. This isn\u0026rsquo;t doable yet, so in the example below the function make_phylo creates a phylogeny using ape::rcoal.\nYou could also do the reverse -\u0026gt; you have a phylogeny and then you could search Phylopic for silhouettes.\nInfo Check out the Phylopic website here, and Phylopic API developer documentation here.\nAlso check out Ben Morris\u0026rsquo; Python wrapper to Phylopic here.\nWhat can you do with fylopic? Install fylopic install.packages(\u0026#34;devtools\u0026#34;) library(devtools) install_github(\u0026#34;fylopic\u0026#34;, \u0026#34;sckott\u0026#34;) library(fylopic, quietly = TRUE) Plot a phylogeny with silhouettes at the tips Here, I search for names based on keyword Homo sapiens - which returns many matche codes. With those results we search for any silhouettes associated with those codes. Then we download images. Finally, make a phylogeny with the silhouettes at the tips. Note that in this eample the phylogeny is just a random coalescent tree made using ape::rcoal - obviously, in the real world you\u0026rsquo;d want to do something more useful.\n## search on Homo sapiens searchres \u0026lt;- search_text(text = \u0026#34;Homo sapiens\u0026#34;, options = \u0026#34;names\u0026#34;) ### which returns UUIDs searchres[1:3] [1] \u0026#34;74aea16b-666b-497a-b2cb-72201ad75a8e\u0026#34; [2] \u0026#34;1ee65cf3-53db-4a52-9960-a9f7093d845d\u0026#34; [3] \u0026#34;cc9ad8ee-3a82-4add-8d50-bc78f4ff6956\u0026#34; ## search for images based on the UUIds output \u0026lt;- search_images(uuid = searchres, options = c(\u0026#34;pngFiles\u0026#34;, \u0026#34;credit\u0026#34;, \u0026#34;canonicalName\u0026#34;)) ### we got eight matches output $`15444b9c-f17f-4d6e-89b5-5990096bcfb0` $`15444b9c-f17f-4d6e-89b5-5990096bcfb0`$supertaxa [1] \u0026#34;e547cd01-7dd1-495b-8239-52cf9971a609\u0026#34; [2] \u0026#34;bd88f674-6976-4cb2-a46e-e6a12a8ba463\u0026#34; $`fedf0e5f-f20a-442c-accf-eb84a3af8c6b` $`fedf0e5f-f20a-442c-accf-eb84a3af8c6b`$supertaxa [1] \u0026#34;e547cd01-7dd1-495b-8239-52cf9971a609\u0026#34; [2] \u0026#34;bd88f674-6976-4cb2-a46e-e6a12a8ba463\u0026#34; $`a88d3a4c-44d3-409e-87b6-516bd188c709` $`a88d3a4c-44d3-409e-87b6-516bd188c709`$supertaxa [1] \u0026#34;e547cd01-7dd1-495b-8239-52cf9971a609\u0026#34; [2] \u0026#34;bd88f674-6976-4cb2-a46e-e6a12a8ba463\u0026#34; $`d88164ec-3152-444b-b41c-4757a344a764` $`d88164ec-3152-444b-b41c-4757a344a764`$supertaxa [1] \u0026#34;9c6af553-390c-4bdd-baeb-6992cbc540b1\u0026#34; $`da5eaeb7-1ed2-4b2e-ad4a-49993881d706` $`da5eaeb7-1ed2-4b2e-ad4a-49993881d706`$supertaxa [1] \u0026#34;9c6af553-390c-4bdd-baeb-6992cbc540b1\u0026#34; ## download images myobjs \u0026lt;- get_image(uuids = output, size = \u0026#34;128\u0026#34;) ## make the phylogeny make_phylo(pngobj = myobjs) Plot a silhouette behind a plot Notice in the below example that you can use normal ggplot2 syntax, and simply add another layer (add_phylopic from fylopic) to the plot.\nlibrary(ggplot2) img \u0026lt;- get_image(\u0026#34;27356f15-3cf8-47e8-ab41-71c6260b2724\u0026#34;, size = \u0026#34;512\u0026#34;)[[1]] qplot(x = Sepal.Length, y = Sepal.Width, data = iris, geom = \u0026#34;point\u0026#34;) + add_phylopic(img) What\u0026rsquo;s next? This is a side project, so if anyone has interest in helping please do contribute code, report bugs, request features, etc.\n","permalink":"http://localhost:1313/2013/06/fylopic/","summary":"\u003ch2 id=\"what-is-phylopic\"\u003eWhat is PhyloPic?\u003c/h2\u003e\n\u003cp\u003ePhyloPic is an awesome new service - I\u0026rsquo;ll let the creator, \u003ca href=\"http://tmkeesey.net/\"\u003eMike Keesey\u003c/a\u003e, explain what it is (paraphrasing here):\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePhyloPic stores silhouette images of organisms, and each image is associated with taxonomic names, and stores the taxonomy of all taxa, allowing searching by taxonomic names. Anyone can submit silhouettes to PhyloPic.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhat is a silhouette?  It\u0026rsquo;s like this:\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"A silhouette\" loading=\"lazy\" src=\"http://phylopic.org/assets/images/submissions/bedd622a-4de2-4067-8c70-4aa44326d229.128.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eby Gareth Monger\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eWhat makes PhyloPic not just awesome, but super awesome? All or most images are licensed under \u003ca href=\"http://creativecommons.org/\"\u003eCreative Commons licenses\u003c/a\u003e. This means you can use the silhouettes without having to ask or pay - just attribute.\u003c/p\u003e","title":"Fylopic, an R wrapper to Phylopic"},{"content":"The USGS recently released a way to search for and get species occurrence records for the USA. The service is called BISON (Biodiversity Information Serving Our Nation). The service has a web interface for human interaction in a browser, and two APIs (application programming interface) to allow machines to interact with their database. One of the APIs allows you to search and retrieve data, and the other gives back maps as either a heatmap or a species occurrence map. The latter is more appropriate for working in a browser, so I\u0026rsquo;ll leave that to the web app folks.\nThe Core Science Analytics and Synthesis (CSAS) program of the US Geological Survey are responsible for BISON, and are the US node of the Global Biodiversity Information Facility (GBIF). BISON data is nested within that of GBIF, but has (or wil have?) additional data not in GBIF, as described on their About page:\nBISON has been initiated with the 110 million records GBIF makes available from the U.S. and is integrating millions more records from other sources each year\nHave a look at their Data providers and Statistics tabs on the BISON website, which list where data comes from and how many searches and downloads have been done on each data provider.\nWe (rOpenSci) started an R package to interact with the BISON search API \u0026raquo; rbison. You may be thinking, but if the data in BISON is also in GBIF, why both making another R package for BISON? Good question. As I just said, BISON will have some data GBIF won\u0026rsquo;t have. Also, the services (search API and map service) are different than those of GBIF.\nCheck out the package on GitHub here https://github.com/ropensci/rbison.\nHere is a quick run through of some things you can do with rbison.\nInstall ribson # Install rbison from GitHub using devtools, uncomment to install # install.packages(\u0026#39;devtools\u0026#39;) library(devtools) install_github(\u0026#39;rbison\u0026#39;, # \u0026#39;ropensci\u0026#39;) library(rbison) Search the BISON database for, of course, bison # Do the search out \u0026lt;- bison(species = \u0026#34;Bison bison\u0026#34;, type = \u0026#34;scientific_name\u0026#34;, start = 0, count = 10) # Check that the returned object is the right class (\u0026#39;bison\u0026#39;) class(out) [1] \u0026#34;bison\u0026#34; Get a summary of the data bison_data(out) total observation fossil specimen unknown 1 761 30 4 709 18 Summary by counties (just the first 6 rows) head(bison_data(input = out, datatype = \u0026#34;counties\u0026#34;)) record_id total county_name state 1 48295 7 Lipscomb Texas 2 41025 15 Harney Oregon 3 49017 8 Garfield Utah 4 35031 2 McKinley New Mexico 5 56013 1 Fremont Wyoming 6 40045 2 Ellis Oklahoma Summary of states bison_data(input = out, datatype = \u0026#34;states\u0026#34;) record_id total county_fips 1 Washington 1 53 2 Texas 8 48 3 New Mexico 8 35 4 Iowa 1 19 5 Montana 9 30 6 Wyoming 155 56 7 Oregon 15 41 8 Oklahoma 14 40 9 Kansas 10 20 10 Arizona 1 04 11 Alaska 29 02 12 Utah 16 49 13 Colorado 17 08 14 Nebraska 1 31 15 South Dakota 61 46 Map the results # Search for Ursus americanus (american black bear) out \u0026lt;- bison(species = \u0026#34;Ursus americanus\u0026#34;, type = \u0026#34;scientific_name\u0026#34;, start = 0, count = 200) # Sweet, got some data bison_data(out) total observation fossil specimen literature unknown centroid 1 3792 59 125 3522 47 39 78 Make some maps! Note that right now the county and state maps just plot the conterminous lower 48. The map of individual occurrences shows the lower 48 + Alaska # By county bisonmap(out, tomap = \u0026#34;county\u0026#34;) # By state bisonmap(out, tomap = \u0026#34;state\u0026#34;) # Individual locations bisonmap(out) ## Rendering map...plotting 199 points When plotting occurrences, you can pass additional arguments into the bisonmap function. For example, you can jitter the points bisonmap(input = out, geom = geom_jitter) ## Rendering map...plotting 199 points And you can specify by how much you want the points to jitter (here an extreme example to make it obvious) library(ggplot2) bisonmap(input = out, geom = geom_jitter, jitter = position_jitter(width = 5)) ## Rendering map...plotting 199 points Let us know if you have any feature requests or find bugs at our GitHub Issues page.\n","permalink":"http://localhost:1313/2013/05/rbison/","summary":"\u003cp\u003eThe USGS recently released a way to search for and get species occurrence records for the USA. The service is called \u003ca href=\"http://bison.usgs.ornl.gov/\"\u003eBISON\u003c/a\u003e (Biodiversity Information Serving Our Nation). The service has \u003ca href=\"http://bison.usgs.ornl.gov/\"\u003ea web interface\u003c/a\u003e for human interaction in a browser, and \u003ca href=\"http://bison.usgs.ornl.gov/services.html\"\u003etwo APIs\u003c/a\u003e (application programming interface) to allow machines to interact with their database. One of the APIs allows you to search and retrieve data, and the other gives back maps as either a heatmap or a species occurrence map. The latter is more appropriate for working in a browser, so I\u0026rsquo;ll leave that to the web app folks.\u003c/p\u003e","title":"BISON USGS species occurrence data"},{"content":"Scholarly metadata - the meta-information surrounding articles - can be super useful. Although metadata does not contain the full content of articles, it contains a lot of useful information, including title, authors, abstract, URL to the article, etc.\nOne of the largest sources of metadata is provided via the Open Archives Initiative Protocol for Metadata Harvesting or OAI-PMH. Many publishers, provide their metadata through their own endpoint, and implement the standard OAI-PMH methods: GetRecord, Identify, ListIdentifiers, ListMetadataFormats, ListRecords, and ListSets. Many providers use OAI-PMH, including DataCite, Dryad, and PubMed.\nSome data-/article-providers provide their metadata via their own APIs. For example, Nature Publishing Group provides their own metadata API here in non OAI-PMH format; you can get PLoS metadata through their search API, and the BHL (see below) provides their own custom metadata service.\nIn addition, CrossRef provides a number of metadata search services: metadata search and openurl.\nWhat about the other publishers? (please tell me if I\u0026rsquo;m wrong about these three)\nSpringer has a metadata API, but it is terrible, soooo\u0026hellip; Elsevier, are you kidding? Well, they do have some sort of API service, but its a pain in the ass. Wiley, no better than Elsevier. Note that metadata can live in other places:\nAnother package being developed by David Springate, rpubmed can get PubMed metadata. Our wrapper to the Mendeley API, RMendeley, gets article metadata via Mendeley\u0026rsquo;s database. Our wrapper to the Biodiversity Heritage Library API here gets their metadata. No, you can\u0026rsquo;t get metadata via Google Scholar - the don\u0026rsquo;t allow scraping, and don\u0026rsquo;t have expose their data via an API.\nI have discussed this package in a previous blog post, but have since worked on the code a bit, and thought it deserved a new post.\nYou can see a tutorial for this package here, and contribute to the code here.\nInstall rmetadata # install_github(\u0026#39;rmetadata\u0026#39;, \u0026#39;ropensci\u0026#39;) # uncomment to install library(rmetadata) Count OAI-PMH identifiers for a data provider. # For DataCite. count_identifiers(\u0026#34;datacite\u0026#34;) provider count 1 datacite 1216193 Lookup article info via CrossRef with DOI and get a citation. As Bibtex print(crossref_citation(\u0026#34;10.3998/3336451.0009.101\u0026#34;), style = \u0026#34;Bibtex\u0026#34;) @Article{, title = {In Google We Trust?}, author = {Geoffrey Bilder}, journal = {The Journal of Electronic Publishing}, year = {2006}, month = {01}, volume = {9}, doi = {10.3998/3336451.0009.101}, } As regular text print(crossref_citation(\u0026#34;10.3998/3336451.0009.101\u0026#34;), style = \u0026#34;text\u0026#34;) Bilder G (2006). \u0026#34;In Google We Trust?\u0026#34; _The Journal of Electronic Publishing_, *9*. \u0026lt;URL: http://dx.doi.org/10.3998/3336451.0009.101\u0026gt;. Search the CrossRef Metatdata for DOIs using free form references. Search with title, author, year, and journal crossref_search_free(query = \u0026#34;Piwowar Sharing Detailed Research Data Is Associated with Increased Citation Rate PLOS one 2007\u0026#34;) text 1 Piwowar Sharing Detailed Research Data Is Associated with Increased Citation Rate PLOS one 2007 match doi score 1 TRUE 10.1038/npre.2007.361 4.905 Get a DOI and get the citation using \\code{crossref_search} # Get a DOI for a paper doi \u0026lt;- crossref_search_free(query = \u0026#34;Piwowar sharing data PLOS one\u0026#34;)$doi # Get the metadata crossref_search(doi = doi)[, 1:3] doi score normalizedScore 1 10.1371/journal.pone.0000308 18.19 100 Get a random set of DOI\u0026rsquo;s through CrossRef. # Default search gets 20 random DOIs crossref_r() [1] \u0026#34;10.4028/www.scientific.net/MSF.126-128.467\u0026#34; [2] \u0026#34;10.2139/ssrn.548523\u0026#34; [3] \u0026#34;10.1016/S0012-821X(02)00562-9\u0026#34; [4] \u0026#34;10.1093/rsq/13.2-3.167\u0026#34; [5] \u0026#34;10.5772/55055\u0026#34; [6] \u0026#34;10.1515/BC.1999.050\u0026#34; [7] \u0026#34;10.1016/S0020-7292(98)90160-6\u0026#34; [8] \u0026#34;10.1111/j.1439-0418.1985.tb02788.x\u0026#34; [9] \u0026#34;10.1089/aid.2012.0115\u0026#34; [10] \u0026#34;10.1016/0002-9378(95)90155-8\u0026#34; [11] \u0026#34;10.1001/jama.1949.02900490055028\u0026#34; [12] \u0026#34;10.1051/jphyscol:1989172\u0026#34; [13] \u0026#34;10.1016/s0301-2115(03)00298-7\u0026#34; [14] \u0026#34;10.1007/BF02735292\u0026#34; [15] \u0026#34;10.1016/0003-4916(65)90026-6\u0026#34; [16] \u0026#34;10.4156/jdcta.vol5.issue5.12\u0026#34; [17] \u0026#34;10.1007/s10904-009-9316-2\u0026#34; [18] \u0026#34;10.1023/A:1021690001832\u0026#34; [19] \u0026#34;10.1007/s12262-012-0724-0\u0026#34; [20] \u0026#34;10.1007/bf02192860\u0026#34; # Specify you want journal articles only crossref_r(type = \u0026#34;journal_article\u0026#34;) [1] \u0026#34;10.1016/j.jacc.2011.09.055\u0026#34; [2] \u0026#34;10.1002/dev.420170603\u0026#34; [3] \u0026#34;10.4315/0362-028X.JFP-10-403\u0026#34; [4] \u0026#34;10.1016/S0925-4927(98)00016-X\u0026#34; [5] \u0026#34;10.1111/j.1933-1592.2002.tb00141.x\u0026#34; [6] \u0026#34;10.1541/ieejfms.127.629\u0026#34; [7] \u0026#34;10.5539/enrr.v3n1p62\u0026#34; [8] \u0026#34;10.1016/S0960-9776(96)90038-7\u0026#34; [9] \u0026#34;10.1016/0925-9635(94)05240-9\u0026#34; [10] \u0026#34;10.1016/s0929-693x(97)86846-7\u0026#34; [11] \u0026#34;10.1002/(SICI)1096-9071(199601)48:1\u0026lt;53::AID-JMV9\u0026gt;3.0.CO;2-K\u0026#34; [12] \u0026#34;10.1016/s0267-7261(01)00016-1\u0026#34; [13] \u0026#34;10.1111/j.1748-0361.2003.tb00575.x\u0026#34; [14] \u0026#34;10.1097/00005721-197701000-00011\u0026#34; [15] \u0026#34;10.1007/s00894-009-0593-z\u0026#34; [16] \u0026#34;10.1071/AR9830063\u0026#34; [17] \u0026#34;10.1186/gb-2009-10-4-r39\u0026#34; [18] \u0026#34;10.2165/00128415-201113540-00038\u0026#34; [19] \u0026#34;10.1007/BF00522986\u0026#34; [20] \u0026#34;10.1080/19407963.2011.539385\u0026#34; Search the CrossRef Metatdata API. # Search for two different query terms crossref_search(query = c(\u0026#34;renear\u0026#34;, \u0026#34;palmer\u0026#34;), rows = 4)[, 1:3] doi score normalizedScore 1 10.1126/science.1157784 3.253 100 2 10.1002/meet.2009.1450460141 2.169 66 3 10.4242/BalisageVol3.Renear01 2.102 64 4 10.4242/BalisageVol5.Renear01 2.102 64 # Get results for a certain year crossref_search(query = c(\u0026#34;renear\u0026#34;, \u0026#34;palmer\u0026#34;), year = 2010)[, 1:3] doi score normalizedScore 1 10.1002/meet.14504701218 1.0509 100 2 10.1002/meet.14504701240 1.0509 100 3 10.5270/OceanObs09.cwp.68 1.0442 99 4 10.1353/mpq.2010.0003 0.6890 65 5 10.1353/mpq.0.0041 0.6890 65 6 10.1353/mpq.0.0044 0.6890 65 7 10.1353/mpq.0.0057 0.6890 65 8 10.1386/fm.1.1.2 0.6890 65 9 10.1386/fm.1.2.2 0.6890 65 10 10.1386/fm.1.3.2 0.6890 65 11 10.1097/ALN.0b013e3181f09404 0.6090 57 12 10.1016/j.urology.2010.02.033 0.6090 57 13 10.1353/ect.2010.0025 0.6090 57 14 10.1117/2.4201001.04 0.6090 57 15 10.1111/j.1835-9310.1977.tb01159.x 0.6090 57 16 10.4067/S0717-69962010000100001 0.6090 57 17 10.4067/S0717-69962010000200001 0.6090 57 18 10.2105/AJPH.2009.191098 0.6029 57 19 10.1353/mpq.2010.0004 0.5167 49 20 10.1353/mpq.0.0048 0.5167 49 Get a short DOI from shortdoi.org. # Geta a short DOI, just the short DOI returned short_doi(doi = \u0026#34;10.1371/journal.pone.0042793\u0026#34;) [1] \u0026#34;10/f2bfz9\u0026#34; # Geta a short DOI, all data returned short_doi(doi = \u0026#34;10.1371/journal.pone.0042793\u0026#34;, justshort = FALSE) $DOI [1] \u0026#34;10.1371/journal.pone.0042793\u0026#34; $ShortDOI [1] \u0026#34;10/f2bfz9\u0026#34; $IsNew [1] FALSE Get a record from a OAI-PMH data provider. # Single provider, one identifier md_getrecord(provider = \u0026#34;pensoft\u0026#34;, identifier = \u0026#34;10.3897/zookeys.1.10\u0026#34;) title 1 A new candidate for a Gondwanaland distribution in the Zodariidae (Araneae): Australutica in Africa creator date type 1 Jocqué,Rudy 2008 Research Article # Single provider, multiple identifiers md_getrecord(provider = \u0026#34;pensoft\u0026#34;, identifier = c(\u0026#34;10.3897/zookeys.1.10\u0026#34;, \u0026#34;10.3897/zookeys.4.57\u0026#34;)) title 1 A new candidate for a Gondwanaland distribution in the Zodariidae (Araneae): Australutica in Africa 2 Studies of Tiger Beetles. CLXXVIII. A new Lophyra (Lophyra) from Somaliland (Coleoptera, Cicindelidae) creator date type 1 Jocqué,Rudy 2008 Research Article 2 Cassola,Fabio 2008 Research Article List available metadata formats from various providers. # List metadata formats for a provider md_listmetadataformats(provider = \u0026#34;dryad\u0026#34;) metadataPrefix 1 oai_dc 2 rdf 3 ore 4 mets schema 1 http://www.openarchives.org/OAI/2.0/oai_dc.xsd 2 http://www.openarchives.org/OAI/2.0/rdf.xsd 3 http://tweety.lanl.gov/public/schemas/2008-06/atom-tron.sch 4 http://www.loc.gov/standards/mets/mets.xsd metadataNamespace 1 http://www.openarchives.org/OAI/2.0/oai_dc/ 2 http://www.openarchives.org/OAI/2.0/rdf/ 3 http://www.w3.org/2005/Atom 4 http://www.loc.gov/METS/ # List metadata formats for a specific identifier for a provider md_listmetadataformats(provider = \u0026#34;pensoft\u0026#34;, identifier = \u0026#34;10.3897/zookeys.1.10\u0026#34;) identifier metadataPrefix 1 10.3897/zookeys.1.10 oai_dc 2 10.3897/zookeys.1.10 mods schema 1 http://www.openarchives.org/OAI/2.0/oai_dc.xsd 2 http://www.loc.gov/standards/mods/v3/mods-3-1.xsd metadataNamespace 1 http://www.openarchives.org/OAI/2.0/oai_dc/ 2 http://www.loc.gov/mods/v3 Some plotting - mean number of authors per paper Okay, so this isn\u0026rsquo;t a super useful visualization, but you can surely think of something better.\nlibrary(ggplot2) library(ggthemes) library(reshape) temp \u0026lt;- md_listrecords(provider = \u0026#34;pensoft\u0026#34;, from = \u0026#34;2011-10-01\u0026#34;, until = \u0026#34;2012-01-01\u0026#34;) temp2 \u0026lt;- ldply(temp)[, -1] auths \u0026lt;- sapply(temp2$creator, function(x) length(strsplit(as.character(x), \u0026#34;;\u0026#34;)[[1]])) toplot \u0026lt;- data.frame(authors = auths, articletype = temp2$type) toplot_ \u0026lt;- ddply(toplot, .(articletype), summarise, authors = mean(authors)) toplot_$articletype \u0026lt;- reorder(toplot_$articletype, toplot_$authors) ggplot(toplot_, aes(articletype, authors)) + theme_tufte(base_size = 16) + geom_bar(stat = \u0026#34;identity\u0026#34;) + coord_flip() Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr, and knitcitations.\n","permalink":"http://localhost:1313/2013/03/r-metadata/","summary":"\u003cp\u003eScholarly metadata - the meta-information surrounding articles - can be super useful.  Although metadata does not contain the full content of articles, it contains a lot of useful information, including title, authors, abstract, URL to the article, etc.\u003c/p\u003e\n\u003cp\u003eOne of the largest sources of metadata is provided via the Open Archives Initiative Protocol for Metadata Harvesting or \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html\"\u003eOAI-PMH\u003c/a\u003e. Many publishers, provide their metadata through their own endpoint, and implement the standard OAI-PMH methods: \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html#GetRecord\"\u003eGetRecord\u003c/a\u003e, \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html#Identify\"\u003eIdentify\u003c/a\u003e, \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html#ListIdentifiers\"\u003eListIdentifiers\u003c/a\u003e, \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html#ListMetadataFormats\"\u003eListMetadataFormats\u003c/a\u003e, \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html#ListRecords\"\u003eListRecords\u003c/a\u003e, and \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html#ListSets\"\u003eListSets\u003c/a\u003e. Many providers use OAI-PMH, including \u003ca href=\"http://oai.datacite.org/\"\u003eDataCite\u003c/a\u003e, \u003ca href=\"http://wiki.datadryad.org/Data_Access#OAI-PMH\"\u003eDryad\u003c/a\u003e, and \u003ca href=\"http://www.ncbi.nlm.nih.gov/pmc/tools/oai/\"\u003ePubMed\u003c/a\u003e.\u003c/p\u003e","title":"Scholarly metadata in R"},{"content":"We (rOpenSci) have been writing code for R packages for a couple years, so it is time to take a look back at the data. What data you ask? The commits data from GitHub ~ data that records who did what and when.\nUsing the Github commits API we can gather data on who commited code to a Github repository, and when they did it. Then we can visualize this hitorical record.\nInstall some functions for interacting with the Github API via R install_github(\u0026#39;sandbox\u0026#39;, \u0026#39;ropensci\u0026#39;) library(sandbox) library(httr) library(ggplot2) library(scales) library(reshape2) library(bipartite) library(doMC) library(plyr) library(ggthemes) library(picante) # And authenticate - pops open a page in your default browser, then tells # you authentication was successful github_auth() Get all repos for an organization, here ropensci of course ropensci_repos \u0026lt;- github_allrepos(userorg = \u0026#34;ropensci\u0026#34;) Get commits broken down in to additions and deletions, though below we just collapse them to all commits registerDoMC(cores = 4) github_commits_safe \u0026lt;- plyr::failwith(NULL, github_commits) out \u0026lt;- llply(ropensci_repos, function(x) github_commits_safe(\u0026#34;ropensci\u0026#34;, x, since = \u0026#34;2009-01-01T\u0026#34;, limit = 500), .parallel = TRUE) names(out) \u0026lt;- ropensci_repos out2 \u0026lt;- compact(out) outdf \u0026lt;- ldply(out2) Plot commits by date and repo outdf_subset \u0026lt;- outdf[!outdf$.id %in% c(\u0026#34;citeulike\u0026#34;, \u0026#34;challenge\u0026#34;, \u0026#34;docs\u0026#34;, \u0026#34;ropensci-book\u0026#34;, \u0026#34;usecases\u0026#34;, \u0026#34;textmine\u0026#34;, \u0026#34;usgs\u0026#34;, \u0026#34;ropenscitoolkit\u0026#34;, \u0026#34;neotoma\u0026#34;, \u0026#34;rEWDB\u0026#34;, \u0026#34;rgauges\u0026#34;, \u0026#34;rodash\u0026#34;, \u0026#34;ropensci.github.com\u0026#34;, \u0026#34;ROAuth\u0026#34;), ] outdf_subset$.id \u0026lt;- tolower(outdf_subset$.id) outdf_subset \u0026lt;- ddply(outdf_subset, .(.id, date), summarise, value = sum(value)) mindates \u0026lt;- llply(unique(outdf_subset$.id), function(x) min(outdf_subset[outdf_subset$.id == x, \u0026#34;date\u0026#34;])) names(mindates) \u0026lt;- unique(outdf_subset$.id) mindates \u0026lt;- sort(do.call(c, mindates)) outdf_subset$.id \u0026lt;- factor(outdf_subset$.id, levels = names(mindates)) ggplot(outdf_subset, aes(date, value, fill = .id)) + geom_bar(stat = \u0026#34;identity\u0026#34;, width = 0.5) + geom_rangeframe(sides = \u0026#34;b\u0026#34;, colour = \u0026#34;grey\u0026#34;) + theme_bw(base_size = 9) + scale_x_date(labels = date_format(\u0026#34;%Y\u0026#34;), breaks = date_breaks(\u0026#34;year\u0026#34;)) + scale_y_log10() + facet_grid(.id ~ .) + labs(x = \u0026#34;\u0026#34;, y = \u0026#34;\u0026#34;) + theme(axis.text.y = element_blank(), axis.text.x = element_text(colour = \u0026#34;black\u0026#34;), axis.ticks.y = element_blank(), strip.text.y = element_text(angle = 0, size = 8, ), strip.background = element_rect(size = 0), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.text = element_text(size = 8), legend.position = \u0026#34;none\u0026#34;, panel.border = element_blank()) The plot above plots the sum of additions+deletions, and is sorted by the first commit date of reach repo, with the first being treebase, which wraps the Treebase API, and the most recent being rwbclimate, which wraps the World Blank climate data API.\nYou can see that some repos have recieved commits more or less consistently over their life time, while others have seen a little development here and there. w\nIn addition, there are quite a few people that have committed code now to rOpenSci repos, calling for a network vizualization of course. outdf_network \u0026lt;- droplevels(outdf[!outdf$.id %in% c(\u0026#34;citeulike\u0026#34;, \u0026#34;challenge\u0026#34;, \u0026#34;docs\u0026#34;, \u0026#34;ropensci-book\u0026#34;, \u0026#34;usecases\u0026#34;, \u0026#34;textmine\u0026#34;, \u0026#34;usgs\u0026#34;, \u0026#34;ropenscitoolkit\u0026#34;, \u0026#34;retriever\u0026#34;, \u0026#34;rodash\u0026#34;, \u0026#34;ropensci.github.com\u0026#34;, \u0026#34;ROAuth\u0026#34;, \u0026#34;rgauges\u0026#34;, \u0026#34;sandbox\u0026#34;, \u0026#34;rfna\u0026#34;, \u0026#34;rmetadata\u0026#34;, \u0026#34;rhindawi\u0026#34;, \u0026#34;rpmc\u0026#34;, \u0026#34;rpensoft\u0026#34;, \u0026#34;ritis\u0026#34;), ]) casted \u0026lt;- dcast(outdf_network, .id + date + name ~ variable, fun.aggregate = length, value.var = \u0026#34;value\u0026#34;) names(casted)[1] \u0026lt;- \u0026#34;repo\u0026#34; casted2 \u0026lt;- ddply(casted, .(repo, name), summarise, commits = sum(additions)) casted2 \u0026lt;- data.frame(repo = casted2$repo, weight = casted2$commits, name = casted2$name) mat \u0026lt;- sample2matrix(casted2) plotweb(sortweb(mat, sort.order = \u0026#34;dec\u0026#34;), method = \u0026#34;normal\u0026#34;, text.rot = 90, adj.high = c(-0.3, 0), adj.low = c(1, -0.3), y.width.low = 0.05, y.width.high = 0.05, ybig = 0.09, labsize = 0.7) The plot above shows repos on one side and contributors on the other. Some folks (the core rOpenSci team: cboettig, karthikram, emhart, and schamberlain) have committed quite a lot to many packages. We also have amny awesome contributors to our packages (some contributors and repos have been removed for clarity).\nrOpenSci is truly a collaborative effort to develop tools for open science, so thanks to all our contributors - keep on forking, pull requesting, and commiting.\n","permalink":"http://localhost:1313/2013/03/ropensci-collaboration/","summary":"\u003cp\u003eWe (\u003ca href=\"http://ropensci.org/\"\u003erOpenSci\u003c/a\u003e) have been writing code for R packages for a couple years, so it is time to take a look back at the data. What data you ask? The commits data from GitHub ~ data that records who did what and when.\u003c/p\u003e\n\u003cp\u003eUsing the \u003ca href=\"http://developer.github.com/v3/repos/commits/\"\u003eGithub commits API\u003c/a\u003e we can gather data on who commited code to a Github repository, and when they did it. Then we can visualize this hitorical record.\u003c/p\u003e","title":"Visualizing rOpenSci collaboration"},{"content":"Reboot We need to reboot academia, at least for graduate training. I am speaking from the point of view of ecology/evolution (EEB). Why you ask? Because of the following line of reasoning:\nFirst, the most important factor for me comes down to supply and demand. We have too much supply (=graduate students) and not enough demand (=faculty positions, etc.) - see this comic at PhDComics for proof. This seems especially apparent when you hear from your fellow postdoc friends that there were hundreds of other people with Ph.D.\u0026rsquo;s applying for the same position.\nSecond, funding is getting thin. I have never received funding from a competitive grant, despite having 12 published papers to my name. Recent cuts to the NSF, NIH, and other federal agencies mean that getting a grant will be harder and harder. Furthermore, the mean age of a first time NIH grant recipient in 2008 was 51 according to a recent study in PLoS One (Matthews et. al. 2011).\nThird, we don\u0026rsquo;t learn the skills we really need. This is many fold. First, we don\u0026rsquo;t learn the appropriate mathematical and statistical techniques in undergraduate and grad school - a forthcoming paper found that in a survey of nearly 1000 ecology and evolution graduate students, most thought they were unprepared wrt to math and stats (interview with author in Soundcloud widget below). Second, we don\u0026rsquo;t learn enough computational skills. Digital data (not on your physical clipboard, but your digital one) is more and more important, requiring knowing how to leverage and keep track of data. Yet, we aren\u0026rsquo;t taught these skills, at least in my experience. The need for training in computation/coding is evident from the sold out Software Carpentry workshops. Third, reproducibility is not something we are taught. Well, we are taught to check over everything in detail (read: proof your data), but there is often no way to reproduce analyses when we use 10 different expensive software programs to do an analysis (read: MS Word, JMP, SAS, SigmaPlot, etc.). And isn\u0026rsquo;t reproduciblity important?\nWhat do we do? To address the supply/demand issue, I think we need fewer graduate students, period. I think this will work for a few reasons. If there are fewer graduate students, those that get in will be of higher quality because profs can be more selective, they may get payed more (hopefully) since there are few students, and they should in theory get more attention from their advisers (if they want it). In addition, there would be less competition for the very few grants out there for grad students. This would then lead to fewer postdocs, and less competition for faculty positions. I think the supply/demand issue in EEB is particularly problematic. That is, in EEB there doesn\u0026rsquo;t seem to be the large quantity of private sector jobs as there is for Ph.D. graduates in engineering, physics, etc.\nThe funding situation is beyond me, but definitely makes me want to leave academia. Crowdfunding, especially #SciFund, is an option for scientists, but mostly only on a small financial scale. Any thoughts?\nThe skills issue will likely be addressed in time, and vary among schools for sure. Some schools will focus on natural history, which is good (that\u0026rsquo;s where I did my undergrad and it was great), and some schools will incorporate more of these science 2.0 skills (advanced stats, better math training, and computer science).\nThoughts? Get the .Rmd file used to create this post at my github account - or .md file. Written in Markdown, with help from knitr, and knitcitations. References Matthews KRW, Calhoun KM, lo N, ho V and Germano G (2011). \u0026ldquo;The Aging of Biomedical Research in The United States.\u0026rdquo; Plos One, 6. http://dx.doi.org/10.1371/journal.pone.0029738. ","permalink":"http://localhost:1313/2013/02/academia-reboot/","summary":"\u003ch2 id=\"reboot\"\u003eReboot\u003c/h2\u003e\n\u003cp\u003eWe need to reboot academia, at least for graduate training. I am speaking from the point of view of ecology/evolution (EEB). Why you ask? Because of the following line of reasoning:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eFirst, the most important factor for me comes down to supply and demand. We have too much supply (=graduate students) and not enough demand (=faculty positions, etc.) - see \u003ca href=\"http://www.phdcomics.com/comics/archive.php?comicid=911\"\u003ethis comic at PhDComics\u003c/a\u003e for proof. This seems especially apparent when you hear from your fellow postdoc friends that there were hundreds of other people with Ph.D.\u0026rsquo;s applying for the same position.\u003c/p\u003e","title":"Academia reboot"},{"content":"I was just at the Phylotastic hackathon in Tucson, AZ at the iPlant facilities at the UofA.\nA problem that needs to be solved is getting the incrasingly vast phylogenetic information to folks not comfortable building their own phylogenies. Phylomatic has made this super easy for people that want plant phylogenies (at least 250 or so papers have used and cited Phylomatic in their papers) - however, there are few options for those that want phylogenies for other taxonomic groups.\nOne cool tool that was brought up was the Common Tree service provided by NCBI. Here\u0026rsquo;s some help on the service. Unlike Phylomatic, Common Tree is purely based off of taxonomic relationships (A and B are both in the C family, so are sisters), not an actual phylogeny as Phylomatic is based on.\nBut how do you use Common Tree?\nGet a species list Grab the taxon list from my github account here\nGo to the site Go to the Common Tree site here\nChoose file Hit the \u0026ldquo;choose file\u0026rdquo; button, then select the species.txt file you downloaded in the first step.\nAdd the species list to make the tree Then hit \u0026ldquo;add from file\u0026rdquo;, and you got a \u0026ldquo;tree\u0026rdquo;\nDownload You can download the tree in a variety of formats, including a .phy file\nPlot the tree on your machine Make a tree, in R for me\n# install.packages(\u0026#39;ape\u0026#39;) # install if you don\u0026#39;t have ape library(ape) # Read the tree in. YOu get the tree back with alot of newlines (\\n) - # can easily take these out with a good text editor. tree \u0026lt;- read.tree(text = \u0026#34;(Lampetra:4,((((((Umbra:4,((Lota:4,Microgadus:4)Gadiformes:4,((Culaea:4,Apeltes:4,Pungitius:4,Gasterosteus:4)Gasterosteidae:4,(Morone:4,(Ambloplites:4,Micropterus:4,Lepomis:4)Centrarchidae:4,(Sander:4,Perca:4)Percidae:4)Percoidei:4,Cottus:4)Percomorpha:4)Holacanthopterygii:4)Neognathi:4,(((Prosopium:4,Coregonus:4)Coregoninae:4,(Salvelinus:4,Salmo:4,Oncorhynchus:4)Salmoninae:4)Salmonidae:4,Osmerus:4)Protacanthopterygii:4)Euteleostei:4,(Alosa:4,(Ameiurus:4,(Catostomus:4,(Semotilus:4,Rhinichthys:4,Margariscus:4,Couesius:4,Pimephales:4,Luxilus:4,Notemigonus:4,Notropis:4,Carassius:4)Cyprinidae:4)Cypriniformes:4)Otophysi:4)Otocephala:4)Clupeocephala:4,Anguilla:4)Elopocephala:4,Acipenser:4)Actinopteri:4,Scyliorhinus:4)Gnathostomata:4)Vertebrata:4;\u0026#34;) # stretch the branches so tips line up tree2 \u0026lt;- compute.brlen(tree, method = \u0026#34;Grafen\u0026#34;) # Plot the tree plot(tree2, no.margin = TRUE, cex = 0.7) w00p, there it is\u0026hellip; And the answer is NO to the question: Is there an API for Common Tree?\nGet the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2013/02/common-tree/","summary":"\u003cp\u003eI was just at the \u003ca href=\"http://www.evoio.org/wiki/Phylotastic\"\u003ePhylotastic hackathon\u003c/a\u003e in Tucson, AZ at the \u003ca href=\"http://www.iplantcollaborative.org/\"\u003eiPlant\u003c/a\u003e facilities at the UofA.\u003c/p\u003e\n\u003cp\u003eA problem that needs to be solved is getting the incrasingly vast phylogenetic information to folks not comfortable building their own phylogenies. \u003ca href=\"http://phylodiversity.net/phylomatic/\"\u003ePhylomatic\u003c/a\u003e has made this super easy for people that want plant phylogenies (at least 250 or so papers have used and cited Phylomatic in their papers) - however, there are few options for those that want phylogenies for other taxonomic groups.\u003c/p\u003e","title":"Getting a simple tree via NCBI"},{"content":"testing ifttt recipe\n","permalink":"http://localhost:1313/2013/01/ifttt-test/","summary":"\u003cp\u003etesting ifttt recipe\u003c/p\u003e","title":"testing ifttt recipe, ignore"},{"content":"Dealing with API tokens in R In my previous post I showed an example of calling the Phylotastic taxonomic name resolution API Taxosaurus here. When you query their API they give you a token which you use later to retrieve the result (see examples on their page above). However, you don\u0026rsquo;t know when the query will be done, so how do we know when to send the query to rerieve the data?\nAs the time this takes depends on how big the query is and other things, we don\u0026rsquo;t know when we can get the result. I struggled with this for a bit, but then settled on using a while loop.\nSo what does this look like? Basically we just keep sending the request for data until we get it.\niter \u0026lt;- 0 # make an iterator so each time we call output \u0026lt;- list() # make an empty list to put data into timeout \u0026lt;- \u0026#34;wait\u0026#34; while (timeout == \u0026#34;wait\u0026#34;) { iter \u0026lt;- iter + 1 # increase the iterator each time temp \u0026lt;- fromJSON(getURL(retrieve)) # send the request and parse the JSON if (grepl(\u0026#34;is still being processed\u0026#34;, temp[\u0026#34;message\u0026#34;]) == TRUE) { timeout \u0026lt;- \u0026#34;wait\u0026#34; } else { output[[iter]] \u0026lt;- temp # put result from query in the list timeout \u0026lt;- \u0026#34;done\u0026#34; # we got the result so timeout is now done, making the while loop stop } } Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2013/01/api-token/","summary":"\u003ch3 id=\"dealing-with-api-tokens-in-r\"\u003eDealing with API tokens in R\u003c/h3\u003e\n\u003cp\u003eIn \u003ca href=\"/posts/2013-01-25-tnrs-use-case/\"\u003emy previous post\u003c/a\u003e I showed an example of calling the Phylotastic taxonomic name resolution API \u003ccode\u003eTaxosaurus\u003c/code\u003e \u003ca href=\"https://api.phylotastic.org/tnrs\"\u003ehere\u003c/a\u003e.  When you query their API they give you a token which you use later to retrieve the result (see examples on their page above). However, you don\u0026rsquo;t know when the query will be done, so how do we know when to send the query to rerieve the data?\u003c/p\u003e","title":"Waiting for an API request to complete"},{"content":"taxize use case: Resolving species names when you have a lot of them Species names can be a pain in the ass, especially if you are an ecologist. We ecologists aren\u0026rsquo;t trained in taxonomy, yet we often end up with huge species lists. Of course we want to correct any spelling errors in the names, and get the newest names for our species, resolve any synonyms, etc.\nWe are building tools into our R package taxize, that will let you check your species names to make sure they are correct.\nAn important use case is when you have a lot of species. Someone wrote to us recently, saying that they had thousands of species, and they wanted to know how to check their species names efficiently in R.\nBelow is an example of how to do this.\nInstall taxize # install_github(\u0026#39;taxize_\u0026#39;, \u0026#39;ropensci\u0026#39;) # install the GitHub version, not # the CRAN version, uncomment if you don\u0026#39;t have it installed library(taxize) Get some species, in this case all species in the Scrophulariaceae family from theplantlist.org tpl_get(dir_ = \u0026#34;~/foo2\u0026#34;, family = \u0026#34;Scrophulariaceae\u0026#34;) ## Reading and writing csv files to ~/foo2... dat \u0026lt;- read.csv(\u0026#34;~/foo2/Scrophulariaceae.csv\u0026#34;) Lets grab the species and concatenate to genus_species species \u0026lt;- as.character(ddply(dat[, c(\u0026#34;Genus\u0026#34;, \u0026#34;Species\u0026#34;)], .(), transform, gen_sp = as.factor(paste(Genus, Species, sep = \u0026#34; \u0026#34;)))[, 4]) It\u0026rsquo;s better to do many smaller calls to a web API instead of few big ones to be nice to the database maintainers. ## Define function to split up your species list into useable chuncks slice \u0026lt;- function(input, by = 2) { starts \u0026lt;- seq(1, length(input), by) tt \u0026lt;- lapply(starts, function(y) input[y:(y + (by - 1))]) llply(tt, function(x) x[!is.na(x)]) } species_split \u0026lt;- slice(species, by = 100) Query for your large species list with pauses between calls, with 3 seconds in between calls to not hit the web service too hard. Using POST method here instead of GET - required when you have a lot of species. tnrs_safe \u0026lt;- failwith(NULL, tnrs) # in case some calls fail, will continue out \u0026lt;- llply(species_split, function(x) tnrs_safe(x, getpost = \u0026#34;POST\u0026#34;, sleep = 3)) Calling http://taxosaurus.org/retrieve/90fcd9ae425ad7c6103b06dd9fd78ae2 Calling http://taxosaurus.org/retrieve/223f73b83fcddcb8b6187966963660a8 Calling http://taxosaurus.org/retrieve/72bacdbb8938316e321d4c709c8cdd09 Calling http://taxosaurus.org/retrieve/979ce9cc4dec376710f61de162e1294e Calling http://taxosaurus.org/retrieve/03a39a124561fec2fdfc0f483d9fb607 Calling http://taxosaurus.org/retrieve/d4bf4e5a1403f45a1be1ca3dd87785d7 Calling http://taxosaurus.org/retrieve/a9a9bdde6fda7e325d80120e27ccb480 Calling http://taxosaurus.org/retrieve/215ccdcf2b00362278bf19d1942e1395 Calling http://taxosaurus.org/retrieve/9d43c0b99b4dfb5ea1b435adab17b980 Calling http://taxosaurus.org/retrieve/42e166f8e43f1fb349e36459cd5938b3 Calling http://taxosaurus.org/retrieve/2c42e4b5227c5464f9bfeeafcdf0651d # Looks like we got some data back for each element of our species list lapply(out, head)[1:2] # just look at the first two [[1]] submittedName acceptedName sourceId 1 Aptosimum welwitschii iPlant_TNRS 2 Anticharis ebracteata Anticharis ebracteata iPlant_TNRS 3 Aptosimum lineare Aptosimum lineare iPlant_TNRS 4 Antherothamnus pearsonii Antherothamnus pearsonii iPlant_TNRS 5 Barthlottia madagascariensis Barthlottia madagascariensis iPlant_TNRS 6 Agathelpis mucronata iPlant_TNRS score matchedName annotations 1 1 Aptosimum welwitschii 2 1 Anticharis ebracteata Schinz 3 1 Aptosimum lineare Marloth \u0026amp; Engl. 4 1 Antherothamnus pearsonii N.E. Br. 5 1 Barthlottia madagascariensis Eb. Fisch. 6 1 Agathelpis mucronata uri 1 2 http://www.tropicos.org/Name/29202501 3 http://www.tropicos.org/Name/29202525 4 http://www.tropicos.org/Name/29202728 5 http://www.tropicos.org/Name/50089700 6 [[2]] submittedName acceptedName sourceId 1 Buddleja pichinchensis x bullata Buddleja pichinchensis iPlant_TNRS 2 Buddleja soratae Buddleja soratae iPlant_TNRS 3 Buddleja euryphylla Buddleja euryphylla iPlant_TNRS 4 Buddleja incana Buddleja incana iPlant_TNRS 5 Buddleja incana Incana NCBI 6 Buddleja nana Buddleja brachystachya iPlant_TNRS score matchedName annotations 1 0.9 Buddleja pichinchensis Kunth 2 1.0 Buddleja soratae Kraenzl. 3 1.0 Buddleja euryphylla Standl. \u0026amp; Steyerm. 4 1.0 Buddleja incana Ruiz \u0026amp; Pav. 5 1.0 Buddleja incana none 6 1.0 Buddleja nana Diels uri 1 http://www.tropicos.org/Name/19000333 2 http://www.tropicos.org/Name/19001018 3 http://www.tropicos.org/Name/19000790 4 http://www.tropicos.org/Name/19000596 5 http://www.ncbi.nlm.nih.gov/taxonomy/405077 6 http://www.tropicos.org/Name/19001133 # Now we can put them back together as so into one data.frame if you like outdf \u0026lt;- ldply(out) head(outdf) submittedName acceptedName sourceId 1 Aptosimum welwitschii iPlant_TNRS 2 Anticharis ebracteata Anticharis ebracteata iPlant_TNRS 3 Aptosimum lineare Aptosimum lineare iPlant_TNRS 4 Antherothamnus pearsonii Antherothamnus pearsonii iPlant_TNRS 5 Barthlottia madagascariensis Barthlottia madagascariensis iPlant_TNRS 6 Agathelpis mucronata iPlant_TNRS score matchedName annotations 1 1 Aptosimum welwitschii 2 1 Anticharis ebracteata Schinz 3 1 Aptosimum lineare Marloth \u0026amp; Engl. 4 1 Antherothamnus pearsonii N.E. Br. 5 1 Barthlottia madagascariensis Eb. Fisch. 6 1 Agathelpis mucronata uri 1 2 http://www.tropicos.org/Name/29202501 3 http://www.tropicos.org/Name/29202525 4 http://www.tropicos.org/Name/29202728 5 http://www.tropicos.org/Name/50089700 6 Note that there are multiple names for some species because data sources have different names for the same species (resulting in more than one row in the data.frame \u0026lsquo;outdf\u0026rsquo; for a species). We are leaving this up to the user to decide which to use. For example, for the species Buddleja montana there are two names for in the output\ndata \u0026lt;- ddply(outdf, .(submittedName), summarize, length(submittedName)) outdf[outdf$submittedName %in% as.character(data[data$..1 \u0026gt; 1, ][6, \u0026#34;submittedName\u0026#34;]), ] submittedName acceptedName sourceId score matchedName 123 Buddleja montana Buddleja montana iPlant_TNRS 1 Buddleja montana 124 Buddleja montana Montana NCBI 1 Buddleja montana annotations uri 123 Britton ex Rusby http://www.tropicos.org/Name/19000601 124 none http://www.ncbi.nlm.nih.gov/taxonomy/441235 The source iPlant matched the name, but NCBI actually gave back a genus of cricket (follow the link under the column uri for Montana). If you look at the page for Buddleja on NCBI here there is no Buddleja montana at all.\nAnother thing we could do is look at the score that is returned. Let\u0026rsquo;s look at those that are less than 1 (i.e., )\noutdf[outdf$score \u0026lt; 1, ] submittedName acceptedName sourceId 94 Buddleja pichinchensis x bullata Buddleja pichinchensis iPlant_TNRS 340 Diascia ellaphieae iPlant_TNRS 495 Eremophila decipiens iPlant_TNRS 500 Eremophila grandiflora Eremophila iPlant_TNRS 808 Jamesbrittneia hilliard iPlant_TNRS 1051 Verbascum patris Verbascum iPlant_TNRS 1081 Verbascum barnadesii Verbascum iPlant_TNRS 1097 Verbascum calycosum Verbascum iPlant_TNRS score matchedName annotations 94 0.90 Buddleja pichinchensis Kunth 340 0.98 Diascia ellaphiae 495 0.98 Eremophila decipiense 500 0.50 Eremophila R. Br. 808 0.50 Jamesbrittenia 1051 0.50 Verbascum L. 1081 0.50 Verbascum L. 1097 0.50 Verbascum L. uri 94 http://www.tropicos.org/Name/19000333 340 495 500 http://www.tropicos.org/Name/40004761 808 1051 http://www.tropicos.org/Name/40023766 1081 http://www.tropicos.org/Name/40023766 1097 http://www.tropicos.org/Name/40023766 As we got this speies list from theplantlist.org, there aren\u0026rsquo;t that many mistakes, but if it was my species list you know there would be many :)\nThat\u0026rsquo;s it. Try it out and let us know if you have any questions at info@ropensci.org, or ask questions/report problems at GitHub.\nGet the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2013/01/tnrs-use-case/","summary":"\u003ch3 id=\"__taxize-use-case-resolving-species-names-when-you-have-a-lot-of-them__\"\u003e\u003cstrong\u003etaxize use case: Resolving species names when you have a lot of them\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eSpecies names can be a pain in the ass, especially if you are an ecologist. We ecologists aren\u0026rsquo;t trained in taxonomy, yet we often end up with huge species lists.  Of course we want to correct any spelling errors in the names, and get the newest names for our species, resolve any synonyms, etc.\u003c/p\u003e\n\u003cp\u003eWe are building tools into our R package \u003ca href=\"http://ropensci.github.com/taxize_/\"\u003e\u003ccode\u003etaxize\u003c/code\u003e\u003c/a\u003e, that will let you check your species names to make sure they are correct.\u003c/p\u003e","title":"Resolving species names when you have a lot of them"},{"content":"\nOpen Science Science is becoming more open in many areas: publishing, data sharing, lab notebooks, and software. There are many benefits to open science. For example, sharing research data alongside your publications leads to increased citation rate (Piwowar et. al. 2007). In addition, data is becoming easier to share and reuse thanks to efforts like FigShare and Dryad.\nIf you don\u0026rsquo;t understand the problem we are currently facing due to lack of open science, watch this video:\nI just want Data Another way to look at this challenge is to think about how you can get data more easily. Right now you probably go to a website that has an interface to a database. You do a search, and then download a .csv file perhaps. Then you open it in Excel, and do some pivot tables to get the data in the right format. Only then will you bring the data in to R.\nThe advantage of using our packages is that they allow you to do that data collection part in a few lines of code. Therefore, you can easily do all those steps in the above paragraph using a few lines of code in one R file. Why does this matter? You can more easily reproduce your own work months later after that summer vacation. In addition, others can reproduce your research more easily.\nThe challenge We (ropensci) have just kicked off the rOpenSci Open Science Challenge. If you aren\u0026rsquo;t familiar with rOpenSci, it is a software collective connecting scientists to open science data on the web. Since R is the most popular programming language for life scientists, it made sense to do this in R (instead of Python e.g.).\nWhat is the challenge about? At rOpenSci, we create R software to make getting open source text from publications and open source data easy. An important result of this is that we are facilitating open science. Why? Because R is an open source programming language, and all of our software is open source. . This challenge asks you to propose a project using one or more of our packages - or perhaps you want to propose a new dataset to connect to R. The rOpenSci core developer team will help you with any problems using our packages, and attempt to modify packages according to feedback from participants. Do you use one or more of our R packages? If you do, great. If not, check out our packages here.\nHow to apply Just send an email to [info@ropensci.org](mailto:info@ropensci.org?subject=rOpenSci Open Science Challenge).\nThe deadline January 31, 2013\nGet the .Rmd file used to create this post at my github account - or .md file. Written in Markdown, with help from knitr, and knitcitations from Carl Boettiger. References Piwowar HA, Day RS, Fridsma DB and Ioannidis J (2007). \u0026ldquo;Sharing Detailed Research Data is Associated With Increased Citation Rate.\u0026rdquo; Plos One, 2. http://dx.doi.org/10.1371/journal.pone.0000308. ","permalink":"http://localhost:1313/2013/01/open-science-challenge/","summary":"\u003cp\u003e\u003cimg alt=\"center\" loading=\"lazy\" src=\"https://raw.github.com/sckott/sckott.github.com/master/public/img/ropensci_challenge.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"__open-science__\"\u003e\u003cstrong\u003eOpen Science\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eScience is becoming more open in many areas: publishing, data sharing, lab notebooks, and software. There are many benefits to open science. For example, sharing research data alongside your publications leads to increased citation rate (Piwowar \u003cem\u003eet. al.\u003c/em\u003e 2007). In addition, data is becoming easier to share and reuse thanks to efforts like \u003ca href=\"http://figshare.com/\"\u003eFigShare\u003c/a\u003e and \u003ca href=\"http://datadryad.org/\"\u003eDryad\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIf you don\u0026rsquo;t understand the problem we are currently facing due to lack of open science, watch this video:\u003c/p\u003e","title":"Open Science Challenge"},{"content":"The Global Invasive Species Database (GISD) (see their website for more info here) has data on the invasiveness status of many species. From taxize you can now query the GISD database.\nIntroducing the function gisd_isinvasive. This function was contributed to taxize by Ignasi Bartomeus, a postdoc at the Swedish University Agricultural Sciences.\nThere are two possible outputs from using gisd_isinvasive: \u0026ldquo;Invasive\u0026rdquo; or \u0026ldquo;Not in GISD\u0026rdquo;. If you use simplify=TRUE in the function you get \u0026ldquo;Invasive\u0026rdquo; or \u0026ldquo;Not in GISD\u0026rdquo;, but if you use simplify=FALSE you get verbose description of the invasive species instead of just \u0026ldquo;Invasive\u0026rdquo; (and you still just get \u0026ldquo;Not in GISD\u0026rdquo;).\nInstall taxize from GitHub # install_github(\u0026#39;taxize_\u0026#39;, \u0026#39;ropensci\u0026#39;) # install if you don\u0026#39;t already # have the GitHub version library(taxize) Make a vector of species sp \u0026lt;- c(\u0026#34;Carpobrotus edulis\u0026#34;, \u0026#34;Rosmarinus officinalis\u0026#34;, \u0026#34;Nasua nasua\u0026#34;, \u0026#34;Martes melampus\u0026#34;, \u0026#34;Centaurea solstitialis\u0026#34;) Using the function gisd_isinvasive you can query the GISD database for the invasiveness status of your species, at least according to GISD. Calling gisd_isinvasive with the second parameter set to default simplify=FALSE, you get verbose output, with details on the species. gisd_isinvasive(sp) Checking species 1 Checking species 2 Checking species 3 Checking species 4 Checking species 5 Done species 1 Carpobrotus edulis 2 Rosmarinus officinalis 3 Nasua nasua 4 Martes melampus 5 Centaurea solstitialis status 1 You searched for invasive species named Carpobrotus edulis: 1. Carpobrotus edulis (succulent) Carpobrotus edulis is a mat-forming succulent native to South Africa which is invasive primarily in coastal habitats in many parts of the world. It was often introduced as an ornamental plant or used for planting along roadsides, from which it has spread to become invasive. Its main impacts are smothering, reduced regeneration of native flora and changes to soil pH and nutrient regimes.\\r\\nCommon Names: balsamo, Cape fig, figue marine, freeway iceplant, ghaukum, ghoenavy, highway ice plant, higo del Cabo, higo marino, Hottentosvy, hottentot fig, Hottentottenfeige, iceplant, ikhambi-lamabulawo, Kaapsevy, patata frita, perdevy, pigface, rankvy, sea fig, sour fig, suurvy, umgongozi, vyerank\\r\\nSynonyms: Mesembryanthemum edule L., Mesembryanthemum edulis 2 Not in GISD 3 You searched for invasive species named Nasua nasua:1. Nasua nasua (mammal) Interim profile, incomplete informationCommon Names: Achuni, Coatí, South American Coati, Tejón 4 You searched for invasive species named Martes melampus:1. Martes melampus (mammal) Interim profile, incomplete informationCommon Names: Japanese Marten, Tsushima Island Marten 5 You searched for invasive species named Centaurea solstitialis: 1. Centaurea solstitialis (herb) Centaurea solstitialis is a winter annual that can form dense impenetrable stands that displace desirable vegetation in natural areas, rangelands, and other places. It is best adapted to open grasslands with deep, well-drained soils and an annual precipitation range of 25 to 150cm per year. It is intolerant of shade. Although populations can occur at elevations as high as 2,400 m, most large infestations are found below 1,500 m. Human activities are the primary mechanisms for the long distance movement of C. solstitialis seed. The short, stiff, pappus bristles are covered with barbs that readily adhere to clothing, hair, and fur. The movement of contaminated hay and uncertified seed are also important long distance transportation mechanisms. Wind disperses seeds over short distances.\\r\\nCommon Names: geeldissel, golden star thistle, sonnwend-Flockenblume, St. Barnaby\u0026#39;s thistle, yellow centaury, yellow cockspur, yellow star thistle\\r\\nSynonyms: Leucantha solstitialis (L.) A.\u0026amp; D. Löve Simpler output, just the invasive status. gisd_isinvasive(sp, simplify = TRUE) Checking species 1 Checking species 2 Checking species 3 Checking species 4 Checking species 5 Done species status 1 Carpobrotus edulis Invasive 2 Rosmarinus officinalis Not in GISD 3 Nasua nasua Invasive 4 Martes melampus Invasive 5 Centaurea solstitialis Invasive Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/12/is-invasive/","summary":"\u003cp\u003eThe Global Invasive Species Database (GISD) (see their website for more info \u003ca href=\"http://www.issg.org/database/welcome/\"\u003ehere\u003c/a\u003e) has data on the invasiveness status of many species. From \u003ccode\u003etaxize\u003c/code\u003e you can now query the GISD database.\u003c/p\u003e\n\u003cp\u003eIntroducing the function \u003ccode\u003egisd_isinvasive\u003c/code\u003e. This function was contributed to \u003ccode\u003etaxize\u003c/code\u003e by \u003ca href=\"http://www.bartomeus.cat/es/ignasi/\"\u003eIgnasi Bartomeus\u003c/a\u003e, a postdoc at the Swedish University Agricultural Sciences.\u003c/p\u003e\n\u003cp\u003eThere are two possible outputs from using \u003ccode\u003egisd_isinvasive\u003c/code\u003e: \u0026ldquo;Invasive\u0026rdquo; or \u0026ldquo;Not in GISD\u0026rdquo;. If you use \u003ccode\u003esimplify=TRUE\u003c/code\u003e in the function you get \u0026ldquo;Invasive\u0026rdquo; or \u0026ldquo;Not in GISD\u0026rdquo;, but if you use \u003ccode\u003esimplify=FALSE\u003c/code\u003e you get verbose description of the invasive species instead of just \u0026ldquo;Invasive\u0026rdquo; (and you still just get \u0026ldquo;Not in GISD\u0026rdquo;).\u003c/p\u003e","title":"Is invasive?"},{"content":"RStudio has a new product called Shiny that, quoting from their website, \u0026ldquo;makes it super simple for R users like you to turn analyses into interactive web applications that anyone can use\u0026rdquo;. See here for more information.\nA Shiny basically consists of two files: a ui.r file and a server.r file. The ui.r file, as it says, provides the user interface, and the server.r file provides the the server logic.\nBelow is what it looks like in the wild (on a browser).\nIt was pretty easy (for Ted Hart of rOpenSci) to build this app to demonstrate output from the ropensci rgbif package.\nYou may need to install packages first. install.packages(c(\u0026#34;shiny\u0026#34;, \u0026#34;ggplot2\u0026#34;, \u0026#34;plyr\u0026#34;, \u0026#34;rgbif\u0026#34;)) We tried to build in making real time API calls to GBIF\u0026rsquo;s servers, but the calls took too long for web speed. So we prepare the data first, and then serve it up from saved data in a .rda file. Let\u0026rsquo;s first prepare the data. \u0026ndash;Well, this is what we do on the app itself, but see the next code block for library(rgbif) splist \u0026lt;- c(\u0026#34;Accipiter erythronemius\u0026#34;, \u0026#34;Junco hyemalis\u0026#34;, \u0026#34;Aix sponsa\u0026#34;, \u0026#34;Haliaeetus leucocephalus\u0026#34;, \u0026#34;Corvus corone\u0026#34;, \u0026#34;Threskiornis molucca\u0026#34;, \u0026#34;Merops malimbicus\u0026#34;) out \u0026lt;- llply(splist, function(x) occurrencelist(x, coordinatestatus = T, maxresults = 100)) names(out) \u0026lt;- splist # name each data.frame with the species names setwd(\u0026#34;~/ShinyApps/rgbif2\u0026#34;) # set directory save(out, file = \u0026#34;speciesdata.rda\u0026#34;) # save the list of data.frames into an .rda file to serve up Here\u0026rsquo;s the server logic library(shiny) library(plyr) library(ggplot2) library(rgbif) ## Set up server output shinyServer(function(input, output) { load(\u0026#34;speciesdata.rda\u0026#34;) # define function for server plot output output$gbifplot \u0026lt;- reactivePlot(function() { species \u0026lt;- input$spec df \u0026lt;- out[names(out) %in% species] print(gbifmap(df)) }) output$cbt \u0026lt;- reactiveText(function() { }) }) The user interface library(shiny) # Define UI for application that plots random distributions shinyUI(pageWithSidebar(headerPanel(\u0026#34;rgbif example\u0026#34;), sidebarPanel(checkboxGroupInput(\u0026#34;spec\u0026#34;, \u0026#34;Species to map:\u0026#34;, c(`Sharp shinned hawk (Accipiter erythronemius)` = \u0026#34;Accipiter erythronemius\u0026#34;, `Dark eyed junco (Junco hyemalis)` = \u0026#34;Junco hyemalis\u0026#34;, `Wood duck (Aix sponsa)` = \u0026#34;Aix sponsa\u0026#34;, `Bald eagle (Haliaeetus leucocephalus)` = \u0026#34;Haliaeetus leucocephalus\u0026#34;, `Carrion crow (Corvus corone)` = \u0026#34;Corvus corone\u0026#34;, `Australian White Ibis (Threskiornis molucca)` = \u0026#34;Threskiornis molucca\u0026#34;, `Rosy Bee-eater (Merops malimbicus)` = \u0026#34;Merops malimbicus\u0026#34;), selected = c(\u0026#34;Bald eagle (Haliaeetus leucocephalus)\u0026#34;))), mainPanel(h5(\u0026#34;A map of your selected species: Please note that GBIF is queried for every selection so loading times vary\u0026#34;), plotOutput(\u0026#34;gbifplot\u0026#34;)))) This should be all you need. To actually serve up the app in the web, request to be part of their beta-test of Shiny server on the web here.\nGo play with our Shiny app here to see the kind of visualization you can do with the rgbif package.\nGet the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/12/shiny-r/","summary":"\u003cp\u003eRStudio has a new product called \u003ccode\u003eShiny\u003c/code\u003e that, quoting from their website, \u0026ldquo;makes it super simple for R users like you to turn analyses into interactive web applications that anyone can use\u0026rdquo;. \u003ca href=\"http://www.rstudio.com/shiny/\"\u003eSee here\u003c/a\u003e for more information.\u003c/p\u003e\n\u003cp\u003eA \u003ccode\u003eShiny\u003c/code\u003e basically consists of two files: a \u003ccode\u003eui.r\u003c/code\u003e file and a \u003ccode\u003eserver.r\u003c/code\u003e file.  The \u003ccode\u003eui.r\u003c/code\u003e file, as it says, provides the user interface, and the \u003ccode\u003eserver.r\u003c/code\u003e file provides the the server logic.\u003c/p\u003e\n\u003cp\u003eBelow is what it looks like in the wild (on a browser).\u003c/p\u003e","title":"Shiny apps are awesome"},{"content":" UPDATE: there were some errors in the tests for taxize, so the binaries aren\u0026rsquo;t avaiable yet. You can install from source though, see below.\nGetting taxonomic information for the set of species you are studying can be a pain in the ass. You have to manually type, or paste in, your species one-by-one. Or, if you are lucky, there is a web service in which you can upload a list of species. Encyclopedia of Life (EOL) has a service where you can do this here. But is this reproducible? No.\nGetting your taxonomic information for your species can now be done programatically in R. Do you want to get taxonomic information from ITIS. We got that. Tropicos? We got that too. uBio? No worries, we got that. What about theplantlist.org? Yep, got that. Encyclopedia of Life? Indeed. What about getting sequence data for a taxon? Oh hell yeah, you can get sequences available for a taxon across all genes, or get all records for a taxon for a specific gene.\nOf course this is all possible because these data providers have open APIs so that we can facilitate your computer talking to their database. Fun!\nWhy get your taxonomic data programatically? Because it\u0026rsquo;s 1) faster than by hand in web sites/looking up in books, 2) reproducible, especially if you share your code (damnit!), and 3) you can easily mash up your new taxonomic data to get sequences to build a phylogeny, etc.\nI\u0026rsquo;ll give a few examples of using taxize based around use cases, that is, stuff someone might actually do instead of what particular functions do.\nInstall packages. You can get from CRAN or GitHub. # install.packages(\u0026#34;ritis\u0026#34;) # uncomment if not already installed # install_github(\u0026#39;taxize_\u0026#39;, \u0026#39;ropensci\u0026#39;) # uncomment if not already installed # install.packages(\u0026#34;taxize\u0026#34;, type=\u0026#34;source\u0026#34;) # uncomment if not already installed library(ritis) library(taxize) Attach family names to a list of species. I often have a list of species that I studied and simply want to get their family names to, for example, make a table for the paper I\u0026rsquo;m writing. # For one species itis_name(query = \u0026#34;Poa annua\u0026#34;, get = \u0026#34;family\u0026#34;) Retrieving data for species \u0026#39; Poa annua \u0026#39; [1] \u0026#34;Poaceae\u0026#34; # For many species species \u0026lt;- c(\u0026#34;Poa annua\u0026#34;, \u0026#34;Abies procera\u0026#34;, \u0026#34;Helianthus annuus\u0026#34;, \u0026#34;Coffea arabica\u0026#34;) famnames \u0026lt;- sapply(species, itis_name, get = \u0026#34;family\u0026#34;, USE.NAMES = F) Retrieving data for species \u0026#39; Poa annua \u0026#39; Retrieving data for species \u0026#39; Abies procera \u0026#39; Retrieving data for species \u0026#39; Helianthus annuus \u0026#39; Retrieving data for species \u0026#39; Coffea arabica \u0026#39; data.frame(species = species, family = famnames) species family 1 Poa annua Poaceae 2 Abies procera Pinaceae 3 Helianthus annuus Asteraceae 4 Coffea arabica Rubiaceae Resolve taxonomic names. This is a common use case for ecologists/evolutionary biologists, or at least should be. That is, species names you have for your own data, or when using other\u0026rsquo;s data, could be old names - and if you need the newest names for your species list, how can you make this as painless as possible? You can query taxonomic data from many different sources with taxize. # The iPlantCollaborative provides access via API to their taxonomic name # resolution service (TNRS) mynames \u0026lt;- c(\u0026#34;shorea robusta\u0026#34;, \u0026#34;pandanus patina\u0026#34;, \u0026#34;oryza sativa\u0026#34;, \u0026#34;durio zibethinus\u0026#34;, \u0026#34;rubus ulmifolius\u0026#34;, \u0026#34;asclepias curassavica\u0026#34;, \u0026#34;pistacia lentiscus\u0026#34;) iplant_tnrsmatch(retrieve = \u0026#34;all\u0026#34;, taxnames = c(\u0026#34;helianthus annuus\u0026#34;, \u0026#34;acacia\u0026#34;, \u0026#34;gossypium\u0026#34;), output = \u0026#34;names\u0026#34;) AcceptedName MatchFam MatchGenus MatchScore Accept? 1 Helianthus annuus Asteraceae Helianthus 1 No opinion 2 Acacia Fabaceae Acacia 1 No opinion 3 Acacia 1 No opinion 4 Gossypium Malvaceae Gossypium 1 No opinion SubmittedNames 1 helianthus annuus 2 acacia 3 acacia 4 gossypium # The global names resolver is another attempt at this, hitting many # different data sources gnr_resolve(names = c(\u0026#34;Helianthus annuus\u0026#34;, \u0026#34;Homo sapiens\u0026#34;), returndf = TRUE) data_source_id submitted_name name_string score 1 4 Helianthus annuus Helianthus annuus 0.988 3 10 Helianthus annuus Helianthus annuus 0.988 5 12 Helianthus annuus Helianthus annuus 0.988 8 110 Helianthus annuus Helianthus annuus 0.988 11 159 Helianthus annuus Helianthus annuus 0.988 13 166 Helianthus annuus Helianthus annuus 0.988 15 169 Helianthus annuus Helianthus annuus 0.988 2 4 Homo sapiens Homo sapiens 0.988 4 10 Homo sapiens Homo sapiens 0.988 6 12 Homo sapiens Homo sapiens 0.988 7 107 Homo sapiens Homo sapiens 0.988 9 122 Homo sapiens Homo sapiens 0.988 10 123 Homo sapiens Homo sapiens 0.988 12 159 Homo sapiens Homo sapiens 0.988 14 168 Homo sapiens Homo sapiens 0.988 16 169 Homo sapiens Homo sapiens 0.988 title 1 NCBI 3 Freebase 5 EOL 8 Illinois Wildflowers 11 CU*STAR 13 nlbif 15 uBio NameBank 2 NCBI 4 Freebase 6 EOL 7 AskNature 9 BioPedia 10 AnAge 12 CU*STAR 14 Index to Organism Names 16 uBio NameBank # We can hit the Plantminer API too plants \u0026lt;- c(\u0026#34;Myrcia lingua\u0026#34;, \u0026#34;Myrcia bella\u0026#34;, \u0026#34;Ocotea pulchella\u0026#34;, \u0026#34;Miconia\u0026#34;, \u0026#34;Coffea arabica var. amarella\u0026#34;, \u0026#34;Bleh\u0026#34;) plantminer(plants) Myrcia lingua Myrcia bella Ocotea pulchella Miconia Coffea arabica var. amarella Bleh fam genus sp author 1 Myrtaceae Myrcia lingua (O. Berg) Mattos 2 Myrtaceae Myrcia bella Cambess. 3 Lauraceae Ocotea pulchella (Nees \u0026amp; Mart.) Mez 4 Melastomataceae Miconia NA Ruiz \u0026amp; Pav. 5 Rubiaceae Coffea arabica var. amarella A. Froehner 6 NA Bleh NA NA source source.id status confidence suggestion database 1 TRO 100227036 NA NA NA Tropicos 2 WCSP 131057 Accepted H NA The Plant List 3 WCSP (in review) 989758 Accepted M NA The Plant List 4 TRO 40018467 NA NA NA Tropicos 5 TRO 100170231 NA NA NA Tropicos 6 NA NA NA NA Baea NA # We made a light wrapper around the Taxonstand package to search # Theplantlist.org too splist \u0026lt;- c(\u0026#34;Heliathus annuus\u0026#34;, \u0026#34;Abies procera\u0026#34;, \u0026#34;Poa annua\u0026#34;, \u0026#34;Platanus occidentalis\u0026#34;, \u0026#34;Carex abrupta\u0026#34;, \u0026#34;Arctostaphylos canescens\u0026#34;, \u0026#34;Ocimum basilicum\u0026#34;, \u0026#34;Vicia faba\u0026#34;, \u0026#34;Quercus kelloggii\u0026#34;, \u0026#34;Lactuca serriola\u0026#34;) tpl_search(taxon = splist) Genus Species Infraspecific Plant.Name.Index 1 Heliathus annuus FALSE 2 Abies procera TRUE 3 Poa annua TRUE 4 Platanus occidentalis TRUE 5 Carex abrupta TRUE 6 Arctostaphylos canescens TRUE 7 Ocimum basilicum TRUE 8 Vicia faba TRUE 9 Quercus kelloggii TRUE 10 Lactuca serriola TRUE Taxonomic.status Family New.Genus New.Species 1 Heliathus annuus 2 Accepted Pinaceae Abies procera 3 Accepted Poaceae Poa annua 4 Accepted Platanaceae Platanus occidentalis 5 Accepted Cyperaceae Carex abrupta 6 Accepted Ericaceae Arctostaphylos canescens 7 Accepted Lamiaceae Ocimum basilicum 8 Accepted Leguminosae Vicia faba 9 Accepted Fagaceae Quercus kelloggii 10 Accepted Compositae Lactuca serriola New.Infraspecific Authority Typo WFormat 1 FALSE FALSE 2 Rehder FALSE FALSE 3 L. FALSE FALSE 4 L. FALSE FALSE 5 \u0026lt;NA\u0026gt; Mack. FALSE FALSE 6 Eastw. FALSE FALSE 7 L. FALSE FALSE 8 L. FALSE FALSE 9 \u0026lt;NA\u0026gt; Newb. FALSE FALSE 10 L. FALSE FALSE Taxonomic hierarchies I often want the full taxonomic hierarchy for a set of species. That is, give me the family, order, class, etc. for my list of species. There are two different easy ways to do this with taxize. The first example uses EOL. Using EOL. pageid \u0026lt;- eol_search(\u0026#34;Quercus douglasii\u0026#34;)$id[1] # first need to search for the taxon\u0026#39;s page on EOL out \u0026lt;- eol_pages(taxonconceptID = pageid) # then we nee to get the taxon ID used by EOL # Notice that there are multiple different sources you can pull the # hierarchy from. Note even that you can get the hierarchy from the ITIS # service via this EOL API. out identifier scientificName 1 46203061 Quercus douglasii Hook. \u0026amp; Arn. 2 48373995 Quercus douglasii Hook. \u0026amp; Arn. nameAccordingTo sourceIdentfier 1 Integrated Taxonomic Information System (ITIS) 19322 2 Species 2000 \u0026amp; ITIS Catalogue of Life: May 2012 9723391 taxonRank 1 Species 2 Species # Then the hierarchy! eol_hierarchy(out[out$nameAccordingTo == \u0026#34;Species 2000 \u0026amp; ITIS Catalogue of Life: May 2012\u0026#34;, \u0026#34;identifier\u0026#34;]) sourceIdentifier taxonID parentNameUsageID taxonConceptID 1 11017504 48276627 0 281 2 11017505 48276628 48276627 282 3 11017506 48276629 48276628 283 4 11022500 48373354 48276629 4184 5 11025284 48373677 48373354 4197 scientificName taxonRank 1 Plantae kingdom 2 Magnoliophyta phylum 3 Magnoliopsida class 4 Fagales order 5 Fagaceae family eol_hierarchy(out[out$nameAccordingTo == \u0026#34;Integrated Taxonomic Information System (ITIS)\u0026#34;, \u0026#34;identifier\u0026#34;]) # and from ITIS, slightly different than ITIS output below, which includes taxa all the way down. sourceIdentifier taxonID parentNameUsageID taxonConceptID 1 202422 46150613 0 281 2 846492 46159776 46150613 8654492 3 846494 46161961 46159776 28818077 4 846496 46167532 46161961 4494 5 846504 46169010 46167532 28825126 6 846505 46169011 46169010 282 7 18063 46169012 46169011 283 8 846548 46202954 46169012 28859070 9 19273 46202955 46202954 4184 10 19275 46203022 46202955 4197 scientificName taxonRank 1 Plantae kingdom 2 Viridaeplantae subkingdom 3 Streptophyta infrakingdom 4 Tracheophyta division 5 Spermatophytina subdivision 6 Angiospermae infradivision 7 Magnoliopsida class 8 Rosanae superorder 9 Fagales order 10 Fagaceae family And getting a taxonomic hierarchy using ITIS. # First, get the taxonomic serial number (TSN) that ITIS uses mytsn \u0026lt;- get_tsn(\u0026#34;Quercus douglasii\u0026#34;, \u0026#34;sciname\u0026#34;) Retrieving data for species \u0026#39; Quercus douglasii \u0026#39; # Get the full taxonomic hierarchy for a taxon from the TSN itis(mytsn, \u0026#34;getfullhierarchyfromtsn\u0026#34;) $`1` parentName parentTsn rankName taxonName tsn 1 Kingdom Plantae 202422 2 Plantae 202422 Subkingdom Viridaeplantae 846492 3 Viridaeplantae 846492 Infrakingdom Streptophyta 846494 4 Streptophyta 846494 Division Tracheophyta 846496 5 Tracheophyta 846496 Subdivision Spermatophytina 846504 6 Spermatophytina 846504 Infradivision Angiospermae 846505 7 Angiospermae 846505 Class Magnoliopsida 18063 8 Magnoliopsida 18063 Superorder Rosanae 846548 9 Rosanae 846548 Order Fagales 19273 10 Fagales 19273 Family Fagaceae 19275 11 Fagaceae 19275 Genus Quercus 19276 12 Quercus 19276 Species Quercus douglasii 19322 # But this can be even easier! classification(get_tsn(\u0026#34;Quercus douglasii\u0026#34;)) # Boom! Retrieving data for species \u0026#39; Quercus douglasii \u0026#39; $`1` parentName parentTsn rankName taxonName tsn 1 Kingdom Plantae 202422 2 Plantae 202422 Subkingdom Viridaeplantae 846492 3 Viridaeplantae 846492 Infrakingdom Streptophyta 846494 4 Streptophyta 846494 Division Tracheophyta 846496 5 Tracheophyta 846496 Subdivision Spermatophytina 846504 6 Spermatophytina 846504 Infradivision Angiospermae 846505 7 Angiospermae 846505 Class Magnoliopsida 18063 8 Magnoliopsida 18063 Superorder Rosanae 846548 9 Rosanae 846548 Order Fagales 19273 10 Fagales 19273 Family Fagaceae 19275 11 Fagaceae 19275 Genus Quercus 19276 12 Quercus 19276 Species Quercus douglasii 19322 # You can also do this easy-peasy route to a taxonomic hierarchy using # uBio classification(get_uid(\u0026#34;Ornithorhynchus anatinus\u0026#34;)) $`1` ScientificName Rank UID 1 cellular organisms no rank 131567 2 Eukaryota superkingdom 2759 3 Opisthokonta no rank 33154 4 Metazoa kingdom 33208 5 Eumetazoa no rank 6072 6 Bilateria no rank 33213 7 Coelomata no rank 33316 8 Deuterostomia no rank 33511 9 Chordata phylum 7711 10 Craniata subphylum 89593 11 Vertebrata no rank 7742 12 Gnathostomata superclass 7776 13 Teleostomi no rank 117570 14 Euteleostomi no rank 117571 15 Sarcopterygii no rank 8287 16 Tetrapoda no rank 32523 17 Amniota no rank 32524 18 Mammalia class 40674 19 Prototheria no rank 9254 20 Monotremata order 9255 21 Ornithorhynchidae family 9256 22 Ornithorhynchus genus 9257 Sequences? While you are at doing taxonomic stuff, you often wonder \u0026ldquo;hmmm, I wonder if there are any sequence data available for my species?\u0026rdquo; So, you can use get_seqs to search for specific genes for a species, and get_genes_avail to find out what genes are available for a certain species. These functions search for data on NCBI. # Get sequences (sequence is provied in output, but hiding here for # brevity). What\u0026#39;s nice about this is that it gets the longest sequence # avaialable for the gene you searched for, and if there isn\u0026#39;t anything # available, it lets you get a sequence from a congener if you set # getrelated=TRUE. The last column in the output data.frame also tells you # what species the sequence is from. out \u0026lt;- get_seqs(taxon_name = \u0026#34;Acipenser brevirostrum\u0026#34;, gene = c(\u0026#34;5S rRNA\u0026#34;), seqrange = \u0026#34;1:3000\u0026#34;, getrelated = T, writetodf = F) out[, !names(out) %in% \u0026#34;sequence\u0026#34;] taxon gene_desc 1 Acipenser brevirostrum Acipenser brevirostrum 5S rRNA gene, clone BRE92A gi_no acc_no length spused 1 60417159 AJ745069.1 121 Acipenser brevirostrum # Search for available sequences out \u0026lt;- get_genes_avail(taxon_name = \u0026#34;Umbra limi\u0026#34;, seqrange = \u0026#34;1:2000\u0026#34;, getrelated = F) out[grep(\u0026#34;RAG1\u0026#34;, out$genesavail, ignore.case = T), ] # does the string \u0026#39;RAG1\u0026#39; exist in any of the gene names spused length 414 Umbra limi 732 427 Umbra limi 959 434 Umbra limi 1631 genesavail 414 isolate UlimA recombinase activating protein 1 (rag1) gene, exon 3 and partial cds 427 recombination-activating protein 1 (RAG1) gene, intron 2 and partial cds 434 recombination-activating protein 1 (RAG1) gene, partial cds predicted 414 JX190826 427 AY459526 434 AY380548 Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/12/taxize/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUPDATE: there were some errors in the tests for \u003ccode\u003etaxize\u003c/code\u003e, so the binaries aren\u0026rsquo;t avaiable yet. You can install from source though, see below.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eGetting taxonomic information for the set of species you are studying can be a pain in the ass. You have to manually type, or paste in, your species one-by-one. Or, if you are lucky, there is a web service in which you can upload a list of species. Encyclopedia of Life (EOL) has a service where you can do this \u003ca href=\"http://gni.globalnames.org/parsers/new\"\u003ehere\u003c/a\u003e. But is this reproducible? No.\u003c/p\u003e","title":"One R package for all your taxonomic needs"},{"content":"Note: This post is cross-posted on Sandra Chung\u0026rsquo;s blog here.\nThe rise of the unconference The Ecological Society of America meeting is holding its 98th annual meeting next year in Minneapolis, MN. Several thousand students and professionals in ecological science and education will gather to hear and read the latest work and ideas in ecology in the familiar poster and lecture formats that are the core of every major scientific conference. But a subset of these people will get a taste of something a little bit different: an unconference within the conference.\nThe most important difference between traditional science conferences and the unconference format is that it prioritizes human interaction. Often the best and most important parts of science meetings are the interactions between talks, next to posters, and at the end of the day over drinks. These connections pave the way for collaborations and friendships that nourish our professional and personal lives with shared opportunities, camaraderie and support.\nIn recognition of the increasing relative importance of the “meeting” part of a science meeting, the unconference format emphasizes interaction over presentation. It attempts to engage participants to the maximum extent reasonable in discussion and doing. Science Online is a good example of this unconference format, in which the session topics are typically decided on democratically by the conference attendees (partly before arrival, partly on arrival), and you vote with your feet by going to and leaving sessions as you desire.\nEcology is changing Ecology is now adopting some of the same online and social tools that are already accelerating innovation in computing and other science disciplines. Ecologists, ecology students and educators are asking many of the same basic questions they have always asked: What should we be doing? How do we do it better and faster? Social media, open source software, open science, altmetrics, crowdsourcing, crowdfunding, data visualization, data sharing, alternative peer review, and an increasing emphasis on more and better communication and collaboration are just some of the newer tools being put forth to help address those questions in the 21st century.\nSocial media is rapidly becoming more common in ecologists’ toolkits to disseminate news of their new papers, communicate about research and research tools, and even filter the deluge of publications. Tools like blogs, Twitter and Facebook are filling the communication gaps between annual meetings and adding a new layer of conversation and connection to conferences and classrooms.\nSocial media, in turn, is connecting scientists directly to people besides their immediate colleagues who appreciate the impact of their work and want it to continue. The crowdfunding movement - exemplified by Kickstarter - has spurred similar alternative science funding projects such as SciFund. #SciFund project has shown that social media engagement increases donations to crowdfunded research (interview with Jarrett Byrnes).\nIn addition, we are in the era of big data, and this includes ecology. To deal with this “data deluge”, ecologists increasingly want to learn how to manage, share, retrieve, use, and visualize data. There are new tools for all of these tasks - we need to aid each other in learning them. Online and offline communities are coalescing around the development and dissemination of these tools for the benefit of ecological science, and they are meeting face-to-face at our ecological unconference.\nScience in general is becoming increasingly complex and calling for larger and larger collaborations. This growth in turn is spurring a drive toward more openness and transparency within the culture of science. The more collaborative and complex scientific study becomes, the more scientists depend upon each other to do good work that we can all build upon with confidence. The often unstated assumption about ecology, and all of science, is that research findings are reproducible; but that assumption is quite shaky given the increasing number of retractions (see the Retraction Watch blog) and findings that much research is not reproducible (see media coverage here and here).\nA recent initiative seeks to facilitate attempts to reproduce research: The Reproducibility Initiative. Jarrett Byrnes spoke at #ESA2012 of how transparent, online discourse enhances our ability to discuss and improve our work and the work of our peers, both before and after publication.\nMuch of the ecological science community shares one or both of these goals: to do the best possible science, and to do it in a way that is most useful and accessible to colleagues and to society at large. The goal of this year’s ecological unconference is to introduce as many people as possible to resources - both tools and people - that can help all of us achieve those goals, on our own, or together.\nOne way we as ecologists can quickly make our research more reproducible is the way we write. By simply using tools that make reproducing what we have done easy to do, we can avoid retracted papers, failed drugs, and ruined careers.\nWhat are you proposing? We originally thought about a separate event from ESA itself, modeled after Science Online, incorporating a variety of topics. However, we thought testing the waters for this sort of non-traditional unconference format would be better in 2013. We are gathering ideas from the community (see “How do I make my voice heard?” below). The ideas on the wiki that get the most traction, and have 1-2 champions that are willing to see the idea through and lead the workshop at ESA will be turned in to proposals for ESA workshops. In addition, we will be submitting a proposal for an Ignite session at ESA. To summarise, we will be running:\nA few workshops (at lunch hours, and half-day). Topics may include: Data sharing Data visualization Data management Alternatives to academia Blogging Social media Reproducible science writing One Ignite session on “Tools for better/faster science”. See more about Ignite sessions here. A “tweetup” event to socialize in a more relaxed atmosphere These will all be loosely aggregated under the #AltEcology hashtag.\nHow do I make my voice heard? We have set up a wiki in which anyone can contribute. Please share your ideas and voice your support for existing ones at the wiki here. You can just throw ideas out there, or even propose new workshops and nominate people to lead them. We’re currently moving to transform existing ideas into ESA workshop and Ignite proposals to meet the November 29 deadline, but we’ll be incorporating input from the wiki right up to the meeting itself in August 2013.\nGet in touch If you have any questions/comments, let us know in the comments section below, tweet us (Sandra: @sandramchung, Scott: @recology_), or email (Sandra, Scott).\nGet the .md file used to create this post at my github account. Written in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/11/altecology/","summary":"\u003cp\u003eNote: This post is cross-posted on Sandra Chung\u0026rsquo;s blog \u003ca href=\"http://sandrachung.com/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"the-rise-of-the-unconference\"\u003eThe rise of the unconference\u003c/h3\u003e\n\u003cp\u003eThe Ecological Society of America meeting is holding its \u003ca href=\"http://www.esa.org/minneapolis/\"\u003e98th annual meeting\u003c/a\u003e next year in Minneapolis, MN. Several thousand students and professionals in ecological science and education will gather to hear and read the latest work and ideas in ecology in the familiar poster and lecture formats that are the core of every major scientific conference. But a subset of these people will get a taste of something a little bit different: an unconference within the conference.\u003c/p\u003e","title":"Altecology, a call to unconference action"},{"content":"Have you ever wanted to easily visualize your ecology data in Google Earth? R2G2 is a new package for R, available via R CRAN and formally described in this Molecular Ecology Resources article, which provides a user-friendly bridge between R and the Google Earth interface. Here, we will provide a brief introduction to the package, including a short tutorial, and then encourage you to try it out with your own data!\nNils Arrigo, with some help from Loren Albert, Mike Barker, and Pascal Mickelson (one of the contributors to Recology), has created a set of R tools to generate KML files to view data with geographic components. Instead of just telling you what the tools can do, though, we will show you a couple of examples using publically available data. Note: a number of individual files are linked to throughout the tutorial below, but just in case you would rather download all the tutorial files in one go, have at it (tutorial zip file).\nAmong the basic tools in R2G2 is the ability to place features—like dots, shapes, or images (including plots you produced in R)— that represent discrete observations at specific geographical locations. For example, in the figure below, we show the migratory path of a particular turkey vulture in autumn of three successive years (red = 2009; blue = 2010; green = 2011).\nGoogle Earth imagery showing migratory path of a particular turkey vulture in 2009, 2010, and 2011. We use the PolyLines2GE function that is part of R2G2 to create line segments between the geographical coordinates which have been obtained from a turkey vulture tagged with a transponder (data accessed via the Movebank Data Repository and is from the Turkey Vulture Acopian Center USA GPS). The PolyLines2GE function looks like the following:\nPolyLines2GE(coords = vulture_path[,2:3], nesting = vulture_path[,1], colors = \u0026#34;auto\u0026#34;, goo = \u0026#34;Vulture_Path.kml\u0026#34;, maxAlt = 1e4, fill = FALSE, closepoly = FALSE, lwd = 2, extrude = 0) It expects to receive an array (\u0026ldquo;coords\u0026rdquo;) containing latitude and longitude coordinates in decimal degrees. Additionally, each individual coordinate has a flag associated with it (\u0026ldquo;nesting\u0026rdquo;) so that each data series can be distinguished. Illustrating what you need is easier than explaining:\nnesting longitude latitude 1\tlong1A\tlat1A 1\tlong1B\tlat1B 1\tlong1C\tlat1C 2\tlong2A\tlat2A 2\tlong2B\tlat2B 3\tlong3A\tlat3A 3\tlong3B\tlat3B 3\tlong3C\tlat3C Feeding the columns of this array to the function results in three differently colored lines: the first would connect the coordinates 1A-1B-1C, while the second would connect 2A-2B, and the third would connect 3A-3B-3C. The only other user-defined input that is strictly necessary is the output file name (\u0026ldquo;Vulture_Path.kml\u0026rdquo; in this case). The other options—which allow you control of the appearance of the lines and of the altitude at which your line displays in Google Earth—have reasonable defaults that are well-documented in the function definition itself. Check out this example in Google Earth by downloading the KML file. Alternatively, download the annotated R script and generate the KML file for yourself.\nNow, let\u0026rsquo;s say you wanted to get a sense of the range and abundance of two congeneric species. In this second example, we use the Hist2GE function to create a histogram—overlaid on the surface of the earth—which shows the species distribution of Mimulus lewisii (red) and Mimulus nasutus (blue) in North America.\nGoogle Earth imagery showing the species distribution of Mimulus lewisii and Mimulus nasutus As you might expect, each polygon represents an occurrence of the species in question, while the height of the polygon represents the abundance of the species at that geographic location. Species occurring within a particular distance of each other have been grouped together for the histogram. For this example, we retrieve data from the GBIF database from within R (see the example code for how that is done). Inputs to the Hist2GE function are:\nHist2GE(coords = MyCompleteData[, 8:7], species = MyCompleteData[, 1], grid = grid10000, goo = \u0026#34;Mimulus\u0026#34;, nedges = 6, orient = 45, maxAlt = 1e4) As in the first example, the function expects to receive an array containing the longitude and latitude (\u0026ldquo;coords\u0026rdquo;), a vector distinguishing individual observations (\u0026ldquo;species\u0026rdquo;), and an output file name (\u0026ldquo;goo\u0026rdquo;). In this case, however, we also need to specify the size of the grid we will use to group observations together to construct the histogram. Several pre-defined grid sizes are included in the package to do this grouping; these all cover large geographic areas and therefore must account for the curvature of the earth. Here is a list of these pre-defined grids:\nGrid NameApproximate Area of Grid Division grid2000025,500 sq. km grid1000051,000 sq. km grid5000102,000 sq. km grid5001,020,000 sq. km grid5010,200,000 sq. km For smaller geographic areas (less than 25,000 square kilometers, or an area of about 158 km per side), you can customize the grid size by specifying the bounds of your region of interest in decimal degrees, as well as the coarseness of the grid within that region. While it is possible to use this custom grid definition for larger sizes, beware that not all areas defined thusly will be of equal size due to the earth\u0026rsquo;s curvature (obviously the bigger you go, the worse it gets\u0026hellip;). Finally, you again have control over the display parameters of the histogram. In particular, the maximum altitude (\u0026ldquo;maxAlt\u0026rdquo;) controls how high the tallest bar in the histogram will go. Here is the resulting KML file, as well as the annotated R script so you can further explore the example.\nMore complex visual representations are also possible using R2G2. For instance, you can also create contour plots or phylogenies overlaid directly on the surface of the earth. We included a couple examples of this type in our Molecular Ecology Resources article, and if the response seems good, we may post a follow up tutorial showing how we went about creating those examples.\nIt is our sincere hope that you will use the tools in R2G2 to more effectively visualize the geographical aspects of your data. In particular, we are excited about the potential for incorporating R2G2 into data analysis pipelines connecting analysis in R with data visualization and exploration in Google Earth. Ultimately, the inclusion of KML files as supplementary materials to journal articles should also enrich one\u0026rsquo;s understanding of the data being presented!\nNote: If you make something cool using R2G2, please post a link to your KML file in the comments; we would love to see! Citation information for R2G2:\nArrigo, N., Albert, L. P., Mickelson, P. G. and Barker, M. S. (2012), Quantitative visualization of biological data in Google Earth using R2G2, an R CRAN package. Molecular Ecology Resources. doi: 10.1111/1755-0998.12012\n","permalink":"http://localhost:1313/2012/10/r2g2-package/","summary":"\u003cp\u003eHave you ever wanted to easily visualize your ecology data in \u003ca href=\"http://earth.google.com\"\u003eGoogle Earth\u003c/a\u003e?  \u003ca href=\"http://cran.r-project.org/web/packages/R2G2/index.html\"\u003eR2G2\u003c/a\u003e is a new package for R, available via \u003ca href=\"http://cran.r-project.org/\"\u003eR CRAN\u003c/a\u003e and formally described in \u003ca href=\"http://onlinelibrary.wiley.com/doi/10.1111/1755-0998.12012/abstract\"\u003ethis Molecular Ecology Resources article\u003c/a\u003e, which provides a user-friendly bridge between R and the Google Earth interface.  Here, we will provide a brief introduction to the package, including a short tutorial, and then encourage you to try it out with your own data!\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://barkerlab.net/nils.html\"\u003eNils Arrigo\u003c/a\u003e, with some help from \u003ca href=\"http://portal.environment.arizona.edu/students/profiles/loren-albert\"\u003eLoren Albert\u003c/a\u003e, \u003ca href=\"http://barkerlab.net/mike.html\"\u003eMike Barker\u003c/a\u003e, and Pascal Mickelson (one of the contributors to \u003ca href=\"http://sckott.github.io/about.html\"\u003eRecology\u003c/a\u003e), has created a set of R tools to generate KML files to view data with geographic components.  Instead of just telling you what the tools can do, though, we will show you a couple of examples using publically available data.  Note: a number of individual files are linked to throughout the tutorial below, but just in case you would rather download all the tutorial files in one go, have at it (\u003ca href=\"/R2G2tutorial/R2G2tutorial.zip\"\u003etutorial zip file\u003c/a\u003e).\u003c/p\u003e","title":"Displaying Your Data in Google Earth Using R2G2"},{"content":"It can be a pain in the ass to get taxonomic names. For example, I sometimes need to get all the Class names for a set of species. This is a relatively easy problem using the ITIS API (example below).\nThe much harder problem is getting all the taxonomic names downstream. ITIS doesn\u0026rsquo;t provide an API method for this - well, they do (getHirerachyDownFromTSN), but it only provides direct children (e.g., the genera within a tribe - but it won\u0026rsquo;t give all the species within each genus).\nSo in the taxize package, we wrote a function called downstream, which allows you to get taxonomic names to any downstream point, e.g.:\nget all Classes within Animalia, get all Species within a Family etc. Install packages. You can get other packages from CRAN, but taxize is only on GitHub for now. # install_github(\u0026#39;ritis\u0026#39;, \u0026#39;ropensci\u0026#39;) # uncomment if not already installed # install_github(\u0026#39;taxize\u0026#39;, \u0026#39;ropensci\u0026#39;) # uncomment if not already library(ritis) library(taxize) Get upstream taxonomic names. # Search for a TSN by scientific name df \u0026lt;- searchbyscientificname(\u0026#34;Tardigrada\u0026#34;) tsn \u0026lt;- df[df$combinedname %in% \u0026#34;Tardigrada\u0026#34;, \u0026#34;tsn\u0026#34;] # Get just one immediate higher taxonomic name gethierarchyupfromtsn(tsn = tsn) parentName parentTsn rankName taxonName tsn 1 Animalia 202423 Phylum Tardigrada 155166 # Get full hierarchy upstream from TSN getfullhierarchyfromtsn(tsn = tsn) parentName parentTsn rankName taxonName tsn 1 Kingdom Animalia 202423 2 Animalia 202423 Phylum Tardigrada 155166 3 Tardigrada 155166 Class Eutardigrada 155362 4 Tardigrada 155166 Class Heterotardigrada 155167 5 Tardigrada 155166 Class Mesotardigrada 155358 Get taxonomc names downstream. # Get genera downstream fromthe Class Bangiophyceae downstream(846509, \u0026#34;Genus\u0026#34;) tsn parentName parentTsn taxonName rankId rankName 1 11531 Bangiaceae 11530 Bangia 180 Genus 2 11540 Bangiaceae 11530 Porphyra 180 Genus 3 11577 Bangiaceae 11530 Porphyrella 180 Genus 4 11580 Bangiaceae 11530 Conchocelis 180 Genus # Get families downstream from Acridoidea downstream(650497, \u0026#34;Family\u0026#34;) tsn parentName parentTsn taxonName rankId rankName 1 102195 Acridoidea 650497 Acrididae 140 Family 2 650502 Acridoidea 650497 Romaleidae 140 Family 3 657472 Acridoidea 650497 Charilaidae 140 Family 4 657473 Acridoidea 650497 Lathiceridae 140 Family 5 657474 Acridoidea 650497 Lentulidae 140 Family 6 657475 Acridoidea 650497 Lithidiidae 140 Family 7 657476 Acridoidea 650497 Ommexechidae 140 Family 8 657477 Acridoidea 650497 Pamphagidae 140 Family 9 657478 Acridoidea 650497 Pyrgacrididae 140 Family 10 657479 Acridoidea 650497 Tristiridae 140 Family 11 657492 Acridoidea 650497 Dericorythidae 140 Family # Get species downstream from Ursus downstream(180541, \u0026#34;Species\u0026#34;) tsn parentName parentTsn taxonName rankId rankName 1 180542 Ursus 180541 Ursus maritimus 220 Species 2 180543 Ursus 180541 Ursus arctos 220 Species 3 180544 Ursus 180541 Ursus americanus 220 Species 4 621850 Ursus 180541 Ursus thibetanus 220 Species Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/10/get-taxa-downstream/","summary":"\u003cp\u003eIt can be a pain in the ass to get taxonomic names. For example, I sometimes need to get all the Class names for a set of species.  This is a relatively easy problem using the \u003ca href=\"http://www.itis.gov/ws_description.html\"\u003eITIS API\u003c/a\u003e (example below).\u003c/p\u003e\n\u003cp\u003eThe much harder problem is getting all the taxonomic names downstream. ITIS doesn\u0026rsquo;t provide an API method for this - well, they do (\u003ca href=\"http://www.itis.gov/ws_hierApiDescription.html#getHierarchyDn\"\u003e\u003ccode\u003egetHirerachyDownFromTSN\u003c/code\u003e\u003c/a\u003e), but it only provides direct children (e.g., the genera within a tribe - but it won\u0026rsquo;t give all the species within each genus).\u003c/p\u003e","title":"Getting taxonomic names downstream"},{"content":"I need to simulate balanced and unbalanced phylogenetic trees for some research I am doing. In order to do this, I do rejection sampling: simulate a tree -\u0026gt; measure tree shape -\u0026gt; reject if not balanced or unbalanced enough. But what is enough? We need to define some cutoff value to determine what will be our set of balanced and unbalanced trees.\ncalculate shape metrics A function to calculate shape metrics, and a custom theme for plottingn phylogenies.\nfoo \u0026lt;- function(x, metric = \u0026#34;colless\u0026#34;) { if (metric == \u0026#34;colless\u0026#34;) { xx \u0026lt;- as.treeshape(x) # convert to apTreeshape format colless(xx, \u0026#34;yule\u0026#34;) # calculate colless\u0026#39; metric } else if (metric == \u0026#34;gamma\u0026#34;) { gammaStat(x) } else stop(\u0026#34;metric should be one of colless or gamma\u0026#34;) } theme_myblank \u0026lt;- function() { stopifnot(require(ggplot2)) theme_blank \u0026lt;- ggplot2::theme_blank ggplot2::theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), plot.background = element_blank(), axis.title.x = element_text(colour = NA), axis.title.y = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.line = element_blank(), axis.ticks = element_blank()) } Simulate some trees library(ape) library(phytools) numtrees \u0026lt;- 1000 # lets simulate 1000 trees trees \u0026lt;- pbtree(n = 50, nsim = numtrees, ape = F) # simulate 500 pure-birth trees with 100 spp each, ape = F makes it run faster Calculate Colless\u0026rsquo; shape metric on each tree library(plyr) library(apTreeshape) colless_df \u0026lt;- ldply(trees, foo, metric = \u0026#34;colless\u0026#34;) # calculate metric for each tree head(colless_df) V1 1 -0.1761 2 0.2839 3 0.4639 4 0.9439 5 -0.6961 6 -0.1161 # Calculate the percent of trees that will fall into the cutoff for balanced and unbalanced trees col_percent_low \u0026lt;- round(length(colless_df[colless_df$V1 \u0026lt; -0.7, \u0026#34;V1\u0026#34;])/numtrees, 2) * 100 col_percent_high \u0026lt;- round(length(colless_df[colless_df$V1 \u0026gt; 0.7, \u0026#34;V1\u0026#34;])/numtrees, 2) * 100 Create a distribution of the metric values library(ggplot2) a \u0026lt;- ggplot(colless_df, aes(V1)) + # plot histogram of distribution of values geom_histogram() + theme_bw(base_size=18) + scale_x_continuous(limits=c(-3,3), breaks=c(-3,-2,-1,0,1,2,3)) + geom_vline(xintercept = -0.7, colour=\u0026#34;red\u0026#34;, linetype = \u0026#34;longdash\u0026#34;) + geom_vline(xintercept = 0.7, colour=\u0026#34;red\u0026#34;, linetype = \u0026#34;longdash\u0026#34;) + ggtitle(paste0(\u0026#34;Distribution of Colless\u0026#39; metric for 1000 trees, cutoffs at -0.7 and 0.7 results in\\n \u0026#34;, col_percent_low, \u0026#34;% (\u0026#34;, numtrees*(col_percent_low/100), \u0026#34;) \u0026#39;balanced\u0026#39; trees (left) and \u0026#34;, col_percent_low, \u0026#34;% (\u0026#34;, numtrees*(col_percent_low/100), \u0026#34;) \u0026#39;unbalanced\u0026#39; trees (right)\u0026#34;)) + labs(x = \u0026#34;Colless\u0026#39; Metric Value\u0026#34;, y = \u0026#34;Number of phylogenetic trees\u0026#34;) + theme(plot.title = element_text(size = 16)) a Create phylogenies representing balanced and unbalanced trees (using the custom theme) library(ggphylo) b \u0026lt;- ggphylo(trees[which.min(colless_df$V1)], do.plot = F) + theme_myblank() c \u0026lt;- ggphylo(trees[which.max(colless_df$V1)], do.plot = F) + theme_myblank() b Now, put it all together in one plot using some gridExtra magic. library(gridExtra) grid.newpage() pushViewport(viewport(layout = grid.layout(1, 1))) vpa_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.49) vpb_ \u0026lt;- viewport(width = 0.35, height = 0.35, x = 0.23, y = 0.7) vpc_ \u0026lt;- viewport(width = 0.35, height = 0.35, x = 0.82, y = 0.7) print(a, vp = vpa_) print(b, vp = vpb_) print(c, vp = vpc_) And the same for Gamma stat, which measures the distribution of nodes in time. gamma_df \u0026lt;- ldply(trees, foo, metric=\u0026#34;gamma\u0026#34;) # calculate metric for each tree gam_percent_low \u0026lt;- round(length(gamma_df[gamma_df$V1 \u0026lt; -1, \u0026#34;V1\u0026#34;])/numtrees, 2)*100 gam_percent_high \u0026lt;- round(length(gamma_df[gamma_df$V1 \u0026gt; 1, \u0026#34;V1\u0026#34;])/numtrees, 2)*100 a \u0026lt;- ggplot(gamma_df, aes(V1)) + # plot histogram of distribution of values geom_histogram() + theme_bw(base_size=18) + scale_x_continuous(breaks=c(-3,-2,-1,0,1,2,3)) + geom_vline(xintercept = -1, colour=\u0026#34;red\u0026#34;, linetype = \u0026#34;longdash\u0026#34;) + geom_vline(xintercept = 1, colour=\u0026#34;red\u0026#34;, linetype = \u0026#34;longdash\u0026#34;) + ggtitle(paste0(\u0026#34;Distribution of Gamma metric for 1000 trees, cutoffs at -1 and 1 results in\\n \u0026#34;, gam_percent_low, \u0026#34;% (\u0026#34;, numtrees*(gam_percent_low/100), \u0026#34;) trees with deeper nodes (left) and \u0026#34;, gam_percent_high, \u0026#34;% (\u0026#34;, numtrees*(gam_percent_high/100), \u0026#34;) trees with shallower nodes (right)\u0026#34;)) + labs(x = \u0026#34;Gamma Metric Value\u0026#34;, y = \u0026#34;Number of phylogenetic trees\u0026#34;) + theme(plot.title = element_text(size = 16)) b \u0026lt;- ggphylo(trees[which.min(gamma_df$V1)], do.plot=F) + theme_myblank() c \u0026lt;- ggphylo(trees[which.max(gamma_df$V1)], do.plot=F) + theme_myblank() grid.newpage() pushViewport(viewport(layout = grid.layout(1,1))) vpa_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.49) vpb_ \u0026lt;- viewport(width = 0.35, height = 0.35, x = 0.23, y = 0.7) vpc_ \u0026lt;- viewport(width = 0.35, height = 0.35, x = 0.82, y = 0.7) print(a, vp = vpa_) print(b, vp = vpb_) print(c, vp = vpc_) Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/10/phylogenetic-tree-balance/","summary":"\u003cp\u003eI need to simulate balanced and unbalanced phylogenetic trees for some research I am doing.  In order to do this, I do rejection sampling: simulate a tree -\u0026gt; measure tree shape -\u0026gt; reject if not balanced or unbalanced \u003cstrong\u003eenough\u003c/strong\u003e.  But what is enough?  We need to define some cutoff value to determine what will be our set of balanced and unbalanced trees.\u003c/p\u003e\n\u003ch3 id=\"calculate-shape-metrics\"\u003ecalculate shape metrics\u003c/h3\u003e\n\u003cp\u003eA function to calculate shape metrics, and a custom theme for plottingn phylogenies.\u003c/p\u003e","title":"Exploring phylogenetic tree balance metrics"},{"content":" UPDATE: In response to Jarrett\u0026rsquo;s query I laid out a separate use case in which you may want to query by higher taxonomic rankings than species. See below. In addition, added examples of querying by location in reply to comments by seminym.\nWe have been working on an R package to get GBIF data from R, with the stable version available through CRAN, and the development version available on GitHub at https://github.com/rgbif\nWe had a Google Summer of code stuent work on the package this summer - you can see his work on the package over at his GitHub page here. We have added some new functionality since his work, and would like to show it off.\nInstall # install_github(\u0026#39;rgbif\u0026#39;, \u0026#39;ropensci\u0026#39;) # uncomment if not already installed library(rgbif) library(plyr) library(XML) library(httr) library(maps) library(ggplot2) Get taxonomic information on a specific taxon or taxa in GBIF by their taxon concept keys. (keys \u0026lt;- taxonsearch(scientificname = \u0026#34;Puma concolor\u0026#34;)) # many matches to this search [1] \u0026#34;51780668\u0026#34; \u0026#34;51758018\u0026#34; \u0026#34;50010499\u0026#34; \u0026#34;51773067\u0026#34; \u0026#34;51078815\u0026#34; \u0026#34;51798065\u0026#34; [7] \u0026#34;51088007\u0026#34; \u0026#34;50410780\u0026#34; \u0026#34;50305290\u0026#34; \u0026#34;51791438\u0026#34; taxonget(keys[[1]]) # let\u0026#39;s get the first one - the first row in the data.frame is the one we searched for (51780668) [[1]] sciname taxonconceptkeys rank 1 Puma concolor 51780668 species 2 Puma 51780667 genus 3 Felidae 51780651 family 4 Carnivora 51780613 order 5 Mammalia 51780547 class 6 Chordata 51775774 phylum 7 Animalia 51775773 kingdom 8 Puma concolor californica 51780669 subspecies 9 Puma concolor improcera 51780670 subspecies The occurrencedensity function was renamed to densitylist because it is in the density API service, not the occurrence API service. You can use densitylist to get a data.frame of total occurrence counts by one-degree cell for a single taxon, country, dataset, data publisher or data network. Just a quick reminder of what the function can do: head(densitylist(originisocountrycode = \u0026#34;CA\u0026#34;)) cellid minLatitude maxLatitude minLongitude maxLongitude count 1 46913 40 41 -67 -66 44 2 46914 40 41 -66 -65 907 3 46915 40 41 -65 -64 510 4 46916 40 41 -64 -63 645 5 46917 40 41 -63 -62 56 6 46918 40 41 -62 -61 143 Using a related function, density_spplist, you can get a species list by one-degree cell as well. # Get a species list by cell, choosing one at random density_spplist(originisocountrycode = \u0026#34;CO\u0026#34;, spplist = \u0026#34;random\u0026#34;)[1:10] [1] \u0026#34;Abarema laeta (Benth.) Barneby \u0026amp; J.W.Grimes\u0026#34; [2] \u0026#34;Abuta grandifolia (Mart.) Sandwith\u0026#34; [3] \u0026#34;Acalypha cuneata Poepp.\u0026#34; [4] \u0026#34;Acalypha diversifolia Jacq.\u0026#34; [5] \u0026#34;Acalypha macrostachya Jacq.\u0026#34; [6] \u0026#34;Acalypha stachyura Pax\u0026#34; [7] \u0026#34;Acanthoscelio acutus\u0026#34; [8] \u0026#34;Accipiter collaris\u0026#34; [9] \u0026#34;Actitis macularia\u0026#34; [10] \u0026#34;Adelobotrys klugii Wurdack\u0026#34; # density_spplist(originisocountrycode = \u0026#39;CO\u0026#39;, spplist = \u0026#39;r\u0026#39;) # can # abbreviate the `spplist` argument # Get a species list by cell, choosing the one with the greatest no. of # records density_spplist(originisocountrycode = \u0026#34;CO\u0026#34;, spplist = \u0026#34;great\u0026#34;)[1:10] # great is abbreviated from `greatest` [1] \u0026#34;Acanthaceae Juss.\u0026#34; [2] \u0026#34;Accipitridae sp.\u0026#34; [3] \u0026#34;Accipitriformes/Falconiformes sp.\u0026#34; [4] \u0026#34;Apodidae sp.\u0026#34; [5] \u0026#34;Apodidae sp. (large swift sp.)\u0026#34; [6] \u0026#34;Apodidae sp. (small swift sp.)\u0026#34; [7] \u0026#34;Arctiinae\u0026#34; [8] \u0026#34;Asteraceae Bercht. \u0026amp; J. Presl\u0026#34; [9] \u0026#34;Asteraceae sp. 1\u0026#34; [10] \u0026#34;Asteraceae sp. 6\u0026#34; # Can also get a data.frame with counts instead of the species list density_spplist(originisocountrycode = \u0026#34;CO\u0026#34;, spplist = \u0026#34;great\u0026#34;, listcount = \u0026#34;counts\u0026#34;)[1:10, ] names_ count 1 Acanthaceae Juss. 2 2 Accipitridae sp. 6 3 Accipitriformes/Falconiformes sp. 2 4 Apodidae sp. 5 5 Apodidae sp. (large swift sp.) 8 6 Apodidae sp. (small swift sp.) 5 7 Arctiinae 7 8 Asteraceae Bercht. \u0026amp; J. Presl 2 9 Asteraceae sp. 1 6 10 Asteraceae sp. 6 10 You can now map point results, from fxns occurrencelist and those from densitylist, which plots them as points or as tiles, respectively. Point map, using output from occurrencelist. out \u0026lt;- occurrencelist(scientificname = \u0026#34;Puma concolor\u0026#34;, coordinatestatus = TRUE, maxresults = 100, latlongdf = T) gbifmap(input = out) # make a map, plotting on world map Point map, using output from occurrencelist, with many species plotted as different colors splist \u0026lt;- c(\u0026#34;Accipiter erythronemius\u0026#34;, \u0026#34;Junco hyemalis\u0026#34;, \u0026#34;Aix sponsa\u0026#34;, \u0026#34;Buteo regalis\u0026#34;) out \u0026lt;- lapply(splist, function(x) occurrencelist(x, coordinatestatus = T, maxresults = 100, latlongdf = T)) gbifmap(out) Tile map, using output from densitylist, using results in Canada only. out2 \u0026lt;- densitylist(originisocountrycode = \u0026#34;CA\u0026#34;) # data for Canada gbifmap(out2) # on world map gbifmap(out2, region = \u0026#34;Canada\u0026#34;) # on Canada map We can also query by higher taxonomic rankings, and map all lower species within that ranking. Above we queried by scientificname, but we can also query by higher taxonomy. 7071443 is the taxonconceptkey for \u0026lsquo;Bacillariophyceae\u0026rsquo;, a Class which includes many lower species. out \u0026lt;- densitylist(taxonconceptKey = 7071443) gbifmap(out) seminym asked about querying by area. You can query by area, though slightly differently for occurrencelist and densitylist functions. For occurrencelist you can search using min and max lat and long values (and min an max altitude, pretty cool, eh). # Get occurrences or density by area, using min/max lat/long coordinates out \u0026lt;- occurrencelist(minlatitude = 30, maxlatitude = 35, minlongitude = -100, maxlongitude = -95, coordinatestatus = T, maxresults = 5000, latlongdf = T) # Using `geom_point` gbifmap(out, \u0026#34;state\u0026#34;, \u0026#34;texas\u0026#34;, geom_point) # Using geom_jitter to move the points apart from one another gbifmap(out, \u0026#34;state\u0026#34;, \u0026#34;texas\u0026#34;, geom_jitter, position_jitter(width = 0.3, height = 0.3)) # And move points a lot gbifmap(out, \u0026#34;state\u0026#34;, \u0026#34;texas\u0026#34;, geom_jitter, position_jitter(width = 1, height = 1)) And you can query by area in densitylist by specifying a place using the originisocountrycode argument (as done in an above example). Just showing the head of the data.frame here. # Get density by place, note that you can\u0026#39;t use the lat and long arguments # in densitylist head(densitylist(originisocountrycode = \u0026#34;CA\u0026#34;)) cellid minLatitude maxLatitude minLongitude maxLongitude count 1 46913 40 41 -67 -66 44 2 46914 40 41 -66 -65 907 3 46915 40 41 -65 -64 510 4 46916 40 41 -64 -63 645 5 46917 40 41 -63 -62 56 6 46918 40 41 -62 -61 143 Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/10/rgbif-newfxns/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUPDATE: In response to Jarrett\u0026rsquo;s query I laid out a separate use case in which you may want to query by higher taxonomic rankings than species. See below.  In addition, added examples of querying by location in reply to comments by seminym.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe have been working on an R package to get GBIF data from R, with the stable version available through CRAN, and the development version available on GitHub at \u003ca href=\"https://github.com/rgbif\"\u003ehttps://github.com/rgbif\u003c/a\u003e\u003c/p\u003e","title":"GBIF biodiversity data from R - more functions"},{"content":"We (rOpenSci) started a repo to wrap the API for VertNet, an open access online database of vertebrate specimen records across many collection holders. Find the open source code here - please contribute if you are so inclined. We had a great Google Summer of Code student, Vijay Barve contributing to the repo this summer, so it is getting close to being CRAN-able.\nMost of the functions in the repo get you the raw data, but there were no functions to visualize the data. Since much of the data records of latitude and longitude data, maps are a natural visualization to use.\nWhat follows is a quick example of using the basic vertmap function.\nInstall rvertnet # install_github(\u0026#39;rvertnet\u0026#39;, \u0026#39;ropensci\u0026#39;) # uncomment if not installed # already library(rvertnet) Get data out \u0026lt;- vertoccurrence(q = \u0026#34;larva\u0026#34;, num = 100) # get records on keyword \u0026#39;larva\u0026#39;, limit to 100 nrow(out) # how many rows? [1] 100 Map Now map it using vertmap. This is a very basic function: it simply cleans up the input data.frame, removing rows without lat/long data, and providing warnings when the input data.frame is not in the correct format. vertmap uses the ggplot2 framework for the map. If you want to make you own map please do so - this is just a simple fxn to get you started if you want to take a quick look at the data.\nvertmap(input = out) # make a map using vertmap Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio.\n","permalink":"http://localhost:1313/2012/09/rvertnet/","summary":"\u003cp\u003eWe (\u003ca href=\"http://ropensci.org/\"\u003erOpenSci\u003c/a\u003e) started a repo to wrap the API for \u003ca href=\"http://vertnet.org/index.php\"\u003eVertNet\u003c/a\u003e, an open access online database of vertebrate specimen records across many collection holders. Find the open source code \u003ca href=\"https://github.com/ropensci/rvertnet\"\u003ehere\u003c/a\u003e - please contribute if you are so inclined.  We had a great Google Summer of Code student, \u003ca href=\"http://vijaybarve.wordpress.com/\"\u003eVijay Barve\u003c/a\u003e contributing to the repo this summer, so it is getting close to being CRAN-able.\u003c/p\u003e\n\u003cp\u003eMost of the functions in the repo get you the raw data, but there were no functions to visualize the data.  Since much of the data records of latitude and longitude data, maps are a natural visualization to use.\u003c/p\u003e","title":"Vertnet - getting vertebrate museum record data and a quick map"},{"content":"The problem: There are a lot of figures in published papers in the scholarly literature, like the below, from (Attwood et. al. 2012)):\nAt some point, a scientist wants to ask a question for which they can synthesize the knowledge on that question by collecting data from the published literature. This often requires something like the following workflow:\nSearch for relevant papers (e.g., via Google Scholar). Collect the papers. Decide which are appropriate for inclusion. Collect data from the figures using software on a native application. Examples include GraphClick and ImageJ. Proof data. Analyze data \u0026amp; publish paper. This workflow needs revamping, particularly in step number 3 - collecting the data. This data remains private, moving from one closed source (original publication) to another (personal computer). We can surely do better.\nA solution The data collection process (Step 3 above) can make use of modern technology, and be based in the browser. Some benefits of a browser based data collection approach:\nCross-platform: a data digitization program that lives in the browser can be more easily cross-platform (Linux/Mac/Windows) than a native app. Linked data: with the increasing abundance of APIs (application programming interfaces), we can link the data going into this app to anything of interest. This is not so easy in a native app. Automatic updates: a web based browser can be updated easily without requiring a user to go get updates. User-based: a web based browser can easily integrate secure user login so that users can be associated with data collected, allowing for quantification of user-based error, and eventually user based scores/badges/etc. if so desired. For those concerned about a browser based approach to data collection from figures, it will likely be possible to make it work offline as well, then send data up to servers when connected to the web again.\nWhat would be great about having data be public by default is that the work would be reproducible easily, at least on the data side of things. Hopefully the researchers would make all their code available publicly to recreate their analyses.\nQuestion: Why would this idea work? Better question: Why wouldn’t it work!\nI think this idea could be awesome. The reason I think it could be is based on two observations:\nThere is a seemingly endless supply of academic papers with figures in them from which data can be extracted.** There is increasing use of meta-analysis in science, which is fed by just this kind of data. ** p.s. in the future, perhaps we will move to all SVG format figures or something even better, in which case data can be extracted from the underlying XML\nOkay, maybe it\u0026rsquo;s a good idea, but who owns the data in figures in published papers? As far as I know, and I\u0026rsquo;ve checked with a few knowledgeable people, no one owns this data. So it\u0026rsquo;s ripe for the digitizing!\nOpen access I want this project to be totally open access (and I hope you do too). I love models like GitHub where everything is public by default (unless you are an enterprise user, exceptions, exceptions), and I think that is what this requires. You may be thinking though: \u0026ldquo;But I am collecting data for my meta-analysis and I don\u0026rsquo;t want to share the data with anyone else\u0026rdquo;. My answer: \u0026ldquo;I understand where you are coming from, but it doesn\u0026rsquo;t seem very likely that someone will be asking the exact same question as you and be looking for the data from the exact same papers\u0026rdquo;. There will just be a huge database of data from figures, and all the appropriate metadata of course. Anyone should be able to use this.\nAPIs It would be great to build this from the start having an API in mind. That is, how do we need to structure the data to be easily served up in an API to other websites, or pulled down to someone\u0026rsquo;s local machine within Python or R to do data manipulation, analysis, and visualization? We are going to need a key-value store database, such as MongoDB/CouchDB because ideally at least we would store the data collected, the figure itself, use information, etc.\nWhat is being done about this? I was fortunate enough to tag along with Ted Hart, a postdoc at UBC, on a recently submitted NCEAS working group proposal. Who knows if we\u0026rsquo;ll get it, but we are already working on a prototype, so we will hit the ground running if we get funded, and just hit the ground, but walk a bit slower if we don\u0026rsquo;t get the funding.\nWhat could this be in the future? At least in my mind, I think of this idea going the direction of gamification, including points, badges, etc., sort of like FoldIt or GalaxyZoo. At first we need alpha-, then beta-testers, which I imagine will most likely be academics exracting data for a meta-analysis for example. But in the future, it would be great to make the interface enjoyable enough to attract non-academics, which could greatly increase the speed of data collection.\nOnce there are a lot of people collecting data we can get many data points for every single data point in a graph. Whereas right now, someone clicks on each data point in a graph one, maybe two times if they are lucky. In the future, we could have ten different users clicking on each mean and each error bar in each graph. So exciting! The following figure illustrates this.\nWhat do you think? Is this idea totally insane? Is it do-able? Is it worth doing?\nGet the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio.\nReferences Attwood AS, Scott-Samuel NE, Stothart G, Munafò MR and Laks J (2012). “Glass Shape Influences Consumption Rate For Alcoholic Beverages.” Plos One, 7. http://dx.doi.org/10.1371/journal.pone.0043007.\n","permalink":"http://localhost:1313/2012/09/getting-data/","summary":"\u003ch2 id=\"the-problem\"\u003eThe problem:\u003c/h2\u003e\n\u003cp\u003eThere are a lot of figures in published papers in the scholarly literature, like the below, from (Attwood \u003cem\u003eet. al.\u003c/em\u003e 2012)):\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/getfig2.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAt some point, a scientist wants to ask a question for which they can synthesize the knowledge on that question by collecting data from the published literature.  This often requires something like the following workflow:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSearch for relevant papers (e.g., via Google Scholar).\u003c/li\u003e\n\u003cli\u003eCollect the papers.\u003c/li\u003e\n\u003cli\u003eDecide which are appropriate for inclusion.\u003c/li\u003e\n\u003cli\u003eCollect data from the figures using software on a native application.  Examples include \u003ca href=\"http://www.arizona-software.ch/graphclick/\"\u003eGraphClick\u003c/a\u003e and \u003ca href=\"http://rsbweb.nih.gov/ij/\"\u003eImageJ\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eProof data.\u003c/li\u003e\n\u003cli\u003eAnalyze data \u0026amp; publish paper.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis workflow needs revamping, particularly in step number 3 - collecting the data.  This data remains private, moving from one closed source (original publication) to another (personal computer).  We can surely do better.\u003c/p\u003e","title":"Getting data from figures in published papers"},{"content":"Metadata! Metadata is very cool. It\u0026rsquo;s super hot right now - everybody is talking about it. Okay, maybe not everyone, but it\u0026rsquo;s an important part of archiving scholarly work.\nWe are working on a repo on GitHub rmetadata to be a one stop shop for querying metadata from around the web. Various repos on GitHub we have started - rpmc, rdatacite, rdryad, rpensoft, rhindawi - will at least in part be folded into rmetadata.\nAs a start we are writing functions to hit any metadata services that use the OAI-PMH: \u0026ldquo;Open Archives Initiative Protocol for Metadata Harvesting\u0026rdquo; framework. OAI-PMH has six methods (or verbs as they are called) for data harvesting that are the same across different metadata providers:\nGetRecord Identify ListIdentifiers ListMetadataFormats ListRecords ListSets OAI-PMH provides an updating list of data providers, which we can easily use to get the base URLs for their data. Then we just use one of the six above methods to query their metadata.\nLet\u0026rsquo;s install rmetadata first. install_github(\u0026#34;rmetadata\u0026#34;, \u0026#34;ropensci\u0026#34;) library(rmetadata) The most basic thing you can do with OAI-PMH is identify the data provider, getting their basic information. The Identify verb. # one provider md_identify(provider = \u0026#34;datacite\u0026#34;) repositoryName baseURL protocolVersion 1 DataCite MDS http://oai.datacite.org/oai 2.0 adminEmail earliestDatestamp deletedRecord 1 admin@datacite.org 2011-01-01T00:00:00Z no granularity compression compression.1 1 YYYY-MM-DDThh:mm:ssZ gzip deflate description 1 oai, oai.datacite.org, :, oai:oai.datacite.org:12425, http://www.openarchives.org/OAI/2.0/oai-identifier http://www.openarchives.org/OAI/2.0/oai-identifier.xsd # many providers md_identify(provider = c(\u0026#34;datacite\u0026#34;, \u0026#34;pensoft\u0026#34;)) repositoryName baseURL protocolVersion 1 DataCite MDS http://oai.datacite.org/oai 2.0 2 Pensoft Publishers http://oai.pensoft.eu 2.0 adminEmail earliestDatestamp deletedRecord 1 admin@datacite.org 2011-01-01T00:00:00Z no 2 NULL 2008-07-04 no granularity compression compression.1 1 YYYY-MM-DDThh:mm:ssZ gzip deflate 2 YYYY-MM-DD NULL NULL description 1 oai, oai.datacite.org, :, oai:oai.datacite.org:12425, http://www.openarchives.org/OAI/2.0/oai-identifier http://www.openarchives.org/OAI/2.0/oai-identifier.xsd 2 NULL # no match for one, two matches for other md_identify(provider = c(\u0026#34;harvard\u0026#34;, \u0026#34;journal\u0026#34;)) $harvard x 1 no match found $journal repo_name 1 Hrcak - Portal of scientific journals of Croatia 2 International journal of Power Electronics Engineering # let\u0026#39;s pick one from the second md_identify(provider = \u0026#34;Hrcak\u0026#34;) repositoryName 1 Hrcak - Portal of scientific journals of Croatia baseURL protocolVersion adminEmail 1 http://hrcak.srce.hr/oai/ 2.0 hrcak@srce.hr earliestDatestamp deletedRecord granularity 1 2005-12-01 no YYYY-MM-DD description 1 oai, hrcak.srce.hr, :, oai:hrcak.srce.hr:anIdentifier, http://www.openarchives.org/OAI/2.0/oai-identifier http://www.openarchives.org/OAI/2.0/oai-identifier.xsd There are a variety of metadata formats, depending on the data provider - list them with the ListMetadataFormats verb. # List metadata formats for a provider md_listmetadataformats(provider = \u0026#34;dryad\u0026#34;) metadataPrefix 1 oai_dc 2 rdf 3 ore 4 mets schema 1 http://www.openarchives.org/OAI/2.0/oai_dc.xsd 2 http://www.openarchives.org/OAI/2.0/rdf.xsd 3 http://tweety.lanl.gov/public/schemas/2008-06/atom-tron.sch 4 http://www.loc.gov/standards/mets/mets.xsd metadataNamespace 1 http://www.openarchives.org/OAI/2.0/oai_dc/ 2 http://www.openarchives.org/OAI/2.0/rdf/ 3 http://www.w3.org/2005/Atom 4 http://www.loc.gov/METS/ # List metadata formats for a specific identifier for a provider md_listmetadataformats(provider = \u0026#34;pensoft\u0026#34;, identifier = \u0026#34;10.3897/zookeys.1.10\u0026#34;) identifier metadataPrefix 1 10.3897/zookeys.1.10 oai_dc 2 10.3897/zookeys.1.10 mods schema 1 http://www.openarchives.org/OAI/2.0/oai_dc.xsd 2 http://www.loc.gov/standards/mods/v3/mods-3-1.xsd metadataNamespace 1 http://www.openarchives.org/OAI/2.0/oai_dc/ 2 http://www.loc.gov/mods/v3 The ListRecords verb is used to harvest records from a repository head(md_listrecords(provider = \u0026#34;datacite\u0026#34;)[[1]][, 2:4]) identifier datestamp setSpec 1 oai:oai.datacite.org:32153 2011-06-08T08:57:11Z TIB 2 oai:oai.datacite.org:32200 2011-06-20T08:11:08Z TIB 3 oai:oai.datacite.org:32220 2011-06-28T14:11:08Z TIB 4 oai:oai.datacite.org:32241 2011-06-30T13:24:45Z TIB 5 oai:oai.datacite.org:32255 2011-07-01T12:09:24Z TIB 6 oai:oai.datacite.org:32282 2011-07-05T09:08:10Z TIB ListIdentifiers is an abbreviated form of ListRecords, retrieving only headers rather than records. # Single provider md_listidentifiers(provider = \u0026#34;datacite\u0026#34;, set = \u0026#34;REFQUALITY\u0026#34;)[[1]][1:10] [1] \u0026#34;oai:oai.datacite.org:32426\u0026#34; \u0026#34;oai:oai.datacite.org:32152\u0026#34; [3] \u0026#34;oai:oai.datacite.org:25453\u0026#34; \u0026#34;oai:oai.datacite.org:25452\u0026#34; [5] \u0026#34;oai:oai.datacite.org:25451\u0026#34; \u0026#34;oai:oai.datacite.org:25450\u0026#34; [7] \u0026#34;oai:oai.datacite.org:25449\u0026#34; \u0026#34;oai:oai.datacite.org:25407\u0026#34; [9] \u0026#34;oai:oai.datacite.org:48328\u0026#34; \u0026#34;oai:oai.datacite.org:48439\u0026#34; md_listidentifiers(provider = \u0026#34;dryad\u0026#34;, from = \u0026#34;2012-07-15\u0026#34;)[[1]][1:10] [1] \u0026#34;oai:datadryad.org:10255/dryad.9106\u0026#34; [2] \u0026#34;oai:datadryad.org:10255/dryad.33780\u0026#34; [3] \u0026#34;oai:datadryad.org:10255/dryad.33901\u0026#34; [4] \u0026#34;oai:datadryad.org:10255/dryad.33902\u0026#34; [5] \u0026#34;oai:datadryad.org:10255/dryad.34472\u0026#34; [6] \u0026#34;oai:datadryad.org:10255/dryad.34558\u0026#34; [7] \u0026#34;oai:datadryad.org:10255/dryad.39975\u0026#34; [8] \u0026#34;oai:datadryad.org:10255/dryad.35065\u0026#34; [9] \u0026#34;oai:datadryad.org:10255/dryad.35081\u0026#34; [10] \u0026#34;oai:datadryad.org:10255/dryad.35082\u0026#34; # Many providers out \u0026lt;- md_listidentifiers(provider = c(\u0026#34;datacite\u0026#34;, \u0026#34;pensoft\u0026#34;), from = \u0026#34;2012-08-21\u0026#34;) llply(out, function(x) x[1:10]) # display just a few of them [[1]] [1] \u0026#34;oai:oai.datacite.org:1099317\u0026#34; \u0026#34;oai:oai.datacite.org:1099572\u0026#34; [3] \u0026#34;oai:oai.datacite.org:1099824\u0026#34; \u0026#34;oai:oai.datacite.org:1099695\u0026#34; [5] \u0026#34;oai:oai.datacite.org:1088239\u0026#34; \u0026#34;oai:oai.datacite.org:1088122\u0026#34; [7] \u0026#34;oai:oai.datacite.org:1088190\u0026#34; \u0026#34;oai:oai.datacite.org:1175749\u0026#34; [9] \u0026#34;oai:oai.datacite.org:1175288\u0026#34; \u0026#34;oai:oai.datacite.org:1115603\u0026#34; [[2]] [1] \u0026#34;10.3897/phytokeys.16.2884\u0026#34; \u0026#34;10.3897/phytokeys.16.3602\u0026#34; [3] \u0026#34;10.3897/phytokeys.16.3186\u0026#34; \u0026#34;10.3897/zookeys.216.3407\u0026#34; [5] \u0026#34;10.3897/zookeys.216.3332\u0026#34; \u0026#34;10.3897/zookeys.216.3224\u0026#34; [7] \u0026#34;10.3897/zookeys.216.3769\u0026#34; \u0026#34;10.3897/zookeys.216.3360\u0026#34; [9] \u0026#34;10.3897/zookeys.216.3646\u0026#34; \u0026#34;10.3897/neobiota.14.3140\u0026#34; With ListSets you can retrieve the set structure of a repository. # arXiv, returns a data.frame head(md_listsets(provider = \u0026#34;arXiv\u0026#34;)[[1]]) setName setSpec 1 Computer Science cs 2 Mathematics math 3 Nonlinear Sciences nlin 4 Physics physics 5 Astrophysics physics:astro-ph 6 Condensed Matter physics:cond-mat # many providers, returns a list md_listsets(provider = c(\u0026#34;pensoft\u0026#34;, \u0026#34;arXiv\u0026#34;)) [[1]] setName setSpec 1 ZooKeys zookeys 2 BioRisk biorisk 3 PhytoKeys phytokeys 4 NeoBiota neobiota 5 Journal of Hymenoptera Research jhr 6 International Journal of Myriapodology ijm 7 Comparative Cytogenetics compcytogen 8 Subterranean Biology subtbiol 9 Nature Conservation natureconservation 10 MycoKeys mycokeys [[2]] setName setSpec 1 Computer Science cs 2 Mathematics math 3 Nonlinear Sciences nlin 4 Physics physics 5 Astrophysics physics:astro-ph 6 Condensed Matter physics:cond-mat 7 General Relativity and Quantum Cosmology physics:gr-qc 8 High Energy Physics - Experiment physics:hep-ex 9 High Energy Physics - Lattice physics:hep-lat 10 High Energy Physics - Phenomenology physics:hep-ph 11 High Energy Physics - Theory physics:hep-th 12 Mathematical Physics physics:math-ph 13 Nuclear Experiment physics:nucl-ex 14 Nuclear Theory physics:nucl-th 15 Physics (Other) physics:physics 16 Quantum Physics physics:quant-ph 17 Quantitative Biology q-bio 18 Quantitative Finance q-fin 19 Statistics stat Retrieve an individual metadata record from a repository using the GetRecord verb. # Single provider, one identifier md_getrecord(provider = \u0026#34;pensoft\u0026#34;, identifier = \u0026#34;10.3897/zookeys.1.10\u0026#34;) identifier datestamp 1 10.3897/zookeys.1.10 2008-07-04 dc.title 1 A new candidate for a Gondwanaland distribution in the Zodariidae (Araneae): Australutica in Africa dc.creator dc.subject dc.subject.1 dc.subject.2 dc.subject.3 1 Jocqué,Rudy new species Gondwanaland Soutpansberg Araneae dc.source 1 ZooKeys 1: 59-66 dc.description 1 Two new species of Australutica Jocqué, 1995, a genus formerly only known from Australia, are described from South Africa: A. africana n. sp. from Soutpansberg and A. normanlarseni n. sp. from the Cape Peninsula. The taxonomic position of the new species is discussed and a key to the species of Australutica is provided. dc.publisher dc.date dc.type dc.format 1 Pensoft Publishers 2008 Research Article text/html dc.identifier 1 http://dx.doi.org/10.3897/zookeys.1.10 dc.identifier.1 dc.language 1 http://www.pensoft.net/journals/zookeys/article/10/ en dc..attrs 1 http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd # Single provider, multiple identifiers md_getrecord(provider = \u0026#34;pensoft\u0026#34;, identifier = c(\u0026#34;10.3897/zookeys.1.10\u0026#34;, \u0026#34;10.3897/zookeys.4.57\u0026#34;)) identifier datestamp 1 10.3897/zookeys.1.10 2008-07-04 2 10.3897/zookeys.4.57 2008-12-17 dc.title 1 A new candidate for a Gondwanaland distribution in the Zodariidae (Araneae): Australutica in Africa 2 Studies of Tiger Beetles. CLXXVIII. A new Lophyra (Lophyra) from Somaliland (Coleoptera, Cicindelidae) dc.creator dc.subject dc.subject.1 dc.subject.2 dc.subject.3 1 Jocqué,Rudy new species Gondwanaland Soutpansberg Araneae 2 Cassola,Fabio Tiger Beetles Cicindelidae Lophyra Somaliland dc.source 1 ZooKeys 1: 59-66 2 ZooKeys 4: 65-69 dc.description 1 Two new species of Australutica Jocqué, 1995, a genus formerly only known from Australia, are described from South Africa: A. africana n. sp. from Soutpansberg and A. normanlarseni n. sp. from the Cape Peninsula. The taxonomic position of the new species is discussed and a key to the species of Australutica is provided. 2 A new tiger beetle species, Lophyra (Lophyra) praetermissa n. sp. (Coleoptera, Cicindelidae), obviously a close relative of L. (L.) histrio (Tschitschérine, 1903), is described from the environs of Erigavo, Somaliland (northern Somalia). Its discovery thus brings up to 73 the number of the species of this genus presently known worldwide (39 species of which - 29 from Africa - belong to the typonominal subgenus). dc.publisher dc.date dc.type dc.format 1 Pensoft Publishers 2008 Research Article text/html 2 Pensoft Publishers 2008 Research Article text/html dc.identifier 1 http://dx.doi.org/10.3897/zookeys.1.10 2 http://dx.doi.org/10.3897/zookeys.4.57 dc.identifier.1 dc.language 1 http://www.pensoft.net/journals/zookeys/article/10/ en 2 http://www.pensoft.net/journals/zookeys/article/57/ en dc..attrs 1 http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd 2 http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd Cool, so I hope people find this post and package useful. Let me know what you think in comments below, or if you have code specific comments or additions, go to the GitHub repo for rmetadata. In a upcoming post I will show an example of what you can do with rmetadata in terms of an actual research question.\nGet the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio.\n","permalink":"http://localhost:1313/2012/09/rmetadata/","summary":"\u003cp\u003eMetadata!  Metadata is very cool.  It\u0026rsquo;s super hot right now - everybody is talking about it.  Okay, maybe not everyone, but it\u0026rsquo;s an important part of archiving scholarly work.\u003c/p\u003e\n\u003cp\u003eWe are working on \u003ca href=\"https://github.com/ropensci/rmetadata\"\u003ea repo on GitHub \u003ccode\u003ermetadata\u003c/code\u003e\u003c/a\u003e to be a one stop shop for querying metadata from around the web.  Various repos on GitHub we have started - \u003ca href=\"https://github.com/ropensci/rpmc\"\u003erpmc\u003c/a\u003e, \u003ca href=\"https://github.com/ropensci/rpmc\"\u003erdatacite\u003c/a\u003e, \u003ca href=\"https://github.com/ropensci/rpmc\"\u003erdryad\u003c/a\u003e, \u003ca href=\"https://github.com/ropensci/rpmc\"\u003erpensoft\u003c/a\u003e, \u003ca href=\"https://github.com/ropensci/rpmc\"\u003erhindawi\u003c/a\u003e - will at least in part be folded into \u003ccode\u003ermetadata\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAs a start we are writing functions to hit any metadata services that use the \u003ca href=\"http://www.openarchives.org/OAI/openarchivesprotocol.html\"\u003eOAI-PMH: \u0026ldquo;Open Archives Initiative Protocol for Metadata Harvesting\u0026rdquo;\u003c/a\u003e framework.  \u003ccode\u003eOAI-PMH\u003c/code\u003e has six methods (or verbs as they are called) for data harvesting that are the same across different metadata providers:\u003c/p\u003e","title":"Scholarly metadata from R"},{"content":"I created an R package a while back to interact with some APIs that serve up data on what our elected represenatives are up to, including the New York Times Congress API, and the Sunlight Labs API.\nWhat kinds of things can you do with govdat? Here are a few examples.\nHow do the two major parties differ in the use of certain words (searches the congressional record using the Sunlight Labs Capitol Words API)?\n# install_github(\u0026#39;govdat\u0026#39;, \u0026#39;sckott\u0026#39;) library(govdat) library(reshape2) library(ggplot2) dems \u0026lt;- sll_cw_dates(phrase = \u0026#34;science\u0026#34;, start_date = \u0026#34;1996-01-20\u0026#34;, end_date = \u0026#34;2012-09-01\u0026#34;, granularity = \u0026#34;year\u0026#34;, party = \u0026#34;D\u0026#34;, printdf = TRUE) repubs \u0026lt;- sll_cw_dates(phrase = \u0026#34;science\u0026#34;, start_date = \u0026#34;1996-01-20\u0026#34;, end_date = \u0026#34;2012-09-01\u0026#34;, granularity = \u0026#34;year\u0026#34;, party = \u0026#34;R\u0026#34;, printdf = TRUE) df \u0026lt;- melt(rbind(data.frame(party = rep(\u0026#34;D\u0026#34;, nrow(dems)), dems), data.frame(party = rep(\u0026#34;R\u0026#34;, nrow(repubs)), repubs))) df$count \u0026lt;- as.numeric(df$count) ggplot(df, aes(yearmonth, count, colour = party, group = party)) + geom_line() + scale_colour_manual(values = c(\u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;)) + labs(y = \u0026#34;use of the word \u0026#39;Science\u0026#39;\u0026#34;) + theme_bw(base_size = 18) + opts(axis.text.x = theme_text(size = 10), panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), legend.position = c(0.2, 0.8)) Let\u0026rsquo;s get some data on donations to individual elected representatives.\nlibrary(plyr) # Let\u0026#39;s get Nancy Pelosi\u0026#39;s entity ID sll_ts_aggregatesearch(\u0026#34;Nancy Pelosi\u0026#34;)[[1]] $name [1] \u0026#34;Nancy Pelosi (D)\u0026#34; $count_given [1] 0 $firm_income [1] 0 $count_lobbied [1] 0 $seat [1] \u0026#34;federal:house\u0026#34; $total_received [1] 13769274 $state [1] \u0026#34;CA\u0026#34; $lobbying_firm NULL $count_received [1] 9852 $party [1] \u0026#34;D\u0026#34; $total_given [1] 0 $type [1] \u0026#34;politician\u0026#34; $id [1] \u0026#34;85ab2e74589a414495d18cc7a9233981\u0026#34; $non_firm_spending [1] 0 $is_superpac NULL # Her entity ID sll_ts_aggregatesearch(\u0026#34;Nancy Pelosi\u0026#34;)[[1]]$id [1] \u0026#34;85ab2e74589a414495d18cc7a9233981\u0026#34; # And search for her top donors by sector nancy \u0026lt;- ldply(sll_ts_aggregatetopsectors(sll_ts_aggregatesearch(\u0026#34;Nancy Pelosi\u0026#34;)[[1]]$id)) nancy # but just abbreviations for sectors sector count amount 1 F 1847 2698672.00 2 P 981 2243050.00 3 H 829 1412700.00 4 K 1345 1409836.00 5 Q 1223 1393154.00 6 N 829 1166187.00 7 B 537 932044.00 8 W 724 760800.00 9 Y 820 664926.00 10 E 201 283575.00 data(sll_ts_sectors) # load sectors abbrevations data nancy2 \u0026lt;- merge(nancy, sll_ts_sectors, by = \u0026#34;sector\u0026#34;) # attach full sector names nancy2_melt \u0026lt;- melt(nancy2[, -1], id.vars = 3) nancy2_melt$value \u0026lt;- as.numeric(nancy2_melt$value) # and lets plot some results ggplot(nancy2_melt, aes(sector_name, value)) + geom_bar() + coord_flip() + facet_wrap(~variable, scales = \u0026#34;free\u0026#34;, ncol = 1) ## It looks like a lot of individual donations (the count facet) by ## finance/insurance/realestate, but by amount, the most (by slim margin) ## is from labor organizations. Or we may want to get a bio of a congressperson. Here we get Todd Akin of MO. And some twitter searching too? Indeed\nout \u0026lt;- nyt_cg_memberbioroles(\u0026#34;A000358\u0026#34;) # cool, lots of info, output cutoff for brevity out[[3]][[1]][1:2] $member_id [1] \u0026#34;A000358\u0026#34; $first_name [1] \u0026#34;Todd\u0026#34; # we can get her twitter id from this bio, and search twitter using # twitteR package akintwitter \u0026lt;- out[[3]][[1]]$twitter_id # install.packages(\u0026#39;twitteR\u0026#39;) library(twitteR) tweets \u0026lt;- userTimeline(akintwitter, n = 100) tweets[1:5] # there\u0026#39;s some gems in there no doubt [[1]] [1] \u0026#34;RepToddAkin: Do you receive my Akin Alert e-newsletter? Pick the issues you’d like to get updates on and sign up here!\\nhttp://t.co/nZfiRjTF\u0026#34; [[2]] [1] \u0026#34;RepToddAkin: If the 2001 \u0026amp;amp; 2003 tax policies expire, taxes will increase over $4 trillion in the next 10 years. America can\u0026#39;t afford it. #stopthetaxhike\u0026#34; [[3]] [1] \u0026#34;RepToddAkin: A govt agency\u0026#39;s order shouldn\u0026#39;t defy constitutional rights. I\u0026#39;m still working for #religiousfreedom and repealing the HHS mandate. #prolife\u0026#34; [[4]] [1] \u0026#34;RepToddAkin: I am a cosponsor of the bill being considered today to limit abortions in DC. RT if you agree! #prolife http://t.co/Mesrjl0w\u0026#34; [[5]] [1] \u0026#34;RepToddAkin: We need to #StopTheTaxHike. Raising taxes like the President wants would destroy more than 700,000 jobs. #4jobs http://t.co/KUTd0M7U\u0026#34; Get the .Rmd file used to create this post at my github account - or .md file.\nWritten in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio.\n","permalink":"http://localhost:1313/2012/09/gov-dat/","summary":"\u003cp\u003eI created an R package a while back to interact with some APIs that serve up data on what our elected represenatives are up to, including the \u003ca href=\"http://developer.nytimes.com/\"\u003eNew York Times Congress API\u003c/a\u003e, and the \u003ca href=\"http://services.sunlightlabs.com/\"\u003eSunlight Labs API\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhat kinds of things can you do with \u003ccode\u003egovdat\u003c/code\u003e?  Here are a few examples.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHow do the two major parties differ in the use of certain words (searches the congressional record using the Sunlight Labs Capitol Words API)?\u003c/strong\u003e\u003c/p\u003e","title":"Getting data on your government"},{"content":"So I want to mine some #altmetrics data for some research I\u0026rsquo;m thinking about doing. The steps would be:\nGet journal titles for ecology and evolution journals. Get DOI\u0026rsquo;s for all papers in all the above journal titles. Get altmetrics data on each DOI. Do some fancy analyses. Make som pretty figs. Write up results. It\u0026rsquo;s early days, so jus working on the first step. However, getting a list of journals in ecology and evolution is frustratingly hard. This turns out to not be that easy if you are (1) trying to avoid Thomson Reuters, and (2) want a machine interface way to do it (read: API).\nUnfortunately, Mendeley\u0026rsquo;s API does not have methods for getting a list of journals by field, or at least I don\u0026rsquo;t know how to do it using their API. No worries though - Crossref comes to save the day. Here\u0026rsquo;s my attempt at this using the Crossref OAI-PMH.\nI wrote a little while loop to get journal titles from the Crossref OAI-PMH. This takes a while to run, but at least it works on my machine - hopefully yours too!\nlibrary(XML) library(RCurl) token \u0026lt;- \u0026#34;characters\u0026#34; # define a iterator, also used for gettingn the resumptionToken nameslist \u0026lt;- list() # define empty list to put joural titles in to while (is.character(token) == TRUE) { baseurl \u0026lt;- \u0026#34;http://oai.crossref.org/OAIHandler?verb=ListSets\u0026#34; if (token == \u0026#34;characters\u0026#34;) { tok2 \u0026lt;- NULL } else { tok2 \u0026lt;- paste(\u0026#34;\u0026amp;resumptionToken=\u0026#34;, token, sep = \u0026#34;\u0026#34;) } query \u0026lt;- paste(baseurl, tok2, sep = \u0026#34;\u0026#34;) crsets \u0026lt;- xmlToList(xmlParse(getURL(query))) names \u0026lt;- as.character(sapply(crsets[[4]], function(x) x[[\u0026#34;setName\u0026#34;]])) nameslist[[token]] \u0026lt;- names if (class(try(crsets[[2]]$.attrs[[\u0026#34;resumptionToken\u0026#34;]])) == \u0026#34;try-error\u0026#34;) { stop(\u0026#34;no more data\u0026#34;) } else token \u0026lt;- crsets[[2]]$.attrs[[\u0026#34;resumptionToken\u0026#34;]] } Yay! Hopefully it worked if you tried it. Let\u0026rsquo;s see how long the list of journal titles is.\nsapply(nameslist, length) # length of each list characters c65ebc3f-b540-4672-9c00-f3135bf849e3 10001 10001 6f61b343-a8f4-48f1-8297-c6f6909ca7f7 6864 allnames \u0026lt;- do.call(c, nameslist) # combine to list length(allnames) [1] 26866 Now, let\u0026rsquo;s use some regex to pull out the journal titles that are likely ecology and evolutionary biology journals. The ^ symbol says \u0026ldquo;the string must start here\u0026rdquo;. The \\\\s means whitespace. The [] lets you specify a set of letters you are looking for, e.g., [Ee] means capital E OR lowercase e. I threw in titles that had the words systematic and natrualist too. Tried to trim any whitespace as well using the stringr package.\nlibrary(stringr) ecotitles \u0026lt;- as.character(allnames[str_detect(allnames, \u0026#34;^[Ee]cology|\\\\s[Ee]cology\u0026#34;)]) evotitles \u0026lt;- as.character(allnames[str_detect(allnames, \u0026#34;^[Ee]volution|\\\\s[Ee]volution\u0026#34;)]) systtitles \u0026lt;- as.character(allnames[str_detect(allnames, \u0026#34;^[Ss]ystematic|\\\\s[Ss]systematic\u0026#34;)]) naturalist \u0026lt;- as.character(allnames[str_detect(allnames, \u0026#34;[Nn]aturalist\u0026#34;)]) ecoevotitles \u0026lt;- unique(c(ecotitles, evotitles, systtitles, naturalist)) # combine to list ecoevotitles \u0026lt;- str_trim(ecoevotitles, side = \u0026#34;both\u0026#34;) # trim whitespace, if any length(ecoevotitles) [1] 188 # Just the first ten titles ecoevotitles[1:10] [1] \u0026#34;Microbial Ecology in Health and Disease\u0026#34; [2] \u0026#34;Population Ecology\u0026#34; [3] \u0026#34;Researches on Population Ecology\u0026#34; [4] \u0026#34;Behavioral Ecology and Sociobiology\u0026#34; [5] \u0026#34;Microbial Ecology\u0026#34; [6] \u0026#34;Biochemical Systematics and Ecology\u0026#34; [7] \u0026#34;FEMS Microbiology Ecology\u0026#34; [8] \u0026#34;Journal of Experimental Marine Biology and Ecology\u0026#34; [9] \u0026#34;Applied Soil Ecology\u0026#34; [10] \u0026#34;Forest Ecology and Management\u0026#34; Get the .Rmd file used to create this post at my github account.\nWritten in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio.\n","permalink":"http://localhost:1313/2012/08/get-ecoevo-journal-titles/","summary":"\u003cp\u003eSo I want to mine some #altmetrics data for some research I\u0026rsquo;m thinking about doing.  The steps would be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGet journal titles for ecology and evolution journals.\u003c/li\u003e\n\u003cli\u003eGet DOI\u0026rsquo;s for all papers in all the above journal titles.\u003c/li\u003e\n\u003cli\u003eGet altmetrics data on each DOI.\u003c/li\u003e\n\u003cli\u003eDo some fancy analyses.\u003c/li\u003e\n\u003cli\u003eMake som pretty figs.\u003c/li\u003e\n\u003cli\u003eWrite up results.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt\u0026rsquo;s early days, so jus working on the first step.  However, getting a list of journals in ecology and evolution is frustratingly hard.  This turns out to not be that easy if you are (1) trying to avoid \u003ca href=\"http://thomsonreuters.com/\"\u003eThomson Reuters\u003c/a\u003e, and (2) want a machine interface way to do it (read: API).\u003c/p\u003e","title":"Getting ecology and evolution journal titles from R"},{"content":"So I heard many people say after or during the recent ESA conference in Portland that they really enjoyed the converstations more than listening to talks or looking at posters.\nThere was some chatter about doing an unconference associated with next year\u0026rsquo;s ESA conference in Minneapolis. And Sandra Chung (@sandramchung) got on it and started a wiki that we can all conribute ideas to. The wiki is here\nWhat is an unconference? The idea of an unconference is to have a community organized meetup in which interactions among people are emphasized over the traditional lecture and poster format. For example, many sessions may just be organized a single idea, and people attending have a discussion around the topic. The format can be decided by the community.\nWhat will we do there? The broadest restriction is that topics should be about science that happens online. You may say, \u0026ldquo;Well, real ecology happens in the field!\u0026rdquo;. Yes, but a lot of the products of ecology are put online, and increasingly the discussion of the practice of ecology happens online. Check out the Science Online 2012 website for a little taste of what we hope to achieve next year.\nHow do I get involved? Go to the wiki and start contributing. There are already some suggestions for topics\u0026hellip;Here\u0026rsquo;s a screenshot of the ideas for Session Proposals page:\nImportant! Use the #esaun13 hashtag to talk about this unconference on Twitter, G+, and app.net.\nGet the .Rmd file used to create this post at my github account.\nWritten in Markdown, with help from knitr.\n","permalink":"http://localhost:1313/2012/08/ecology-unconference/","summary":"\u003cp\u003eSo I heard many people say after or during the recent ESA conference in Portland that they really enjoyed the converstations more than listening to talks or looking at posters.\u003c/p\u003e\n\u003cp\u003eThere was some chatter about doing an unconference associated with next year\u0026rsquo;s ESA conference in Minneapolis. And Sandra Chung (@sandramchung) got on it and started a wiki that we can all conribute ideas to. The wiki is \u003ca href=\"http://ecologyunconference.wikispaces.com/\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWhat is an unconference? The idea of an unconference is to have a community organized meetup in which interactions among people are emphasized over the traditional lecture and poster format. For example, many sessions may just be organized a single idea, and people attending have a discussion around the topic. The format can be decided by the community.\u003c/p\u003e","title":"Ecology unconference at ESA 2013"},{"content":"So I was trying to figure out a fast way to make matrices with randomly allocated 0 or 1 in each cell of the matrix. I reached out on Twitter, and got many responses (thanks tweeps!).\nHere is the solution I came up with. See if you can tell why it would be slow.\n{% highlight r linenos %} mm \u0026lt;- matrix(0, 10, 5) apply(mm, c(1, 2), function(x) sample(c(0, 1), 1)) {% endhighlight %}\n{% highlight text %} [,1] [,2] [,3] [,4] [,5] [1,] 1 0 1 0 1 [2,] 0 0 1 1 1 [3,] 0 0 0 0 1 [4,] 0 1 1 0 1 [5,] 0 1 1 1 1 [6,] 1 0 1 1 1 [7,] 0 1 0 1 0 [8,] 0 0 1 0 1 [9,] 1 0 1 1 1 [10,] 1 0 0 1 1 {% endhighlight %}\nTed Hart (@distribecology) replied first with: {% highlight r linenos %} matrix(rbinom(10 * 5, 1, 0.5), ncol = 5, nrow = 10) {% endhighlight %}\n{% highlight text %} [,1] [,2] [,3] [,4] [,5] [1,] 1 1 0 1 1 [2,] 1 0 0 1 0 [3,] 0 1 0 0 0 [4,] 0 0 1 0 0 [5,] 1 0 1 0 0 [6,] 0 0 0 0 1 [7,] 1 0 0 0 0 [8,] 0 1 0 1 0 [9,] 1 1 1 1 0 [10,] 0 1 1 0 0 {% endhighlight %}\nNext, David Smith (@revodavid) and Rafael Maia (@hylospar) came up with about the same solution. {% highlight r linenos %} m \u0026lt;- 10 n \u0026lt;- 5 matrix(sample(0:1, m * n, replace = TRUE), m, n) {% endhighlight %}\n{% highlight text %} [,1] [,2] [,3] [,4] [,5] [1,] 0 0 0 0 1 [2,] 0 0 0 0 0 [3,] 0 1 1 0 1 [4,] 1 0 0 1 0 [5,] 0 0 0 0 1 [6,] 1 0 1 1 1 [7,] 1 1 1 1 0 [8,] 0 0 0 1 1 [9,] 1 0 0 0 1 [10,] 0 1 0 1 1 {% endhighlight %}\nThen there was the solution by Luis Apiolaza (@zentree). {% highlight r linenos %} m \u0026lt;- 10 n \u0026lt;- 5 round(matrix(runif(m * n), m, n)) {% endhighlight %}\n{% highlight text %} [,1] [,2] [,3] [,4] [,5] [1,] 0 1 1 0 0 [2,] 1 0 1 1 0 [3,] 1 0 1 0 0 [4,] 1 0 0 0 1 [5,] 1 0 1 1 0 [6,] 1 0 0 0 0 [7,] 1 0 0 0 0 [8,] 1 1 1 0 0 [9,] 0 0 0 0 1 [10,] 1 0 0 1 1 {% endhighlight %}\nLast, a solution was proposed using RcppArmadillo, but I couldn\u0026rsquo;t get it to work on my machine, but here is the function anyway if someone can. {% highlight r linenos %} library(inline) library(RcppArmadillo) f \u0026lt;- cxxfunction(body = \u0026ldquo;return wrap(arma::randu(5,10));\u0026rdquo;, plugin = \u0026ldquo;RcppArmadillo\u0026rdquo;) {% endhighlight %}\nAnd here is the comparison of system.time for each solution. {% highlight r linenos %} mm \u0026lt;- matrix(0, 10, 5) m \u0026lt;- 10 n \u0026lt;- 5\nsystem.time(replicate(1000, apply(mm, c(1, 2), function(x) sample(c(0, 1), 1)))) # @recology_ {% endhighlight %}\n{% highlight text %} user system elapsed 0.470 0.002 0.471 {% endhighlight %}\n{% highlight r linenos %} system.time(replicate(1000, matrix(rbinom(10 * 5, 1, 0.5), ncol = 5, nrow = 10))) # @distribecology {% endhighlight %}\n{% highlight text %} user system elapsed 0.014 0.000 0.015 {% endhighlight %}\n{% highlight r linenos %} system.time(replicate(1000, matrix(sample(0:1, m * n, replace = TRUE), m, n))) # @revodavid \u0026amp; @hylospar {% endhighlight %}\n{% highlight text %} user system elapsed 0.015 0.000 0.014 {% endhighlight %}\n{% highlight r linenos %} system.time(replicate(1000, round(matrix(runif(m * n), m, n)), )) # @zentree {% endhighlight %}\n{% highlight text %} user system elapsed 0.014 0.000 0.014 {% endhighlight %}\nIf you want to take the time to learn C++ or already know it, the RcppArmadillo option would likely be the fastest, but I think (IMO) for many scientists, especially ecologists, we probably don\u0026rsquo;t already know C++, so will stick to the next fastest options. Get the .Rmd file used to create this post at my github account. Written in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio. ","permalink":"http://localhost:1313/2012/08/making-matrices/","summary":"\u003cp\u003eSo I was trying to figure out a fast way to make matrices with randomly allocated 0 or 1 in each cell of the matrix. I reached out on Twitter, and got many responses (thanks tweeps!).\u003c/p\u003e\n\u003ch3 id=\"here-is-the-solution-i-came-up-with\"\u003eHere is the solution I came up with.\u003c/h3\u003e\n\u003cp\u003eSee if you can tell why it would be slow.\u003c/p\u003e\n\u003cp\u003e{% highlight r linenos %}\nmm \u0026lt;- matrix(0, 10, 5)\napply(mm, c(1, 2), function(x) sample(c(0, 1), 1))\n{% endhighlight %}\u003c/p\u003e","title":"Making matrices with zeros and ones"},{"content":" UPDATE: changed data source so that the entire example can be run by anyone on their own machine. Also, per Joachim\u0026rsquo;s suggestion, I put a box around the blown up area of the map. In addition, rgeos and maptools removed, not needed.\nHere\u0026rsquo;s a quick demo of creating a map with an inset within it using ggplot. The inset is achieved using the gridExtra package.\nInstall libraries install.packages(c(\u0026#34;ggplot2\u0026#34;, \u0026#34;maps\u0026#34;, \u0026#34;grid\u0026#34;, \u0026#34;gridExtra\u0026#34;)) library(\u0026#34;ggplot2\u0026#34;) library(\u0026#34;maps\u0026#34;) library(\u0026#34;grid\u0026#34;) library(\u0026#34;gridExtra\u0026#34;) Create a data frame dat \u0026lt;- data.frame(ecosystem = rep(c(\u0026#34;oak\u0026#34;, \u0026#34;steppe\u0026#34;, \u0026#34;prairie\u0026#34;), each = 8), lat = rnorm(24, mean = 51, sd = 1), lon = rnorm(24, mean = -113, sd = 5)) head(dat) #\u0026gt; ecosystem lat lon #\u0026gt; 1 oak 49.58285 -107.6930 #\u0026gt; 2 oak 52.58942 -116.6920 #\u0026gt; 3 oak 50.49277 -114.5542 #\u0026gt; 4 oak 50.05943 -116.5660 #\u0026gt; 5 oak 51.76492 -112.1457 #\u0026gt; 6 oak 52.82153 -112.8858 Get maps using the maps library Get a map of Canada\ncanadamap \u0026lt;- data.frame(map(\u0026#34;world\u0026#34;, \u0026#34;Canada\u0026#34;, plot = FALSE)[c(\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)]) Get a map of smaller extent\ncanadamapsmall \u0026lt;- canadamap[canadamap$x \u0026lt; -90 \u0026amp; canadamap$y \u0026lt; 54, ] canadamapsmall_ \u0026lt;- na.omit(canadamapsmall) This should get your corner points for the box, picking min and max of lat and lon\n(insetrect \u0026lt;- data.frame(xmin = min(canadamapsmall_$x), xmax = max(canadamapsmall_$x), ymin = min(canadamapsmall_$y), ymax = max(canadamapsmall_$y))) #\u0026gt; xmin xmax ymin ymax #\u0026gt; 1 -133.0975 -90.38942 48.04721 53.99915 Make the maps Create a theme to be used by both plots\nptheme \u0026lt;- theme( panel.border = element_rect(colour = \u0026#39;black\u0026#39;, size = 1, linetype = 1), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = \u0026#39;white\u0026#39;), legend.key = element_blank() ) The inset map, all of Canada\na \u0026lt;- ggplot(canadamap) + theme_bw(base_size = 22) + geom_path(data = canadamap, aes(x, y), colour = \u0026#34;black\u0026#34;, fill = \u0026#34;white\u0026#34;) + geom_rect(data = insetrect, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0, colour = \u0026#34;blue\u0026#34;, size = 1, linetype = 1) + ptheme %+% theme( legend.position = c(0.15, 0.80), axis.ticks = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank() ) + labs(x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) The larger map, zoomed in, with the data\nb \u0026lt;- ggplot(dat, aes(lon, lat, colour = ecosystem)) + theme_bw(base_size = 22) + geom_jitter(size = 4, alpha = 0.6) + geom_path(data = canadamapsmall, aes(x, y), colour = \u0026#34;black\u0026#34;, fill = \u0026#34;white\u0026#34;) + scale_size(guide = \u0026#34;none\u0026#34;) + ptheme %+% theme( legend.position = c(0.1, 0.20), legend.text = element_text(size = 12, face = \u0026#39;bold\u0026#39;), legend.title = element_text(size = 12, face = \u0026#39;bold\u0026#39;), axis.ticks = element_line(size = 2) ) + labs(x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) Print maps One an inset of the other. This approach uses the gridExtra package for flexible alignment, etc. of ggplot graphs.\ngrid.newpage() vpb_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) # the larger map vpa_ \u0026lt;- viewport(width = 0.4, height = 0.4, x = 0.8, y = 0.8) # the inset in upper right print(b, vp = vpb_) print(a, vp = vpa_) Written in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio.\n","permalink":"http://localhost:1313/2012/08/ggplot-inset-map/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUPDATE: changed data source so that the entire example can be run by anyone on their own machine. Also, per Joachim\u0026rsquo;s suggestion, I put a box around the blown up area of the map. In addition, rgeos and maptools removed, not needed.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHere\u0026rsquo;s a quick demo of creating a map with an inset within it using ggplot. The inset is achieved using the \u003ccode\u003egridExtra\u003c/code\u003e package.\u003c/p\u003e\n\u003ch3 id=\"install-libraries\"\u003eInstall libraries\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall.packages\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ggplot2\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;maps\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;grid\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gridExtra\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ggplot2\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;maps\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;grid\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;gridExtra\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"create-a-data-frame\"\u003eCreate a data frame\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(ecosystem \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erep\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;oak\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;steppe\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;prairie\u0026#34;\u003c/span\u003e), each \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    lat \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ernorm\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e24\u003c/span\u003e, mean \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e51\u003c/span\u003e, sd \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e), lon \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ernorm\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e24\u003c/span\u003e, mean \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e-113\u003c/span\u003e, sd \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehead\u003c/span\u003e(dat)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt;   ecosystem      lat       lon\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 1       oak 49.58285 -107.6930\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 2       oak 52.58942 -116.6920\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 3       oak 50.49277 -114.5542\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 4       oak 50.05943 -116.5660\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 5       oak 51.76492 -112.1457\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#\u0026gt; 6       oak 52.82153 -112.8858\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"get-maps-using-the-maps-library\"\u003eGet maps using the maps library\u003c/h3\u003e\n\u003cp\u003eGet a map of Canada\u003c/p\u003e","title":"ggplot2 maps with insets"},{"content":"Example of using the Global Names Resolver API to check species names There are a number of options for resolution of taxonomic names. The Taxonomic Name Resolution Service (TNRS) comes to mind. There is a new service for taxonomic name resoultion called the Global Names Resolver. They describe the service thusly \u0026ldquo;Resolve lists of scientific names against known sources. This service parses incoming names, executes exact or fuzzy matching as required, and displays a confidence score for each match along with its identifier.\u0026rdquo;.\nLoad required packages Just uncomment the code to use. # If you don\u0026#39;t have them already # install.packages(c(\u0026#39;RJSONIO\u0026#39;,\u0026#39;plyr\u0026#39;,\u0026#39;devtools\u0026#39;)) require(devtools) # install_github(\u0026#39;taxize_\u0026#39;,\u0026#39;ropensci\u0026#39;) library(RJSONIO) library(plyr) library(taxize) Get the data sources available Get just id\u0026rsquo;s and names of sources in a data.frame tail(gnr_datasources(todf = T)) ## id title ## 82 164 BioLib.cz ## 83 165 Tropicos - Missouri Botanical Garden ## 84 166 nlbif ## 85 167 IPNI ## 86 168 Index to Organism Names ## 87 169 uBio NameBank Give me the id for EOL (Encyclopedia of Life) out \u0026lt;- gnr_datasources(todf = T) out[out$title == \u0026#34;EOL\u0026#34;, \u0026#34;id\u0026#34;] ## [1] 12 Fuzzy search for sources with the word \u0026ldquo;zoo\u0026rdquo; out \u0026lt;- gnr_datasources(todf = T) outdf \u0026lt;- out[agrep(\u0026#34;zoo\u0026#34;, out$title, ignore.case = T), ] outdf[1:2, ] ## id title ## 20 100 Mushroom Observer ## 25 105 ZooKeys Resolve some names Search for Helianthus annuus and Homo sapiens, return a data.frame gnr(names = c(\u0026#34;Helianthus annuus\u0026#34;, \u0026#34;Homo sapiens\u0026#34;), returndf = TRUE)[1:2, ] ## data_source_id submitted_name name_string score title ## 1 4 Helianthus annuus Helianthus annuus 0.988 NCBI ## 3 10 Helianthus annuus Helianthus annuus 0.988 Freebase Search for the same species, with only using data source 12 (i.e., EOL) gnr(names = c(\u0026#34;Helianthus annuus\u0026#34;, \u0026#34;Homo sapiens\u0026#34;), data_source_ids = \u0026#34;12\u0026#34;, returndf = TRUE) ## data_source_id submitted_name name_string score title ## 1 12 Helianthus annuus Helianthus annuus 0.988 EOL ## 2 12 Homo sapiens Homo sapiens 0.988 EOL That\u0026rsquo;s it. Have fun! And put bugs/comments/etc. here. Written in Markdown, with help from knitr, and nice knitr highlighting/etc. in in RStudio.\nI prepared the markdown for this post by: KnitPost \u0026lt;- function(input, base.url = \u0026#34;/\u0026#34;) { require(knitr) opts_knit$set(base.url = base.url) fig.path \u0026lt;- paste0(\u0026#34;img/\u0026#34;, sub(\u0026#34;.Rmd$\u0026#34;, \u0026#34;\u0026#34;, basename(input)), \u0026#34;/\u0026#34;) opts_chunk$set(fig.path = fig.path) opts_chunk$set(fig.cap = \u0026#34;center\u0026#34;) render_jekyll() knit(input, envir = parent.frame()) } setwd(\u0026#34;/path/to/_posts\u0026#34;) KnitPost(\u0026#34;/path/to/postfile.Rmd\u0026#34;) from jfisher.\n","permalink":"http://localhost:1313/2012/07/global-names-resolver/","summary":"\u003ch2 id=\"example-of-using-the-global-names-resolver-api-to-check-species-names\"\u003eExample of using the Global Names Resolver API to check species names\u003c/h2\u003e\n\u003cp\u003eThere are a number of options for resolution of taxonomic names. The \u003ca href=\"http://tnrs.iplantcollaborative.org/\"\u003eTaxonomic Name Resolution Service (TNRS)\u003c/a\u003e comes to mind. There is a new service for taxonomic name resoultion called the \u003ca href=\"http://resolver.globalnames.org/\"\u003eGlobal Names Resolver\u003c/a\u003e. They describe the service thusly \u0026ldquo;\u003cem\u003eResolve lists of scientific names against known sources. This service parses incoming names, executes exact or fuzzy matching as required, and displays a confidence score for each match along with its identifier.\u003c/em\u003e\u0026rdquo;.\u003c/p\u003e","title":"Hitting the Global Names Resolver API"},{"content":"Many R packages/tools have come out recently for doing ecology and evolution. All of the below were described in Methods in Ecology and Evolution, except for spider, which came out in Molecular Ecology Resources. Here are some highlights.\nmvabund paper - get R pkg Model-based analysis of multivariate abundance data. Visualising data, fitting predictive models, checking assumptions, hypothesis testing. popdemo paper - get R pkg Population demography using projection matrix analysis. motmot paper - get R pkg Models of trait macroevolution on trees spider paper - get R pkg Analysis of species identity and evolution, with particular reference to DNA barcoding BaSTA paper - get R pkg Bayesian estimation of age-specific survival from incomplete mark–recapture/recovery data with covariates abc paper - get R pkg Approximate Bayesian Computation (ABC) RNetLogo paper - get R pkg Running and exploring individual-based models implemented in NetLogo phytools paper - get R pkg Tools for phylogenetic comparative biology smatr paper - get R pkg Estimation and inference about allometric lines RBrownie paper - get R pkg ? Testing hypotheses about rates of evolutionary change polytomy resolver paper - get R pkg Resolve polytomies on dated phylogenies with their R scripts [here][]. And a cool tool came out for the Python programming language.\nNichePy paper - get python Modular tools for estimating the similarity of ecological niche and species distribution models ","permalink":"http://localhost:1313/2012/06/recent-r-eeb-packages/","summary":"\u003cp\u003eMany R packages/tools have come out recently for doing ecology and evolution. All of the below were described in Methods in Ecology and Evolution, except for spider, which came out in \u003ca href=\"https://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1755-0998\"\u003eMolecular Ecology Resources\u003c/a\u003e. Here are some highlights.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emvabund \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00190.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/mvabund/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eModel-based analysis of multivariate abundance data. Visualising data, fitting predictive models, checking assumptions, hypothesis testing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003epopdemo \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00222.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/popdemo/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003ePopulation demography using projection matrix analysis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003emotmot \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00132.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/motmot/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eModels of trait macroevolution on trees\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003espider \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.1755-0998.2011.03108.x/abstract?deniedAccessCustomisedMessage=\u0026amp;userIsAuthenticated=false\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/spider/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eAnalysis of species identity and evolution, with particular reference to DNA barcoding\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBaSTA \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00186.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/BaSTA/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eBayesian estimation of age-specific survival from incomplete mark–recapture/recovery data with covariates\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eabc \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00179.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/abc/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eApproximate Bayesian Computation (ABC)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRNetLogo \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00180.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/RNetLogo/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eRunning and exploring individual-based models implemented in NetLogo\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ephytools \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00169.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/phytools/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eTools for phylogenetic comparative biology\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003esmatr \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00153.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://cran.r-project.org/web/packages/smatr/index.html\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eEstimation and inference about allometric lines\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRBrownie \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00112.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://www.brianomeara.info/tutorials/brownie\"\u003eget R pkg ?\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eTesting hypotheses about rates of evolutionary change\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003epolytomy resolver \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00103.x/abstract\"\u003epaper\u003c/a\u003e - \u003ca href=\"https://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00103.x/suppinfo\"\u003eget R pkg\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eResolve polytomies on dated phylogenies with their R scripts [here][].\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd a cool tool came out for the Python programming language.\u003c/p\u003e","title":"Recent R packages for ecology and evolution"},{"content":"So OpenCPU is pretty awesome. You can run R in a browser using URL calls with an alphanumeric code (e.g., x3e50ee0780) defining a stored function, and any arguments you pass to it.\nGo here to store a function. And you can output lots of different types of things: png, pdf, json, etc - see here.\nHere\u0026rsquo;s a function I created (originally from https://gist.github.com/2602432):\n# Store a function with man lines # Go Here: http://beta.opencpu.org/apps/opencpu.demo/storefunction/ # number: x3e50ee0780 # link: http://beta.opencpu.org/R/call/store:tmp/x3e50ee0780/png?id=\u0026#39;ropensci\u0026#39;\u0026amp;type=\u0026#39;org\u0026#39; the \u0026lt;- function (id = \u0026#39;hadley\u0026#39;, type = \u0026#39;user\u0026#39;) { require(RCurl); require(RJSONIO); require(ggplot2); require(reshape2); require(plyr) if(type == \u0026#39;user\u0026#39;){ url = \u0026#34;https://api.github.com/users/\u0026#34; } else if(type == \u0026#39;org\u0026#39;){ url = \u0026#34;https://api.github.com/orgs/\u0026#34; } else stop(\u0026#34;parameter \u0026#39;type\u0026#39; has to be either \u0026#39;user\u0026#39; or \u0026#39;org\u0026#39; \u0026#34;) url2 \u0026lt;- paste(url, id, \u0026#34;/repos?per_page=100\u0026#34;, sep = \u0026#34;\u0026#34;) xx \u0026lt;- getURL(url2) tt \u0026lt;- fromJSON(xx) if(!length(tt) == 1) { tt \u0026lt;- tt } else { stop(\u0026#34;user or organization not found - search GitHub? - https://github.com/\u0026#34;) } out \u0026lt;- ldply(tt, function(x) t(c(x$name, x$forks, x$watchers))) names(out) \u0026lt;- c(\u0026#34;Repo\u0026#34;, \u0026#34;Forks\u0026#34;, \u0026#34;Watchers\u0026#34;) out$Forks \u0026lt;- as.integer(out$Forks) out$Watchers \u0026lt;- as.integer(out$Watcher) out2 \u0026lt;- melt(out, id = 1) out2$value \u0026lt;- as.numeric(out2$value) out2$Repo \u0026lt;- as.factor(out2$Repo) repoorder \u0026lt;- unique(out2[order(out2$value, decreasing=FALSE),][,1]) out2$Repo \u0026lt;- factor(out2$Repo, levels = repoorder) ggplot(out2, aes(Repo, value)) + geom_bar() + coord_flip() + facet_wrap(~variable) + theme_bw(base_size = 18) } the() # default for hadley the(id=\u0026#39;defunkt\u0026#39;, type=\u0026#39;user\u0026#39;) # works - a user with even more repos than Hadley the(id=\u0026#39;ropensci\u0026#39;, type=\u0026#39;org\u0026#39;) # works - organization example the(id=\u0026#39;jeroenooms\u0026#39;, type=\u0026#39;user\u0026#39;) # works - organization example the(id=\u0026#39;SChamberlain\u0026#39;, type=\u0026#39;org\u0026#39;) # error message - mismatch of username with org type the(id=\u0026#39;adsff\u0026#39;, type=\u0026#39;user\u0026#39;) # error message - name does not exist It makes a ggplot2 graphic of your watchers and forks on each repo (up to 100 repos), sorted by descending number of forks/watchers.\nHere\u0026rsquo;s an example from the function. Paste the following in to your browser and you should get the below figure.\nhttp://beta.opencpu.org/R/call/opencpu.demo/gitstats/png\nAnd you can specify user or organization name using arguments in the URL\nhttp://beta.opencpu.org/R/call/opencpu.demo/gitstats/png?type='org'\u0026amp;id='ropensci'\nSweet. Have fun.\n","permalink":"http://localhost:1313/2012/05/opencpu-github-stats/","summary":"\u003cp\u003eSo \u003ca href=\"http://opencpu.org/\"\u003eOpenCPU\u003c/a\u003e is pretty awesome.  You can run R in a browser using URL calls with an alphanumeric code (e.g., x3e50ee0780) defining a stored function, and any arguments you pass to it.\u003c/p\u003e\n\u003cp\u003eGo \u003ca href=\"http://beta.opencpu.org/apps/opencpu.demo/storefunction/\"\u003ehere\u003c/a\u003e to store a function.  And you can output lots of different types of things: png, pdf, json, etc - see \u003ca href=\"http://opencpu.org/documentation/outputs/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s a function I created (originally from \u003ca href=\"https://gist.github.com/2602432\"\u003ehttps://gist.github.com/2602432\u003c/a\u003e):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Store a function with man lines\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Go Here: http://beta.opencpu.org/apps/opencpu.demo/storefunction/\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# number: x3e50ee0780\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# link: http://beta.opencpu.org/R/call/store:tmp/x3e50ee0780/png?id=\u0026#39;ropensci\u0026#39;\u0026amp;type=\u0026#39;org\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ethe \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e (id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;hadley\u0026#39;\u003c/span\u003e, type \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;user\u0026#39;\u003c/span\u003e) \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(RCurl); \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(RJSONIO); \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(ggplot2); \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(reshape2); \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(plyr)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e(type \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;user\u0026#39;\u003c/span\u003e){ url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://api.github.com/users/\u0026#34;\u003c/span\u003e } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e(type \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;org\u0026#39;\u003c/span\u003e){ url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://api.github.com/orgs/\u0026#34;\u003c/span\u003e } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;parameter \u0026#39;type\u0026#39; has to be either \u0026#39;user\u0026#39; or \u0026#39;org\u0026#39; \u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  url2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003epaste\u003c/span\u003e(url, id, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/repos?per_page=100\u0026#34;\u003c/span\u003e, sep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  xx \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetURL\u003c/span\u003e(url2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  tt \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efromJSON\u003c/span\u003e(xx)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e(\u003cspan style=\"color:#f92672\"\u003e!\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(tt) \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e) { tt \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e tt } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    { \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user or organization not found - search GitHub? - https://github.com/\u0026#34;\u003c/span\u003e) }   \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  out \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eldply\u003c/span\u003e(tt, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) \u003cspan style=\"color:#a6e22e\"\u003et\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(x\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ename, x\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eforks, x\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ewatchers)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003enames\u003c/span\u003e(out) \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Repo\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Forks\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Watchers\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  out\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eForks \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eas.integer\u003c/span\u003e(out\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eForks)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  out\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eWatchers \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eas.integer\u003c/span\u003e(out\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eWatcher)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  out2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emelt\u003c/span\u003e(out, id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  out2\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eas.numeric\u003c/span\u003e(out2\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  out2\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eRepo \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eas.factor\u003c/span\u003e(out2\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eRepo)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  repoorder \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(out2\u003cspan style=\"color:#a6e22e\"\u003e[order\u003c/span\u003e(out2\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue, decreasing\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFALSE\u003c/span\u003e),][,\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  out2\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eRepo \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efactor\u003c/span\u003e(out2\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eRepo, levels \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e repoorder)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(out2, \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(Repo, value)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_bar\u003c/span\u003e() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecoord_flip\u003c/span\u003e() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003efacet_wrap\u003c/span\u003e(\u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003evariable) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_bw\u003c/span\u003e(base_size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e18\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ethe\u003c/span\u003e() \u003cspan style=\"color:#75715e\"\u003e# default for hadley\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ethe\u003c/span\u003e(id\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;defunkt\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;user\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# works - a user with even more repos than Hadley\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ethe\u003c/span\u003e(id\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;ropensci\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;org\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# works - organization example\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ethe\u003c/span\u003e(id\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;jeroenooms\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;user\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# works - organization example\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ethe\u003c/span\u003e(id\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;SChamberlain\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;org\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# error message - mismatch of username with org type\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ethe\u003c/span\u003e(id\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;adsff\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;user\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# error message - name does not exist\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIt makes a \u003ca href=\"http://had.co.nz/ggplot2/\"\u003eggplot2\u003c/a\u003e graphic of your watchers and forks on each repo (up to 100 repos), sorted by descending number of forks/watchers.\u003c/p\u003e","title":"Visualize your Github stats (forks and watchers) in a browser with R!"},{"content":"There is a new R package in town, mvabund, which does, as they say \u0026ldquo;statistical methods for analysing multivariate abundance data\u0026rdquo;. The authors introduced the paper in an online early paper in Methods in Ecology and Evolution here, R package here.\nThe package is meant to visualize data, fit predictive models, check model assumptions, and test hypotheses about community-environment associations.\nHere is a quick example (originally from https://gist.github.com/2112141)\n#### mvabund play # install mvabund from CRAN pkg repository install.packages(\u0026#34;mvabund\u0026#34;) require(mvabund) # plot abundance by copepod species data(Tasmania) attach(Tasmania) tasmvabund \u0026lt;- mvabund(Tasmania$copepods) plot(tasmvabund ~ treatment, col = as.numeric(block)) # fit negative binomial model for each species and plot residuals vs. fitted tas.nb \u0026lt;- manyglm(copepods ~ block*treatment, family=\u0026#34;negative.binomial\u0026#34;) plot(tas.nb) ","permalink":"http://localhost:1313/2012/03/mvabund/","summary":"\u003cp\u003eThere is a new R package in town, mvabund, which does, as they say \u0026ldquo;statistical methods for analysing multivariate abundance data\u0026rdquo;.  The authors introduced the paper in an online early paper in Methods in Ecology and Evolution \u003ca href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2012.00190.x/full\"\u003ehere\u003c/a\u003e, R package \u003ca href=\"http://cran.r-project.org/web/packages/mvabund/index.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe package is meant to visualize data, fit predictive models, check model assumptions, and test hypotheses about community-environment associations.\u003c/p\u003e\n\u003cp\u003eHere is a quick example (originally from \u003ca href=\"https://gist.github.com/2112141\"\u003ehttps://gist.github.com/2112141\u003c/a\u003e)\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#### mvabund play\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# install mvabund from CRAN pkg repository\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall.packages\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mvabund\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(mvabund)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# plot abundance by copepod species \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003edata\u003c/span\u003e(Tasmania)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eattach\u003c/span\u003e(Tasmania)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etasmvabund \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emvabund\u003c/span\u003e(Tasmania\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003ecopepods)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eplot\u003c/span\u003e(tasmvabund \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e treatment, col \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eas.numeric\u003c/span\u003e(block))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# fit negative binomial model for each species and plot residuals vs. fitted\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etas.nb \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emanyglm\u003c/span\u003e(copepods \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e block\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003etreatment, family\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;negative.binomial\u0026#34;\u003c/span\u003e) \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eplot\u003c/span\u003e(tas.nb)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"mvabund1\" loading=\"lazy\" src=\"/mvabund1.png\"\u003e\u003c/p\u003e","title":"mvabund - new R pkg for multivariate abundance data"},{"content":"I should have thought of it earlier: In a day and age when we are increasingly reading scientific literature on computer screens, why is it that we limit our peer-reviewed data representation to static, unchanging graphs and plots? Why do we not try to create dynamic visualizations of our rich and varied data sets? Would we not derive benefits in the quality and clarity of scientific discourse from publishing these visualizations?\nAn article in the very good (and under-appreciated, in my opinion) American Scientist magazine written by Brian Hayes started me thinking about these questions. \u0026ldquo;Pixels or Perish\u0026rdquo; begins by recapping the evolution of graphics in scientific publications and notes that before people were good at making plots digitally, they were good at making figures from using photographic techniques; and before that, from elaborate engravings. Clearly, the state-of-the-art in scientific publishing is a moving target.\nHayes points out that one of the primary advantages of static images is that everyone knows how to use them and that almost no one lacks the tools to view them. That is, printed images in a magazine or static digital images in the portable document format (pdf) are easily viewed on paper or on a screen and can be readily interpreted by a wide audience. While I agree that this feature is very important, why have we not, as scientists, moved to the next level? We do not lack the ability to interpret data\u0026ndash;it is our job to do so\u0026ndash;not to mention that we are some of the heaviest generators of data in the first place.\nThe obstacles to progress towards interactive data are two-fold. First, generating dynamic data visualizations is not as easy as generating static plots. The data visualization tools simply are not as well developed and they do not show up as frequently in the programming environments in which scientists work. One example Hayes cites is that the ideas from programs such as D^3 have not yet made an appearance in software, like R and Matlab, that more scientists use. This is one reason why I am so excited by the work that our very own Scott has been doing with this Recology blog, in trying to promote awareness of tools in R.\nThe second is that neither of our currently dominant publishing formats (physical paper and digital pdf files) support dynamic graphics. Hayes says it better than I could: \u0026ldquo;…the Web is not where scientists publish…[publications are]…available through the Web, not on the Web.\u0026rdquo; So, not many current publications really take advantage of the new capabilities that the Web has offered us to showcase dynamic data sets. In fact, while Science and Nature\u0026ndash;just to name two prominent examples of scientific journals\u0026ndash;make available HTML versions of their articles, it seems like most of the interactivity is limited to looking at larger versions of figures in the articles*. I myself usually just download the pdf version of articles rather than viewing the HTML version. This obstacle, however, is not a fundamental one; it is only the current situation.\nThe more serious obstacle that Hayes foresees in transitioning to dynamic graphics is one of archiving. Figures in journal articles printed in 1900 are still readable today, but there is no guarantee that a particular file format will survive in usable form to 2100, or even 2020. I do not know the answer to this conundrum. A balance might need to be struck between generating static and dynamic data. At least in the medium term, papers should probably also contain static versions of figures representing dynamic data sets. It is inelegant, but it could avoid the situation where we lose access to information that was once there.\nThat said, if the New York Times can do it, so can we. We should not wait to make our data presentation more dynamic and interactive. At first, it will be difficult to incorporate these kinds of figures into the articles themselves, and they will likely be relegated to the \u0026ldquo;supplemental material\u0026rdquo; dead zone that is infrequently viewed. But the more dynamic material that journals receive from authors, the more incentive they will have to expand upon their current offerings. Ultimately, doing so will greatly improve the quality of scientific discourse.\n* Whether the lack of dynamic data visualization on these journals' websites is due to the authors not submitting such material or due to restrictions from the journals themselves, I do not know. I suspect the burden falls more on the authors' shoulders at this point than the journals'. ","permalink":"http://localhost:1313/2012/02/science-publications-need-interactive-graphics/","summary":"\u003cp\u003eI should have thought of it earlier: In a day and age when we are increasingly reading scientific literature on computer screens, why is it that we limit our peer-reviewed data representation to static, unchanging graphs and plots? Why do we not try to create dynamic visualizations of our rich and varied data sets? Would we not derive benefits in the quality and clarity of scientific discourse from publishing these visualizations?\u003c/p\u003e","title":"Journal Articles Need Interactive Graphics"},{"content":"Many ecologists are R users, but we vary in our understanding of the math and statistical theory behind models we use. There is no clear consensus on what should be the basic mathematical training of ecologists.\nTo learn what the community thinks, we invite you to fill out a short and anonymous questionnaire on this topic here.\nThe questionnaire was designed by Frédéric Barraquand, a graduate student at Université Pierre et Marie Curie, in collaboration with the International Network of Next-Generation Ecologists (INNGE).\n","permalink":"http://localhost:1313/2012/02/math-ecology-survey/","summary":"\u003cp\u003eMany ecologists are R users, but we vary in our understanding of the math and statistical theory behind models we use. There is no clear consensus on what should be the basic mathematical training of ecologists.\u003c/p\u003e\n\u003cp\u003eTo learn what the community thinks, we invite you to fill out a short and anonymous questionnaire on this topic \u003ca href=\"https://sites.google.com/site/mathematicsandecologysurvey/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe questionnaire was designed by \u003ca href=\"http://www.cebc.cnrs.fr/Fidentite/barraquand/barraquand.htm\"\u003eFrédéric Barraquand\u003c/a\u003e, a graduate student at Université Pierre et Marie Curie, in collaboration with the International Network of Next-Generation Ecologists (\u003ca href=\"http://www.innge.net\"\u003eINNGE\u003c/a\u003e).\u003c/p\u003e","title":"Take the INNGE survey on math and ecology"},{"content":"So Flora of North America is an awesome collection of taxonomic information for plants across the continent. However, the information within is not easily machine readable.\nSo, a little web scraping is called for.\nrfna is an R package to collect information from the Flora of North America.\nSo far, you can:\nGet taxonomic names from web pages that index the names. Then get daughter URLs for those taxa, which then have their own 2nd order daughter URLs you can scrape, or scrape the 1st order daughter page. Query Asteraceae taxa for whether they have paleate or epaleate receptacles. This function is something I needed, but more functions will be made like this to get specific traits. Further functions will do search, etc.\nYou can install by:\ninstall.packages(\u0026#34;devtools\u0026#34;) require(devtools) install_github(\u0026#34;rfna\u0026#34;, \u0026#34;rOpenSci\u0026#34;) require(rfna) Here is an example where a set of URLs is acquired using function getdaughterURLs, then the function receptacle is used to ask whether of each the taxa at those URLs have paleate or epaleate receptacles.\n# A web page with taxa names you want to get trait data from pg1 \u0026lt;- \u0026#39;http://www.efloras.org/browse.aspx?flora_id=1\u0026amp;start_taxon_id=10074\u0026amp;page=1\u0026#39; # Get the daughter URLs from the taxa on the page, using doMC to speed things up urls \u0026lt;- getdaughterURLs(pg1, cores=TRUE, no_cores=2) |======================================================================================================| 100% # Get the receptacle trait state for the taxa ldply(urls, receptacle, .progress=\u0026#39;text\u0026#39;) |======================================================================================================| 100% V1 V2 1 Acamptopappus epaleate 2 Acanthospermum paleate 3 Achillea paleate 4 Achyrachaena paleate 5 Acmella paleate 6 Acourtia paleate 7 Acroptilon epaleate 8 Adenocaulon epaleate 9 Adenophyllum epaleate 10 Ageratina epaleate 11 Ageratum epaleate 12 Agnorhiza paleate 13 Agoseris paleate 14 Almutaster epaleate 15 Amauriopsis epaleate 16 Amberboa epaleate 17 Amblyolepis epaleate 18 Amblyopappus epaleate 19 Ambrosia not found 20 Ampelaster epaleate 21 Amphiachyris epaleate 22 Amphipappus epaleate ----#RESULTS CUT OFF FOR BREVITY#---- ","permalink":"http://localhost:1313/2012/01/flora-north-america-scraping/","summary":"\u003cp\u003eSo \u003ca href=\"http://fna.huh.harvard.edu/\"\u003eFlora of North America\u003c/a\u003e is an awesome collection of taxonomic information for plants across the continent.  However, the information within is not easily machine readable.\u003c/p\u003e\n\u003cp\u003eSo, a little web scraping is called for.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ropensci/rfna\"\u003erfna\u003c/a\u003e is an R package to collect information from the Flora of North America.\u003c/p\u003e\n\u003cp\u003eSo far, you can:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGet taxonomic names from web pages that index the names.\u003c/li\u003e\n\u003cli\u003eThen get daughter URLs for those taxa, which then have their own 2nd order daughter URLs you can scrape, or scrape the 1st order daughter page.\u003c/li\u003e\n\u003cli\u003eQuery Asteraceae taxa for whether they have paleate or epaleate receptacles.  This function is something I needed, but more functions will be made like this to get specific traits.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFurther functions will do search, etc.\u003c/p\u003e","title":"Scraping Flora of North America"},{"content":"Described in a new Methods in Ecology and Evolution paper here, a new R package RNetLogo allows you to use NetLogo from R.\nNetLogo is software is a \u0026ldquo;multi-agent programmable modeling environment\u0026rdquo;. NetLogo can be used in individual- and agent-based modeling, and is used in the book Agent-based and Individual-based Modeling: A Practical Introduction by Railsback \u0026amp; Grimm.\nI have not tried the package yet, but looks interesting. I am always a fan of running stand-alone programs from R if possible.\n","permalink":"http://localhost:1313/2012/01/rnetlogo/","summary":"\u003cp\u003eDescribed in a new Methods in Ecology and Evolution paper \u003ca href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00180.x/abstract\"\u003ehere\u003c/a\u003e, a new \u003ca href=\"http://cran.r-project.org/\"\u003eR\u003c/a\u003e package \u003ca href=\"http://cran.r-project.org/web/packages/RNetLogo/index.html\"\u003eRNetLogo\u003c/a\u003e allows you to use \u003ca href=\"http://ccl.northwestern.edu/netlogo/\"\u003eNetLogo\u003c/a\u003e from R.\u003c/p\u003e\n\u003cp\u003eNetLogo is software is a \u0026ldquo;multi-agent programmable modeling environment\u0026rdquo;. NetLogo can be used in individual- and agent-based modeling, and is used in the book \u003ca href=\"http://www.railsback-grimm-abm-book.com/\"\u003e\u003cem\u003eAgent-based and Individual-based Modeling: A Practical Introduction\u003c/em\u003e\u003c/a\u003e by Railsback \u0026amp; Grimm.\u003c/p\u003e\n\u003cp\u003eI have not tried the package yet, but looks interesting. I am always a fan of running stand-alone programs from R if possible.\u003c/p\u003e","title":"RNetLogo - A package for running NetLogo from R"},{"content":"This post is only tangentially about open science. It is more directly about the process of peer review and how it might be improved. I am working on a follow-up post about how these points can be addressed in an open publishing environment.\nA recent paper on the arXiv got me thinking about the sticking points in the publishing pipeline. As it stands, most scientists have a pretty good understanding of how peer reviewed publishing is supposed to work. Once an author—or more likely, a group of authors—decides that a manuscript is ready for action, the following series of events will occur:\nthe authors submit the manuscript to the journal of choice; the journal\u0026rsquo;s editor makes a general decision about whether the article is appropriate for the journal; in the affirmative case, the editor selects referees for the manuscript and sends them the text for review; the referees return reviews of the manuscript (the referees are not typically identified to the authors); the editor makes the decision to reject the manuscript, accept it with minor revisions, or accept it with major revisions. Rejected manuscripts usually start over the process in another journal. Minor revisions to accepted manuscripts are usually made quickly and publication proceeds. In the case of major revisions, the suggested changes are made, if possible, and the manuscript is returned to the editor. At this point, the referees may get a second crack at the material (but not necessarily), before the editor makes a final accept/reject decision based on the feedback from the referees. Peer review of manuscripts exists for several reasons. For one, self-regulation determines the suitability of the material for publication if it was not already obvious to the editor of the journal. Having peer reviewers also improves the material and its presentation. Furthermore, having expert reviewers lends credibility to the work and insures that misleading, wrong, or crackpot material does not receive the stamp of credibility. Finally, finding appropriately skilled referees spreads the workload beyond the editors, who may not have the resources to evaluate every paper arriving at their desk.\nThough peer review has a storied history, it also has its drawbacks. First, and perhaps foremost, the process is often a slow one, with many months elapsing during even one round of communications between the authors, the editor, and the referees. Peer review is not always an objective process either: referees have the power to delay, or outright reject, work that their competitors have completed, and thus they may lose their impartiality in the process. Additionally, the publishing process does not reveal the feedback process that occurs between authors and referees, which can be a scientifically and pedagogically valuable exchange.\nOne proposal to address the shortcomings of the peer review process (alluded to in the first paragraph) was posted by Sergey Bozhevolnyi on the arXiv, a pre-publication website for many physics-related manuscripts. Bozhevolnyi calls his model of publishing Rapid, Impartial, and Comprehensive (RIC) publishing. To him, \u0026ldquo;rapid\u0026rdquo; means that editors should approve or reject manuscripts before the manuscripts are sent to the referees for review. Then, \u0026ldquo;impartial\u0026rdquo; means that referees, who might otherwise have an interest in rejecting a perfectly fine paper, lose the power to dictate whether or not a manuscript is published. Instead, the referees critique the paper without assessing whether it is publication-worthy. Lastly, \u0026ldquo;comprehensive\u0026rdquo; involves publishing everything having to do with the manuscript. That is, all positive and negative reviews are published in conjunction with the all versions of a manuscript.\nThe primary benefit of RIC, according to Bozhevolnyi, is that it saves the energies of authors, editors, and referees, thus allowing them all to do more research and less wrangling. Since most papers are ultimately accepted somewhere, then we should not cause additional delays in publishing by first rejecting them in multiple places. Instead, collate the manuscript and the reviews and publish them all together, along with any revisions to the manuscript. Having the reviews be publicly viewable will encourage referees to be more careful about writing their critiques and supporting their assertions, and the process as a whole will be more transparent than it currently is.\nBefore I critique the RIC publishing proposal, I should point out that some aspects of the proposal are very appealing. I particularly like the idea of publishing all reviews in addition to the manuscript. That said, I find it difficult to believe that the incentives for authors and referees change for the better under this proposal. For example, what happens if authors receive feedback, do not wish to invest the time to address the critique, and subsequently allow the original manuscript and the reviews to stand as they are? This situation seems like a moral hazard for authors that does a disservice to the quality of scientific literature. On the part of the referees, does removing decision-making authority make reviewing less appealing? Disempowering the referees by potentially ignoring their critique and only counting it as a minor part of the publishing process will not motivate them to write better reviews. In the case of editors, what makes us believe that an editor, or an editorial board, has the background to properly evaluate the suitability of work for acceptance into a journal? The reason we have referees in the current peer review system is because they have the very expertise and familiarity needed for this task.\nDoes the fact that Bozhevolnyi\u0026rsquo;s RIC proposal does not make sense mean that peer review is fine as it is? I do not think so. Instead, it is worth asking what parts of peer review we like and what parts we would like to improve. I posit that rejection, or the threat of rejection, is the greatest motivator for authors to make necessary changes to their manuscript. As such, rejection by peers is still the best way to require and receive revisions. Though I think that referees should retain their rejecting power (and their anonymity!), I feel strongly that the entire peer review process would benefit from the increased transparency and accountability that publishing unsigned reviews would add. As far as editors, they play a role in shaping the kind of journal they run by selecting appropriate material on a general level, but they should not play too large a role in determining the \u0026ldquo;important\u0026rdquo; research in any field. The model used by the journal [Public Library of Science One][] is a promising one in this regard, with the only acceptance criterion being whether the science is sound.\nThe amount of time that it takes to publish is one of the most frustrating aspects of peer review, however. Journals could voluntarily publish time-to-publication figures, a number which could then be used by authors—along with impact factors and acceptance rates—to decide which journals to submit to. For instance, an editor of the Journal of Orthodontics writes about just this fact in an editorial. A Google search for \u0026ldquo;journal time to publication\u0026rdquo; reveals that people have been thinking about this problem for a while (e.g. computer science comparisons), but no general standard exists across journals. In fact, I suspect these are numbers most journals are afraid will hurt them more than help them. Nevertheless, journals acknowledge the demand for rapid publication when they offer services like [Springer\u0026rsquo;s Fast Track publishingfasttrackpub or Physical Review\u0026rsquo;s Rapid Communications.\nUltimately, it may not matter what journals do because authors are routing around this problem via pre-publication archives such as the arXiv for physics-related subject matter. Though not without complications, especially in the health sciences (see, for example, \u0026ldquo;The Promise and Perils of Pre-Publication Review\u0026rdquo;), pre-publication allows authors to communicate results and establish priority without stressing about getting through the peer review process as fast as possible. Instead, the process takes its normal, slower course while authors move along their on-going research.\nI will conclude by leaving an open question that I may address in a future post: how do you encourage peer reviewers to do the best possible job, in a timely manner, without only relying on their altruism to doing good science and being good members of the community? It is this question about peer review, I feel, that is the most fraught with complication and subject to the law of unintended consequences if the incentives are changed.\n","permalink":"http://localhost:1313/2012/01/reviewing-peer-review-process/","summary":"\u003cp\u003eThis post is only tangentially about open science.  It is more directly about the process of peer review and how it might be improved.  I am working on a follow-up post about how these points can be addressed in an open publishing environment.\u003c/p\u003e\n\u003cp\u003eA \u003ca href=\"http://arxiv.org/abs/1110.0791\"\u003erecent paper on the arXiv\u003c/a\u003e got me thinking about the sticking points in the publishing pipeline.  As it stands, most scientists have a pretty good understanding of how peer reviewed publishing is supposed to work.  Once an author—or more likely, a group of authors—decides that a manuscript is ready for action, the following series of events will occur:\u003c/p\u003e","title":"Taking a Closer Look at Peer Review"},{"content":"UPDATE: Yeah, so the treeresstats function had a problem in one of the calculations. I fixed that and added some more calulcations to the function.\nI couldn\u0026rsquo;t find any functions to calculate number of polytomies, and related metrics.\nHere\u0026rsquo;s a simple function that gives four metrics on a phylo tree object:\n# calculate tree resolution stats treeresstats \u0026lt;- function(x) { require(phangorn) # load the phangorn package todo \u0026lt;- ( 1+Ntip(x)) : (Ntip(x) + Nnode(x) ) trsize_tips \u0026lt;- Ntip(x) trsize_nodes \u0026lt;- Nnode(x) polytomyvec \u0026lt;- sapply(todo, function(y) length(Children(x, y))) numpolys \u0026lt;- length(polytomyvec[polytomyvec \u0026gt; 2]) numpolysbytrsize_tips \u0026lt;- numpolys/trsize_tips numpolysbytrsize_nodes \u0026lt;- numpolys/trsize_nodes proptipsdescpoly \u0026lt;- sum(polytomyvec[polytomyvec \u0026gt; 2])/trsize_tips propnodesdich \u0026lt;- length(polytomyvec[polytomyvec == 2])/trsize_nodes list(trsize_tips = trsize_tips, trsize_nodes = trsize_nodes, numpolys = numpolys, numpolysbytrsize_tips = numpolysbytrsize_tips, numpolysbytrsize_nodes = numpolysbytrsize_nodes, proptipsdescpoly = proptipsdescpoly, propnodesdich = propnodesdich) } # Single tree example tree \u0026lt;- read.tree(text=\u0026#34;((((((artemisia_species:44,lactuca_species:44,senecio_species:44)6:46,campanula_species:90)5:17.75,((asclepias_species:71,galium_species:71)8:18.375,plantago_species:89.375)7:18.375)4:17.75,((cerastium_species:41.833332,silene_species:41.833332)10:41.833332,chenopodium_species:83.666664)9:41.833336)3:17.75,((geum_species:47,potentilla_species:47)12:48.125,lepidium_species:95.125)11:48.125)2:17.75,(bromus_species:12,elymus_species:12)13:149)1;\u0026#34;) dat \u0026lt;- treeresstats(tree) dat # Many trees example maketrees \u0026lt;- function(numtrees) { require(ape); require(plyr) trees \u0026lt;- rmtree(numtrees, 20) llply(trees, di2multi, tol = 0.5) } trees \u0026lt;- maketrees(30) dat \u0026lt;- ldply(trees, function(x) data.frame(treeresstats(x))) dat Here\u0026rsquo;s output from the gist above:\n$trsize_tips [1] 15 $trsize_nodes [1] 13 $numpolys [1] 1 $numpolysbytrsize_tips [1] 0.06666667 $numpolysbytrsize_nodes [1] 0.07692308 $proptipsdescpoly [1] 0.2 $propnodesdich [1] 0.9230769 And an example with many trees:\ntrsize_tips trsize_nodes numpolys numpolysbytrsize_tips numpolysbytrsize_nodes proptipsdescpoly propnodesdich 20 13 4 0.20 0.31 0.7 0.69 20 7 3 0.15 0.43 0.9 0.57 20 11 6 0.30 0.55 1.0 0.45 20 13 4 0.20 0.31 0.7 0.69 20 9 5 0.25 0.56 1.0 0.44 ","permalink":"http://localhost:1313/2012/01/phylogeny-resolution/","summary":"\u003cp\u003eUPDATE:  Yeah, so the treeresstats function had a problem in one of the calculations.  I fixed that and added some more calulcations to the function.\u003c/p\u003e\n\u003cp\u003eI couldn\u0026rsquo;t find any functions to calculate number of polytomies, and related metrics.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s a simple function that gives four metrics on a phylo tree object:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# calculate tree resolution stats\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etreeresstats \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(phangorn) \u003cspan style=\"color:#75715e\"\u003e# load the phangorn package\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  todo \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e ( \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eNtip\u003c/span\u003e(x)) \u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eNtip\u003c/span\u003e(x) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNnode\u003c/span\u003e(x) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  trsize_tips \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNtip\u003c/span\u003e(x)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  trsize_nodes \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNnode\u003c/span\u003e(x)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  polytomyvec \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esapply\u003c/span\u003e(todo, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(y) \u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eChildren\u003c/span\u003e(x, y)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  numpolys \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(polytomyvec[polytomyvec \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  numpolysbytrsize_tips \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e numpolys\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etrsize_tips\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  numpolysbytrsize_nodes \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e numpolys\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etrsize_nodes\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  proptipsdescpoly \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esum\u003c/span\u003e(polytomyvec[polytomyvec \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e])\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etrsize_tips\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  propnodesdich \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(polytomyvec[polytomyvec \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e])\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003etrsize_nodes\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003elist\u003c/span\u003e(trsize_tips \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e trsize_tips, trsize_nodes \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e trsize_nodes, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e       numpolys \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e numpolys, numpolysbytrsize_tips \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e numpolysbytrsize_tips,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e       numpolysbytrsize_nodes \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e numpolysbytrsize_nodes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e       proptipsdescpoly \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e proptipsdescpoly, propnodesdich \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e propnodesdich)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Single tree example\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etree \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eread.tree\u003c/span\u003e(text\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;((((((artemisia_species:44,lactuca_species:44,senecio_species:44)6:46,campanula_species:90)5:17.75,((asclepias_species:71,galium_species:71)8:18.375,plantago_species:89.375)7:18.375)4:17.75,((cerastium_species:41.833332,silene_species:41.833332)10:41.833332,chenopodium_species:83.666664)9:41.833336)3:17.75,((geum_species:47,potentilla_species:47)12:48.125,lepidium_species:95.125)11:48.125)2:17.75,(bromus_species:12,elymus_species:12)13:149)1;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etreeresstats\u003c/span\u003e(tree)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Many trees example\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emaketrees \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(numtrees) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(ape); \u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(plyr)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  trees \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ermtree\u003c/span\u003e(numtrees, \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003ellply\u003c/span\u003e(trees, di2multi, tol \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etrees \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emaketrees\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e30\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eldply\u003c/span\u003e(trees, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003etreeresstats\u003c/span\u003e(x)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere\u0026rsquo;s output from the gist above:\u003c/p\u003e","title":"Function for phylogeny resolution"},{"content":"Recology used to be hosted on Blogger, and my personal website was hosted on Wordpress. Neither platform was very satisfying. Blogger is very limited in their layouts, unless you use dynamic views, which suck because they don\u0026rsquo;t allow javascript snippets to render GitHub gists. Wordpress is just limited all around as you can\u0026rsquo;t put in hardly anythig excep text and some pictures. They both have their place, but not so much for content that requires syntax highlighting, references, etc.\nJekyll powered sites on GitHub are an awesome alternative. You do have to write the code yourself, but you can copy any number of templates on GitHub with a simple git clone onto your machine, edit the text a bit, push it up to GitHub, and that\u0026rsquo;s it.\nOn Blogger and Wordpress you can\u0026rsquo;t see the code behind why different blogs/sites look different. But on Jekyll/GitHub you can see the code behind each site (see here for a list of Jekyll/GitHub sites and their source code), which makes learning so easy.\nHere is a video on YouTube that explains in some detail Jekyll/GitHub sites:\nA great point in the video above is that a Jekyll site allows a workflow that is great not only for code-junkies, but for scientists. What is the most important thing about science? That it is reproducible of course. Documenting your code and sharing with everyone on GitHub or SVN, etc. is great for science in facilitating collaboration and facilitating transparency. Having your website/blog on Jekyll fits right in to this workflow (that is, pull down any changes - write/edit something - commit - push to GitHub). Although this sort of worklow isn\u0026rsquo;t necessary for a blog, it is nice for scientists to use this workflow all the time.\nHere\u0026rsquo;s how to get started:\nInstall git Get a free GitHub account and configure GitHub. If you are afraid of the command line, there is a great GitHub app here. git clone a jekyll template to your machine. There are hundreds of these now. Look here for your favorite, and git clone it. *** Edit the template you have cloned, and commit and push to GitHub. That\u0026rsquo;s it. It will take just a bit to render. There is more to it than that, but that is how you can get started. If you want to add comments, Disqus is a great option. Once you fork someones jekyll site, make sure to change all the personal/site specific information to your information, including the RSS feed.\n*** Note: You can name your repo for your site/blog as yourgithubname.github.com if you want your URL for the site to be http://yourgithubname.github.com. Or you can name your repo whatever you want, e.g., disrepo, then the URL will be http://yourgithubname.github.com/disrepo.\n","permalink":"http://localhost:1313/2012/01/moving-from-blogger-wordpress-to-jekyll/","summary":"\u003cp\u003eRecology used to be hosted on Blogger, and my personal website was hosted on Wordpress.  Neither platform was very satisfying.  Blogger is very limited in their layouts, unless you use dynamic views, which suck because they don\u0026rsquo;t allow javascript snippets to render GitHub gists.  Wordpress is just limited all around as you can\u0026rsquo;t put in hardly anythig excep text and some pictures. They both have their place, but not so much for content that requires syntax highlighting, references, etc.\u003c/p\u003e","title":"Moving from blogger and wordpress to jekyll"},{"content":"So my advisor pointed out this \u0026rsquo;new\u0026rsquo; (well, 2004), way of plotting results of logistic regression results. The idea was presented in a 2004 Bulletin of the Ecological Society of America issue (here). I tried to come up with a solution using, what else, ggplot2. I don\u0026rsquo;t have it quite all the way down - I am missing the second y-axis values for the histograms, but someone smarter than me can figure that part out (note that Hadley doesn\u0026rsquo;t want to support second y-axes in ggplot2, but they can probably be hacked on).\nHere\u0026rsquo;s the code (originally was in https://gist.github.com/1589136):\n# Define the function loghistplot \u0026lt;- function(data) { require(ggplot2); require(gridExtra) # load packages names(data) \u0026lt;- c(\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;) # rename columns # get min and max axis values min_x \u0026lt;- min(data$x) max_x \u0026lt;- max(data$x) min_y \u0026lt;- min(data$y) max_y \u0026lt;- max(data$y) # get bin numbers bin_no \u0026lt;- max(hist(data$x)$counts) + 5 # create plots a \u0026lt;- ggplot(data, aes(x = x, y = y)) + theme_bw(base_size=16) + geom_smooth(method = \u0026#34;glm\u0026#34;, family = \u0026#34;binomial\u0026#34;, se = TRUE, colour=\u0026#39;black\u0026#39;, size=1.5, alpha = 0.3) + # scale_y_continuous(limits=c(0,1), breaks=c(0,1)) + scale_x_continuous(limits=c(min_x,max_x)) + opts(panel.grid.major = theme_blank(), panel.grid.minor=theme_blank(), panel.background = theme_blank()) + labs(y = \u0026#34;Probability\\n\u0026#34;, x = \u0026#34;\\nYour X Variable\u0026#34;) b \u0026lt;- ggplot(data[data$y == unique(data$y)[1], ], aes(x = x)) + theme_bw(base_size=16) + geom_histogram(fill = \u0026#34;grey\u0026#34;) + scale_y_continuous(limits=c(0,bin_no)) + scale_x_continuous(limits=c(min_x,max_x)) + opts(panel.grid.major = theme_blank(), panel.grid.minor=theme_blank(), axis.text.y = theme_blank(), axis.text.x = theme_blank(), axis.ticks = theme_blank(), panel.border = theme_blank(), panel.background = theme_blank()) + labs(y=\u0026#39;\\n\u0026#39;, x=\u0026#39;\\n\u0026#39;) c \u0026lt;- ggplot(data[data$y == unique(data$y)[2], ], aes(x = x)) + theme_bw(base_size=16) + geom_histogram(fill = \u0026#34;grey\u0026#34;) + scale_y_continuous(trans=\u0026#39;reverse\u0026#39;) + scale_y_continuous(trans=\u0026#39;reverse\u0026#39;, limits=c(bin_no,0)) + scale_x_continuous(limits=c(min_x,max_x)) + opts(panel.grid.major = theme_blank(),panel.grid.minor=theme_blank(), axis.text.y = theme_blank(), axis.text.x = theme_blank(), axis.ticks = theme_blank(), panel.border = theme_blank(), panel.background = theme_blank()) + labs(y=\u0026#39;\\n\u0026#39;, x=\u0026#39;\\n\u0026#39;) grid.newpage() pushViewport(viewport(layout = grid.layout(1,1))) vpa_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) vpb_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) vpc_ \u0026lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) print(b, vp = vpb_) print(c, vp = vpc_) print(a, vp = vpa_) } # Examples # loghistplot(mtcars[,c(\u0026#34;mpg\u0026#34;,\u0026#34;vs\u0026#34;)]) # loghistplot(movies[,c(\u0026#34;rating\u0026#34;,\u0026#34;Action\u0026#34;)]) logpointplot \u0026lt;- function(data) { require(ggplot2); require(gridExtra) # load packages names(data) \u0026lt;- c(\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;) # rename columns # get min and max axis values min_x \u0026lt;- min(data$x) max_x \u0026lt;- max(data$x) min_y \u0026lt;- min(data$y) max_y \u0026lt;- max(data$y) # create plots ggplot(data, aes(x = x, y = y)) + theme_bw(base_size=16) + geom_point(alpha = 0.5, position = position_jitter(w=0, h=0.02)) + geom_smooth(method = \u0026#34;glm\u0026#34;, family = \u0026#34;binomial\u0026#34;, se = TRUE, colour=\u0026#39;black\u0026#39;, size=1.5, alpha = 0.3) + scale_x_continuous(limits=c(min_x,max_x)) + opts(panel.grid.major = theme_blank(), panel.grid.minor=theme_blank(), panel.background = theme_blank()) + labs(y = \u0026#34;Probability\\n\u0026#34;, x = \u0026#34;\\nYour X Variable\u0026#34;) } # Examples # logpointplot(mtcars[,c(\u0026#34;mpg\u0026#34;,\u0026#34;vs\u0026#34;)]) # logpointplot(movies[,c(\u0026#34;rating\u0026#34;,\u0026#34;Action\u0026#34;)]) Here\u0026rsquo;s a few examples using datasets provided with the ggplot2 package:\nloghistplot(mtcars[,c(\u0026#34;mpg\u0026#34;,\u0026#34;vs\u0026#34;)]) loghistplot(movies[,c(\u0026#34;rating\u0026#34;,\u0026#34;Action\u0026#34;)]) And two examples of the logpointplot function:\nlogpointplot(mtcars[,c(\u0026#34;mpg\u0026#34;,\u0026#34;vs\u0026#34;)]) logpointplot(movies[,c(\u0026#34;rating\u0026#34;,\u0026#34;Action\u0026#34;)]) ","permalink":"http://localhost:1313/2012/01/logistic-regression-barplot-fig/","summary":"\u003cp\u003eSo my advisor pointed out this \u0026rsquo;new\u0026rsquo; (well, 2004), way of plotting results of logistic regression results.  The idea was presented in a 2004 Bulletin of the Ecological Society of America issue (\u003ca href=\"http://esapubs.org/bulletin/backissues/085-3/bulletinjuly2004_2column.htm#tools1\"\u003ehere\u003c/a\u003e).  I tried to come up with a  solution using, what else, ggplot2.  I don\u0026rsquo;t have it quite all the way down - I am missing the second y-axis values for the histograms, but someone smarter than me can figure that part out (note that Hadley doesn\u0026rsquo;t want to support second y-axes in ggplot2, but they can probably be hacked on).\u003c/p\u003e","title":"Presenting results of logistic regression"},{"content":"Does this work on twitterfeed?\n","permalink":"http://localhost:1313/2012/01/testing-twitterfeed/","summary":"\u003cp\u003eDoes this work on twitterfeed?\u003c/p\u003e","title":"Testing twitterfeed"},{"content":"So the Weecology folks have published a large dataset on mammal communities in a data paper in Ecology. I know nothing about mammal communities, but that doesn\u0026rsquo;t mean one can\u0026rsquo;t play with the data\u0026hellip;\nTheir dataset consists of five csv files: communities, references, sites, species, and trapping data Where are these sites, and by the way, do they vary much in altitude? Let\u0026rsquo;s zoom in on just the states What phylogenies can we get for the species in this dataset? We can use the rOpenSci package treebase to search the online phylogeny repository TreeBASE. Limiting to returning a max of 1 tree (to save time), we can see that X species are in at least 1 tree on the TreeBASE database. Nice.\nSo there are 321 species in the database with at least 1 tree in the TreeBASE database. Of course there could be many more, but we limited results from TreeBASE to just 1 tree per query.\nHere\u0026rsquo;s the code: ","permalink":"http://localhost:1313/2011/12/weecology-can-has-new-mammal-dataset/","summary":"\u003cp\u003eSo the \u003ca href=\"http://weecology.org/\"\u003eWeecology\u003c/a\u003e folks have published a large dataset on mammal communities in a data paper in \u003ca href=\"http://www.esajournals.org/doi/abs/10.1890/11-0262.1\"\u003eEcology\u003c/a\u003e. I know nothing about mammal communities, but that doesn\u0026rsquo;t mean one can\u0026rsquo;t play with the data\u0026hellip;\u003c/p\u003e\n\u003ch3 id=\"their-dataset-consists-of-five-csv-files\"\u003eTheir dataset consists of five csv files:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecommunities,\u003c/li\u003e\n\u003cli\u003ereferences,\u003c/li\u003e\n\u003cli\u003esites,\u003c/li\u003e\n\u003cli\u003especies, and\u003c/li\u003e\n\u003cli\u003etrapping data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"where-are-these-sites-and-by-the-way-do-they-vary-much-in-altitude\"\u003eWhere are these sites, and by the way, do they vary much in altitude?\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"http://3.bp.blogspot.com/-BKqBoPCDA_A/Tvx9nLbMlkI/AAAAAAAAFPA/9_pG_Ihx33I/s1600/usmap.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"http://1.bp.blogspot.com/-KkU_EcX8-EY/Tvx9n7hiP9I/AAAAAAAAFPI/7LoV0IjRiAM/s1600/worldmap.png%22\"\u003e\u003c/p\u003e\n\u003ch3 id=\"lets-zoom-in-on-just-the-states\"\u003eLet\u0026rsquo;s zoom in on just the states\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"http://3.bp.blogspot.com/-BKqBoPCDA_A/Tvx9nLbMlkI/AAAAAAAAFPA/9_pG_Ihx33I/s1600/usmap.png%22\"\u003e\u003c/p\u003e","title":"Weecology can has new mammal dataset"},{"content":"This blog has lasted a whole year already. Thanks for reading and commenting.\nThere are a couple of announcements: Less blogging: I hope to put in many more years blogging here, but in full disclosure I am blogging for Journal of Ecology now, so I am going to be (and already have been) blogging less here. More blogging: If anyone wants to write guest posts at Recology on the topics of using R for ecology and evolution, or open science, please contact me. Different blogging: I was going to roll out the new dynamic views for this blog, but Google doesn\u0026rsquo;t allow javascript, which is how I include code using GitHub gists. Oh well\u0026hellip; Anywho, here is the breakdown of visits to this blog, visualized using #ggplot2, of course. There were a total of about 23,000 pageviews in the first year of this blog.\nHere is the pie chart code I used:\nVisits to top ten posts:\nVisits by by pages:\nVisits by top referring sites:\nVisits by country:\nVisits by browsers:\nVisits by operating system:\n","permalink":"http://localhost:1313/2011/12/recology-is-1-yr-old/","summary":"\u003cp\u003eThis blog has lasted a whole year already. Thanks for reading and commenting.\u003c/p\u003e\n\u003ch3 id=\"there-are-a-couple-of-announcements\"\u003eThere are a couple of announcements:\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eLess blogging: I hope to put in many more years blogging here, but in full disclosure \u003ca href=\"http://jecologyblog.wordpress.com/\"\u003eI am blogging for Journal of Ecology\u003c/a\u003e now, so I am going to be (and already have been) blogging less here.\u003c/li\u003e\n\u003cli\u003eMore blogging: If anyone wants to write guest posts at Recology on the topics of using R for ecology and evolution, or open science, please contact me.\u003c/li\u003e\n\u003cli\u003eDifferent blogging: I was going to roll out the new dynamic views for this blog, but Google doesn\u0026rsquo;t allow javascript, which is how I include code using GitHub gists. Oh well\u0026hellip;\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAnywho, here is the breakdown of visits to this blog, visualized using #ggplot2, of course. There were a total of about 23,000 pageviews in the first year of this blog.\u003c/p\u003e","title":"Recology is 1 yr old"},{"content":"Sorry for the temporary loss of GitHub gists\u0026hellip;Hopefully dynamic views will support javascript soon!!\n","permalink":"http://localhost:1313/2011/12/dynamic-views-don-t-support-javascript-so-reverting-back-to-simple-views/","summary":"\u003cp\u003eSorry for the temporary loss of GitHub gists\u0026hellip;Hopefully dynamic views will support javascript soon!!\u003c/p\u003e","title":"Dynamic views don't support javascript-so reverting back to simple views"},{"content":"UPDATE: code and figure updated at 647 AM CST on 19 Dec \u0026lsquo;11. Also, see Jarrett Byrnes (improved) fork of my gist here.\nThe site I WORK FOR THE INTERNET is collecting pictures and first names (last name initials only) to show collective support against SOPA (the Stop Online Piracy Act). Please stop by their site and add your name/picture.\nI used the #rstats package twitteR, created by Jeff Gentry, to search for tweets from people signing this site with their picture, then plotted using ggplot2, and also used Hadley\u0026rsquo;s lubridate to round timestamps on tweets to be able to bin tweets in to time slots for plotting.\nTweets containing the phrase \u0026lsquo;I work for the internet\u0026rsquo; by time:\nHere\u0026rsquo;s the code as a GitHub gist. Sometimes the searchTwitter fxn doesn\u0026rsquo;t returns an error, which I don\u0026rsquo;t understand, but you can play with it:\n","permalink":"http://localhost:1313/2011/12/i-work-for-internet/","summary":"\u003cp\u003eUPDATE: code and figure updated at 647 AM CST on 19 Dec \u0026lsquo;11. Also, see Jarrett Byrnes (improved) fork of my gist \u003ca href=\"https://gist.github.com/1474802\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe site \u003ca href=\"http://iworkfortheinternet.org/\"\u003eI WORK FOR THE INTERNET\u003c/a\u003e is collecting pictures and first names (last name initials only) to show collective support against SOPA (the Stop Online Piracy Act). Please stop by their site and add your name/picture.\u003c/p\u003e\n\u003cp\u003eI used the #rstats package twitteR, created by Jeff Gentry, to search for tweets from people signing this site with their picture, then plotted using ggplot2, and also used Hadley\u0026rsquo;s lubridate to round timestamps on tweets to be able to bin tweets in to time slots for plotting.\u003c/p\u003e","title":"I Work For The Internet !"},{"content":"Three presentations uploaded on LondonR meetings website. I especially enjoyed the JD Long presentation on the seque package for simulations using Amazon\u0026rsquo;s EC2.\n","permalink":"http://localhost:1313/2011/12/londonr-meetings-presentations/","summary":"\u003cp\u003eThree presentations uploaded on \u003ca href=\"http://www.londonr.org/Presentations/Agenda.html\"\u003eLondonR meetings website\u003c/a\u003e.  I especially enjoyed the \u003ca href=\"http://www.londonr.org/Presentations/segue-presentation-LondonRUG%20(1).ppt\"\u003eJD Long presentation\u003c/a\u003e on the seque package for simulations using Amazon\u0026rsquo;s EC2.\u003c/p\u003e","title":"LondonR meetings presentations"},{"content":"I am part of the rOpenSci development team (along with Carl Boettiger, Karthik Ram, and Nick Fabina).\nOur website: http://ropensci.org/.\nCode at Github: https://github.com/ropensci\nWe entered two of our R packages for integrating with PLoS Journals (rplos) and Mendeley (RMendeley) in the Mendeley-PLoS Binary Battle. Get them at GitHub (rplos; RMendeley). These two packages allow users (from R! of course) to search and retrieve data from PLoS journals (including their altmetrics data), and from Mendeley. You could surely mash up data from both PLoS and Mendeley. That\u0026rsquo;s what\u0026rsquo;s cool about rOpenSci - we provide the tools, and leave it up to users vast creativity to do awesome things. 3rd place gives us a $1,000 prize, plus a Parrot AR Drone helicopter.\n","permalink":"http://localhost:1313/2011/11/ropensci-won-3rd-place-in-the-plos-mendeley-binary-battle/","summary":"\u003cp\u003eI am part of the rOpenSci development team (\u003ca href=\"http://ropensci.org/developers/\"\u003ealong with Carl Boettiger, Karthik Ram, and Nick Fabina\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eOur website: \u003ca href=\"http://ropensci.org/\"\u003ehttp://ropensci.org/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCode at Github: \u003ca href=\"https://github.com/ropensci\"\u003ehttps://github.com/ropensci\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe entered two of our R packages for integrating with PLoS Journals (rplos) and Mendeley (RMendeley) in the \u003ca href=\"http://dev.mendeley.com/api-binary-battle\"\u003eMendeley-PLoS Binary Battle\u003c/a\u003e. Get them at GitHub (\u003ca href=\"https://github.com/ropensci/rplos\"\u003erplos\u003c/a\u003e; \u003ca href=\"https://github.com/ropensci/RMendeley\"\u003eRMendeley\u003c/a\u003e). These two packages allow users (from R! of course) to search and retrieve data from PLoS journals (including their altmetrics data), and from Mendeley.  You could surely mash up data from both PLoS and Mendeley.  That\u0026rsquo;s what\u0026rsquo;s cool about rOpenSci - we provide the tools, and leave it up to users vast creativity to do awesome things. 3rd place gives us a $1,000 prize, plus a \u003ca href=\"http://ardrone.parrot.com/parrot-ar-drone/uk/\"\u003eParrot AR Drone helicopter\u003c/a\u003e.\u003c/p\u003e","title":"rOpenSci won 3rd place in the PLoS-Mendeley Binary Battle!"},{"content":"See http://www.surveygizmo.com/s3/722753/Mendeley-PLoS-Binary-Battle-Public-Vote\n","permalink":"http://localhost:1313/2011/11/public-vote-open-for-mendely-plos/","summary":"\u003cp\u003eSee \u003ca href=\"http://www.surveygizmo.com/s3/722753/Mendeley-PLoS-Binary-Battle-Public-Vote\"\u003ehttp://www.surveygizmo.com/s3/722753/Mendeley-PLoS-Binary-Battle-Public-Vote\u003c/a\u003e\u003c/p\u003e","title":"Public vote open for Mendely-PLoS Binary Battle: vote rOpenSci!"},{"content":"I gave a talk today on doing very basic phylogenetics in R, including getting sequence data, aligning sequence data, plotting trees, doing trait evolution stuff, etc.\nPlease comment if you have code for doing bayesian phylogenetic inference in R. I know phyloch has function mrbayes, but can\u0026rsquo;t get it to work\u0026hellip;\nPhylogenetics in R View more presentations from schamber ","permalink":"http://localhost:1313/2011/11/my-talk-on-doing-phylogenetics-in-r/","summary":"\u003cp\u003eI gave a talk today on doing very basic phylogenetics in R, including getting sequence data, aligning sequence data, plotting trees, doing trait evolution stuff, etc.\u003c/p\u003e\n\u003cp\u003ePlease comment if you have code for doing bayesian phylogenetic inference in R.  I know phyloch has function mrbayes, but can\u0026rsquo;t get it to work\u0026hellip;\u003c/p\u003e\n\n\u003cdiv id=\"__ss_10222772\" style=\"width: 425px;\"\u003e\u003cstrong style=\"display: block; margin: 12px 0 4px;\"\u003e\u003ca href=\"http://www.slideshare.net/schamber/phylogenetics-in-r\" target=\"_blank\" title=\"Phylogenetics in R\"\u003ePhylogenetics in R\u003c/a\u003e\u003c/strong\u003e \u003ciframe frameborder=\"0\" height=\"355\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"http://www.slideshare.net/slideshow/embed_code/10222772\" width=\"425\"\u003e\u003c/iframe\u003e \u003cbr /\u003e\u003cdiv style=\"padding: 5px 0 12px;\"\u003eView more \u003ca href=\"http://www.slideshare.net/\" target=\"_blank\"\u003epresentations\u003c/a\u003e from \u003ca href=\"http://www.slideshare.net/schamber\" target=\"_blank\"\u003eschamber\u003c/a\u003e \u003c/div\u003e\u003c/div\u003e","title":"My talk on doing phylogenetics in R"},{"content":"Okay, so this post isn\u0026rsquo;t at all about R - but I can\u0026rsquo;t resist begging my readers for some help.\nI’m trying to get some crowdfunding for my research on the evolution of native plants in agricultural landscapes. My campaign is part of a larger project by about 50 other scientists and me to see how well it works to go straight to the public to get funding for science research. All these projects, including mine, are hosted at a site called RocketHub - a site that hosts crowdfunding projects of all sorts – and now they have science.\nIt is important to get a few bucks at the beginning so that the people that don’t know me with deep pockets will hopefully chip in once they see the money ball rolling.\nThe funding will go towards paying some students to collect data in the lab for me.\u0026lt;\nHere’s the link if you want to donate, or just to check out the video I made about my research! http://www.rockethub.com/projects/3790-evolution-in-agriculture\nAnd watch the video here too:\n","permalink":"http://localhost:1313/2011/11/check-out-video-of-my-research-at/","summary":"\u003cp\u003eOkay, so this post isn\u0026rsquo;t at all about R - but I can\u0026rsquo;t resist begging my readers for some help.\u003c/p\u003e\n\u003cp\u003eI’m trying to get some crowdfunding for my research on the evolution of native plants in agricultural landscapes.  My campaign is part of a larger project by about 50 other scientists and me to see how well it works to go straight to the public to get funding for science research.  All these projects, including mine, are hosted at a site called RocketHub - a site that hosts crowdfunding projects of all sorts – and now they have science.\u003c/p\u003e","title":"Check out a video of my research at RocketHub"},{"content":" With examples from rOpenSci R packages.\u0026nbsp;p.s. I am no expert at this...\nWeb data from R View more presentations from schamber ","permalink":"http://localhost:1313/2011/10/my-little-presentation-on-getting-web/","summary":"\u003cdiv id=\"__ss_9926321\" style=\"width: 425px;\"\u003e\u003cspan style=\"display: block; margin: 12px 0 4px;\"\u003eWith examples from \u003ca href=\"http://ropensci.org/\"\u003erOpenSci\u003c/a\u003e R packages.\u0026nbsp;\u003c/span\u003e\u003cspan style=\"display: block; margin: 12px 0 4px;\"\u003ep.s. I am no expert at this...\u003c/span\u003e\u003cstrong style=\"display: block; margin: 12px 0 4px;\"\u003e\u003cbr /\u003e\u003c/strong\u003e\u003cstrong style=\"display: block; margin: 12px 0 4px;\"\u003e\u003ca href=\"http://www.slideshare.net/schamber/web-data-from-r\" target=\"_blank\" title=\"Web data from R\"\u003eWeb data from R\u003c/a\u003e\u003c/strong\u003e \u003ciframe frameborder=\"0\" height=\"355\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"http://www.slideshare.net/slideshow/embed_code/9926321\" width=\"425\"\u003e\u003c/iframe\u003e \u003cbr /\u003e\u003cdiv style=\"padding: 5px 0 12px;\"\u003eView more \u003ca href=\"http://www.slideshare.net/\" target=\"_blank\"\u003epresentations\u003c/a\u003e from \u003ca href=\"http://www.slideshare.net/schamber\" target=\"_blank\"\u003eschamber\u003c/a\u003e \u003c/div\u003e\u003c/div\u003e","title":"My little presentation on getting web data through R"},{"content":"Carl Boettiger, a graduate student at UC Davis, just got two packages on CRAN. One is treebase, which which handshakes with the Treebase API. The other is rfishbase, which connects with the Fishbase, although I believe just scrapes XML content as there is no API. See development on GitHub for treebase here, and for rfishbase here. Carl has some tutorials on treebase and rfishbase at his website here, and we have an official rOpenSci tutorial for treebase here.\nBasically, these two R packages let you search and pull down data from Treebase and Fishbase - pretty awesome. This improves workflow, and puts your data search and acquisition component into your code, instead of being a bunch of mouse clicks in a browser.\nThese two packages are part of the rOpenSci project.\n","permalink":"http://localhost:1313/2011/10/two-new-ropensci-r-packages-are-on-cran/","summary":"\u003cp\u003e\u003ca href=\"http://www.carlboettiger.info/\"\u003eCarl Boettiger\u003c/a\u003e, a graduate student at UC Davis, just got two packages on \u003ca href=\"http://cran.r-project.org/web/packages/available_packages_by_name.html\"\u003eCRAN\u003c/a\u003e.  One is \u003ca href=\"http://cran.r-project.org/web/packages/treebase/index.html\"\u003etreebase\u003c/a\u003e, which which handshakes with the \u003ca href=\"http://www.treebase.org/treebase-web/home.html\"\u003eTreebase\u003c/a\u003e API.  The other is \u003ca href=\"http://cran.r-project.org/web/packages/rfishbase/index.html\"\u003erfishbase\u003c/a\u003e, which connects with the \u003ca href=\"http://www.fishbase.org/search.php\"\u003eFishbase\u003c/a\u003e, although I believe just scrapes XML content as there is no API.  See development on GitHub for treebase \u003ca href=\"https://github.com/ropensci/treeBASE\"\u003ehere\u003c/a\u003e, and for rfishbase \u003ca href=\"https://github.com/ropensci/rfishbase\"\u003ehere\u003c/a\u003e.  Carl has some tutorials on treebase and rfishbase at his website \u003ca href=\"http://www.carlboettiger.info/\"\u003ehere\u003c/a\u003e, and we have an official rOpenSci tutorial for treebase \u003ca href=\"http://ropensci.org/tutorials/r-treebase-tutorial/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBasically, these two R packages let you search and pull down data from Treebase and Fishbase - pretty awesome.  This improves workflow, and puts your data search and acquisition component into your code, instead of being a bunch of mouse clicks in a browser.\u003c/p\u003e","title":"Two new rOpenSci R packages are on CRAN"},{"content":"Tom Miller (a prof here at Rice) and Brian Inouye have a paper out in Ecology (paper, appendices) that confronts two-sex models of dispersal with empirical data. They conducted the first confrontation of two-sex demographic models with empirical data on lab populations of bean beetles Callosobruchus. Their R code for the modeling work is available at Ecological Archives (link here). Here is a figure made from running the five blocks of code in \u0026lsquo;Miller_and_Inouye_figures.txt\u0026rsquo; that reproduces Fig. 4 (A-E) in their Ecology paper (p = proportion female, Nt = density). Nice!\nA: Saturating density dependence\nB: Over-compensatory density dependence\nC: Sex-specific gamma\u0026rsquo;s (but bM=bF=0.5)\nD: Sex-specific b\u0026rsquo;s (but gammaM=gammaF=1) Sex-specific b\u0026rsquo;s (but gammaM=gammaF=2)\n","permalink":"http://localhost:1313/2011/10/two-sex-demographic-models-in-r/","summary":"\u003cp\u003eTom Miller (a prof here at Rice) and Brian Inouye have a paper out in Ecology (\u003ca href=\"http://www.esajournals.org/doi/abs/10.1890/11-0028.1\"\u003epaper\u003c/a\u003e, \u003ca href=\"http://www.esapubs.org/archive/archive_E.htm\"\u003eappendices\u003c/a\u003e) that confronts two-sex models of dispersal with empirical data. They conducted the first confrontation of two-sex demographic models with empirical data on lab populations of bean beetles \u003cem\u003eCallosobruchus\u003c/em\u003e. Their R code for the modeling work is available at Ecological Archives (link \u003ca href=\"http://www.esapubs.org/archive/ecol/E092/186/\"\u003ehere\u003c/a\u003e). Here is a figure made from running the five blocks of code in \u0026lsquo;Miller_and_Inouye_figures.txt\u0026rsquo; that reproduces Fig. 4 (A-E) in their Ecology paper (p = proportion female, Nt = density). Nice!\u003c/p\u003e","title":"Two-sex demographic models in R"},{"content":"So, there is a new food web dataset out that was put in Ecological Archives here, and I thought I would play with it. The food web is from Otago Harbour, an intertidal mudflat ecosystem in New Zealand. The web contains 180 nodes, with 1,924 links.\nFun stuff\u0026hellip;\nigraph, default layout plot\nigraph, circle layout plot, nice\nMy funky little gggraph function plotget the gggraph function, and make it better, here at Github\n","permalink":"http://localhost:1313/2011/10/new-food-web-dataset/","summary":"\u003cp\u003eSo, there is a new food web dataset out that was put in Ecological Archives \u003c!-- raw HTML omitted --\u003ehere\u003c!-- raw HTML omitted --\u003e, and I thought I would play with it.  The food web is from Otago Harbour, an intertidal mudflat ecosystem in New Zealand.  The web contains 180 nodes, with 1,924 links.\u003c/p\u003e\n\u003cp\u003eFun stuff\u0026hellip;\u003c/p\u003e\n\n\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003eigraph, default layout plot\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003e\u003ca href=\"http://3.bp.blogspot.com/-2lQOoeAqGCM/Tphf9GJI8LI/AAAAAAAAFEA/EPwum7GfwXg/s1600/igraphplot.jpeg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003e\u003cimg border=\"0\" height=\"303\" src=\"http://3.bp.blogspot.com/-2lQOoeAqGCM/Tphf9GJI8LI/AAAAAAAAFEA/EPwum7GfwXg/s400/igraphplot.jpeg\" width=\"400\" /\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003e\u003cbr /\u003e\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003eigraph, circle layout plot, nice\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003e\u003ca href=\"http://1.bp.blogspot.com/--hGl2IwHi4M/TphhJYdBO0I/AAAAAAAAFEI/8GsLuUkbYcM/s1600/igraphcircleplot.jpeg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003e\u003cimg border=\"0\" height=\"303\" src=\"http://1.bp.blogspot.com/--hGl2IwHi4M/TphhJYdBO0I/AAAAAAAAFEI/8GsLuUkbYcM/s400/igraphcircleplot.jpeg\" width=\"400\" /\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003e\u003cbr /\u003e\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003eMy funky little gggraph function plot\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003eget the gggraph function, and make it better, \u003ca href=\"https://github.com/sckott/gggraph\"\u003ehere at Github\u003c/a\u003e\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003e\u003ca href=\"http://4.bp.blogspot.com/-MBPHlFaVWos/Tphf82gWUpI/AAAAAAAAFD4/qaxCX4PP-C0/s1600/gggraphplot.jpeg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"\u003e\u003cimg border=\"0\" height=\"303\" src=\"http://4.bp.blogspot.com/-MBPHlFaVWos/Tphf82gWUpI/AAAAAAAAFD4/qaxCX4PP-C0/s400/gggraphplot.jpeg\" width=\"400\" /\u003e\u003c/a\u003e\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003e\u003cbr /\u003e\u003c/div\u003e\u003cdiv class=\"separator\" style=\"clear: both; text-align: center;\"\u003e\u003cbr /\u003e\u003c/div\u003e\u003cbr /\u003e\u003cbr /\u003e\u003cscript src=\"https://gist.github.com/1287545.js?file=newfoodwb.R\"\u003e\u003c/script\u003e","title":"New food web dataset"},{"content":"So, I\u0026rsquo;ve blogged about this topic before, way back on 5 Jan this year.\nMatt Helmus, a postdoc in the Wootton lab at the University of Chicago, published a paper with Anthony Ives in Ecological Monographs this year (abstract here). The paper addressed a new statistical approach to phylogenetic community structure.\nAs I said in the original post, part of the power of the PGLMM (phylogenetic generalized linear mixed models) approach is that you don\u0026rsquo;t have to conduct quite so many separate statistical tests as with the previous null model/randomization approach.\nTheir original code was written in Matlab. Here I provide the R code that Matt has so graciously shared with me. There are four functions and a fifth file has an example use case. The example and output are shown below.\nLook for the inclusion of Matt\u0026rsquo;s PGLMM to the picante R package in the future.\nHere are links to the files as GitHub gists:\nPGLMM.data.R: https://gist.github.com/1278205\nPGLMM.fit.R: https://gist.github.com/1284284\nPGLMM.reml.R: https://gist.github.com/1284287\nPGLMM.sim.R: https://gist.github.com/1284288\nPGLMM_example.R: https://gist.github.com/1284442\nEnjoy!\nThe example\n","permalink":"http://localhost:1313/2011/10/phylogenetic-community-structure-pglmms/","summary":"\u003cp\u003eSo, \u003ca href=\"http://r-ecology.blogspot.com/2011/01/new-approach-to-analysis-of.html\"\u003eI\u0026rsquo;ve blogged about this topic before\u003c/a\u003e, way back on 5 Jan this year.\u003c/p\u003e\n\u003cp\u003eMatt Helmus, a postdoc in the \u003ca href=\"http://woottonlab.uchicago.edu/\"\u003eWootton lab at the University of Chicago\u003c/a\u003e, published a paper with Anthony Ives in Ecological Monographs this year (\u003ca href=\"http://www.esajournals.org/doi/abs/10.1890/10-1264.1\"\u003eabstract here\u003c/a\u003e). The paper addressed a new statistical approach to phylogenetic community structure.\u003c/p\u003e\n\u003cp\u003eAs I said in the original post, part of the power of the PGLMM (phylogenetic generalized linear mixed models) approach is that you don\u0026rsquo;t have to conduct quite so many separate statistical tests as with the previous null model/randomization approach.\u003c/p\u003e","title":"Phylogenetic community structure: PGLMMs"},{"content":"Regular expressions are a powerful in any language to manipulate, search, etc. data.\nFor example:\n\u0026gt; fruit \u0026lt;- c(\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;pear\u0026#34;, \u0026#34;pineapple\u0026#34;) \u0026gt; fruit [1] \u0026#34;apple\u0026#34; \u0026#34;banana\u0026#34; \u0026#34;pear\u0026#34; \u0026#34;pineapple\u0026#34; \u0026gt; grep(\u0026#34;a\u0026#34;, fruit) # there is an \u0026#34;a\u0026#34; in each of the words [1] 1 2 3 4 \u0026gt; \u0026gt; strsplit(\u0026#34;a string\u0026#34;, \u0026#34;s\u0026#34;) # strsplit splits the string on the \u0026#34;s\u0026#34; [[1]] [1] \u0026#34;a \u0026#34; \u0026#34;tring\u0026#34; R base has many functions for regular expressions, see slide 9 of Ed\u0026rsquo;s talk below. The package stringr, created by Hadley Wickham, is a nice alternative that wraps the base regex functions for easier use. I highly recommend stringr.\nEd Goodwin, the coordinator of the Houston R Users group, gave a presentation to the group last night on regular expressions in R. It was a great talk, and he is allowing me to post his talk here.\nEnjoy! And thanks for sharing Ed!\nregex-presentation_ed_goodwin View more presentations from schamber ","permalink":"http://localhost:1313/2011/10/r-talk-on-regular-expressions-regex/","summary":"\u003cp\u003eRegular expressions are a powerful in any language to manipulate, search, etc. data.\u003c/p\u003e\n\u003cp\u003eFor example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e fruit \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;apple\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;banana\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;pear\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;pineapple\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e fruit\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[1] \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;apple\u0026#34;\u003c/span\u003e     \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;banana\u0026#34;\u003c/span\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;pear\u0026#34;\u003c/span\u003e      \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;pineapple\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egrep\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;a\u0026#34;\u003c/span\u003e, fruit) \u003cspan style=\"color:#75715e\"\u003e# there is an \u0026#34;a\u0026#34; in each of the words\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[1] \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003estrsplit\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;a string\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;s\u0026#34;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# strsplit splits the string on the \u0026#34;s\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[[1]]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[1] \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;a \u0026#34;\u003c/span\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;tring\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eR base has many functions for regular expressions, see slide 9 of Ed\u0026rsquo;s talk below.  The package stringr, created by Hadley Wickham, is a nice alternative that wraps the base regex functions for easier use. I highly recommend \u003ca href=\"http://cran.r-project.org/web/packages/stringr/index.html\"\u003estringr\u003c/a\u003e.\u003c/p\u003e","title":"R talk on regular expressions (regex)"},{"content":"Rolf Lohaus, a Huxley postdoctoral fellow here in the EEB dept at Rice University, gave our R course a talk on basic visualizations in R this morning.\nEnjoy!\n","permalink":"http://localhost:1313/2011/09/r-tutorial-on-visualizationsgraphics/","summary":"\u003cp\u003eRolf Lohaus, a Huxley postdoctoral fellow here in the EEB dept at Rice University, gave our R course a talk on basic visualizations in R this morning.\u003c/p\u003e\n\u003cp\u003eEnjoy!\u003c/p\u003e\n\n\u003cscript src=\"https://gist.github.com/1254174.js?file=visualizations_tutorial.R\"\u003e\u003c/script\u003e","title":"R tutorial on visualizations/graphics"},{"content":"Crowdsourced funding is becoming a sustainable way for various artists, entrepreneurs, etc. to get their idea funded from individuals. For example, think of Kickstarter and RocketHub.\nJai Ranganathan and Jarrett Byrnes have started an experiment to determine how well crowdfunding can work for scientists: The SciFund Challenge. Go here to signup and here for their website.\nThe deadline to sign up is Oct. 1\n","permalink":"http://localhost:1313/2011/09/short-on-funding-cant-get-grant/","summary":"\u003cp\u003eCrowdsourced funding is becoming a sustainable way for various artists, entrepreneurs, etc. to get their idea funded from individuals. For example, think of \u003ca href=\"http://www.kickstarter.com/\"\u003eKickstarter\u003c/a\u003e and \u003ca href=\"http://www.rockethub.com/\"\u003eRocketHub\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eJai Ranganathan and Jarrett Byrnes have started an experiment to determine how well crowdfunding can work for scientists: The SciFund Challenge. Go here to \u003ca href=\"http://scifund.wordpress.com/sign-up/\"\u003esignup\u003c/a\u003e and here for their \u003ca href=\"http://scifund.wordpress.com/\"\u003ewebsite\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe deadline to sign up is Oct. 1\u003c/p\u003e","title":"Short on funding? Can't get a grant? Crowdfunding! #SciFund"},{"content":" Ten Simple Rules for Open Access Publishers View more presentations from Philip Bourne ","permalink":"http://localhost:1313/2011/09/ten-simple-rules-for-oa-publishers-talk/","summary":"\u003cdiv id=\"__ss_9354451\" style=\"width: 425px;\"\u003e\u003cstrong style=\"display: block; margin: 12px 0 4px;\"\u003e\u003ca href=\"http://www.slideshare.net/pebourne/ten-simple-rules-for-open-access-publishers\" target=\"_blank\" title=\"Ten Simple Rules for Open Access Publishers\"\u003eTen Simple Rules for Open Access Publishers\u003c/a\u003e\u003c/strong\u003e \u003ciframe frameborder=\"0\" height=\"355\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"http://www.slideshare.net/slideshow/embed_code/9354451\" width=\"425\"\u003e\u003c/iframe\u003e \u003cbr /\u003e\u003cbr /\u003e\u003cdiv style=\"padding: 5px 0 12px;\"\u003eView more \u003ca href=\"http://www.slideshare.net/\" target=\"_blank\"\u003epresentations\u003c/a\u003e from \u003ca href=\"http://www.slideshare.net/pebourne\" target=\"_blank\"\u003ePhilip Bourne\u003c/a\u003e \u003c/div\u003e\u003c/div\u003e","title":"Ten Simple Rules for OA Publishers talk by Philip Bourne"},{"content":"The O\u0026rsquo;Reilly Media Strata Summit has many interviews on YouTube (just search YouTube for it)Drew Conway is the author of a R packages, including infochimps, an R wrapper to the Infochimps API service.\nThe YouTube video:\n","permalink":"http://localhost:1313/2011/09/drewconway-interview-on-datanoborders/","summary":"\u003cp\u003eThe O\u0026rsquo;Reilly Media Strata Summit has many interviews on YouTube (just search YouTube for it)\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eDrew Conway is the author of a R packages, including \u003c!-- raw HTML omitted --\u003einfochimps\u003c!-- raw HTML omitted --\u003e, an R wrapper to the \u003c!-- raw HTML omitted --\u003eInfochimps\u003c!-- raw HTML omitted --\u003e API service.\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\n\u003cp\u003eThe YouTube video:\u003c/p\u003e\n\n\u003ciframe allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"http://www.youtube.com/embed/fsnnwTWoOLk\" width=\"560\"\u003e\u003c/iframe\u003e","title":"@drewconway interview on @DataNoBorders at the Strata conference"},{"content":"Carl Boettiger gave a talk on the topic of open science to incoming UC Davis graduate students.\nHere is the audio click here\nHere are the slides clickhere\n","permalink":"http://localhost:1313/2011/09/open-science-talk-by-carl-boettiger/","summary":"\u003cp\u003eCarl Boettiger gave a talk on the topic of open science to incoming UC Davis graduate students.\u003c/p\u003e\n\u003cp\u003eHere is the audio \u003ca href=\"http://www.archive.org/details/ThingsIWishIKnewThreeYearsAgo-ByTheDavisOpenScienceGroup\u0026amp;reCache=1\"\u003eclick here\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHere are the slides \u003ca href=\"http://hazelnusse.github.com/DOS_WOW2011/#title-slide\"\u003eclickhere\u003c/a\u003e\u003c/p\u003e","title":"Open science talk by Carl Boettiger"},{"content":"UPDATE: I put in an R tutorial as a Github gist below.\nHere is a short intro R talk I gave today\u0026hellip;for what it\u0026rsquo;s worth\u0026hellip;\nR Introduction View more presentations from schamber Here\u0026rsquo;s the tutorial in a GitHub gist: https://gist.github.com/1208321\n","permalink":"http://localhost:1313/2011/09/my-take-on-r-introduction-talk/","summary":"\u003cp\u003eUPDATE: I put in an R tutorial as a Github gist below.\u003c/p\u003e\n\u003cp\u003eHere is a short intro R talk I gave today\u0026hellip;for what it\u0026rsquo;s worth\u0026hellip;\u003c/p\u003e\n\n\u003cdiv id=\"__ss_9195930\" style=\"width: 425px;\"\u003e\u003cstrong style=\"display: block; margin: 12px 0 4px;\"\u003e\u003ca href=\"http://www.slideshare.net/schamber/r-introduction\" target=\"_blank\" title=\"R Introduction\"\u003eR Introduction\u003c/a\u003e\u003c/strong\u003e \u003ciframe frameborder=\"0\" height=\"355\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"http://www.slideshare.net/slideshow/embed_code/9195930\" width=\"425\"\u003e\n\n\u003c/iframe\u003e \u003cbr /\u003e\u003cdiv style=\"padding: 5px 0 12px;\"\u003eView more \u003ca href=\"http://www.slideshare.net/\" target=\"_blank\"\u003epresentations\u003c/a\u003e from \u003ca href=\"http://www.slideshare.net/schamber\" target=\"_blank\"\u003eschamber\u003c/a\u003e \u003cbr /\u003e\u003cbr /\u003e\u003cbr /\u003e\u003cbr /\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eHere\u0026rsquo;s the tutorial in a GitHub gist: \u003ca href=\"https://gist.github.com/1208321\"\u003ehttps://gist.github.com/1208321\u003c/a\u003e\u003c/p\u003e","title":"My take on an R introduction talk"},{"content":" Note: thanks to Scott for inviting me to contribute to the Recology blog despite being an ecology outsider; my work is primarily in atomic physics. -Pascal\nA part of me has always liked thinking about how to effectively present information, but until the past year, I had not read much to support my (idle) interest in information visualization. That changed in the spring when I read Edward Tufte\u0026rsquo;s The Visual Display of Quantitative Information, a book that stimulated me to think more deeply about presenting information. I originally started with a specific task in mind\u0026ndash;a wonderful tool for focusing one\u0026rsquo;s interests\u0026ndash;but quickly found that Tufte\u0026rsquo;s book was less a practical guide and more a list of general design principles. Then, a few months ago, I stumbled upon Nathan Yau\u0026rsquo;s blog, FlowingData, and found out he was writing a practical guide to design and visualization. Conveniently enough for me, Yau\u0026rsquo;s book, Visualize This, would be released within a month of my discovery of his blog; what follows are my impressions of Visualize This.\nI have liked Visualize This a lot. Yau writes with much the same informal tone as on his blog, and the layout is visually pleasing (good thing, too, for a book about visualizing information!). The first few chapters are pretty basic if you have done much data manipulation before, but it is really nice to have something laid out so concisely. The examples are good, too, in that he is very explicit about every step: there is no intuiting what that missing step should be. The author even acknowledges in the introduction that the first part of the book is at an introductory level.\nEarly in the book, Yau discusses where to obtain data. This compilation of sources is potentially a useful reference for someone, like me, who almost always generates his own data in the lab. Unfortunately, Yau does not talk much about preparation of (or best practices for) your own data. Additionally, from the perspective of a practicing scientist, it would have been nice to hear about how to archive data to make sure it is readable far into the future, but that is probably outside the scope of the book.\nYau seems really big into using open source software for getting and analyzing data (e.g. Python, R, etc…), but he is surprisingly attached to the proprietary Adobe Illustrator for turning figures into presentation quality graphics. He says that he feels like the default options in most analysis programs do not make for very good quality graphics (and he is right), but he does not really acknowledge that you can generate nice output if you go beyond the default settings. For me, the primary advantage of generating output programmatically is that it is easy to regenerate when you need to change the data or the formatting on the plot. Using a graphical user interface, like in Adobe Illustrator, is nice if you are only doing something once (how often does that happen?), but when you have to regenerate the darn figure fifty times to satisfy your advisor, it gets tedious to move things around pixel by pixel.\nBy the time I reached the middle chapters, I started finding many of the details to be repetitive. Part of this repetition stems from the fact that Yau divides these chapters by the type of visualization. For example, \u0026ldquo;Visualizing Proportions\u0026rdquo; and \u0026ldquo;Visualizing Relationships\u0026rdquo; are two of the chapter titles. While I think these distinctions are important ones for telling the right story about one\u0026rsquo;s data, creating figures for the different data types often boils down to choosing different functions in R or Python. People with less analysis and presentation experience should find the repetition helpful, but I increasingly skimmed these sections as I went along.\nWorking through Yau\u0026rsquo;s examples for steps you do not already know would probably be the most useful way of getting something out of the book. So, for example, I started trying to use Python to scrape data from a webpage, something I had not previously done. I followed the book\u0026rsquo;s example of this data-scraping just fine, but as with most things in programming, you find all sorts of minor hurdles to clear when you try your own thing. In my case, I am re-learning the Python I briefly learned about 10 years ago\u0026ndash;partly in anticipation of not having access to Matlab licenses once I vacate the academy\u0026ndash;since I have forgotten a lot of the syntax. A lot of this stuff would be faster if I were working in Matlab which I grew more familiar with in graduate school.\nOverall, Visualize This is a really nice looking book and will continue to be useful to me as a reference. Yau concludes his book with a refreshing reminder to provide context for the data we present. This advice is particularly relevant when presenting to a wider or lay audience, but it is still important for us, as scientists, to clearly communicate our findings in the literature. Patterns in the data are not often self-evident, and therefore we should think carefully about which visualization tools will best convey the meaning of our results.\nNote: Edited to add a link to Visualize This here and in the introductory paragraph.\n","permalink":"http://localhost:1313/2011/09/data-visualization-book/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNote:  thanks to Scott for inviting me to contribute to the Recology blog despite being an ecology outsider; my work is primarily in atomic physics. -Pascal\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA part of me has always liked thinking about how to effectively present information, but until the past year, I had not read much to support my (idle) interest in information visualization. That changed in the spring when I read Edward Tufte\u0026rsquo;s \u003cem\u003eThe Visual Display of Quantitative Information\u003c/em\u003e, a book that stimulated me to think more deeply about presenting information. I originally started with a specific task in mind\u0026ndash;a wonderful tool for focusing one\u0026rsquo;s interests\u0026ndash;but quickly found that Tufte\u0026rsquo;s book was less a practical guide and more a list of general design principles. Then, a few months ago, I stumbled upon Nathan Yau\u0026rsquo;s blog, \u003ca href=\"http://flowingdata.com/\"\u003eFlowingData\u003c/a\u003e, and found out he was writing a practical guide to design and visualization. Conveniently enough for me, Yau\u0026rsquo;s book, \u003cem\u003e\u003ca href=\"http://book.flowingdata.com/\"\u003eVisualize This\u003c/a\u003e\u003c/em\u003e, would be released within a month of my discovery of his blog; what follows are my impressions of \u003cem\u003eVisualize This\u003c/em\u003e.\u003c/p\u003e","title":"A Data Visualization Book"},{"content":"FigShare - I very much like this idea of a place to put your data online that is NOT published. Dryad is a nice place for datastes linked with published papers, but there isn\u0026rsquo;t really a place for datasets that perhaps did not make the cut for a published paper, and if known to the scientific community, could potentially help resolve the \u0026ldquo;file-drawer\u0026rdquo; effect in meta-analyses. (wow, run on sentence)\n\"Figshare - Why don't you publish all your research?\" Mark Hahnel Imperial College London from London Biogeeks on Vimeo. ","permalink":"http://localhost:1313/2011/09/figshare-talk/","summary":"\u003cp\u003eFigShare - I very much like this idea of a place to put your data online that is NOT published. Dryad is a nice place for datastes linked with published papers, but there isn\u0026rsquo;t really a place for datasets that perhaps did not make the cut for a published paper, and if known to the scientific community, could potentially help resolve the \u0026ldquo;file-drawer\u0026rdquo; effect in meta-analyses. (wow, run on sentence)\u003c/p\u003e","title":"FigShare Talk"},{"content":" Stalking the Fourth Domain with Jonathan Eisen, Ph D from mendelspod on Vimeo. ","permalink":"http://localhost:1313/2011/09/jonathan-eisen-on-fourth-domain-and/","summary":"\u003ciframe frameborder=\"0\" height=\"227\" src=\"http://player.vimeo.com/video/28444926?portrait=0\" width=\"400\"\u003e\u003c/iframe\u003e\u003cbr /\u003e\u003cbr /\u003e\u003cbr /\u003e\u003ca href=\"http://vimeo.com/28444926\"\u003eStalking the Fourth Domain with Jonathan Eisen, Ph D\u003c/a\u003e from \u003ca href=\"http://vimeo.com/user8340111\"\u003emendelspod\u003c/a\u003e on \u003ca href=\"http://vimeo.com/\"\u003eVimeo\u003c/a\u003e.","title":"Jonathan Eisen on the Fourth Domain and Open Science"},{"content":"The team at rOpenSci and I have been working on a wrapper for the USA National Phenology Network API. The following is a demo of some of the current possibilities. We will have more functions down the road. Get the publicly available code, and contribute, at Github here. If you try this out look at the Description file for the required R packages to run rnpn. Let us know at Github (here) or at our website http://ropensci.org/, or in the comments below, or on twitter (@rOpenSci), what use cases you would like to see with the rnpn package.\nMethod and demo of each:\nGet observations for species by day\nFrom the documentation:\nThis function will return a list of species, containing all the dates which observations were made about the species, and a count of the number of such observations made on that date.\nNote, the data below is truncated for blogging brevity\u0026hellip;\n# Searched for any individuals at stations 507 and 523 \u0026gt; getindsatstations(c(507, 523)) individual_id individual_name species_id kingdom 1 1200 dogwood 12 Plantae 2 1197 purple lilac 36 Plantae 3 1193 white t 38 Plantae 4 3569 forsythia-1 73 Plantae 5 1206 jack 150 Plantae 6 1199 trout lily 161 Plantae 7 1198 dandy 189 Plantae 8 1192 red t 192 Plantae 9 1710 common lilac 36 Plantae 10 1711 common lilac 2 36 Plantae 11 1712 dandelion 189 Plantae Get individuals of species at stations\nFrom the documentation: \u0026ldquo;This function will return a list of all the individuals, which are members of a species, among any number of stations.\u0026rdquo;\nSearch for individuals of species 35 at stations 60 and 259 in year 2009\n\u0026gt; getindspatstations(35, c(60, 259), 2009) individual_id individual_name number_observations 1 1715 west 5 2 1716 east 5 Get observation associated with particular observation\nFrom the documentation: \u0026ldquo;This function will return the comment associated with a particular observation.\u0026rdquo;\n# The observation for observation number 1938 \u0026gt; getobscomm(1938) $observation_comment [1] \u0026#34;some lower branches are bare\u0026#34; ","permalink":"http://localhost:1313/2011/08/rnpn-r-interface-for-national-phenology/","summary":"\u003cp\u003eThe team at \u003ca href=\"http://ropensci.org/\"\u003erOpenSci\u003c/a\u003e and I have been working on a wrapper for the \u003ca href=\"http://www.usanpn.org/\"\u003eUSA National Phenology Network\u003c/a\u003e API. The following is a demo of some of the current possibilities. We will have more functions down the road. Get the publicly available code, and contribute, at Github \u003ca href=\"https://github.com/ropensci/rnpn\"\u003ehere\u003c/a\u003e. If you try this out look at the \u003ca href=\"https://github.com/ropensci/rnpn/blob/master/DESCRIPTION\"\u003eDescription file\u003c/a\u003e for the required R packages to run rnpn. Let us know at Github (\u003ca href=\"https://github.com/ropensci\"\u003ehere\u003c/a\u003e) or at our website  \u003ca href=\"http://ropensci.org/\"\u003ehttp://ropensci.org/\u003c/a\u003e, or in the comments below, or on twitter (@rOpenSci), what use cases you would like to see with the rnpn package.\u003c/p\u003e","title":"rnpn: An R interface for the National Phenology Network"},{"content":"There is an awesome position opening up for an assistant professor in systematics at the University of Vermont. Below is the announcement, and see the original post at the Distributed Ecology blog. Why is this related to R? One can do a lot of systematics work in R, including retrieving scientific collections data through an upcoming package handshaking with VertNet (part of the rOpenSci project), managing large data sets, retrieval of GenBank data through the ape package (see fxn read.genbank), phylogenetic reconstruction and analysis, and more. So I am sure a systematist with R ninja skills will surely have a head up on the rest of the field.\nAssistant Professor in Systematics\nDepartment of Biology\nUniversity of Vermont\nBurlington, Vermont\nThe Department of Biology of the University of Vermont seeks applications for a tenure- track Assistant Professor position in Systematics and Evolutionary Biology of arthropods, especially insects. The position will be open in the fall of 2012. The successful candidate will have expertise in classical and molecular systematics, including analysis of complex data sets. Candidates pursuing phylogenomics and innovative methods in bioinformatics in combination with taxonomy are especially encouraged to apply. Department information at: http://www.uvm.edu/~biology/.\nAll applicants are expected to: 1) hold a Ph.D. degree in relevant disciplines and have two or more years of postdoctoral experience; 2) develop a competitively funded research program; 3) teach undergraduate courses (chosen from among general biology, evolution, systematic entomology, and others in the candidate\u0026rsquo;s expertise); 4) teach, mentor and advise undergraduate and graduate students; and 5) oversee a natural history collection of historic significance.\nCandidates must apply online: http://www.uvmjobs.com/. On left see \u0026ldquo;Search Postings\u0026rdquo; then find \u0026ldquo;Biology\u0026rdquo; under \u0026ldquo;HCM Department\u0026rdquo; then posting 0040090 (first posting). Sorry, but we cannot supply the direct link because it will time out.\nAttach a cover letter with a statement of research focus and teaching interests (one document), a curriculum vitae, representative publications, and the contact information of three references.\nReview of applications will begin on September 15, 2011, and will continue until the position is filled. Questions and up to three additional publications may be directed to Dr. Jos. J. Schall: jschall@uvm.edu.\nThe University of Vermont recently identified several \u0026ldquo;Spires of Excellence\u0026rdquo; in which it will strategically focus institutional investments and growth over the next several years. One spire associated with the position is Complex Systems. Candidates whose research interests align with this spire are especially encouraged to applyhttp://www.uvm.edu/~tri/.\nThe University seeks faculty who can contribute to the diversity and excellence of the academic community through their research, teaching, and/or service. Applicants are requested to include in their cover letter information about how they will further this goal. The University of Vermont is an Affirmative Action/Equal Opportunity employer. The Department is committed to increasing faculty diversity and welcomes applications from women, underrepresented ethnic, racial and cultural groups, and from people with disabilities.\n","permalink":"http://localhost:1313/2011/08/tenure-track-position-in-systematics-at/","summary":"\u003cp\u003eThere is an awesome position opening up for an assistant professor in systematics at the University of Vermont. Below is the announcement, and see the \u003ca href=\"http://currentecology.blogspot.com/2011/08/tenure-track-position-in-systematics-at.html\"\u003eoriginal post\u003c/a\u003e at the \u003ca href=\"http://currentecology.blogspot.com/\"\u003eDistributed Ecology blog\u003c/a\u003e. Why is this related to R? One can do a lot of systematics work in R, including retrieving scientific collections data through an upcoming package handshaking with \u003ca href=\"http://vertnet.org/index.php\"\u003eVertNet\u003c/a\u003e (part of the \u003ca href=\"http://ropensci.org/\"\u003erOpenSci\u003c/a\u003e project), managing large data sets, retrieval of GenBank data through the ape package (see fxn read.genbank), phylogenetic reconstruction and analysis, and more. So I am sure a systematist with R ninja skills will surely have a head up on the rest of the field.\u003c/p\u003e","title":"Tenure track position in systematics at the University of Vermont"},{"content":"Interesting talks/posters:\nRichard Lankau presented research on trade-offs and competitive ability. He suggests that during range expansion selection for increased intraspecific competitive ability in older populations leads to loss of traits for interspecific competitive traits due to trade-offs between these traits.\nEllner emphatically states that rapid evolution DOES matter for ecological responses, and longer-term evolutionary patterns as well. [His paper on the talk he was giving came out prior to his talk, which he pointed out, good form sir]\nLauren Sullivan gave an interesting talk on bottom up and top down effects on plant reproduction in one site of a huge network of sites doing similar nutrient and herbivory manipulations around the globe - NutNet (go here: http://nutnet.science.oregonstate.edu/).\nLaura Prugh shows in California that the engineering effects (i.e., the mounds that they make) of giant kangaroo rats are more important for the associated food web than the species interaction effects (the proxy used was just density of rats).\nKristy Deiner suggests that chironomids are more phylogenetic similar in lakes with stocked fish relative to fishless lakes, in high elevation lakes in the Sierra Nevada. She used barcode data to generate her phylogeny of chironomids. If you have barcode data and want to search BOLD Systems site, one option is doing it from R using rbold, a package under development at rOpenSci (code at Github).\nJessica Gurevitch presented a large working group\u0026rsquo;s methods/approach to a set of reviews on invasion biology. We didn\u0026rsquo;t get to see a lot of results from this work, but I personally was glad to see her explaining to a packed room the utility of meta-analysis, and comparing to the medical field in which meta-analysis is sort of the gold standard by which to draw conclusions.\nFollowing Jessica, Jason Fridley told us about the Evolutionary Imbalance Hypothesis (EIH) (see my notes here). He posed the problem of, when two biotas come together, what determines which species are retained in this new community and which species are left out. He listed a litany of traits/responses to measure to get at this problem, but suggested that with a little bit of \u0026ldquo;desktop ecology\u0026rdquo;, we could simply ask: Is the invasability of X region related to the phylogenetic diversity of that region? In three destination regions (Eastern Deciduous Forests, Mediterranean California, and the Czech Republic) out of four there was a positive relationship between proportion of invasive plant species in a source region and the phylogenetic diversity of the source regions.\n","permalink":"http://localhost:1313/2011/08/thursday-at-esa11/","summary":"\u003cp\u003eInteresting talks/posters:\u003c/p\u003e\n\u003cp\u003eRichard Lankau presented research on trade-offs and competitive ability. He suggests that during range expansion selection for increased intraspecific competitive ability in older populations leads to loss of traits for interspecific competitive traits due to trade-offs between these traits.\u003c/p\u003e\n\u003cp\u003eEllner emphatically states that rapid evolution DOES matter for ecological responses, and longer-term evolutionary patterns as well. [His paper on the talk he was giving came out prior to his talk, which he pointed out, good form sir]\u003c/p\u003e","title":"Thursday at #ESA11"},{"content":"Interesting talks/posters:\nEthan White\u0026rsquo;s poster describing EcologicalData.org was of course awesome given my interest in getting data into the hands of ecologists over at rOpenSci.org. Ethan also has software you can download on your machine to get the datasets you want easily - EcoData Retriever. [rOpenSci will try to take advantage of their work and allow you to call the retriever from R] Carl Boettiger\u0026rsquo;s talk was awesome. He explained how we need better tools to be able to predict collapses using early warning signals. He developed a way to estimate the statistical distribution of probabilities of system collapse. Jennifer Dunne: Explained how she put together an ancient network from Germany. Bravo. Carlos Melian explained his model of network buildup that starts from individuals, allows speciation, and other evolutionary processes. Rachel Winfree told us that in two sets of mutualistic plant-pollinator networks in New Jersey and California, that the least connected pollinator species were the most likely to be lost from the network with increasing agricultural intensity. Dan Cariveau suggests that pollination crop services can be stabilized even with increasing agriculture intensity if in fact pollinator species respond in different ways. That is, some pollinators may decrease in abundance with increasing ag intensity, while other species may increase - retaining overall pollination services to crops. ","permalink":"http://localhost:1313/2011/08/wednesday-at-esa11/","summary":"\u003cp\u003eInteresting talks/posters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEthan White\u0026rsquo;s poster describing \u003ca href=\"http://ecologicaldata.org/\"\u003eEcologicalData.org\u003c/a\u003e was of course awesome given my interest in getting data into the hands of ecologists over at \u003ca href=\"http://ropensci.org/\"\u003erOpenSci.org\u003c/a\u003e. Ethan also has software you can download on your machine to get the datasets you want easily - \u003ca href=\"http://ecologicaldata.org/ecodata-retriever\"\u003eEcoData Retriever\u003c/a\u003e. [rOpenSci will try to take advantage of their work and allow you to call the retriever from R]\u003c/li\u003e\n\u003cli\u003eCarl Boettiger\u0026rsquo;s talk was awesome. He explained how we need better tools to be able to predict collapses using early warning signals. He developed a way to estimate the statistical distribution of probabilities of system collapse. \u003c/li\u003e\n\u003cli\u003eJennifer Dunne: Explained how she put together an ancient network from Germany. Bravo. \u003c/li\u003e\n\u003cli\u003eCarlos Melian explained his model of network buildup that starts from individuals, allows speciation, and other evolutionary processes. \u003c/li\u003e\n\u003cli\u003eRachel Winfree told us that in two sets of mutualistic plant-pollinator networks in New Jersey and California, that the least connected pollinator species were the most likely to be lost from the network with increasing agricultural intensity. \u003c/li\u003e\n\u003cli\u003eDan Cariveau suggests that pollination crop services can be stabilized even with increasing agriculture intensity if in fact pollinator species respond in different ways. That is, some pollinators may decrease in abundance with increasing ag intensity, while other species may increase - retaining overall pollination services to crops.\u003c/li\u003e\n\u003c/ul\u003e","title":"Wednesday at #ESA11"},{"content":"Monday was a good day at ESA in Austin. There were a few topics I promised to report on in my blogging/tweeting.\n\u0026hellip;focused on open source data. Carly Strasser\u0026rsquo;s presentation on guidelines for data management was awesome (including other talks in the symposium on Creating Effective Data Management Plans for Ecological Research). Although this was a good session, I can\u0026rsquo;t help but wish that they had hammered home the need for open science more. Oh well. Also, they talked a lot about how, and not a lot of why we should properly curate data. Still, a good session. One issue Carly and I talked about was tracking code in versioning systems such as Github. There doesn\u0026rsquo;t seem to be a culture of versioning code for analyses/simulations in ecology. But when we get there\u0026hellip;it will be easier to share/track/collaborate on code.\n\u0026hellip;used R software. David Jennings talked about a meta-analysis asking if phylogenetic distance influences competition strength in pairwise experiments. David used the metafor package in R to do his meta-analysis. Good form sir.\n\u0026hellip;did cool science. Matt Helmus presented a great talk on phylogenetic species area curves (likely using R, or Matlab maybe?).\np.s. We launched rOpenSci today.\nOddities:\nThe Tilman effect - Tilman\u0026rsquo;s talk was so packed it looked like there was a line waiting to get into a trendy bar. Here\u0026rsquo;s a picture (credit: Jaime Ashander). Bigger room next time anyone? Wiley came out with an open source journal called Ecology and Evolution. This brings them to 3 open source journals (the other two are in other fields). We (rOpenSci) will attempt to hand-shake with these journals. The vegetarian lunch option was surprisingly good. Nice. ","permalink":"http://localhost:1313/2011/08/monday-at-esa11/","summary":"\u003cp\u003eMonday was a good day at ESA in Austin. There were a few topics I promised to report on in my blogging/tweeting.\u003c/p\u003e\n\u003cp\u003e\u0026hellip;focused on open source data. \u003ca href=\"http://www.nceas.ucsb.edu/~strasser/Site/Home.html\"\u003eCarly Strasser\u0026rsquo;s\u003c/a\u003e presentation on guidelines for data management was awesome (including other talks in the symposium on Creating Effective Data Management Plans for Ecological Research). Although this was a good session, I can\u0026rsquo;t help but wish that they had hammered home the need for open science more. Oh well. Also, they talked a lot about how, and not a lot of why we should properly curate data. Still, a good session. One issue Carly and I talked about was tracking code in versioning systems such as \u003ca href=\"https://github.com/\"\u003eGithub\u003c/a\u003e. There doesn\u0026rsquo;t seem to be a culture of versioning code for analyses/simulations in ecology. But when we get there\u0026hellip;it will be easier to share/track/collaborate on  code.\u003c/p\u003e","title":"Monday at ESA11"},{"content":"Our development team would like to announce the launch of rOpenSci. As the title states, this project aims to create R packages to make open science more available to researchers.\nhttp://ropensci.org/\nWhat this means is that we seek to connect researchers using R with as much open data as possible, mainly through APIs. There are a number of R packages that already do this (e.g., infochimps, twitteR), but we are making more packages, e.g., for Mendeley, PLoS Journals, and taxonomic sources (ITIS, EOL, TNRS, Phylomatic, UBio).\nImportantly, we are creating a package called rOpenSci, which aims to integrate functions from packages for individual open data sources.\nIf you are somewhat interested, follow our progress on our website, on Twitter, or contact us. If you are really^2 interested you could go to Github and contribute. If you are really^3 interested, join our development team.\n","permalink":"http://localhost:1313/2011/08/esa11-ropensci-collaborative-effort-to_08/","summary":"\u003cp\u003eOur \u003ca href=\"http://ropensci.org/developers/\"\u003edevelopment team\u003c/a\u003e would like to announce the launch of \u003ca href=\"http://ropensci.org/\"\u003erOpenSci\u003c/a\u003e. As the title states, this project aims to create \u003ca href=\"http://www.r-project.org/\"\u003eR\u003c/a\u003e \u003ca href=\"http://cran.r-project.org/web/packages/available_packages_by_name.html\"\u003epackages\u003c/a\u003e to make open science more available to researchers.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://1.bp.blogspot.com/-9YIk1e1liUU/Tj8YAoOPyQI/AAAAAAAAElQ/9gCMQ8CJORI/s1600/ropensci.png\"\u003e\u003cimg loading=\"lazy\" src=\"http://1.bp.blogspot.com/-9YIk1e1liUU/Tj8YAoOPyQI/AAAAAAAAElQ/9gCMQ8CJORI/s1600/ropensci.png\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://ropensci.org/\"\u003ehttp://ropensci.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWhat this means is that we seek to connect researchers using R with as much open data as possible, mainly through APIs. There are a number of R packages that already do this (e.g., \u003ca href=\"http://cran.r-project.org/web/packages/infochimps/index.html\"\u003einfochimps\u003c/a\u003e, \u003ca href=\"http://cran.r-project.org/web/packages/twitteR/index.html\"\u003etwitteR\u003c/a\u003e), but we are making more packages, e.g., for \u003ca href=\"https://github.com/cboettig/RMendeley\"\u003eMendeley\u003c/a\u003e, \u003ca href=\"https://github.com/sckott/rplos\"\u003ePLoS Journals\u003c/a\u003e, and \u003ca href=\"https://github.com/sckott/taxize_\"\u003etaxonomic sources\u003c/a\u003e (\u003ca href=\"http://www.itis.gov/\"\u003eITIS\u003c/a\u003e, \u003ca href=\"http://www.eol.org/\"\u003eEOL\u003c/a\u003e, \u003ca href=\"http://tnrs.iplantcollaborative.org/\"\u003eTNRS\u003c/a\u003e, \u003ca href=\"http://www.phylodiversity.net/phylomatic/\"\u003ePhylomatic\u003c/a\u003e, \u003ca href=\"http://www.ubio.org/\"\u003eUBio\u003c/a\u003e).\u003c/p\u003e","title":"(#ESA11) rOpenSci: a collaborative effort to develop R-based tools for facilitating Open Science"},{"content":"I will be blogging about the upcoming Ecological Society of America meeting in Austin, TX. I will focus on discussing talks/posters that:\nHave taken a cool approach to using data, or Have focused on open science/data, or Done something cool with R software, or Are just exciting in general I will also tweet throughout the meeting from @recology_ (yes the underscore is part of the name, recology was already taken). The hashtag for the meeting this year is #ESA11\n","permalink":"http://localhost:1313/2011/07/bloggingtweeting-from-esa11/","summary":"\u003cp\u003eI will be blogging about the upcoming Ecological Society of America meeting in Austin, TX. I will focus on discussing talks/posters that:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHave taken a cool approach to using data, or\u003c/li\u003e\n\u003cli\u003eHave focused on open science/data, or\u003c/li\u003e\n\u003cli\u003eDone something cool with R software, or\u003c/li\u003e\n\u003cli\u003eAre just exciting in general\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI will also tweet throughout the meeting from @recology_ (yes the underscore is part of the name, recology was already taken). \u003c/p\u003e","title":"Blogging/tweeting from ESA11"},{"content":"Here is one of the talks by Thomas Brouquet, and see the rest here.\nThomas Broquet by mez_en_video ","permalink":"http://localhost:1313/2011/07/models-in-evolutionary-ecology-seminar/","summary":"\u003cp\u003eHere is one of the talks by Thomas Brouquet, and see the rest \u003c!-- raw HTML omitted --\u003ehere\u003c!-- raw HTML omitted --\u003e.\u003c/p\u003e\n\n\n\u003ciframe frameborder=\"0\" height=\"270\" src=\"http://www.dailymotion.com/embed/video/xjylea\" width=\"480\"\u003e\u003c/iframe\u003e\u003cbr /\u003e\n\n\u003ca href=\"http://www.dailymotion.com/video/xjylea_thomas-broquet_tech\" target=\"_blank\"\u003eThomas Broquet\u003c/a\u003e \u003ci\u003eby \u003ca href=\"http://www.dailymotion.com/mez_en_video\" target=\"_blank\"\u003emez_en_video\u003c/a\u003e\u003c/i\u003e","title":"Models in Evolutionary Ecology seminar, organized by Timothee Poisot"},{"content":"We now have many options for archiving data sets online:\nDryad, KNB, Ecological Archives, Ecology Data Papers, Ecological Data, etc.\nHowever, these portals largely do not communicate with one another as far as I know, and there is no way to search over all data set sources, again, as far as I know. So, I wonder if it would ease finding of all these different data sets to get these different sites to get their data sets cloned on a site like Infochimps, or have links from Infochimps. Infochimps already has APIs (and there\u0026rsquo;s an R wrapper for the Infochimps API already set up here: http://cran.r-project.org/web/packages/infochimps/index.html by Drew Conway), and they have discussions set up there, etc.\nDoes it make sense to post data sets linked to published works on Infochimps? I think probably not know that I think about it. But perhaps it makes sense for other data sets, or subsets of data sets that are not linked with published works to be posted there as I know at least Dryad only accepts data sets linked with published papers.\nOne use case is there was a tweet from someone recently that his students were excited about getting their data sets on their resume/CV, but didn\u0026rsquo;t think there was a way to put them any place where there wasn\u0026rsquo;t a precondition that the data set was linked with a published work. Seems like this could be a good opportunity to place these datasets on Infcohimps, and at least they are available then where a lot of people are searching for data sets, etc.\nWhat I think would be ideal is if Dryad, KNB, etc. could link their datasets to Infochimps, where they could be found, then users can either get them from Infochimps, or perhaps you would have to go to the Dryad site, e.g. But at least you could search over all ecological data sets then.\n","permalink":"http://localhost:1313/2011/07/archiving-ecologyevolution-data-sets/","summary":"\u003cp\u003eWe now have many options for archiving data sets online:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://datadryad.org/\"\u003eDryad\u003c/a\u003e, \u003ca href=\"http://knb.ecoinformatics.org/index.jsp\"\u003eKNB\u003c/a\u003e, \u003ca href=\"http://www.esapubs.org/archive/\"\u003eEcological Archives\u003c/a\u003e, \u003ca href=\"http://www.esapubs.org/archive/archive_D.htm\"\u003eEcology Data Papers\u003c/a\u003e, \u003ca href=\"http://ecologicaldata.org/\"\u003eEcological Data\u003c/a\u003e, etc.\u003c/p\u003e\n\u003cp\u003eHowever, these portals largely do not communicate with one another as far as I know, and there is no way to search over all data set sources, again, as far as I know. So, I wonder if it would ease finding of all these different data sets to get these different sites to get their data sets cloned on a site like Infochimps, or have links from Infochimps.  Infochimps already has APIs (and there\u0026rsquo;s an R wrapper for the Infochimps API already set up here: \u003ca href=\"http://cran.r-project.org/web/packages/infochimps/index.html\"\u003ehttp://cran.r-project.org/web/packages/infochimps/index.html\u003c/a\u003e by Drew Conway), and they have discussions set up there, etc.\u003c/p\u003e","title":"Archiving ecology/evolution data sets online"},{"content":"Cloudnumbers and CRdata are two new cloud computing services.\nI tested the two services with a very simple script. The script simply creates a dataframe of 10000 numbers via rnorm, and assigns them to a factor of one of two levels (a or b). I then take the mean of the two factor levels with the aggregate function.\nIn CRdata you need to put in some extra code to format the output in a browser window. For example, the last line below needs to have \u0026lsquo;\u0026lt;crdata_object\u0026gt;\u0026rsquo; on both sides of the output object so it can be rendered in a browser. And etc. for other things that one would print to a console. Whereas you don\u0026rsquo;t need this extra code for using Cloudnumbers.\ndat \u0026lt;- data.frame(n = rnorm(10000), p = rep(c(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;), each=5000)) out \u0026lt;- aggregate(n ~ p, data = dat, mean) Here is a screenshot of the output from CRdata with the simple script above.This simple script ran in about 20 seconds or so from starting the job to finishing. However, it seems like the only output option is html. Can this be right? This seems like a terrible only option.\nIn Cloudnumbers you have to start a workspace, upload your R code file.Then, start a session\u0026hellip;choose your software platform\u0026hellip;choose packages (one at a time, very slow)\u0026hellip;then choose number of clusters, etc.Then finally star the job.Then it initializes, then finally you can open the console, andThen from here it is like running R as you normally would, except on the web.\nWho wins (at least for our very minimal example above)\n","permalink":"http://localhost:1313/2011/07/crdata-vs-cloudnumbers/","summary":"\u003cp\u003e\u003c!-- raw HTML omitted --\u003eCloudnumbers\u003c!-- raw HTML omitted --\u003e and \u003c!-- raw HTML omitted --\u003eCRdata\u003c!-- raw HTML omitted --\u003e are two new cloud computing services.\u003c/p\u003e\n\u003cp\u003eI tested the two services with a very simple script. The script simply creates a dataframe of 10000 numbers via rnorm, and assigns them to a factor of one of two levels (a or b). I then take the mean of the two factor levels with the aggregate function.\u003c/p\u003e","title":"CRdata vs. Cloudnumbers"},{"content":"Have you ever wanted to search and fetch barcode data from Bold Systems?\nI am developing functions to interface with Bold from R. I just started, but hopefully folks will find it useful.\nThe code is at Github here. The two functions are still very buggy, so please bring up issues below, or in the Issues area on Github. For example, some searches work and other similar searches don\u0026rsquo;t. Apologies in advance for the bugs.\nBelow is a screenshot of an example query using function getsampleids to get barcode identifiers for specimens. You can then use getseqs function to grab barcode data for specific specimens or many specimens.\n","permalink":"http://localhost:1313/2011/06/rbold-an-r-interface-for-bold-systems-barcode-repository/","summary":"\u003cp\u003eHave you ever wanted to \u003ca href=\"http://services.boldsystems.org/index.php?page=1_esearch\u0026amp;status=\"\u003esearch\u003c/a\u003e and \u003ca href=\"http://services.boldsystems.org/index.php?page=2_efetch\u0026amp;status=\"\u003efetch\u003c/a\u003e barcode data from \u003ca href=\"http://www.boldsystems.org/views/login.php\"\u003eBold Systems\u003c/a\u003e?\u003c/p\u003e\n\u003cp\u003eI am developing functions to interface with Bold from R. I just started, but hopefully folks will find it useful.\u003c/p\u003e\n\u003cp\u003eThe code is at Github \u003ca href=\"https://github.com/ropensci/rbold\"\u003ehere\u003c/a\u003e. The two functions are still very buggy, so please bring up issues below, or in the Issues area on Github. For example, some searches work and other similar searches don\u0026rsquo;t. Apologies in advance for the bugs.\u003c/p\u003e","title":"rbold: An R Interface for Bold Systems barcode repository"},{"content":"We just wrapped up the 2011 iEvoBio meeting. It was awesome! If you didn\u0026rsquo;t go this year or last year, definitely think about going next year.\nHere is a list of the cool projects that were discussed at the meeting (apologies if I left some out):\nVistrails: workflow tool, awesome project by Claudio Silva Commplish: purpose is to use via API\u0026rsquo;s, not with the web UI Phylopic: a database of life-form silouhettes, including an API for remote access, sweet! Gloome MappingLife: awesome geographic/etc data visualization interace on the web SuiteSMA: visualizating multiple alignments treeBASE: R interface to treebase, by Carl Boettiger VertNet: database for vertebrate natural history collections RevBayes: revamp of MrBayes, with GUI, etc. Phenoscape Knowledge Base Peter Midford lightning talk: talked about matching taxonomic and genetic data BiSciCol: biological science collections tracker Ontogrator TNRS: taxonomic name resolution service Barcode of Life data systems, and remote access Moorea Biocode Project Microbial LTER\u0026rsquo;s data BirdVis: interactive bird data visualization (Claudio Silva in collaboration with Cornell Lab of Ornithology) Crowdlabs: I think the site is down right now, another project by Claudio Silva Phycas: Bayesian phylogenetics, can you just call this from R? RIP MrBayes!!!! replaced by RevBayes (see 9 above) Slides of presentations will be at Slideshare (not all presentations up yet) A birds of a feather group I was involved in proposed an idea (TOL-o-matic) like Phylomatic, but of broader scope, for easy access and submission of trees, and perhaps even social (think just pushing a \u0026lsquo;SHARE\u0026rsquo; button within PAUP, RevBayes, or other phylogenetics software)! Synopses of Birds of a Feather discussion groups: http://piratepad.net/iEvoBio11-BoF-reportouts ","permalink":"http://localhost:1313/2011/06/ievobio-2011-synopsis/","summary":"\u003cp\u003eWe just wrapped up the \u003c!-- raw HTML omitted --\u003e2011 iEvoBio meeting\u003c!-- raw HTML omitted --\u003e. It was awesome! If you didn\u0026rsquo;t go this year or last year, definitely think about going next year.\u003c/p\u003e\n\u003cp\u003eHere is a list of the cool projects that were discussed at the meeting (apologies if I left some out):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eVistrails\u003c!-- raw HTML omitted --\u003e: workflow tool, awesome project by Claudio Silva\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eCommplish\u003c!-- raw HTML omitted --\u003e: purpose is to use via API\u0026rsquo;s, not with the web UI\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003ePhylopic\u003c!-- raw HTML omitted --\u003e: a database of life-form silouhettes, including an API for remote access, sweet!\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eGloome\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eMappingLife\u003c!-- raw HTML omitted --\u003e: awesome geographic/etc data visualization interace on the web\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eSuiteSMA\u003c!-- raw HTML omitted --\u003e: visualizating multiple alignments\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003etreeBASE\u003c!-- raw HTML omitted --\u003e: R interface to treebase, by \u003c!-- raw HTML omitted --\u003eCarl Boettiger\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eVertNet\u003c!-- raw HTML omitted --\u003e: database for vertebrate natural history collections\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eRevBayes\u003c!-- raw HTML omitted --\u003e: revamp of MrBayes, with GUI, etc. \u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003ePhenoscape Knowledge Base\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003ePeter Midford lightning talk: talked about matching taxonomic and genetic data\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eBiSciCol\u003c!-- raw HTML omitted --\u003e: biological science collections tracker\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eOntogrator\u003c!-- raw HTML omitted --\u003e \u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eTNRS\u003c!-- raw HTML omitted --\u003e: taxonomic name resolution service\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eBarcode of Life data systems\u003c!-- raw HTML omitted --\u003e, and \u003c!-- raw HTML omitted --\u003eremote access\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eMoorea Biocode Project\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eMicrobial LTER\u0026rsquo;s data\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eBirdVis\u003c!-- raw HTML omitted --\u003e: interactive bird data visualization (Claudio Silva in collaboration with Cornell Lab of Ornithology)\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003eCrowdlabs\u003c!-- raw HTML omitted --\u003e: I think the site is down right now, another project by Claudio Silva\u003c/li\u003e\n\u003cli\u003e\u003c!-- raw HTML omitted --\u003ePhycas\u003c!-- raw HTML omitted --\u003e: Bayesian phylogenetics, can you just call this from R?\u003c/li\u003e\n\u003cli\u003eRIP MrBayes!!!! replaced by RevBayes (see 9 above)\u003c/li\u003e\n\u003cli\u003eSlides of presentations will be at \u003c!-- raw HTML omitted --\u003eSlideshare\u003c!-- raw HTML omitted --\u003e (not all presentations up yet)          \u003c/li\u003e\n\u003cli\u003eA birds of a feather group I was involved in proposed an idea (TOL-o-matic) like Phylomatic, but of broader scope, for easy access and submission of trees, and perhaps even social (think just pushing a \u0026lsquo;SHARE\u0026rsquo; button within PAUP, RevBayes, or other phylogenetics software)! \u003c/li\u003e\n\u003cli\u003eSynopses of Birds of a Feather discussion groups: \u003c!-- raw HTML omitted --\u003e\u003ca href=\"http://piratepad.net/iEvoBio11-BoF-reportouts\"\u003ehttp://piratepad.net/iEvoBio11-BoF-reportouts\u003c/a\u003e\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"iEvoBio 2011 Synopsis"},{"content":"The Public Libraries of Science (PLOS) has an API so that developers can create cool tools to access their data (including full text papers!!).\nCarl Boettiger at UC Davis and I are working on R functions that use the PLoS API. See our code on Github here. See the wiki at the Github page for examples of use. We hope to deploy rplos as a package someday soon. Please feel free to suggest changes/additions rplos in the comments below or on the Github/rplos site.\nGet your own API key here.\n","permalink":"http://localhost:1313/2011/06/plos-journals-api-from-r-rplos-/","summary":"\u003cp\u003eThe Public Libraries of Science (PLOS) has an API so that developers can create cool tools to access their data (including full text papers!!).\u003c/p\u003e\n\u003cp\u003eCarl Boettiger at UC Davis and I are working on R functions that use the PLoS API. See our code on Github \u003ca href=\"https://github.com/ropensci/rplos\"\u003ehere\u003c/a\u003e. See the wiki at the Github page for examples of use. We hope to deploy rplos as a package someday soon. Please feel free to suggest changes/additions rplos in the comments below or on the Github/rplos site.\u003c/p\u003e","title":"PLoS journals API from R: \"rplos\""},{"content":" ","permalink":"http://localhost:1313/2011/06/ggplot2-talk-by-hadley-whickam-at-google/","summary":"\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/TaxJwC_MP9Q?autoplay=0\u0026controls=1\u0026end=0\u0026loop=0\u0026mute=0\u0026start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\n      \u003e\u003c/iframe\u003e\n    \u003c/div\u003e","title":"ggplot2 talk by Hadley Whickam at Google"},{"content":"I am writing some functions to acquire data from the OpenStates project, via their API. They have a great support community at Google Groups as well.\nOn its face this post is not obviously about ecology or evolution, but well, our elected representatives do, so to speak, hold our environment in a noose, ready to let the Earth hang any day.\nCode I am developing is over at Github. Here is an example of its use in R, in this case using the Bill Search option (billsearch.R on my Github site), and in this case you do not provide your API key in the function call, but instead put it in your .Rprofile file, which is called when you open R.\nWe are searching here for the term \u0026lsquo;agriculture\u0026rsquo; in Texas (\u0026rsquo;tx\u0026rsquo;), in the \u0026lsquo;upper\u0026rsquo; chamber.\n\u0026gt; temp \u0026lt;- billsearch(\u0026#39;agriculture\u0026#39;, state = \u0026#39;tx\u0026#39;, chamber = \u0026#39;upper\u0026#39;) \u0026gt; length(temp) [1] 21 \u0026gt; temp[[1]] $title [1] \u0026#34;Congratulating John C. Padalino of El Paso for being appointed to the United States Department of Agriculture.\u0026#34; $created_at [1] \u0026#34;2010-08-11 07:59:46\u0026#34; $updated_at [1] \u0026#34;2010-09-02 03:34:39\u0026#34; $chamber [1] \u0026#34;upper\u0026#34; $state [1] \u0026#34;tx\u0026#34; $session [1] \u0026#34;81\u0026#34; $type $type[[1]] [1] \u0026#34;resolution\u0026#34; $subjects $subjects[[1]] [1] \u0026#34;Resolutions\u0026#34; $subjects[[2]] [1] \u0026#34;Other\u0026#34; $bill_id [1] \u0026#34;SR 1042\u0026#34; Apparently, the first bill (SR 2042, see $bill_id at the bottom of the list output) that came up was to congratulate John Paladino for being appointed to the USDA.\nThe other function I have ready is getting basic metadata on a state, called statemetasearch.\nI plan to develop more functions for all the possible API calls to the OpenStates project.\n","permalink":"http://localhost:1313/2011/06/openstates-from-r-via-api-watch-your-elected-representatives/","summary":"\u003cp\u003eI am writing some functions to acquire data from the \u003ca href=\"http://openstates.sunlightlabs.com/\"\u003eOpenStates project,\u003c/a\u003e via \u003ca href=\"http://openstates.sunlightlabs.com/api/\"\u003etheir API\u003c/a\u003e. They have \u003ca href=\"http://groups.google.com/group/fifty-state-project\"\u003ea great support community\u003c/a\u003e at Google Groups as well.\u003c/p\u003e\n\u003cp\u003eOn its face this post is not obviously about ecology or evolution, but well, our elected representatives do, so to speak, hold our environment in a noose, ready to let the Earth hang any day.\u003c/p\u003e\n\u003cp\u003eCode \u003ca href=\"https://SChamberlain@github.com/SChamberlain/ropstates.git\"\u003eI am developing is over at Github\u003c/a\u003e. Here is an example of its use in R, in this case using the Bill Search option (billsearch.R on my Github site), and in this case you do not provide your API key in the function call, but instead put it in your .Rprofile file, which is called when you open R.\u003c/p\u003e","title":"OpenStates from R via API: watch your elected representatives"},{"content":"Thanks so much everyone for stopping by!\n![]( https://f.cl.ly/items/3m2g3b3r2o1b1k1b3x27/Screen%20shot%202011-06-09%20at%2012.07.33%20PM.png\n","permalink":"http://localhost:1313/2011/06/10000-visits-to-my-recology/","summary":"\u003cp\u003eThanks so much everyone for stopping by!\u003c/p\u003e\n\u003cp\u003e![](\n\u003ca href=\"https://f.cl.ly/items/3m2g3b3r2o1b1k1b3x27/Screen%20shot%202011-06-09%20at%2012.07.33%20PM.png\"\u003ehttps://f.cl.ly/items/3m2g3b3r2o1b1k1b3x27/Screen%20shot%202011-06-09%20at%2012.07.33%20PM.png\u003c/a\u003e\u003c/p\u003e","title":"More than 10,000 visits to Recology!!!!"},{"content":"A new paper out in Ecology by Xiao and colleagues (in press, here) compares the use of log-transformation to non-linear regression for analyzing power-laws.\nThey suggest that the error distribution should determine which method performs better. When your errors are additive, homoscedastic, and normally distributed, they propose using non-linear regression. When errors are multiplicative, heteroscedastic, and lognormally distributed, they suggest using linear regression on log-transformed data. The assumptions about these two methods are different, so cannot be correct for a single dataset.\nThey will provide their R code for their methods once they are up on Ecological Archives (they weren\u0026rsquo;t up there by the time of this post).\n","permalink":"http://localhost:1313/2011/06/how-to-fit-power-laws/","summary":"\u003cp\u003eA new paper out in Ecology by Xiao and colleagues (in press, \u003ca href=\"http://www.esajournals.org/doi/abs/10.1890/11-0538.1\"\u003ehere\u003c/a\u003e) compares the use of log-transformation to non-linear regression for analyzing power-laws.\u003c/p\u003e\n\u003cp\u003eThey suggest that the error distribution should determine which method performs better. When your errors are additive, homoscedastic, and normally distributed, they propose using non-linear regression. When errors are multiplicative, heteroscedastic, and lognormally distributed, they suggest using linear regression on log-transformed data. The assumptions about these two methods are different, so cannot be correct for a single dataset.\u003c/p\u003e","title":"How to fit power laws"},{"content":"I am writing a set of functions to search ITIS for taxonomic information (more databases to come) and functions to fetch plant phylogenetic trees from Phylomatic. Code at github.\nAlso, see the examples in the demos folder on the Github site above.\n","permalink":"http://localhost:1313/2011/06/searching-itis-and-fetching-phylomatic-trees/","summary":"\u003cp\u003eI am writing a set of functions to search ITIS for taxonomic information (more databases to come) and functions to fetch plant phylogenetic trees from Phylomatic. \u003ca href=\"https://github.com/sckott/taxize_\"\u003eCode at github\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAlso, see the examples in the demos folder on the Github site above.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://1.bp.blogspot.com/-rcw5OIf3Hak/Telhj896L0I/AAAAAAAAEh4/p6GhpNRW6IA/s1600/examplephylogenyplot.png\"\u003e\u003cimg loading=\"lazy\" src=\"http://1.bp.blogspot.com/-rcw5OIf3Hak/Telhj896L0I/AAAAAAAAEh4/p6GhpNRW6IA/s400/examplephylogenyplot.png\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"searching ITIS and fetching Phylomatic trees"},{"content":"I did a little simulation to examine how K and lambda vary in response to tree size (and how they compare to each other on the same simulated trees). I use Liam Revell\u0026rsquo;s functions fastBM to generate traits, and phylosig to measure phylogenetic signal.\nTwo observations:\nFirst, it seems that lambda is more sensitive than K to tree size, but then lambda levels out at about 40 species, whereas K continues to vary around a mean of 1. Second, K is more variable than lambda at all levels of tree size (compare standard error bars). Does this make sense to those smart folks out there?\n#### Simulations install.packages(c(\u0026#34;ape\u0026#34;,\u0026#34;reshape2\u0026#34;,\u0026#34;ggplot2\u0026#34;)) require(ape); require(reshape2); require(ggplot2) source(\u0026#34;http://anolis.oeb.harvard.edu/~liam/R-phylogenetics/phylosig/v0.3/phylosig.R\u0026#34;) source(\u0026#34;http://anolis.oeb.harvard.edu/~liam/R-phylogenetics/fastBM/v0.4/fastBM.R\u0026#34;) # Simulation function physig_sim # input: x = number of species in tree # output: a vector length two with (K, lamba) physig_sim \u0026lt;- function(x) { tree \u0026lt;- rcoal(x) traits \u0026lt;- fastBM(tree) physig_k \u0026lt;- phylosig(tree, traits, method = \u0026#34;K\u0026#34;) physig_l \u0026lt;- phylosig(tree, traits, method = \u0026#34;lambda\u0026#34;)$lambda sigs \u0026lt;- c(physig_k, physig_l) return(sigs) } # Run simulation spnumbs \u0026lt;- c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 65, 80, 95) rands \u0026lt;- 1000 siglist \u0026lt;- alply( spnumbs, 1, function(x) t(replicate(rands, physig_sim(x))), .progress=\u0026#34;text\u0026#34;) sigdf \u0026lt;- as.data.frame(do.call(rbind, siglist)) names(sigdf) \u0026lt;- c(\u0026#34;K\u0026#34;, \u0026#34;L\u0026#34;) sigdf$numsp_ \u0026lt;- rep(spnumbs, each=rands) sigdf_m \u0026lt;- melt(sigdf, id = 3) # Plot results plotdf \u0026lt;- ddply(sigdf_m, .(numsp_, variable), summarise, mean = mean(value), se = sd(value)/sqrt(length(value)) ) limits \u0026lt;- aes(ymax = mean + se, ymin = mean - se) dodge \u0026lt;- position_dodge(width=0.9) ggplot(plotdf, aes(x = numsp_, y = mean, shape = variable)) + geom_point(position=dodge) + geom_errorbar(limits, position=dodge, width=0.25) + geom_smooth() ggsave(\u0026#34;physig_sim.jpeg\u0026#34;) ","permalink":"http://localhost:1313/2011/05/phylogenetic-signal-simulations/","summary":"\u003cp\u003eI did a little simulation to examine how K and lambda vary in response to tree size (and how they compare to each other on the same simulated trees). I use Liam Revell\u0026rsquo;s functions fastBM to generate traits, and phylosig to measure phylogenetic signal.\u003c/p\u003e\n\u003cp\u003eTwo observations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirst, it seems that lambda is more sensitive than K to tree size, but then lambda levels out at about 40 species, whereas K continues to vary around a mean of 1.\u003c/li\u003e\n\u003cli\u003eSecond, K is more variable than lambda at all levels of tree size (compare standard error bars).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDoes this make sense to those smart folks out there?\u003c/p\u003e","title":"phylogenetic signal simulations"},{"content":"UPDATE: Greg jordan has a much more elegant way of plotting trees with ggplot2. See his links in the comments below.\nI wrote a simple function for plotting a phylogeny in ggplot2. However, it only handles a 3 species tree right now, as I haven\u0026rsquo;t figured out how to generalize the approach to N species. It\u0026rsquo;s at https://gist.github.com/977207\nAny ideas on how to improve this?\n","permalink":"http://localhost:1313/2011/05/simple-function-for-plotting/","summary":"\u003cp\u003eUPDATE: Greg jordan has a much more elegant way of plotting trees with ggplot2. See his links in the comments below.\u003c/p\u003e\n\u003cp\u003eI wrote a simple function for plotting a phylogeny in ggplot2. However, it only handles a 3 species tree right now, as I haven\u0026rsquo;t figured out how to generalize the approach to N species. It\u0026rsquo;s at \u003ca href=\"https://gist.github.com/977207\"\u003ehttps://gist.github.com/977207\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAny ideas on how to improve this?\u003c/p\u003e","title":"A simple function for plotting phylogenies in ggplot2"},{"content":" I had seen the function idata.frame in plyr before, but not really tested it. From the plyr documentation:\n\u0026ldquo;An immutable data frame works like an ordinary data frame, except that when you subset it, it returns a reference to the original data frame, not a a copy. This makes subsetting substantially faster and has a big impact when you are working with large datasets with many groups.\u0026rdquo;\nFor example, although baseball is a data.frame, its immutable counterpart is a reference to it:\n\u0026gt; idata.frame(baseball) \u0026lt;environment: 0x1022c74e8\u0026gt; attr(,\u0026#34;class\u0026#34;) [1] \u0026#34;idf\u0026#34; \u0026#34;environment\u0026#34; Here are a few comparisons of operations on normal data frames and immutable data frames. Immutable data frames don\u0026rsquo;t work with the doBy package, but do work with aggregate in base functions. Overall, the speed gains using idata.frame are quite impressive - I will use it more often for sure.\nHere\u0026rsquo;s the comparisons of idata.frames and data.frames:\n# load packages require(plyr) require(reshape2) # Make immutable data frame baseball_i \u0026lt;- idata.frame(baseball) Example 1 - idata.frame more than twice as fast system.time(replicate(50, ddply(baseball, \u0026#34;year\u0026#34;, summarise, mean(rbi)))) user system elapsed 8.509 0.266 8.798 system.time(replicate(50, ddply(baseball_i, \u0026#34;year\u0026#34;, summarise, mean(rbi)))) user system elapsed 7.233 0.025 7.334 Example 2 - Bummer, this does not work with idata.frame\u0026rsquo;s colwise(max, is.numeric)(baseball) # works year stint g ab r h X2b X3b hr rbi sb cs bb so ibb hbp sh sf 1 2007 4 165 705 177 257 64 28 73 NA NA NA 232 NA NA NA NA NA gidp 1 NA colwise(max, is.numeric)(baseball_i) # doesn\u0026#39;t work Error: is.data.frame(df) is not TRUE Example 3 - idata.frame twice as fast system.time(replicate(100, baseball[baseball$year == \u0026#34;1884\u0026#34;, ])) user system elapsed 1.329 0.035 1.378 system.time(replicate(100, baseball_i[baseball_i$year == \u0026#34;1884\u0026#34;, ])) user system elapsed 0.674 0.015 0.689 Example 4 - idata.frame faster system.time(replicate(50, melt(baseball[, 1:4], id = 1))) user system elapsed 7.129 0.506 7.691 system.time(replicate(50, melt(baseball_i[, 1:4], id = 1))) user system elapsed 0.852 0.162 1.015 And you can go back to a data frame by d \u0026lt;- as.data.frame(baseball_i) str(d) \u0026#39;data.frame\u0026#39;: 21699 obs. of 22 variables: $ id : chr \u0026#34;ansonca01\u0026#34; \u0026#34;forceda01\u0026#34; \u0026#34;mathebo01\u0026#34; \u0026#34;startjo01\u0026#34; ... $ year : int 1871 1871 1871 1871 1871 1871 1871 1872 1872 1872 ... $ stint: int 1 1 1 1 1 1 1 1 1 1 ... $ team : chr \u0026#34;RC1\u0026#34; \u0026#34;WS3\u0026#34; \u0026#34;FW1\u0026#34; \u0026#34;NY2\u0026#34; ... $ lg : chr \u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34; ... $ g : int 25 32 19 33 29 29 29 46 37 25 ... $ ab : int 120 162 89 161 128 146 145 217 174 130 ... $ r : int 29 45 15 35 35 40 36 60 26 40 ... $ h : int 39 45 24 58 45 47 37 90 46 53 ... $ X2b : int 11 9 3 5 3 6 5 10 3 11 ... $ X3b : int 3 4 1 1 7 5 7 7 0 0 ... $ hr : int 0 0 0 1 3 1 2 0 0 0 ... $ rbi : int 16 29 10 34 23 21 23 50 15 16 ... $ sb : int 6 8 2 4 3 2 2 6 0 2 ... $ cs : int 2 0 1 2 1 2 2 6 1 2 ... $ bb : int 2 4 2 3 1 4 9 16 1 1 ... $ so : int 1 0 0 0 0 1 1 3 1 0 ... $ ibb : int NA NA NA NA NA NA NA NA NA NA ... $ hbp : int NA NA NA NA NA NA NA NA NA NA ... $ sh : int NA NA NA NA NA NA NA NA NA NA ... $ sf : int NA NA NA NA NA NA NA NA NA NA ... $ gidp : int NA NA NA NA NA NA NA NA NA NA ... idata.frame doesn\u0026rsquo;t work with the doBy package require(doBy) summaryBy(rbi ~ year, baseball_i, FUN = c(mean), na.rm = T) Error: cannot coerce type \u0026#39;environment\u0026#39; to vector of type \u0026#39;any\u0026#39; But idata.frame works with aggregate in base (but with minimal speed gains) and aggregate is faster than ddply system.time(replicate(100, aggregate(rbi ~ year, baseball, mean))) user system elapsed 4.998 0.346 5.373 system.time(replicate(100, aggregate(rbi ~ year, baseball_i, mean))) user system elapsed 4.745 0.283 5.045 system.time(replicate(100, ddply(baseball_i, \u0026#34;year\u0026#34;, summarise, mean(rbi)))) user system elapsed 13.293 0.042 13.428 ","permalink":"http://localhost:1313/2011/05/plyrs-idataframe-vs-dataframe/","summary":"\u003chr\u003e\n\u003cp\u003eI had seen the function idata.frame in plyr before, but not really tested it. From the plyr documentation:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003e\u0026ldquo;An immutable data frame works like an ordinary data frame, except that when you subset it, it returns a reference to the original data frame, not a a copy. This makes subsetting substantially faster and has a big impact when you are working with large datasets with many groups.\u0026rdquo;\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor example, although baseball is a data.frame, its immutable counterpart is a reference to it:\u003c/p\u003e","title":"plyr's idata.frame VS. data.frame"},{"content":"I just realized that the gists code blocks don\u0026rsquo;t show up in Google Reader, so you have to click the link to my blog to see the gists. Apologies for that!\n-S\n","permalink":"http://localhost:1313/2011/05/google-reader/","summary":"\u003cp\u003eI just realized that the gists code blocks don\u0026rsquo;t show up in Google Reader, so you have to click the link to my blog to see the gists. Apologies for that!\u003c/p\u003e\n\u003cp\u003e-S\u003c/p\u003e","title":"google reader"},{"content":"With all the packages (and beta stage groups of functions) for comparative phylogenetics in R (tested here: picante, geiger, ape, motmot, Liam Revell\u0026rsquo;s functions), I was simply interested in which functions to use in cases where multiple functions exist to do the same thing. I only show default settings, so perhaps these functions would differ under different parameter settings. [I am using a Mac 2.4 GHz i5, 4GB RAM]\nGet motmot here: https://r-forge.r-project.org/R/?group_id=782\nGet Liam Revell\u0026rsquo;s functions here: http://anolis.oeb.harvard.edu/~liam/R-phylogenetics/\n\u0026gt; # Load require(motmot); require(geiger); require(picante) source(\u0026#34;http://anolis.oeb.harvard.edu/~liam/R-phylogenetics/phylosig/v0.3/phylosig.R\u0026#34;) source(\u0026#34;http://anolis.oeb.harvard.edu/~liam/R-phylogenetics/fastBM/v0.4/fastBM.R\u0026#34;) # Make tree tree \u0026lt;- rcoal(10) # Transform branch lengths \u0026gt; system.time( replicate(1000, transformPhylo(tree, model = \u0026#34;lambda\u0026#34;, lambda = 0.5)) ) # motmot user system elapsed 1.757 0.004 1.762 \u0026gt; system.time( replicate(1000, lambdaTree(tree, 0.9)) ) # geiger user system elapsed 3.708 0.008 3.716 \u0026gt; # motmot wins!!! # Simulate trait evolution system.time( replicate(1000, transformPhylo.sim(tree, model = \u0026#34;bm\u0026#34;)) ) # motmot user system elapsed 3.732 0.007 3.741 \u0026gt; system.time( replicate(1000, rTraitCont(tree, model = \u0026#34;BM\u0026#34;)) ) # ape user system elapsed 0.312 0.009 0.321 \u0026gt; system.time( replicate(1000, fastBM(tree)) ) # Revell user system elapsed 1.315 0.005 1.320 \u0026gt; # ape wins!!! # Phylogenetically independent contrasts trait \u0026lt;- rnorm(10) names(trait) \u0026lt;- tree$tip.label \u0026gt; system.time( replicate(10000, pic.motmot(trait, tree)$contr[,1]) ) # motmot user system elapsed 3.062 0.007 3.070 \u0026gt; system.time( replicate(10000, pic(trait, tree)) ) # ape user system elapsed 2.846 0.007 2.853 \u0026gt; # ape wins!!! # Phylogenetic signal, Blomberg\u0026#39;s K \u0026gt; system.time( replicate(100, Kcalc(trait, tree)) ) # picante user system elapsed 1.311 0.005 1.316 \u0026gt; system.time( replicate(100, phylosig(tree, trait, method = \u0026#34;K\u0026#34;)) ) # Revell user system elapsed 0.201 0.000 0.202 \u0026gt; # Liam Revell wins!!! # Ancestral character state estimation \u0026gt; system.time( replicate(100, ace(trait, tree)$ace) ) # ape user system elapsed 4.988 0.018 5.007 \u0026gt; system.time( replicate(100, getAncStates(trait, tree)) ) # geiger user system elapsed 2.253 0.005 2.258 \u0026gt; # geiger wins!!! It\u0026rsquo;s hard to pick an overall winner because not all functions are available in all packages, but there are definitely some functions that are faster than others.\n","permalink":"http://localhost:1313/2011/05/comparison-of-functions-for-comparative-phylogenetics/","summary":"\u003cp\u003eWith all the packages (and beta stage groups of functions) for comparative phylogenetics in R (tested here: picante, geiger, ape, motmot, Liam Revell\u0026rsquo;s functions), I was simply interested in which functions to use in cases where multiple functions exist to do the same thing. I only show default settings, so perhaps these functions would differ under different parameter settings. [I am using a Mac 2.4 GHz i5, 4GB RAM]\u003c/p\u003e\n\u003cp\u003eGet motmot here: \u003ca href=\"https://r-forge.r-project.org/R/?group_id=782\"\u003ehttps://r-forge.r-project.org/R/?group_id=782\u003c/a\u003e\u003c/p\u003e","title":"Comparison of functions for comparative phylogenetics"},{"content":"RHIPE: An Interface Between Hadoop and R\nPresented by Saptarshi Guha\nAnd this review of methods for interfacing with Hadoop suggests R\u0026rsquo;s RHIPE is quite nice.\n","permalink":"http://localhost:1313/2011/05/rhipe-package-in-r-for-interfacing-between-hadoop-and-r/","summary":"\u003cp\u003e\u003cstrong\u003e\u003ca href=\"http://www.lecturemaker.com/2011/02/rhipe/#video\" title=\"Click link to go to the video page\"\u003eRHIPE: An Interface Between Hadoop and R\u003c/a\u003e\u003c/strong\u003e\u003cbr\u003e\nPresented by Saptarshi Guha\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.lecturemaker.com/2011/02/rhipe/#video\"\u003e\u003cimg alt=\"Video Link\" loading=\"lazy\" src=\"http://www.lecturemaker.com/lectures/RMeetUp2010/RHIPE_Lecture.jpg\" title=\"Click image to go to the video page\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAnd \u003ca href=\"http://blog.piccolboni.info/2011/04/looking-for-map-reduce-language.html\"\u003ethis review\u003c/a\u003e of methods for interfacing with Hadoop suggests R\u0026rsquo;s RHIPE is quite nice.\u003c/p\u003e","title":"RHIPE package in R for interfacing between Hadoop and R"},{"content":"UPDATE: See Carl Boettiger\u0026rsquo;s functions/package at Github for searching Treebase here.\nTreebase is a great resource for phylogenetic trees, and has a nice interface for searching for certain types of trees. However, if you want to simply download a lot of trees for analyses (like that in Davies et al.), then you want to be able to access trees in bulk (I believe Treebase folks are working on an API though). I wrote some simple code for extracting trees from Treebase.org.It reads an xml file of (in this case consensus) URL\u0026rsquo;s for each tree, parses the xml, makes a vector of URL\u0026rsquo;s, reads the nexus files with error checking, remove trees that gave errors, then a simple plot looking at metrics of the trees.\nIs there an easier way to do this?\n(note: code is gone, was at https://gist.github.com/953468.js?file=treebase_code.R)\n","permalink":"http://localhost:1313/2011/05/treebase-trees-from-r/","summary":"\u003cp\u003eUPDATE: See Carl Boettiger\u0026rsquo;s functions/package at Github for searching Treebase \u003c!-- raw HTML omitted --\u003ehere\u003c!-- raw HTML omitted --\u003e.\u003c/p\u003e\n\u003cp\u003eTreebase is a great resource for phylogenetic trees, and has a nice interface for searching for certain types of trees. However, if you want to simply download a lot of trees for analyses (like that in \u003c!-- raw HTML omitted --\u003eDavies et al.\u003c!-- raw HTML omitted --\u003e), then you want to be able to access trees in bulk (I believe Treebase folks are working on an API though). I wrote some simple code for extracting trees from \u003c!-- raw HTML omitted --\u003eTreebase.org\u003c!-- raw HTML omitted --\u003e.\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eIt reads an xml file of (in this case consensus) URL\u0026rsquo;s for each tree, parses the xml, makes a vector of URL\u0026rsquo;s, reads the nexus files with error checking, remove trees that gave errors, then a simple plot looking at metrics of the trees.\u003c/p\u003e","title":"Treebase trees from R"},{"content":"So perhaps you have all figured this out already, but I was excited to figure out how to finally neatly get all the data frames, lists, vectors, etc. out of a nested list. It is as easy as nesting calls to the apply family of functions, in the case below, using plyr\u0026rsquo;s apply like functions. Take this example:\n# Nested lists code, an example # Make a nested list mylist \u0026lt;- list() mylist_ \u0026lt;- list() for(i in 1:5) { for(j in 1:5) { mylist[[j]] \u0026lt;- i*j } mylist_[[i]] \u0026lt;- mylist } # return values from first part of list laply(mylist_[[1]], identity) [1] 1 2 3 4 5 # return all values laply(mylist_, function(x) laply(x, identity)) 1 2 3 4 5 [1,] 1 2 3 4 5 [2,] 2 4 6 8 10 [3,] 3 6 9 12 15 [4,] 4 8 12 16 20 [5,] 5 10 15 20 25 # perform some function, in this case sqrt of each value laply(mylist_, function(x) laply(x, function(x) sqrt(x))) 1 2 3 4 5 [1,] 1.000000 1.414214 1.732051 2.000000 2.236068 [2,] 1.414214 2.000000 2.449490 2.828427 3.162278 [3,] 1.732051 2.449490 3.000000 3.464102 3.872983 [4,] 2.000000 2.828427 3.464102 4.000000 4.472136 [5,] 2.236068 3.162278 3.872983 4.472136 5.000000 ","permalink":"http://localhost:1313/2011/04/processing-nested-lists/","summary":"\u003cp\u003eSo perhaps you have all figured this out already, but I was excited to figure out how to finally neatly get all the data frames, lists, vectors, etc. out of a nested list. It is as easy as nesting calls to the apply family of functions, in the case below, using plyr\u0026rsquo;s apply like functions. Take this example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Nested lists code, an example\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Make a nested list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emylist \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003elist\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emylist_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003elist\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e(i \u003cspan style=\"color:#66d9ef\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e(j \u003cspan style=\"color:#66d9ef\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  mylist[[j]] \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e i\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003ej\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emylist_[[i]] \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e mylist\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# return values from first part of list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elaply\u003c/span\u003e(mylist_[[1]], identity)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[1] \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# return all values\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elaply\u003c/span\u003e(mylist_, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) \u003cspan style=\"color:#a6e22e\"\u003elaply\u003c/span\u003e(x, identity))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[1,] \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[2,] \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[3,] \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[4,] \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[5,] \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e25\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# perform some function, in this case sqrt of each value\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elaply\u003c/span\u003e(mylist_, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) \u003cspan style=\"color:#a6e22e\"\u003elaply\u003c/span\u003e(x, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) \u003cspan style=\"color:#a6e22e\"\u003esqrt\u003c/span\u003e(x)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[1,] \u003cspan style=\"color:#ae81ff\"\u003e1.000000\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1.414214\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1.732051\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2.000000\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2.236068\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[2,] \u003cspan style=\"color:#ae81ff\"\u003e1.414214\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2.000000\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2.449490\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2.828427\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.162278\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[3,] \u003cspan style=\"color:#ae81ff\"\u003e1.732051\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2.449490\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.000000\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.464102\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.872983\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[4,] \u003cspan style=\"color:#ae81ff\"\u003e2.000000\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2.828427\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.464102\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4.000000\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4.472136\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e[5,] \u003cspan style=\"color:#ae81ff\"\u003e2.236068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.162278\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.872983\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4.472136\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e5.000000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Processing nested lists"},{"content":"Here is some code to run Phylip\u0026rsquo;s contrast application from R and get the output within R to easily manipulate yourself. Importantly, the code is written specifically for trait pairs only as the regular expression work in the code specifically grabs data from contast results when only two traits are input. You could easily change the code to do N traits. Note that the p-value calculated for the chi-square statistic is not output from contrast, but is calculated within the function \u0026lsquo;PhylipWithinSpContr\u0026rsquo;. In the code below there are two functions that make a lot of busy work easier: \u0026lsquo;WritePhylip\u0026rsquo; and \u0026lsquo;PhylipWithinSpContr\u0026rsquo;. The first function is nice because the formatting required for data input to Phylip programs is so, well, awkward - and this function does it for you. The second function runs contrast and retrieves the output data. The example data set I produce in the code below has multiple individuals per species, so that contrasts are calculated taking into account within species variation. Get Phylip\u0026rsquo;s contrast documentation here.\nNote that the data input format allows only 10 characters for the species name, so I suggest if your species names are longer than 10 characters use the function abbreviate, for example, to shorten all names to no longer than 10 characters. Also, within the function WritePhylip I concatenate species names and their number of individuals per species leaving plenty of space.\nAlso, mess around with the options in the \u0026ldquo;system\u0026rdquo; call to get what you want. For example, I used \u0026ldquo;R\u0026rdquo;, \u0026ldquo;W\u0026rdquo; and \u0026ldquo;Y\u0026rdquo;, meaning replace old outfile (R), then turn on within species analyses (W), then accept all options (Y). E..g, if you don\u0026rsquo;t have an old outfile, then you obviously don\u0026rsquo;t need to replace the old file with the \u0026ldquo;R\u0026rdquo; command.\n(p.s. I have not tried this on a windows machine).\nHere is example output:\n\u0026gt; datout\nnames2 dat...1. dat...2.\n1 VarAIn_VarAest 0.000110 -0.000017\n2 VarAIn_VarAest -0.000017 0.000155\n3 VarAIn_VarEest 0.790783 -0.063097\n4 VarAIn_VarEest -0.063097 0.981216\n5 VarAIn_VarAreg 1.000000 -0.107200\n6 VarAIn_VarAreg -0.151800 1.000000\n7 VarAIn_VarAcorr 1.000000 -0.127600\n8 VarAIn_VarAcorr -0.127600 1.000000\n9 VarAIn_VarEreg 1.000000 -0.064300\n10 VarAIn_VarEreg -0.079800 1.000000\n11 VarAIn_VarEcorr 1.000000 -0.071600\n12 VarAIn_VarEcorr -0.071600 1.000000\n13 VarAOut_VarEest 0.790734 -0.063104\n14 VarAOut_VarEest -0.063104 0.981169\n15 VarAOut_VarEreg 1.000000 -0.064300\n16 VarAOut_VarEreg -0.079800 1.000000\n17 VarAOut_VarEcorr 1.000000 -0.071600\n18 VarAOut_VarEcorr -0.071600 1.000000\n19 logL_withvar_df -68.779770 6.000000\n20 logL_withoutvar_df -68.771450 3.000000\n21 chisq_df -0.016640 3.000000\n22 chisq_p 1.000000 -999.000000 ","permalink":"http://localhost:1313/2011/04/running-phylip-s-contrast-application-for-trait-pairs-from-r/","summary":"\u003cp\u003eHere is some code to run Phylip\u0026rsquo;s contrast application from R and get the output within R to easily manipulate yourself. Importantly, the code is written specifically for trait pairs only as the regular expression work in the code specifically grabs data from contast results when only two traits are input. You could easily change the code to do N traits. Note that the p-value calculated for the chi-square statistic is not output from contrast, but is calculated within the function \u0026lsquo;PhylipWithinSpContr\u0026rsquo;. In the code below there are two functions that make a lot of busy work easier: \u0026lsquo;WritePhylip\u0026rsquo; and \u0026lsquo;PhylipWithinSpContr\u0026rsquo;. The first function is nice because the formatting required for data input to Phylip programs is so, well, awkward  - and this function does it for you. The second function runs contrast and retrieves the output data. The example data set I produce in the code below has multiple individuals per species, so that contrasts are calculated taking into account within species variation. Get Phylip\u0026rsquo;s contrast documentation \u003ca href=\"http://evolution.genetics.washington.edu/phylip/doc/contrast.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Running Phylip's contrast application for trait pairs from R"},{"content":"\u0026mdash;UPDATE: I am now using code formatting from gist.github, so I replaced the old prettyR code (sorry guys). The github way is much easier and prettier. I hope readers like the change.\nI wrote earlier about some code I wrote for running Phylometa (software to do phylogenetic meta-analysis) from R.\nI have been concerned about what exactly is the right penalty for including phylogeny in a meta-analysis. E.g.: AIC is calculated from Q in Phylometa, and Q increases with tree size.\nSo, I wrote some code to shuffle the tips of your tree N number of times, run Phylometa, and extract just the \u0026ldquo;Phylogenetic MA\u0026rdquo; part of the output. So, we compare the observed output (without tip shuffling) to the distribution of the tip shuffled output, and we can calculate a P-value from that. The code I wrote simply extracts the pooled effect size for fixed and also random-effects models. But you could change the code to extract whatever you like for the randomization.\nI think the point of this code is not to estimate your pooled effects, etc., but may be an alternative way to compare traditional to phylogenetic MA where hopefully simply incorporating a tree is not penalizing the meta-analysis so much that you will always accept the traditional MA as better.\nGet the code here, and also below. Get the example tree file and data file, named \u0026ldquo;phylogeny.txt\u0026rdquo; and \u0026ldquo;metadata_2g.txt\u0026rdquo;, respectively below (or use your own data!). You need the file \u0026ldquo;phylometa_fxn.r\u0026rdquo; from my website, get here, but just call it using source as seen below.\n`\nAs you can see, the observed values fall well within the distribution of values obtained from shuffling tips. P-values were 0.64 and 0.68 for fixed- and random-effects MA\u0026rsquo;s, respectively. This suggests, to me at least, that the traditional (distribution of tip shuffled analyses, the histograms below) and phylogenetic (red lines) MA\u0026rsquo;s are not different. The way I would use this is as an additional analysis to the actual Phylometa output.\n","permalink":"http://localhost:1313/2011/04/phylometa-from-r-randomization-via-tip-shuffle/","summary":"\u003cp\u003e\u0026mdash;UPDATE: I am now using code formatting from gist.github, so I replaced the old prettyR code (sorry guys). The github way is much easier and prettier. I hope readers like the change.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/2011-04-01-phylometa-from-r-udpate/\"\u003eI wrote earlier\u003c/a\u003e about some code I wrote for running Phylometa (software to do phylogenetic meta-analysis) from R.\u003c/p\u003e\n\u003cp\u003eI have been concerned about what exactly is the right penalty for including phylogeny in a meta-analysis. E.g.: AIC is calculated from Q in Phylometa, and Q increases with tree size.\u003c/p\u003e","title":"Phylometa from R: Randomization via Tip Shuffle"},{"content":"RStudio Beta 2 (v0.93) « RStudio Blog\nA new beta version of RStudio is out!\n","permalink":"http://localhost:1313/2011/04/rstudio-beta-2-is-out-/","summary":"\u003cp\u003e\u003ca href=\"http://blog.rstudio.org/2011/04/11/rstudio-beta2/\"\u003eRStudio Beta 2 (v0.93) « RStudio Blog\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA new beta version of RStudio is out!\u003c/p\u003e","title":"RStudio Beta 2 is Out!"},{"content":"Here is an approach for comparing two methods of adjusting branch lengths on trees: bladj in the program Phylocom and a fxn written by Gene Hunt at the Smithsonian.\nGet the code and example files (tree and node ages) at https://gist.github.com/938313\nGet phylocom at http://www.phylodiversity.net/phylocom/\nGene Hunt\u0026rsquo;s method has many options you can mess with, including setting tip ages (not available in bladj), setting node ages, and minimum branch length imposed. You will notice that Gene\u0026rsquo;s method may be not the appropriate if you only have extant taxa.\nThe function AdjBrLens uses as input a newick tree file and a text file of node ages, and uses functions you can simply run by \u0026ldquo;source\u0026rdquo; the R file bladjing_twomethods.R file from here.\nNote that blad does not like numbers for node names, so you have to put a character in front of a number of just character names for nodes.\n# This is where the work happens... # Directory below needs to have at least three items: # 1. phylocom executable for windows or mac # 2. tree newick file # 3. node ages file as required by phylocom, see their manual # Output: trees_out is a list of three trees, the original, bladj, and Gene Hunt\u0026#39;s method # Also, within the function all three trees are written to file as PDFs setwd(\u0026#34;/Mac/R_stuff/Blog_etc/Bladjing\u0026#34;) # set working directory source(\u0026#34;bladjing_twomethods.R\u0026#34;) # run functions from source file trees_out \u0026lt;- AdjBrLens(\u0026#34;tree.txt\u0026#34;, \u0026#34;nodeages.txt\u0026#34;) # plot trees of three methods together, # with nodes with age estimates labeled jpeg(\u0026#34;threeplots.jpeg\u0026#34;, quality=100) layout(matrix(1:3, 1, 3)) plot(trees_out[[1]]) nodelabels(trees_out[[1]]$node.label, cex = 0.6) title(\u0026#34;original tree\u0026#34;) plot(trees_out[[2]]) nodelabels(trees_out[[2]]$node.label, cex = 0.6) title(\u0026#34;bladj method\u0026#34;) plot(trees_out[[3]]) nodelabels(trees_out[[3]]$node.label, cex = 0.6) title(\u0026#34;gene hunt method, scalePhylo\u0026#34;) dev.off() ","permalink":"http://localhost:1313/2011/04/adjust-branch-lengths-with-node-ages-comparison-of-two-methods/","summary":"\u003cp\u003eHere is an approach for comparing two methods of adjusting branch lengths on trees: bladj in the program Phylocom and a fxn written by Gene Hunt at the Smithsonian.\u003c/p\u003e\n\u003cp\u003eGet the code and example files (tree and node ages) at \u003ca href=\"https://gist.github.com/938313\"\u003ehttps://gist.github.com/938313\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGet phylocom at \u003ca href=\"http://www.phylodiversity.net/phylocom/\"\u003ehttp://www.phylodiversity.net/phylocom/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGene Hunt\u0026rsquo;s method has many options you can mess with, including setting tip ages (not available in bladj), setting node ages, and minimum branch length imposed. You will notice that Gene\u0026rsquo;s method may be not the appropriate if you only have extant taxa.\u003c/p\u003e","title":"Adjust branch lengths with node ages: comparison of two methods"},{"content":"A while back I posted some messy code to run Phylometa from R, especially useful for processing the output data from Phylometa which is not easily done. The code is still quite messy, but it should work now. I have run the code with tens of different data sets and phylogenies so it should work.\nI fixed errors when parentheses came up against numbers in the output, and other things. You can use the code for up to 4 levels of your grouping variable. In addition, there are some lines of code to plot the effect sizes with confidence intervals, comparing random and fixed effects models and phylogenetic and traditional models.\nThe code is in a Gist at https://gist.github.com/sckott/939970\nUse the first file to do the work, calling the second file using source().\nThis new code works with Marc\u0026rsquo;s new version of Phylometa, so please update: http://lajeunesse.myweb.usf.edu\nAgain, please let me know if it doesn\u0026rsquo;t work, if it\u0026rsquo;s worthless, what changes could make it better.\nSome notes on tree formatting for Phylometa.\nTrees cannot have node labels - remove them (e.g., tree$node.label \u0026lt; NULL) Trees cannot have zero length branches. This may seem like a non-problem, but it might be for example if you have resolved polytomies and zero length branches are added to resolve the polytomy. I think you cannot have a branch length on the root branch. ","permalink":"http://localhost:1313/2011/04/phylometa-from-r-udpate/","summary":"\u003cp\u003eA while back I posted some messy code to run Phylometa from R, especially useful for processing the output data from Phylometa which is not easily done. The code is still quite messy, but it should work now. I have run the code with tens of different data sets and phylogenies so it should work.\u003c/p\u003e\n\u003cp\u003eI fixed errors when parentheses came up against numbers in the output, and other things. You can use the code for up to 4 levels of your grouping variable. In addition, there are some lines of code to plot the effect sizes with confidence intervals, comparing random and fixed effects models and phylogenetic and traditional models.\u003c/p\u003e","title":"Phylometa from R - UDPATE"},{"content":"Bio-ORACLE\nA new dataset available of geophysical, biotic and climate data. Should be fun to play with in R.\n","permalink":"http://localhost:1313/2011/03/bio-oracle/","summary":"\u003cp\u003e\u003ca href=\"http://www.oracle.ugent.be/index.html\"\u003eBio-ORACLE\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA new dataset available of geophysical, biotic and climate data. Should be fun to play with in R.\u003c/p\u003e","title":"Bio-ORACLE"},{"content":"I posted last week a simple function to plot networks using ggplot2 package. Here is version 2. I still need to work on figuring out efficient vertex placement.\nChanges in version 2:\nYou have one of three options: use an igraph object, a matrix, or a dataframe (matrices will be converted to data frames within the function) If you have data on food webs similar to that provided in the Takapoto dataset provided in the NetIndices package, you can set trophic = \u0026ldquo;TRUE\u0026rdquo;, and gggraph will use the function TrophInd to assign trophic levels (the y axis value) to each vertex/node. You have to provide additional information along with this option such as what the imports and exports are, see NetIndices documentation. I added some simple error checking. if using method=\u0026ldquo;df\u0026rdquo; and trophic=\u0026ldquo;FALSE\u0026rdquo;, x axis placement of vertices is now done using the function degreex (see inside the fxn), which sorts vertices according to their degree (so the least connected species are on the left of the graph; note that species with the same degree are not stacked on the y-axis because e.g., two vertices of degree=5 would get x=3 then x=4). # ggraph Version 2 require(bipartite) require(igraph) require(ggplot2) # gggraph, version 3 g = an igraph graph object, a matrix, or data frame # vplace = type of vertex placement assignment, one of rnorm, runif, etc. # method = one of \u0026#39;df\u0026#39; for data frame, \u0026#39;mat\u0026#39; for matrix or \u0026#39;igraph\u0026#39; for an # igraph graph object trophic = TRUE or FALSE for using Netindices # function TrophInd to determine trophic level (y value in graph) # trophinames = columns in matrix or dataframe to use for calculating # trophic level import = named or refereced by col# columns of matrix or # dataframe to use for import argument of TrophInd export = named or # refereced by col# columns of matrix or dataframe to use for export # argument of TrophInd dead = named or refereced by col# columns of matrix # or dataframe to use for dead argument of TrophInd gggraph \u0026lt;- function(g, vplace = rnorm, method, trophic = \u0026#34;FALSE\u0026#34;, trophinames, import, export) { degreex \u0026lt;- function(x) { degreecol \u0026lt;- apply(x, 2, function(y) length(y[y \u0026gt; 0])) degreerow \u0026lt;- apply(x, 1, function(y) length(y[y \u0026gt; 0])) degrees \u0026lt;- sort(c(degreecol, degreerow)) df \u0026lt;- data.frame(degrees, x = seq(1, length(degrees), 1)) df$value \u0026lt;- rownames(df) df } # require igraph if (!require(igraph)) stop(\u0026#34;must first install \u0026#39;igraph\u0026#39; package.\u0026#34;) # require ggplot2 if (!require(ggplot2)) stop(\u0026#34;must first install \u0026#39;ggplot2\u0026#39; package.\u0026#34;) if (method == \u0026#34;df\u0026#34;) { if (class(g) == \u0026#34;matrix\u0026#34;) { g \u0026lt;- as.data.frame(g) } if (class(g) != \u0026#34;data.frame\u0026#34;) stop(\u0026#34;object must be of class \u0026#39;data.frame.\u0026#39;\u0026#34;) if (trophic == \u0026#34;FALSE\u0026#34;) { # data preparation from adjacency matrix temp \u0026lt;- data.frame(expand.grid(dimnames(g))[1:2], as.vector(as.matrix(g))) temp \u0026lt;- temp[(temp[, 3] \u0026gt; 0) \u0026amp; !is.na(temp[, 3]), ] temp \u0026lt;- temp[sort.list(temp[, 1]), ] g_df \u0026lt;- data.frame(rows = temp[, 1], cols = temp[, 2], freqint = temp[, 3]) g_df$id \u0026lt;- 1:length(g_df[, 1]) g_df \u0026lt;- data.frame(id = g_df[, 4], rows = g_df[, 1], cols = g_df[, 2], freqint = g_df[, 3]) g_df_ \u0026lt;- melt(g_df, id = c(1, 4)) xy_s \u0026lt;- data.frame(degreex(g), y = rnorm(length(unique(g_df_$value)))) g_df_2 \u0026lt;- merge(g_df_, xy_s, by = \u0026#34;value\u0026#34;) } else if (trophic == \u0026#34;TRUE\u0026#34;) { # require NetIndices if (!require(NetIndices)) stop(\u0026#34;must first install \u0026#39;NetIndices\u0026#39; package.\u0026#34;) # data preparation from adjacency matrix temp \u0026lt;- data.frame(expand.grid(dimnames(g[-trophinames, -trophinames]))[1:2], as.vector(as.matrix(g[-trophinames, -trophinames]))) temp \u0026lt;- temp[(temp[, 3] \u0026gt; 0) \u0026amp; !is.na(temp[, 3]), ] temp \u0026lt;- temp[sort.list(temp[, 1]), ] g_df \u0026lt;- data.frame(rows = temp[, 1], cols = temp[, 2], freqint = temp[, 3]) g_df$id \u0026lt;- 1:length(g_df[, 1]) g_df \u0026lt;- data.frame(id = g_df[, 4], rows = g_df[, 1], cols = g_df[, 2], freqint = g_df[, 3]) g_df_ \u0026lt;- melt(g_df, id = c(1, 4)) xy_s \u0026lt;- data.frame(value = unique(g_df_$value), x = rnorm(length(unique(g_df_$value))), y = TrophInd(g, Import = import, Export = export)[, 1]) g_df_2 \u0026lt;- merge(g_df_, xy_s, by = \u0026#34;value\u0026#34;) } # plotting p \u0026lt;- ggplot(g_df_2, aes(x, y)) + geom_point(size = 5) + geom_line(aes(size = freqint, group = id)) + geom_text(size = 3, hjust = 1.5, aes(label = value)) + theme_bw() + opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), axis.text.x = theme_blank(), axis.text.y = theme_blank(), axis.title.x = theme_blank(), axis.title.y = theme_blank(), axis.ticks = theme_blank(), panel.border = theme_blank(), legend.position = \u0026#34;none\u0026#34;) p # return graph } else if (method == \u0026#34;igraph\u0026#34;) { if (class(g) != \u0026#34;igraph\u0026#34;) stop(\u0026#34;object must be of class \u0026#39;igraph.\u0026#39;\u0026#34;) # data preparation from igraph object g_ \u0026lt;- get.edgelist(g) g_df \u0026lt;- as.data.frame(g_) g_df$id \u0026lt;- 1:length(g_df[, 1]) g_df \u0026lt;- melt(g_df, id = 3) xy_s \u0026lt;- data.frame(value = unique(g_df$value), x = vplace(length(unique(g_df$value))), y = vplace(length(unique(g_df$value)))) g_df2 \u0026lt;- merge(g_df, xy_s, by = \u0026#34;value\u0026#34;) # plotting p \u0026lt;- ggplot(g_df2, aes(x, y)) + geom_point(size = 2) + geom_line(size = 0.3, aes(group = id, linetype = id)) + geom_text(size = 3, hjust = 1.5, aes(label = value)) + theme_bw() + opts(panel.grid.major = theme_blank(), panel.grid.minor = theme_blank(), axis.text.x = theme_blank(), axis.text.y = theme_blank(), axis.title.x = theme_blank(), axis.title.y = theme_blank(), axis.ticks = theme_blank(), panel.border = theme_blank(), legend.position = \u0026#34;none\u0026#34;) p # return graph } else stop(paste(\u0026#34;do not recognize method = \\\u0026#34;\u0026#34;, method, \u0026#34;\\\u0026#34;;\\nmethods are \\\u0026#34;df\\\u0026#34; and \\\u0026#34;igraph\\\u0026#34;\u0026#34;, sep = \u0026#34;\u0026#34;)) } # Eg library(NetIndices) data(Takapoto) gggraph(Takapoto, vplace = rnorm, method = \u0026#34;df\u0026#34;, trophic = \u0026#34;TRUE\u0026#34;, trophinames = c(8:10), import = \u0026#34;CO2\u0026#34;, export = c(\u0026#34;CO2\u0026#34;, \u0026#34;Sedimentation\u0026#34;, \u0026#34;Grazing\u0026#34;)) plants \u0026lt;- round(rlnorm(n = 5, meanlog = 2, sdlog = 1)) animals \u0026lt;- round(rlnorm(n = 5, meanlog = 2, sdlog = 1)) plants \u0026lt;- plants * (100/sum(plants)) animals \u0026lt;- animals * (100/sum(animals)) z \u0026lt;- r2dtable(1, animals, plants) # if errors, rerun again until no error z \u0026lt;- as.data.frame(z[[1]]) rownames(z) \u0026lt;- c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;) gggraph(z, vplace = rnorm, method = \u0026#34;df\u0026#34;, trophic = \u0026#34;FALSE\u0026#34;) ","permalink":"http://localhost:1313/2011/03/basic-ggplot2-network-graphs-ver2/","summary":"\u003cp\u003eI posted last week a simple function to plot networks using ggplot2 package. Here is version 2. I still need to work on figuring out efficient vertex placement.\u003c/p\u003e\n\u003cp\u003eChanges in version 2:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYou have one of three options: use an igraph object, a matrix, or a dataframe (matrices will be converted to data frames within the function)\u003c/li\u003e\n\u003cli\u003eIf you have data on food webs similar to that provided in the Takapoto dataset provided in the NetIndices package, you can set trophic = \u0026ldquo;TRUE\u0026rdquo;, and gggraph will use the function TrophInd to assign trophic levels (the y axis value) to each vertex/node. You have to provide additional information along with this option such as what the imports and exports are, see NetIndices documentation.\u003c/li\u003e\n\u003cli\u003eI added some simple error checking.\u003c/li\u003e\n\u003cli\u003eif using method=\u0026ldquo;df\u0026rdquo; and trophic=\u0026ldquo;FALSE\u0026rdquo;, x axis placement of vertices is now done using the function degreex (see inside the fxn), which sorts vertices according to their degree (so the least connected species are on the left of the graph; note that species with the same degree are not stacked on the y-axis because e.g., two vertices of degree=5 would get x=3 then x=4).\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# ggraph Version 2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(bipartite)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(igraph)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(ggplot2)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# gggraph, version 3 g = an igraph graph object, a matrix, or data frame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# vplace = type of vertex placement assignment, one of rnorm, runif, etc.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# method = one of \u0026#39;df\u0026#39; for data frame, \u0026#39;mat\u0026#39; for matrix or \u0026#39;igraph\u0026#39; for an\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# igraph graph object trophic = TRUE or FALSE for using Netindices\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# function TrophInd to determine trophic level (y value in graph)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# trophinames = columns in matrix or dataframe to use for calculating\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# trophic level import = named or refereced by col# columns of matrix or\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# dataframe to use for import argument of TrophInd export = named or\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# refereced by col# columns of matrix or dataframe to use for export\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# argument of TrophInd dead = named or refereced by col# columns of matrix\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# or dataframe to use for dead argument of TrophInd\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egggraph \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(g, vplace \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e rnorm, method, trophic \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;FALSE\u0026#34;\u003c/span\u003e, trophinames,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    import, export) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    degreex \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(x) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        degreecol \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eapply\u003c/span\u003e(x, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(y) \u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(y[y \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        degreerow \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eapply\u003c/span\u003e(x, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(y) \u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(y[y \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        degrees \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esort\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(degreecol, degreerow))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        df \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(degrees, x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eseq\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(degrees), \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        df\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erownames\u003c/span\u003e(df)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        df\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# require igraph\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#f92672\"\u003e!\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(igraph))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;must first install \u0026#39;igraph\u0026#39; package.\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# require ggplot2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#f92672\"\u003e!\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(ggplot2))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;must first install \u0026#39;ggplot2\u0026#39; package.\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (method \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;df\u0026#34;\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eclass\u003c/span\u003e(g) \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;matrix\u0026#34;\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eas.data.frame\u003c/span\u003e(g)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eclass\u003c/span\u003e(g) \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;data.frame\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;object must be of class \u0026#39;data.frame.\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (trophic \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;FALSE\u0026#34;\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# data preparation from adjacency matrix\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eexpand.grid\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003edimnames\u003c/span\u003e(g))[1\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e], \u003cspan style=\"color:#a6e22e\"\u003eas.vector\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eas.matrix\u003c/span\u003e(g)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e temp\u003cspan style=\"color:#a6e22e\"\u003e[\u003c/span\u003e(temp[, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eis.na\u003c/span\u003e(temp[, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e]), ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e temp\u003cspan style=\"color:#a6e22e\"\u003e[sort.list\u003c/span\u003e(temp[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]), ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(rows \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e temp[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e], cols \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e temp[, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e], freqint \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e temp[,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eid \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(g_df[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e], rows \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e], cols \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e], freqint \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emelt\u003c/span\u003e(g_df, id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            xy_s \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003edegreex\u003c/span\u003e(g), y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ernorm\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(g_df_\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue))))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df_2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emerge\u003c/span\u003e(g_df_, xy_s, by \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;value\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (trophic \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;TRUE\u0026#34;\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# require NetIndices\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#f92672\"\u003e!\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(NetIndices))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;must first install \u0026#39;NetIndices\u0026#39; package.\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# data preparation from adjacency matrix\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eexpand.grid\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003edimnames\u003c/span\u003e(g[\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003etrophinames, \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003etrophinames]))[1\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e],\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#a6e22e\"\u003eas.vector\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eas.matrix\u003c/span\u003e(g[\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003etrophinames, \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003etrophinames])))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e temp\u003cspan style=\"color:#a6e22e\"\u003e[\u003c/span\u003e(temp[, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eis.na\u003c/span\u003e(temp[, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e]), ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temp \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e temp\u003cspan style=\"color:#a6e22e\"\u003e[sort.list\u003c/span\u003e(temp[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]), ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(rows \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e temp[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e], cols \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e temp[, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e], freqint \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e temp[,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eid \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(g_df[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e], rows \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e], cols \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e], freqint \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e g_df[, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emelt\u003c/span\u003e(g_df, id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            xy_s \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(value \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(g_df_\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue), x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ernorm\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(g_df_\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue))),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eTrophInd\u003c/span\u003e(g, Import \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e import, Export \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e export)[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            g_df_2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emerge\u003c/span\u003e(g_df_, xy_s, by \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;value\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# plotting\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        p \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(g_df_2, \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(x, y)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_point\u003c/span\u003e(size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_line\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e freqint,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            group \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e id)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_text\u003c/span\u003e(size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e, hjust \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(label \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#a6e22e\"\u003etheme_bw\u003c/span\u003e() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eopts\u003c/span\u003e(panel.grid.major \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), panel.grid.minor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            axis.text.x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), axis.text.y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), axis.title.x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            axis.title.y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), axis.ticks \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), panel.border \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            legend.position \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;none\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        p  \u003cspan style=\"color:#75715e\"\u003e# return graph\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (method \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;igraph\u0026#34;\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (\u003cspan style=\"color:#a6e22e\"\u003eclass\u003c/span\u003e(g) \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;igraph\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;object must be of class \u0026#39;igraph.\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# data preparation from igraph object\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        g_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eget.edgelist\u003c/span\u003e(g)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        g_df \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eas.data.frame\u003c/span\u003e(g_)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        g_df\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eid \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(g_df[, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        g_df \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emelt\u003c/span\u003e(g_df, id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        xy_s \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(value \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(g_df\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue), x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003evplace\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(g_df\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue))),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003evplace\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003elength\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eunique\u003c/span\u003e(g_df\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003evalue))))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        g_df2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emerge\u003c/span\u003e(g_df, xy_s, by \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;value\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#75715e\"\u003e# plotting\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        p \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(g_df2, \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(x, y)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_point\u003c/span\u003e(size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_line\u003c/span\u003e(size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(group \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e id, linetype \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e id)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_text\u003c/span\u003e(size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e, hjust \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(label \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e value)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_bw\u003c/span\u003e() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eopts\u003c/span\u003e(panel.grid.major \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            panel.grid.minor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), axis.text.x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), axis.text.y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            axis.title.x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), axis.title.y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), axis.ticks \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            panel.border \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etheme_blank\u003c/span\u003e(), legend.position \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;none\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        p  \u003cspan style=\"color:#75715e\"\u003e# return graph\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003estop\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003epaste\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;do not recognize method = \\\u0026#34;\u0026#34;\u003c/span\u003e, method, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\\\u0026#34;;\\nmethods are \\\u0026#34;df\\\u0026#34; and \\\u0026#34;igraph\\\u0026#34;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        sep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Eg\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(NetIndices)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003edata\u003c/span\u003e(Takapoto)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003egggraph\u003c/span\u003e(Takapoto, vplace \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e rnorm, method \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;df\u0026#34;\u003c/span\u003e, trophic \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;TRUE\u0026#34;\u003c/span\u003e, trophinames \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    import \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;CO2\u0026#34;\u003c/span\u003e, export \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;CO2\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Sedimentation\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Grazing\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"center\" loading=\"lazy\" src=\"/gggraph1.png\"\u003e\u003c/p\u003e","title":"basic ggplot2 network graphs ver2"},{"content":"I have been looking around on the web and have not found anything yet related to using ggplot2 for making graphs/networks. I put together a few functions to make very simple graphs. The bipartite function especially is not ideal, as of course we only want to allow connections between unlike nodes, not all nodes. These functions do not, obviously, take full advantage of the power of ggplot2, but it’s a start.\n# ggplot network graphics functions # g = an igraph graph object, any igraph graph object # vplace = type of vertex placement assignment, one of rnorm, runif, etc. gggraph \u0026lt;- function(g, vplace = rnorm) { require(ggplot2) g_ \u0026lt;- get.edgelist(g) g_df \u0026lt;- as.data.frame(g_) g_df$id \u0026lt;- 1:length(g_df[,1]) g_df \u0026lt;- melt(g_df, id=3) xy_s \u0026lt;- data.frame(value = unique(g_df$value), x = vplace(length(unique(g_df$value))), y = vplace(length(unique(g_df$value)))) g_df2 \u0026lt;- merge(g_df, xy_s, by = \u0026#34;value\u0026#34;) p \u0026lt;- ggplot(g_df2, aes(x, y)) + geom_point() + geom_line(size = 0.3, aes(group = id, linetype = id)) + geom_text(size = 3, hjust = 1.5, aes(label = value)) + theme_bw() + opts(panel.grid.major=theme_blank(), panel.grid.minor=theme_blank(), axis.text.x=theme_blank(), axis.text.y=theme_blank(), axis.title.x=theme_blank(), axis.title.y=theme_blank(), axis.ticks=theme_blank(), panel.border=theme_blank(), legend.position=\u0026#34;none\u0026#34;) p } ggbigraph \u0026lt;- function(g) { require(ggplot2) g_ \u0026lt;- get.edgelist(g) g_df \u0026lt;- as.data.frame(g_) g_df$id \u0026lt;- 1:length(g_df[,1]) g_df \u0026lt;- melt(g_df, id=3) xy_s \u0026lt;- data.frame(value = unique(g_df$value), x = c(rep(2, length(unique(g_df$value))/2), rep(4, length(unique(g_df$value))/2)), y = rep(seq(1, length(unique(g_df$value))/2, 1), 2)) g_df2 \u0026lt;- merge(g_df, xy_s, by = \u0026#34;value\u0026#34;) p \u0026lt;- ggplot(g_df2, aes(x, y)) + geom_point() + geom_line(size = 0.3, aes(group = id, linetype = id)) + geom_text(size = 3, hjust = 1.5, aes(label = value)) + theme_bw() + opts(panel.grid.major=theme_blank(), panel.grid.minor=theme_blank(), axis.text.x=theme_blank(), axis.text.y=theme_blank(), axis.title.x=theme_blank(), axis.title.y=theme_blank(), axis.ticks=theme_blank(), panel.border=theme_blank(), legend.position=\u0026#34;none\u0026#34;) p } Created by Pretty R at inside-R.org g \u0026lt;- erdos.renyi.game(20, 5, type=\u0026#34;gnm\u0026#34;) gggraph(g, rnorm) g \u0026lt;- barabasi.game(20) gggraph(g, rnorm) g \u0026lt;- grg.game(20, 0.45, torus=FALSE) gggraph(g, rnorm) g \u0026lt;- growing.random.game(20, citation=FALSE) gggraph(g, rnorm) g \u0026lt;- watts.strogatz.game(1, 20, 5, 0.05) gggraph(g, rnorm) # A bipartite graphs g \u0026lt;- grg.game(20, 0.45, torus=FALSE) ggbigraph(g) ","permalink":"http://localhost:1313/2011/03/basic-ggplot2-network-graphs/","summary":"\u003cp\u003eI have been looking around on the web and have not found anything yet related to using ggplot2 for making graphs/networks. I put together a few functions to make very simple graphs. The bipartite function especially is not ideal, as of course we only want to allow connections between unlike nodes, not all nodes. These functions do not, obviously, take full advantage of the power of ggplot2, but it’s a start.\u003c/p\u003e","title":"basic ggplot2 network graphs"},{"content":"A post over at the Phased blog (http://www.nasw.org/users/mslong/) highlights a recent paper in PLoS One by Robert Warren et al. Similar results were obtained in a 2007 Ecology Letters paper by Nekola and Brown, who showed that abundance distributions found in ecology are similar to those found for scientific citations, Eastern North American precipitation, among other things. A similar argument was made by Nee et al. in 1991 (in the journal PRSL-B). The author of the blog appears to agree with the outcome of the Warren et al. study.\nI tend to disagree.\nIn the field of graphs/networks, many networks (social, sexual intercourse among humans, etc.) are found to have similar statistical properties to those of ecological networks (food webs, interactions among mutualists, etc.). However, just because these networks have similar statistical properties does not mean that the statistical properties of ecological networks have no biological meaning.\nThey make the argument that the common SAD fit may be an artifact of large data sets alone. However, I don\u0026rsquo;t see any explanation of why they think large data sets is a valid explanation of SADs. Surely SAD\u0026rsquo;s are fit to varying sizes of datasets. The problem with small datasets is lack of statistical power to detect a particular pattern, but surely you can get a fit for a particular SAD to a small dataset.\nThere are ecological mechanistic theories behind different SAD models. They argue that because very similar SADs are found in ecological and non-ecological datasets alike one option is that a universal mechanism structures ecological and non-ecological data (with the mechanism unknown in both). Why can\u0026rsquo;t the same SAD pattern be generated by different mechanisms?\nAre Warren et al, Nekola, and Nee right in questioning the utility of SADs? Questioning our theories and ideas only makes the theories better in the end by weeding out shortcomings, etc.\nWarren, R., Skelly, D., Schmitz, O., \u0026amp; Bradford, M. (2011). Universal Ecological Patterns in College Basketball Communities PLoS ONE, 6 (3) DOI: 10.1371/journal.pone.0017342 ","permalink":"http://localhost:1313/2011/03/species-abundance-distributions-and/","summary":"\u003cp\u003eA post over at the Phased blog (\u003ca href=\"http://www.nasw.org/users/mslong/\"\u003ehttp://www.nasw.org/users/mslong/\u003c/a\u003e) highlights a recent paper in PLoS One by Robert Warren et al. Similar results were obtained in a 2007 Ecology Letters paper by Nekola and Brown, who showed that abundance distributions found in ecology are similar to those found for scientific citations, Eastern North American precipitation, among other things. A similar argument was made by Nee et al. in 1991 (in the journal PRSL-B). The author of the blog appears to agree with the outcome of the Warren et al. study.\u003c/p\u003e","title":"Species abundance distributions and basketball"},{"content":"UPDATE: I guess it still is not actually available. Bummer\u0026hellip;\nHas anyone used cloudnumbers.com?\nhttp://www.cloudnumbers.com/\nThey provide cloud computing, and have built in applications, including R.\nHow well does it work? Does it increase processing speed? I guess it may at the least free up RAM and processor space on your own machine.\n","permalink":"http://localhost:1313/2011/03/cloudnumbers-com/","summary":"\u003cp\u003eUPDATE: I guess it still is not actually available. Bummer\u0026hellip;\u003c/p\u003e\n\u003cp\u003eHas anyone used cloudnumbers.com?\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.cloudnumbers.com/\"\u003ehttp://www.cloudnumbers.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThey provide cloud computing, and have built in applications, including R.\u003c/p\u003e\n\u003cp\u003eHow well does it work? Does it increase processing speed? I guess it may at the least free up RAM and processor space on your own machine.\u003c/p\u003e","title":"cloudnumbers.com"},{"content":"UPDATE: At the bottom are two additional methods, and some additions (underlined) are added to the original 5 methods. Thanks for all the feedback\u0026hellip;\n-Also, another post here about ordered-categorical data-Also #2, a method combining splom and hexbin packages here, for larger datasets\nIn data analysis it is often nice to look at all pairwise combinations of continuous variables in scatterplots. Up until recently, I have used the function splom in the package lattice, but ggplot2 has superior aesthetics, I think anyway.\nHere a few ways to accomplish the task:\nrequire(lattice)\nrequire(ggplot2)\u0026nbsp;\nrequire(car)\nUsing base graphics, function \u0026ldquo;pairs\u0026rdquo; pairs(iris[1:4], pch = 21)Created by Pretty R at inside-R.org\nUsing lattice package, function \u0026ldquo;splom\u0026rdquo; -Additional code to improve splom plots here (and see Oscar's code below in comments)\nsplom(~iris[1:4])Created by Pretty R at inside-R.org\nUsing package ggplot2, function \u0026ldquo;plotmatrix\u0026rdquo; plotmatrix(iris[1:4])Created by Pretty R at inside-R.org\na function called ggcorplot by Mike Lawrence at Dalhousie University get ggcorplot function at this link\n-ggcorplot is also built in to Deducer (get here); see Ian's code below in the comments\n-Lastly, an improved version of ggcorplot is built in to the ez package (get here)\nggcorplot(\ndata = iris[1:4],\nvar_text_size = 5,\ncor_text_limits = c(5,10))Created by Pretty R at inside-R.org\npanel.cor function using pairs, similar to ggcorplot, but using base graphics. Not sure who wrote this function, but here is where I found it. panel.cor \u0026lt;- function(x, y, digits=2, prefix=\"\", cex.cor) {\nusr \u0026lt;- par(\"usr\"); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r \u0026lt;- abs(cor(x, y)) txt \u0026lt;- format(c(r, 0.123456789), digits=digits)[1] txt \u0026lt;- paste(prefix, txt, sep=\"\") if(missing(cex.cor)) cex \u0026lt;- 0.8/strwidth(txt) \u0026nbsp;\ntest \u0026lt;- cor.test(x,y) # borrowed from printCoefmat\nSignif \u0026lt;- symnum(test$p.value, corr = FALSE, na = FALSE, cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),\nsymbols = c(\"***\", \"**\", \"*\", \".\", \" \")) \u0026nbsp;\ntext(0.5, 0.5, txt, cex = cex * r) text(.8, .8, Signif, cex=cex, col=2) }\n\u0026nbsp;pairs(iris[1:4], lower.panel=panel.smooth, upper.panel=panel.cor)Created by Pretty R at inside-R.org\nA comparison of run times...\n\u0026gt; system.time(pairs(iris[1:4]))\nuser system elapsed 0.138 0.008 0.156 \u0026gt; system.time(splom(~iris[1:4]))\nuser system elapsed 0.003 0.000 0.003 \u0026gt; system.time(plotmatrix(iris[1:4]))\nuser system elapsed 0.052 0.000 0.052 \u0026gt; system.time(ggcorplot(\n+ data = iris[1:4],\nvar_text_size = 5,\ncor_text_limits = c(5,10)))\n\u0026nbsp;\nuser system elapsed 0.130 0.001 0.131 \u0026gt; system.time(pairs(iris[1:4], lower.panel=panel.smooth, upper.panel=panel.cor))\nuser system elapsed 0.170 0.011 0.200Created by Pretty R at inside-R.org\n\u0026hellip;shows that splom is the fastest method, with the method using the panel.cor function pulling up the rear.\ngiven by a reader in the comments (get her/his code here). This one is nice as it gives 95% CI\u0026rsquo;s for the correlation coefficients, AND histograms of each variable. 7) a reader in the comments suggested the scatterplotMatrix (spm can be used) function in the car package. This one has the advantage of plotting distributions of each variable, and providing fits to each data with confidence intervals.\nspm(iris[1:4])\n","permalink":"http://localhost:1313/2011/03/for-all-your-pairwise-comparison-needs/","summary":"\u003cp\u003eUPDATE: \u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eAt the bottom are two additional methods, and some additions (underlined) are added to the original 5 methods. Thanks for all the feedback\u0026hellip;\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e-Also, another post \u003c!-- raw HTML omitted --\u003ehere\u003c!-- raw HTML omitted --\u003e about ordered-categorical data\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e-Also #2, a method combining splom and hexbin packages \u003c!-- raw HTML omitted --\u003ehere\u003c!-- raw HTML omitted --\u003e, for larger datasets\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e","title":"Five ways to visualize your pairwise comparisons"},{"content":"They have online seminars that you can join in on live, and watch later as recorded videos. Check it out at phyloseminar.org\n","permalink":"http://localhost:1313/2011/03/check-out-phyloseminar-org/","summary":"\u003cp\u003eThey have online seminars that you can join in on live, and watch later as recorded videos. Check it out at \u003ca href=\"http://phyloseminar.org/index.html\"\u003ephyloseminar.org\u003c/a\u003e\u003c/p\u003e","title":"Check out Phyloseminar.org"},{"content":"New thoughts: After actually using it more, it is quite nice, but I have a couple of major issues.\nThe text editor is quite slow to scroll through. ggplot2 graphics look bad, worse than if just running R alone RStudio\nEveryone seems to be excited about this\u0026hellip; Is it any good? Seems great for folks just learning R, but perhaps less ideal for advanced R users? ","permalink":"http://localhost:1313/2011/02/rstudio/","summary":"\u003cp\u003eNew thoughts: After actually using it more, it is quite nice, but I have a couple of major issues.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe text editor is quite slow to scroll through.\u003c/li\u003e\n\u003cli\u003eggplot2 graphics look bad, worse than if just running R alone\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eRStudio\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEveryone seems to be excited about this\u0026hellip;\u003c/li\u003e\n\u003cli\u003eIs it any good? Seems great for folks just learning R, but perhaps less ideal for advanced R users?\u003c/li\u003e\n\u003c/ul\u003e","title":"RStudio"},{"content":"TIOBE Software: Tiobe Index\n","permalink":"http://localhost:1313/2011/02/r-overtakes-sas-in-popularity/","summary":"\u003cp\u003e\u003ca href=\"http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html\"\u003eTIOBE Software: Tiobe Index\u003c/a\u003e\u003c/p\u003e","title":"R overtakes SAS in popularity"},{"content":"I have up to recently always done my phenotypic selection analyses in SAS. I finally got some code I think works to do everything SAS would do. Feedback much appreciated!\n########################Selection analyses############################# install.packages(c(\u0026#34;car\u0026#34;,\u0026#34;reshape\u0026#34;,\u0026#34;ggplot2\u0026#34;)) require(car) require(reshape) require(ggplot2) # Create data set dat \u0026lt;- data.frame(plant = seq(1,100,1), trait1 = rep(c(0.1,0.15,0.2,0.21,0.25,0.3,0.5,0.6,0.8,0.9,1,3,4,10,11,12,13,14,15,16), each = 5), trait2 = runif(100), fitness = rep(c(1,5,10,20,50), each = 20)) # Make relative fitness column dat_ \u0026lt;- cbind(dat, dat$fitness/mean(dat$fitness)) names(dat_)[5] \u0026lt;- \u0026#34;relfitness\u0026#34; # Standardize traits dat_ \u0026lt;- cbind(dat_[,-c(2:3)], rescaler(dat_[,c(2:3)],\u0026#34;sd\u0026#34;)) ####Selection differentials and correlations among traits, cor.prob uses function in functions.R file ############################################################################ ####### Function for calculating correlation matrix, corrs below diagonal, ####### and P-values above diagonal ############################################################################ cor.prob \u0026lt;- function(X, dfr = nrow(X) - 2) { R \u0026lt;- cor(X) above \u0026lt;- row(R) \u0026lt; col(R) r2 \u0026lt;- R[above]^2 Fstat \u0026lt;- r2 * dfr / (1 - r2) R[above] \u0026lt;- 1 - pf(Fstat, 1, dfr) R } # Get selection differentials and correlations among traits in one data frame dat_seldiffs \u0026lt;- cov(dat_[,c(3:5)]) # calculates sel\u0026#39;n differentials using cov dat_selcorrs \u0026lt;- cor.prob(dat_[,c(3:5)]) # use P-values above diagonal for significance of sel\u0026#39;n differentials in dat_seldiffs dat_seldiffs_selcorrs \u0026lt;- data.frame(dat_seldiffs, dat_selcorrs) # combine the two ########################################################################## ####Selection gradients dat_selngrad \u0026lt;- lm(relfitness ~ trait1 * trait2, data = dat_) summary(dat_selngrad) # where \u0026#34;Estimate\u0026#34; is our sel\u0026#39;n gradient ####Check assumptions shapiro.test(dat_selngrad$residuals) # normality, bummer, non-normal hist(dat_selngrad$residuals) # plot residuals vif(dat_selngrad) # check variance inflation factors (need package car), everything looks fine plot(dat_selngrad) # cycle through diagnostic plots ############################################################################ # Plot data ggplot(dat_, aes(trait1, relfitness)) + geom_point() + geom_smooth(method = \u0026#34;lm\u0026#34;) + labs(x=\u0026#34;Trait 1\u0026#34;,y=\u0026#34;Relative fitness\u0026#34;) ggsave(\u0026#34;myplot.jpeg\u0026#34;) Plot of relative fitness vs. trait 1 standardized\n","permalink":"http://localhost:1313/2011/02/phenotypic-selection-analysis-in-r/","summary":"\u003cp\u003eI have up to recently always done my phenotypic selection analyses in SAS. I finally got some code I think works to do everything SAS would do. Feedback much appreciated!\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e########################Selection analyses#############################\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003einstall.packages\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;car\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reshape\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;ggplot2\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(car)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(reshape)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003erequire\u003c/span\u003e(ggplot2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create data set\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(plant \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eseq\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e trait1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erep\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e0.1\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.15\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.21\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.25\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.3\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.6\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.8\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e0.9\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e13\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e14\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e), each \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e), trait2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erunif\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e fitness \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erep\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e,\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e), each \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Make relative fitness column\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecbind\u003c/span\u003e(dat, dat\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003efitness\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003emean\u003c/span\u003e(dat\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003efitness))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enames\u003c/span\u003e(dat_)[5] \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;relfitness\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Standardize traits\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat_ \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecbind\u003c/span\u003e(dat_[,\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e)], \u003cspan style=\"color:#a6e22e\"\u003erescaler\u003c/span\u003e(dat_[,\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e)],\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sd\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e####Selection differentials and correlations among traits, cor.prob uses function in functions.R file\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e############################################################################\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e####### Function for calculating correlation matrix, corrs below diagonal,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e####### and P-values above diagonal\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e############################################################################\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecor.prob \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efunction\u003c/span\u003e(X, dfr \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003enrow\u003c/span\u003e(X) \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         R \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecor\u003c/span\u003e(X)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         above \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erow\u003c/span\u003e(R) \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecol\u003c/span\u003e(R)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         r2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e R[above]^2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         Fstat \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e r2 \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e dfr \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e (\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e r2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         R[above] \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003epf\u003c/span\u003e(Fstat, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, dfr)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         R\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Get selection differentials and correlations among traits in one data frame\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat_seldiffs \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecov\u003c/span\u003e(dat_[,\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e)]) \u003cspan style=\"color:#75715e\"\u003e# calculates sel\u0026#39;n differentials using cov\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat_selcorrs \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecor.prob\u003c/span\u003e(dat_[,\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e)]) \u003cspan style=\"color:#75715e\"\u003e# use P-values above diagonal for significance of sel\u0026#39;n differentials in dat_seldiffs\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat_seldiffs_selcorrs \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(dat_seldiffs, dat_selcorrs) \u003cspan style=\"color:#75715e\"\u003e# combine the two\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e##########################################################################\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e####Selection gradients\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edat_selngrad \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003elm\u003c/span\u003e(relfitness \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e trait1 \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e trait2, data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dat_)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003esummary\u003c/span\u003e(dat_selngrad) \u003cspan style=\"color:#75715e\"\u003e# where \u0026#34;Estimate\u0026#34; is our sel\u0026#39;n gradient\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e####Check assumptions\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eshapiro.test\u003c/span\u003e(dat_selngrad\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eresiduals) \u003cspan style=\"color:#75715e\"\u003e# normality, bummer, non-normal\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ehist\u003c/span\u003e(dat_selngrad\u003cspan style=\"color:#f92672\"\u003e$\u003c/span\u003eresiduals) \u003cspan style=\"color:#75715e\"\u003e# plot residuals\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003evif\u003c/span\u003e(dat_selngrad) \u003cspan style=\"color:#75715e\"\u003e# check variance inflation factors (need package car), everything looks fine\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eplot\u003c/span\u003e(dat_selngrad) \u003cspan style=\"color:#75715e\"\u003e# cycle through diagnostic plots\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e############################################################################\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Plot data\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eggplot\u003c/span\u003e(dat_, \u003cspan style=\"color:#a6e22e\"\u003eaes\u003c/span\u003e(trait1, relfitness)) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_point\u003c/span\u003e() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \u003cspan style=\"color:#a6e22e\"\u003egeom_smooth\u003c/span\u003e(method \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;lm\u0026#34;\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \u003cspan style=\"color:#a6e22e\"\u003elabs\u003c/span\u003e(x\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Trait 1\u0026#34;\u003c/span\u003e,y\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Relative fitness\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eggsave\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;myplot.jpeg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePlot of relative fitness vs. trait 1 standardized\u003c/p\u003e","title":"Phenotypic selection analysis in R"},{"content":"The phangorn package is a relatively new package in R for the analysis and comparison of phylogenies. See here for the Bioinformatics paper and here for the package. Here is an example of using phangorn from getting sequences to making phylogenies and visualizing them:Getting sequences from GenbankMultiple alignmentMaximum likelihood tree reconstructionVisualizing treesVisualizing trees and traitsMake fake traits:Visualize them on trees:\n","permalink":"http://localhost:1313/2011/02/phylogenetic-analysis-with-the-phangorn-package-an-example/","summary":"\u003cp\u003eThe phangorn package is a relatively new package in R for the analysis and comparison of phylogenies. See \u003c!-- raw HTML omitted --\u003ehere\u003c!-- raw HTML omitted --\u003e for the Bioinformatics paper and \u003c!-- raw HTML omitted --\u003ehere\u003c!-- raw HTML omitted --\u003e for the package. Here is an example of using phangorn from getting sequences to making phylogenies and visualizing them:\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eGetting sequences from Genbank\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eMultiple alignment\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eMaximum likelihood tree reconstruction\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eVisualizing trees\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eVisualizing trees and traits\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eMake fake traits:\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003eVisualize them on trees:\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e","title":"Phylogenetic analysis with the phangorn package: an example"},{"content":"I combined USDA data on farmer\u0026rsquo;s markets in the US with population census data to get an idea of the disparity in farmers markets by state, and then also expressed per capita.\nDownload USDA data here. The formatted file I used below is here (in excel format, although I read into R as csv file). The census data is read from url as below.\nCalifornia has a ton of absolute number of farmer\u0026rsquo;s markets, but Vermont takes the cake by far with number of markets per capita. Iowa comes in a distant second behind Vermont in markets per capita.\nThe code:\n######## Farmer\u0026#39;s Markets ############# setwd(\u0026#34;/Mac/R_stuff/Blog_etc/USDAFarmersMarkets\u0026#34;) # Set to your working directory, this is where you want to call files from and write files to install.packages(c(\u0026#34;ggplot2\u0026#34;, \u0026#34;RCurl\u0026#34;)) # install all packags required below require(ggplot2) # plyr is libraried along with ggplot2, as ggplot2 uses plyr (as well as package reshape) functions # read market data markets \u0026lt;- read.csv(\u0026#34;farmmarkets.csv\u0026#34;) markets$state \u0026lt;- as.factor(gsub(\u0026#34;Wyoming \u0026#34;, \u0026#34;Wyoming\u0026#34;, markets$LocAddState)) # there was a typo for Wyoming markets \u0026lt;- na.omit(markets) str(markets) # read population census data popcen \u0026lt;- read.csv(\u0026#34;http://www.census.gov/popest/national/files/NST_EST2009_ALLDATA.csv\u0026#34;) popcen \u0026lt;- popcen[,c(4,5,6,17)] str(popcen) # summarize markets_ \u0026lt;- ddply(markets, .(state), summarise, markets_n = length(LocAddState) ) markets_pop_ \u0026lt;- merge(markets_, popcen[,-1], by.x = \u0026#34;state\u0026#34;, by.y = \u0026#34;NAME\u0026#34;) # merge two data sets markets_pop_$marketspercap \u0026lt;- markets_pop_$markets_n/markets_pop_$POPESTIMATE2009 # create column of markets per capita markets_pop_$markets_n_st \u0026lt;- markets_pop_$markets_n/max(markets_pop_$markets_n) markets_pop_$marketspercap_st \u0026lt;- markets_pop_$marketspercap/max(markets_pop_$marketspercap) # plot ggplot(melt(markets_pop_[,-c(2:5)]), aes(x = state, y = value, fill = variable)) + geom_bar(position = \u0026#34;dodge\u0026#34;) + coord_flip() ggsave(\u0026#34;fmarkets_barplot.jpeg\u0026#34;) # maps try_require(\u0026#34;maps\u0026#34;) states \u0026lt;- map_data(\u0026#34;state\u0026#34;) markets_pop_$statelow \u0026lt;- tolower(markets_pop_$state) survey_sum_map \u0026lt;- merge(states, markets_pop_, by.x = \u0026#34;region\u0026#34;, by.y = \u0026#34;statelow\u0026#34;) survey_sum_map \u0026lt;- survey_sum_map[order(survey_sum_map$order), ] str(survey_sum_map) qplot(long, lat, data = survey_sum_map, group = group, fill = markets_n, geom = \u0026#34;polygon\u0026#34;, main = \u0026#34;Total farmer\u0026#39;s markets\u0026#34;) + scale_fill_gradient(low=\u0026#34;green\u0026#34;, high=\u0026#34;black\u0026#34;) ggsave(\u0026#34;fmarkets_map_green.jpeg\u0026#34;) ","permalink":"http://localhost:1313/2011/02/farmer-s-markets-data/","summary":"\u003cp\u003eI combined USDA data on farmer\u0026rsquo;s markets in the US with population census data to get an idea of the disparity in farmers markets by state, and then also expressed per capita.\u003c/p\u003e\n\u003cp\u003eDownload USDA data \u003ca href=\"http://www.ams.usda.gov/AMSv1.0/getfile?dDocName=STELPRDC5087258\u0026amp;acct=frmrdirmkt\"\u003ehere\u003c/a\u003e. The formatted file I used below is \u003ca href=\"http://schamber.files.wordpress.com/2011/02/farmmarkets.xls\"\u003ehere\u003c/a\u003e (in excel format, although I read into R as csv file). The census data is read from url as below.\u003c/p\u003e\n\u003cp\u003eCalifornia has a ton of absolute number of farmer\u0026rsquo;s markets, but Vermont takes the cake by far with number of markets per capita. Iowa comes in a distant second behind Vermont in markets per capita.\u003c/p\u003e","title":"Farmer's markets data"},{"content":"[UPDATE: i remade the maps in green, hope that helps\u0026hellip;]\nA recent survey reported in Science (\u0026ldquo;Defeating Creationism in the Courtroom, but not in the Classroom\u0026rdquo;) found that biology teachers in high school do not often accept the basis of their discipline, as do teachers in other disciplines, and thus may not teach evolution appropriately. Read more here: New York Times.\nI took a little time to play with the data provided online along with the Science article. The data is available on the Science website along with the article, and the dataset I read into R is unchanged from the original. The states abbreviations file is here (as a .xls). Here goes:\nI only played with two survey questions: q1b (no. of hours ecology is taught per year), and q1d (no. of hours evolution is taught per year). I looked at ecology and evolution as this blog is about ecology and evolution. It seems that some states that teach a lot of ecology teach a lot of evolution, but I found no correlation between the two without extreme outliers. I couldn’t help but notice my home state, TX, is near the bottom of the list on both counts - go TX! The teaching of evolution on the map produced below is less predictable than I would have though just based on my assumptions about political will in each state.\n# Analyses of Conditionality Data set of all variables, except for latitude, etc. setwd(\u0026#34;/Mac/R_stuff/Blog_etc/EvolutionTeaching/\u0026#34;) # Set working directory library(ggplot2) # read in data, and prepare new columns survey \u0026lt;- read.csv(\u0026#34;berkmandata.csv\u0026#34;) str(survey) # (I do realize that survey is a data object in the MASS package) # Assign actual hours to survey answers ecol \u0026lt;- gsub(1, 0, survey$q1b) ecol \u0026lt;- gsub(2, 1.5, ecol) ecol \u0026lt;- gsub(3, 4, ecol) ecol \u0026lt;- gsub(4, 8, ecol) ecol \u0026lt;- gsub(5, 13, ecol) ecol \u0026lt;- gsub(6, 18, ecol) ecol \u0026lt;- gsub(7, 20, ecol) evol \u0026lt;- gsub(1, 0, survey$q1d) evol \u0026lt;- gsub(2, 1.5, evol) evol \u0026lt;- gsub(3, 4, evol) evol \u0026lt;- gsub(4, 8, evol) evol \u0026lt;- gsub(5, 13, evol) evol \u0026lt;- gsub(6, 18, evol) evol \u0026lt;- gsub(7, 20, evol) survey$ecol \u0026lt;- as.numeric(ecol) survey$evol \u0026lt;- as.numeric(evol) # ddply it survey_sum \u0026lt;- ddply(survey, .(st_posta), summarise, mean_ecol_hrs = mean(ecol, na.rm=T), mean_evol_hrs = mean(evol, na.rm=T), se_ecol_hrs = sd(ecol, na.rm=T)/sqrt(length(ecol)), se_evol_hrs = sd(evol, na.rm=T)/sqrt(length(evol)), num_teachers = length(st_posta) ) # plotting limits_ecol \u0026lt;- aes(ymax = mean_ecol_hrs + se_ecol_hrs, ymin = mean_ecol_hrs - se_ecol_hrs) limits_evol \u0026lt;- aes(ymax = mean_evol_hrs + se_evol_hrs, ymin = mean_evol_hrs - se_evol_hrs) ggplot(survey_sum, aes(x = reorder(st_posta, mean_ecol_hrs), y = mean_ecol_hrs)) + geom_point() + geom_errorbar(limits_ecol) + geom_text(aes(label = num_teachers), vjust = 1, hjust = -3, size = 3) + coord_flip() + labs(x = \u0026#34;State\u0026#34;, y = \u0026#34;Mean hours of ecology taught \\n per year (+/- 1 se)\u0026#34;) ####SMALL NUMBERS BY BARS ARE NUMBER OF TEACHERS THAT RESPONDED TO ","permalink":"http://localhost:1313/2011/02/troubling-news-for-teaching-of/","summary":"\u003cp\u003e[UPDATE: i remade the maps in green, hope that helps\u0026hellip;]\u003c/p\u003e\n\u003cp\u003eA recent survey reported in \u003ca href=\"http://www.sciencemag.org.silk.library.umass.edu/content/331/6016/404.full\"\u003eScience\u003c/a\u003e (\u0026ldquo;Defeating Creationism in the Courtroom, but not in the Classroom\u0026rdquo;) found that biology teachers in high school do not often accept the basis of their discipline, as do teachers in other disciplines, and thus may not teach evolution appropriately. Read more here: \u003ca href=\"http://www.nytimes.com/2011/02/08/science/08creationism.html?emc=eta1\"\u003eNew York Times\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI took a little time to play with the data provided online along with the Science article. The data is available on the Science website along with the article, and the dataset I read into R is unchanged from the original. The states abbreviations file is \u003ca href=\"http://schamber.files.wordpress.com/2011/02/states_abbreviations.xls\"\u003ehere\u003c/a\u003e (as a .xls). Here goes:\u003c/p\u003e","title":"Troubling news for the teaching of evolution"},{"content":"A new early online paper in American Journal of Botany by Risa Sargent and colleagues suggests that plants are less sex deprived (pollen limited) in vernal pools that have more closely related plant species. Vernal pools are (at least in my experience) small (to quite large) depressions that fill up with water with winter rains, and dry out completely in the summer. Vernal pool adapted plants flower in rings down the pool as the water dries up. Aquatic invertebrates and some herps can last through the summer by burrowing in the soil.\nThe study did hand pollination experiments with a focal species, Lasthenia fremontii. They examined the relationship between these pollen limitation experiments and the relatedness of L. fremontii to the rest of the plant community in each pool.\nPlant species richness was not related to pollen limitation. Thus, at least in their study with vernal pools in California, relatedness to your plant neighbors has a greater impact than plant richness.\nThe great thing about vernal pools is that they are truly terrestrial islands of habitat, surrounded by inhospitable habitat for pool species. Many vernal pools are created artificially for habitat conservation easements (e.g., here. Perhaps someone can experimentally manipulate phylogenetic diversity in artificially created pools to really get at the causal links.\np.s. Ok, this post is not terribly R related, except for that this paper used R for some of their statistics.\n","permalink":"http://localhost:1313/2011/02/plants-are-less-sex-deprived-when-next-to-closely-related-neighbors/","summary":"\u003cp\u003eA new early online paper in \u003ca href=\"http://www.amjbot.org/cgi/content/abstract/ajb.1000329v1\"\u003eAmerican Journal of Botany\u003c/a\u003e by \u003ca href=\"http://mysite.science.uottawa.ca/rsargent/\"\u003eRisa Sargent\u003c/a\u003e and colleagues suggests that plants are less sex deprived (pollen limited) in vernal pools that have more closely related plant species. Vernal pools are (at least in my experience) small (to quite large) depressions that fill up with water with winter rains, and dry out completely in the summer. Vernal pool adapted plants flower in rings down the pool as the water dries up. Aquatic invertebrates and some herps can last through the summer by burrowing in the soil.\u003c/p\u003e","title":"Plants are less sex deprived when next to closely related neighbors"},{"content":"Excel pivot tables have been how I have reorganized data\u0026hellip;up until now. These are just a couple of examples why R is superior to Excel for reorganizing data:\nUPDATE: I fixed the code to use \u0026lsquo;dcast\u0026rsquo; instead of \u0026lsquo;cast\u0026rsquo;. And library(ggplot2) instead of library(plyr) [plyr is called along with ggplot2]. Thanks Bob!\nAlso, see another post on this topic here.\nlibrary(reshape2) library(ggplot2) dataset \u0026lt;- data.frame(var1 = rep(c(\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;d\u0026#34;,\u0026#34;e\u0026#34;,\u0026#34;f\u0026#34;), each = 4), var2 = rep(c(\u0026#34;level1\u0026#34;,\u0026#34;level1\u0026#34;,\u0026#34;level2\u0026#34;,\u0026#34;level2\u0026#34;), 6), var3 = rep(c(\u0026#34;h\u0026#34;,\u0026#34;m\u0026#34;), 12), meas = rep(1:12)) Created by Pretty R at inside-R.org # simply pivot table dcast(dataset, var1 ~ var2 + var3) Using meas as value column. Use the value argument to cast to override this choice var1 level1_h level1_m level2_h level2_m 1 a 1 2 3 4 2 b 5 6 7 8 3 c 9 10 11 12 4 d 1 2 3 4 5 e 5 6 7 8 6 f 9 10 11 12 # mean by var1 and var2 dcast(dataset, var1 ~ var2, mean) Using meas as value column. Use the value argument to cast to override this choice var1 level1 level2 1 a 1.5 3.5 2 b 5.5 7.5 3 c 9.5 11.5 4 d 1.5 3.5 5 e 5.5 7.5 6 f 9.5 11.5 # mean by var1 and var3 dcast(dataset, var1 ~ var3, mean) Using meas as value column. Use the value argument to cast to override this choice var1 h m 1 a 2 3 2 b 6 7 3 c 10 11 4 d 2 3 5 e 6 7 6 f 10 11 # mean by var1, var2 and var3 (version 1) dcast(dataset, var1 ~ var2 + var3, mean) Using meas as value column. Use the value argument to cast to override this choice var1 level1_h level1_m level2_h level2_m 1 a 1 2 3 4 2 b 5 6 7 8 3 c 9 10 11 12 4 d 1 2 3 4 5 e 5 6 7 8 6 f 9 10 11 12 # mean by var1, var2 and var3 (version 2) dcast(dataset, var1 + var2 ~ var3, mean) Using meas as value column. Use the value argument to cast to override this choice var1 var2 h m 1 a level1 1 2 2 a level2 3 4 3 b level1 5 6 4 b level2 7 8 5 c level1 9 10 6 c level2 11 12 7 d level1 1 2 8 d level2 3 4 9 e level1 5 6 10 e level2 7 8 11 f level1 9 10 12 f level2 11 12 # use package plyr to create flexible data frames... dataset_plyr \u0026lt;- ddply(dataset, .(var1, var2), summarise, mean = mean(meas), se = sd(meas), CV = sd(meas)/mean(meas) ) \u0026gt; dataset_plyr var1 var2 mean se CV 1 a level1 1.5 0.7071068 0.47140452 2 a level2 3.5 0.7071068 0.20203051 3 b level1 5.5 0.7071068 0.12856487 4 b level2 7.5 0.7071068 0.09428090 5 c level1 9.5 0.7071068 0.07443229 6 c level2 11.5 0.7071068 0.06148755 7 d level1 1.5 0.7071068 0.47140452 8 d level2 3.5 0.7071068 0.20203051 9 e level1 5.5 0.7071068 0.12856487 10 e level2 7.5 0.7071068 0.09428090 11 f level1 9.5 0.7071068 0.07443229 12 f level2 11.5 0.7071068 0.06148755 # ...to use for plotting qplot(var1, mean, colour = var2, size = CV, data = dataset_plyr, geom = \u0026#34;point\u0026#34;) ","permalink":"http://localhost:1313/2011/01/good-riddance-to-excel-pivot-tables/","summary":"\u003cp\u003eExcel pivot tables have been how I have reorganized data\u0026hellip;up until now. These are just a couple of examples why R is superior to Excel for reorganizing data:\u003c/p\u003e\n\u003cp\u003eUPDATE: I fixed the code to use \u0026lsquo;dcast\u0026rsquo; instead of \u0026lsquo;cast\u0026rsquo;. And \u003ccode\u003elibrary(ggplot2)\u003c/code\u003e instead of \u003ccode\u003elibrary(plyr)\u003c/code\u003e [\u003ccode\u003eplyr\u003c/code\u003e is called along with \u003ccode\u003eggplot2\u003c/code\u003e]. Thanks Bob!\u003c/p\u003e\n\u003cp\u003eAlso, see another post on this topic \u003ca href=\"http://news.mrdwab.com/2010/08/08/using-the-reshape-packagein-r/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-r\" data-lang=\"r\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(reshape2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003elibrary\u003c/span\u003e(ggplot2) \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edataset \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edata.frame\u003c/span\u003e(var1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erep\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;a\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;b\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;c\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;d\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;e\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;f\u0026#34;\u003c/span\u003e), each \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e), \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e var2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erep\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;level1\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;level1\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;level2\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;level2\u0026#34;\u003c/span\u003e), \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e), \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e var3 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erep\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ec\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;h\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;m\u0026#34;\u003c/span\u003e), \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e), meas \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erep\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eCreated by Pretty R at inside\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003eR.org\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# simply pivot table\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003edcast\u003c/span\u003e(dataset, var1 \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e var2 \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e var3)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eUsing meas as value column.  Use the value argument to cast to override this choice\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  var1 level1_h level1_m level2_h level2_m\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e    a        \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e    b        \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e    c        \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e    d        \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e    e        \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e    f        \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# mean by var1 and var2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003edcast\u003c/span\u003e(dataset, var1 \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e var2, mean)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eUsing meas as value column.  Use the value argument to cast to override this choice\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  var1 level1 level2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e    a    \u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e    \u003cspan style=\"color:#ae81ff\"\u003e3.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e    b    \u003cspan style=\"color:#ae81ff\"\u003e5.5\u003c/span\u003e    \u003cspan style=\"color:#ae81ff\"\u003e7.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e    c    \u003cspan style=\"color:#ae81ff\"\u003e9.5\u003c/span\u003e   \u003cspan style=\"color:#ae81ff\"\u003e11.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e    d    \u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e    \u003cspan style=\"color:#ae81ff\"\u003e3.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e    e    \u003cspan style=\"color:#ae81ff\"\u003e5.5\u003c/span\u003e    \u003cspan style=\"color:#ae81ff\"\u003e7.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e    f    \u003cspan style=\"color:#ae81ff\"\u003e9.5\u003c/span\u003e   \u003cspan style=\"color:#ae81ff\"\u003e11.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# mean by var1 and var3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003edcast\u003c/span\u003e(dataset, var1 \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e var3, mean)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eUsing meas as value column.  Use the value argument to cast to override this choice\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  var1  h  m\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e    a  \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e    b  \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e    c \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e    d  \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e    e  \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e    f \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# mean by var1, var2 and var3 (version 1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003edcast\u003c/span\u003e(dataset, var1 \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e var2 \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e var3, mean)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eUsing meas as value column.  Use the value argument to cast to override this choice\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  var1 level1_h level1_m level2_h level2_m\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e    a        \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e    b        \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e    c        \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e    d        \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e    e        \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e        \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e    f        \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# mean by var1, var2 and var3 (version 2)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003edcast\u003c/span\u003e(dataset, var1 \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e var2 \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003e var3, mean)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eUsing meas as value column.  Use the value argument to cast to override this choice\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   var1   var2  h  m\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e     a level1  \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e     a level2  \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e     b level1  \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e     b level2  \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e     c level1  \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e     c level2 \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e     d level1  \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e     d level2  \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e     e level1  \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e    e level2  \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e  \u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e    f level1  \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e    f level2 \u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# use package plyr to create flexible data frames...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edataset_plyr \u003cspan style=\"color:#f92672\"\u003e\u0026lt;-\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eddply\u003c/span\u003e(dataset, .(var1, var2), summarise, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e mean \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emean\u003c/span\u003e(meas), \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e se \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esd\u003c/span\u003e(meas),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e CV \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003esd\u003c/span\u003e(meas)\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003emean\u003c/span\u003e(meas)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e dataset_plyr\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   var1   var2 mean        se         CV\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e     a level1  \u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.47140452\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e     a level2  \u003cspan style=\"color:#ae81ff\"\u003e3.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.20203051\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e     b level1  \u003cspan style=\"color:#ae81ff\"\u003e5.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.12856487\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e     b level2  \u003cspan style=\"color:#ae81ff\"\u003e7.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.09428090\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e     c level1  \u003cspan style=\"color:#ae81ff\"\u003e9.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.07443229\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e     c level2 \u003cspan style=\"color:#ae81ff\"\u003e11.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.06148755\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e     d level1  \u003cspan style=\"color:#ae81ff\"\u003e1.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.47140452\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e8\u003c/span\u003e     d level2  \u003cspan style=\"color:#ae81ff\"\u003e3.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.20203051\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e     e level1  \u003cspan style=\"color:#ae81ff\"\u003e5.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.12856487\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e    e level2  \u003cspan style=\"color:#ae81ff\"\u003e7.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.09428090\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e    f level1  \u003cspan style=\"color:#ae81ff\"\u003e9.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.07443229\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e    f level2 \u003cspan style=\"color:#ae81ff\"\u003e11.5\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.7071068\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.06148755\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# ...to use for plotting\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eqplot\u003c/span\u003e(var1, mean, colour \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e var2, size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e CV, data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dataset_plyr, geom \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;point\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg alt=\"Figure\" loading=\"lazy\" src=\"/pivottable1.jpeg\"\u003e\u003c/p\u003e","title":"Good riddance to Excel pivot tables"},{"content":"I recently gathered fish harvest data from the U.S. National Oceanic and Atmospheric Administarion (NOAA), which I downloaded from Infochimps. The data is fish harvest by weight and value, by species for 21 years, from 1985 to 2005.\nHere is a link to a google document of the data I used below. I had to do some minor pocessing in Excel first; thus the link to this data.\nhttps://spreadsheets.google.com/ccc?key=0Aq6aW8n11tS_dFRySXQzYkppLXFaU2F5aC04d19ZS0E\u0026amp;amp;hl=en\nGet the original data from Infochimps here http://infochimps.com/datasets/domestic-fish-and-shellfish-catch-value-and-price-by-species-198\n# Fish harvest data setwd(\u0026#34;/Mac/R_stuff/Blog_etc/Infochimps/Fishharvest\u0026#34;) # Set path library(ggplot2) library(googleVis) library(Hmisc) fish \u0026lt;- read.csv(\u0026#34;fishharvest.csv\u0026#34;) # read data fish2 \u0026lt;- melt(fish,id=1:3,measure=4:24) # melt table year \u0026lt;- rep(1985:2005, each = 117) fish2 \u0026lt;- data.frame(fish2,year) # replace year with actual values # Google visusalization API fishdata \u0026lt;- data.frame(subset(fish2,fish2$var == \u0026#34;quantity_1000lbs\u0026#34;,-4),value_1000dollars=subset(fish2,fish2$var == \u0026#34;value_1000dollars\u0026#34;,-4)[,4]) names(fishdata)[4] \u0026lt;- \u0026#34;quantity_1000lbs\u0026#34; fishharvest \u0026lt;- gvisMotionChart(fishdata, idvar=\u0026#34;species\u0026#34;, timevar=\u0026#34;year\u0026#34;) plot(fishharvest) fishdatagg2 \u0026lt;- ddply(fish2,.(species,var),summarise, mean = mean(value), se = sd(value)/sqrt(length(value)) ) fishdatagg2 \u0026lt;- subset(fishdatagg2,fishdatagg2$var %in% c(\u0026#34;quantity_1000lbs\u0026#34;,\u0026#34;value_1000dollars\u0026#34;)) limit3 \u0026lt;- aes(ymax = mean + se, ymin = mean - se) bysppfgrid \u0026lt;- ggplot(fishdatagg2,aes(x=reorder(species,rank(mean)),y=mean,colour=species)) + geom_point() + geom_errorbar(limit3) + facet_grid(. ~ var, scales=\u0026#34;free\u0026#34;) + opts(legend.position=\u0026#34;none\u0026#34;) + coord_flip() + scale_y_continuous(trans=\u0026#34;log\u0026#34;) ggsave(\u0026#34;bysppfgrid.jpeg\u0026#34;) ","permalink":"http://localhost:1313/2011/01/r-and-google-visualization-api-fish/","summary":"\u003cp\u003eI recently gathered fish harvest data from the U.S. National Oceanic and Atmospheric Administarion (NOAA), which I downloaded from \u003ca href=\"http://infochimps.com/\"\u003eInfochimps\u003c/a\u003e. The data is fish harvest by weight and value, by species for 21 years, from 1985 to 2005.\u003c/p\u003e\n\u003cp\u003eHere is a link to a google document of the data I used below. I had to do some minor pocessing in Excel first; thus the link to this data.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://spreadsheets.google.com/ccc?key=0Aq6aW8n11tS_dFRySXQzYkppLXFaU2F5aC04d19ZS0E\u0026amp;amp;hl=en\"\u003ehttps://spreadsheets.google.com/ccc?key=0Aq6aW8n11tS_dFRySXQzYkppLXFaU2F5aC04d19ZS0E\u0026amp;amp;hl=en\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGet the original data from Infochimps here \u003ca href=\"http://infochimps.com/datasets/domestic-fish-and-shellfish-catch-value-and-price-by-species-198\"\u003ehttp://infochimps.com/datasets/domestic-fish-and-shellfish-catch-value-and-price-by-species-198\u003c/a\u003e\u003c/p\u003e","title":"R and Google Visualization API: Fish harvests"},{"content":"Wikispeedia is a website trying to gather all speed limit signs on Earth. I recently created a Google Visualization for some of their data, specifically on speed limit signs that change speed throughout the day. Check it out here. Here is how to see and comment on what they are doing: website, and Google groups.\n","permalink":"http://localhost:1313/2011/01/r-and-google-visualization-api-wikispeedia/","summary":"\u003cp\u003eWikispeedia is a website trying to gather all speed limit signs on Earth.  I recently created a Google Visualization for some of their data, specifically on speed limit signs that change speed throughout the day.  Check it out \u003ca href=\"http://groups.google.com/group/wikispeedia/browse_thread/thread/c9c712125a597b16\"\u003ehere\u003c/a\u003e.  Here is how to see and comment on what they are doing: \u003ca href=\"http://www.wikispeedia.org/\"\u003ewebsite\u003c/a\u003e, and \u003ca href=\"http://groups.google.com/group/wikispeedia?lnk=\"\u003eGoogle groups\u003c/a\u003e.\u003c/p\u003e","title":"R and Google Visualization API: Wikispeedia"},{"content":"Earlier, I posted about generating networks from abundance distributions that you specify. If this post was interesting, check out Jeff Kilpatrick\u0026rsquo;s website, where he provides code he produced in R and Octave to compare real bipartite networks to ones generated based on ecological variables measured in the field (in our case it was abundance, body size, and nectar production). We used that code for a paper we published. Code was modified from code produced by Diego P. Vazquez.\n","permalink":"http://localhost:1313/2011/01/bipartite-networks-and-r/","summary":"\u003cp\u003eEarlier, I posted about \u003ca href=\"http://r-ecology.blogspot.com/2011/01/ecological-networks-from-abundance.html\"\u003egenerating networks from abundance distributions that you specify\u003c/a\u003e. If this post was interesting, check out Jeff Kilpatrick\u0026rsquo;s website, where he provides code he produced in R and Octave to compare real bipartite networks to ones generated based on ecological variables measured in the field (in our case it was abundance, body size, and nectar production). We used that code for a paper \u003ca href=\"http://www.springerlink.com/content/1055615l6m74mp30/\"\u003ewe published\u003c/a\u003e. Code was modified from code produced by \u003ca href=\"http://www.cricyt.edu.ar/interactio/dvazquez/html/index_e.html\"\u003eDiego P. Vazquez\u003c/a\u003e.\u003c/p\u003e","title":"Bipartite networks and R"},{"content":" Okay, so this isn\u0026rsquo;t ecology related at all, but I like exploring data sets. So here goes\u0026hellip;\nPropublica has some awesome data sets available at their website: http://www.propublica.org/tools/\nI played around with their data set on Recovery.gov (see hyperlink below in code). Here\u0026rsquo;s some figures:\nMean award amount, ranked by mean amount, and also categorized by number of grants received (\u0026ldquo;nfund\u0026rdquo;) by state (by size and color of point). Yes, there are 56 \u0026ldquo;states\u0026rdquo;, which includes things like Northern Marian Islands (MP). Notice that California got the largest number of awards, but the mean award size was relatively small.\nHere is a figure by government organization that awarded each award, by mean award size (y-axis), number of awards (x-axis), and number of jobs created (numjobs=text size). Notice that the FCC (Federal Communications Commission) created nearly the most jobs despite not giving very large awards (although they did give a lot of awards).\nHere is the code:\n# Propublica Recovery.gov data install.packages(c(\u0026#34;ggplot2\u0026#34;,\u0026#34;maps\u0026#34;,\u0026#34;stringr\u0026#34;)) library(ggplot2) library(maps) library(stringr) setwd(\u0026#34;/Mac/R_stuff/Blog_etc\u0026#34;) # Set working directory theme_set(theme_bw()) # Read propublica data from file (download from here: http://propublica.s3.amazonaws.com/assets/recoverygov/propublica-recoverygov-primary-2.xls propubdat \u0026lt;- read.csv(\u0026#34;propublica-recoverygov-primary-2.csv\u0026#34;) str(propubdat) # Summarize data fundbystate \u0026lt;- ddply(propubdat,.(prime_state),summarise, meanfund = mean(award_amount), sefund = sd(award_amount)/sqrt(length(award_amount)), nfund = length(award_amount), numjobs = mean(number_of_jobs) ) fundbyagency \u0026lt;- ddply(propubdat,.(funding_agency_name),summarise, meanfund = mean(award_amount), sefund = sd(award_amount)/sqrt(length(award_amount)), nfund = length(award_amount), numjobs = mean(number_of_jobs) ) fun1 \u0026lt;- function(a) {str_c(paste(na.omit(str_extract(unlist(str_split(unlist(as.character(a[1])), \u0026#34; \u0026#34;)), \u0026#34;[A-Z]{1}\u0026#34;))), collapse=\u0026#34;\u0026#34;)} # Fxn to make funding agency name abbreviations within ddply below fundbyagency2 \u0026lt;- ddply(fundbyagency,.(funding_agency_name),transform, # add to table funding agency name abbreviations agency_abbrev = fun1(funding_agency_name) ) # Plot data, means and se\u0026#39;s by state limits \u0026lt;- aes(ymax = meanfund + sefund, ymin = meanfund - sefund) dodge \u0026lt;- position_dodge(width=0.6) awardbystate \u0026lt;- ggplot(fundbystate,aes(x=reorder(prime_state,meanfund),y=meanfund,colour=nfund)) + geom_point(aes(size=nfund),position=dodge) + coord_flip() + geom_errorbar(limits, width=0.2,position=dodge) + opts(panel.grid.major = theme_blank(),panel.grid.minor=theme_blank(),legend.position=c(0.7,0.2)) + labs(x=\u0026#34;State\u0026#34;,y=\u0026#34;Mean grant amount awarded +/- 1 s.e.\u0026#34;) ggsave(\u0026#34;awardbystate.jpeg\u0026#34;) # Plot data, means and se\u0026#39;s by funding agency limits2 \u0026lt;- aes(ymax = meanfund + sefund, ymin = meanfund - sefund) dodge \u0026lt;- position_dodge(width=0.6) awardbyagency \u0026lt;- ggplot(fundbyagency2,aes(y=log(meanfund),x=log(nfund),label=agency_abbrev)) + geom_text(aes(size=numjobs)) ggsave(\u0026#34;awardbyagency.jpeg\u0026#34;) # On US map fundbystate2 \u0026lt;- read.csv(\u0026#34;fundbystate.csv\u0026#34;) states \u0026lt;- map_data(\u0026#34;state\u0026#34;) # get state geographic data from the maps package recovmap \u0026lt;- merge(states,fundbystate2,by=\u0026#34;region\u0026#34;) # merage datasets qplot(long,lat,data=recovmap,group=group,fill=meanfund,geom=\u0026#34;polygon\u0026#34;) ggsave(\u0026#34;bystatemapmeans.jpeg\u0026#34;) qplot(long,lat,data=recovmap,group=group,fill=nfund,geom=\u0026#34;polygon\u0026#34;) ggsave(\u0026#34;bystatemapnumber.jpeg\u0026#34;) And the text file fundbystate2 here. I had the make this file separately so I could get in the spelled out state names as they were not provided in the propublica dataset.\nSource and disclaimer:\nData provided by Propublica. Data may contain errors and/or omissions.\n","permalink":"http://localhost:1313/2011/01/just-for-fun-recoverygov-data-snooping/","summary":"\u003cscript async=\"true\" src=\"http://pixel.propublica.org/pixel.js\" type=\"text/javascript\"\u003e\u003c/script\u003e\n\n\u003cp\u003eOkay, so this isn\u0026rsquo;t ecology related at all, but I like exploring data sets. So here goes\u0026hellip;\u003c/p\u003e\n\u003cp\u003ePropublica has some awesome data sets available at their website: \u003ca href=\"http://www.propublica.org/tools/\"\u003ehttp://www.propublica.org/tools/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eI played around with their data set on Recovery.gov (see hyperlink below in code). Here\u0026rsquo;s some figures:\u003c/p\u003e\n\u003cp\u003eMean award amount, ranked by mean amount, and also categorized by number of grants received (\u0026ldquo;nfund\u0026rdquo;) by state (by size and color of point).  Yes, there are 56 \u0026ldquo;states\u0026rdquo;, which includes things like Northern Marian Islands (MP). Notice that California got the largest number of awards, but the mean award size was relatively small.\u003c/p\u003e","title":"Just for fun: Recovery.gov data snooping"},{"content":"Another grad student and I tried recently to make a contribution to our understanding of the relationship between ecological network structure (e.g., nestedness) and community structure (e.g., evenness)\u0026hellip;\n\u0026hellip;Alas, I had no luck making new insights. However, I am providing the code used for this failed attempt in hopes that someone may find it useful. This is very basic code. It was roughly based off of the paper by Bluthgen et al. 2008 Ecology (here). In my code the number of interactions is set to 600, and there are 30 plant species, and 10 animal species. This assumes they share the same abundance distributions and sigma values.\nUPDATE: I changed the below code a bit to just output the metrics links per species, interaction evenness and H2.\nUPDATE on 27-Aug-12: Now using a github gist, which should actually work:\n# Community-Network Structure Simulation library(bipartite) # Set of mean and sd combinations of log-normal distribution mu\u0026lt;-c(0.5,2.9,5.3) sig\u0026lt;-c(0.75,1.6,2.45) make.matrices\u0026lt;-function(a,b,nmats){ plants\u0026lt;-round(rlnorm(n=30, meanlog=mu[a], sdlog=sig[b])) animals\u0026lt;-round(rlnorm(n=10, meanlog=mu[a], sdlog=sig[b])) plants\u0026lt;-plants*(600/sum(plants)) animals\u0026lt;-animals*(600/sum(animals)) r2dtable(nmats,animals,plants) } # Make matrices matrices \u0026lt;- make.matrices(1,1,100) # Calculate some network metrics-e.g., for one combination of mu and sigma linkspersp \u0026lt;- numeric(100) h2 \u0026lt;- numeric(100) inteven \u0026lt;- numeric(100) for(i in 1:length(matrices)){ m\u0026lt;-matrix(unlist(matrices[i]),ncol=30,byrow=F) metrics\u0026lt;-t(networklevel(m,index=c(\u0026#34;links per species\u0026#34;,\u0026#34;H2\u0026#34;,\u0026#34;interaction evenness\u0026#34;))) linkspersp[i]\u0026lt;-metrics[1] h2[i]\u0026lt;-metrics[2] inteven[i]\u0026lt;-metrics[3] } linkspersp h2 inteven ","permalink":"http://localhost:1313/2011/01/ecological-networks-from-abundance/","summary":"\u003cp\u003eAnother grad student and I tried recently to make a contribution to our understanding of the relationship between ecological network structure (e.g., nestedness) and community structure (e.g., evenness)\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;Alas, I had no luck making new insights. However, I am providing the code used for this failed attempt in hopes that someone may find it useful. This is very basic code. It was roughly based off of the paper by Bluthgen et al. 2008 Ecology (\u003ca href=\"http://www.esajournals.org/doi/abs/10.1890/07-2121.1?journalCode=ecol\"\u003ehere\u003c/a\u003e). In my code the number of interactions is set to 600, and there are 30 plant species, and 10 animal species. This assumes they share the same abundance distributions and sigma values.\u003c/p\u003e","title":"Ecological networks from abundance distributions"},{"content":"Just a quick FYI note in case you haven\u0026rsquo;t seen this site.\nR-bloggers is an awesome site, bringing together more than 140 blogs (including mine) about R in a single location. See Tal Galili\u0026rsquo;s motivation for creating the site, and his notes on the site here.\n","permalink":"http://localhost:1313/2011/01/r-bloggers/","summary":"\u003cp\u003eJust a quick FYI note in case you haven\u0026rsquo;t seen this site.\u003c/p\u003e\n\u003cp\u003eR-bloggers is an awesome site, bringing together more than 140 blogs (including mine) about R in a single location. See Tal Galili\u0026rsquo;s motivation for creating the site, and his notes on the site \u003ca href=\"http://www.r-bloggers.com/r-bloggers-in-2010-top-14-r-posts-site-statistics-and-invitation-for-sponsors/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"R-bloggers"},{"content":"Anthony Ives, of University of Wisconsin-Madison, and Matthew Helmus of the Xishuangbanna Tropical Botanical Garden, present a new statistical method for analyzing phylogenetic community structure in an early view paper in Ecological Monographs. See the abstract here.\nUp to now, most phylogenetic community structure papers have calculated metrics and used randomization tests to determine if observed metrics are different from random. The approach of Ives and Helmus fits models to observed data, instead of calculating single metrics.\nFurthermore, their approach gets around the limitation in studies of phylogenetic community structure of conducting many separate statistical tests, thereby inflating your chances of finding a significant effect purely by chance.\nTheir approach uses generalized linear mixed models (GLMMs). They provide Matlab code for running these models, but R code will be available in the Picante package in the future.\n","permalink":"http://localhost:1313/2011/01/new-approach-to-analysis-of-phylogenetic-community-structure/","summary":"\u003cp\u003eAnthony Ives, of University of Wisconsin-Madison, and Matthew Helmus of the Xishuangbanna Tropical Botanical Garden, present a new statistical method for analyzing phylogenetic community structure in an early view paper in Ecological Monographs. See the abstract \u003ca href=\"http://www.esajournals.org/doi/abs/10.1890/10-1264.1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eUp to now, most phylogenetic community structure papers have calculated metrics and used randomization tests to determine if observed metrics are different from random. The approach of Ives and Helmus fits models to observed data, instead of calculating single metrics.\u003c/p\u003e","title":"New approach to analysis of phylogenetic community structure"},{"content":"The recent availability of google ngram data is a great source of data on language use. Here are some terms from ecology from 1890 to 2000 (from here: https://books.google.com/ngrams/). Note that the word \u0026ldquo;ecology\u0026rdquo; doesn\u0026rsquo;t appear at all until about 1890.\nNotice the close alignment of \u0026ldquo;ecology\u0026rdquo; and \u0026ldquo;ecosystem\u0026rdquo;, while \u0026ldquo;predation\u0026rdquo; and \u0026ldquo;facilitation\u0026rdquo; put in somewhat equal showings. \u0026ldquo;Parasitism\u0026rdquo; is almost constant through time, while \u0026ldquo;mutualism\u0026rdquo; falls way below predation and facilitation and parasitism.\nGoogle Ngram plot of six ecological terms + environmentalism for giggles\n","permalink":"http://localhost:1313/2010/12/ngram-ecological-terms/","summary":"\u003cp\u003eThe recent availability of google ngram data is a great source of data on language use. Here are some terms from ecology from 1890 to 2000 (from here: \u003ca href=\"https://books.google.com/ngrams/\"\u003ehttps://books.google.com/ngrams/\u003c/a\u003e). Note that the word \u0026ldquo;ecology\u0026rdquo; doesn\u0026rsquo;t appear at all until about 1890.\u003c/p\u003e\n\u003cp\u003eNotice the close alignment of \u0026ldquo;ecology\u0026rdquo; and \u0026ldquo;ecosystem\u0026rdquo;, while \u0026ldquo;predation\u0026rdquo; and \u0026ldquo;facilitation\u0026rdquo; put in somewhat equal showings. \u0026ldquo;Parasitism\u0026rdquo; is almost constant through time, while \u0026ldquo;mutualism\u0026rdquo; falls way below predation and facilitation and parasitism.\u003c/p\u003e","title":"Ngram ecological terms"},{"content":"Here is some code to run Phylometa from R. Phylometa is a program that conducts phylogenetic meta-analyses. The great advantage of the approach below is that you can easily run Phylometa from R, and manipulate the output from Phylometa in R.\nPhylometa was created by Marc Lajeunesse at University of South Florida, and is described in his 2009 AmNat paper. Phylometa can be downloaded free here.\nSave phylometa_fxn.R (get here) to your working directory. Then use the block of code below to call the functions within phylometa_fxn.R.\nThe program Phylometa needs to be in the working directory you are calling from.\nLet me know what doesn\u0026rsquo;t work, and what improvements can be made; I\u0026rsquo;m sure there are many!\n\u0026mdash;This code below is also available here on Github.\n########Directions #Place phylometa software to your working directory #Put your phylogeny, in format required by phylometa, in your working directory #Put your meta-analysis dataset, in format required by phylometa, in your working directory #Set working directory #Use below functions #Beware: only use a moderator variable with up to 6 groups ########Install packages install.packages(c(\u0026#34;plyr\u0026#34;,\u0026#34;ggplot2\u0026#34;)) library(plyr) library(ggplot2) ########Set the working directory [NOTE:CHANGE TO YOUR WORKING DIRECTORY] setwd(\u0026#34;/Users/Scott/Documents/phylometa\u0026#34;) #Call and run functions (used below) in the working directory [NOTE:CHANGE TO YOUR WORKING DIRECTORY] source(\u0026#34;/Users/Scott/Documents/phylometa\u0026#34;) ###########################Functions to to a phylogenetic meta-analysis #Define number of groups in moderator variable groups \u0026lt;- 2 ####Run phylometa. Change file names as needed phylometa.run \u0026lt;- system(paste(\u0026#39;\u0026#34;phyloMeta_v1-2_beta.exe\u0026#34; phylogeny.txt metadata_2g.txt\u0026#39;),intern=T) ####Process phylometa output #E.g. myoutput \u0026lt;- phylometa.process(phylometa.run,groups) ####Get output from phylometa.run phylometa.output(myoutput) #Prints all five tables phylometa.output.table(myoutput,2) #Prints the table you specify, from 1 to 5, in this example, table 2 is output ################################################### #########Plot effect sizes. These are various ways to look at the data. Go through them to see what they do. Output pdf\u0026#39;s are in your working directory #Make table for plotting analysis \u0026lt;- c(rep(\u0026#34;fixed\u0026#34;,groups+1),rep(\u0026#34;random\u0026#34;,groups+1)) trad_effsizes \u0026lt;- data.frame(analysis,phylometa.output.table(myoutput,2)) #Tradiational effect size table phylog_effsizes \u0026lt;- data.frame(analysis,phylometa.output.table(myoutput,4)) #Phylogenetic effect size table #The arrange method limits \u0026lt;- aes(ymax = effsize + (CI_high-effsize), ymin = effsize - (effsize-CI_low)) dodge \u0026lt;- position_dodge(width=0.3) plot01 \u0026lt;- ggplot(trad_effsizes,aes(y=effsize,x=analysis,colour=Group)) + geom_point(size=3,position=dodge) + theme_bw() + opts(panel.grid.major = theme_blank(),panel.grid.minor=theme_blank(),title=\u0026#34;Traditional meta-analysis\u0026#34;) + labs(x=\u0026#34;Group\u0026#34;,y=\u0026#34;Effect size\u0026#34;) + geom_errorbar(limits, width=0.2, position=dodge) + geom_hline(yintercept=0,linetype=2) plot02 \u0026lt;- ggplot(phylog_effsizes,aes(y=effsize,x=analysis,colour=Group)) + geom_point(size=3,position=dodge) + theme_bw() + opts(panel.grid.major = theme_blank(),panel.grid.minor=theme_blank(),title=\u0026#34;Phylogenetic meta-analysis\u0026#34;) + labs(x=\u0026#34;Group\u0026#34;,y=\u0026#34;Effect size\u0026#34;) + geom_errorbar(limits, width=0.2, position=dodge) + geom_hline(yintercept=0,linetype=2) pdf(\u0026#34;plots_effsizes_arrange.pdf\u0026#34;,width = 8, height = 11) arrange(plot01,plot02,ncol=1) dev.off() #used in the two plotting methods below bothanalyses\u0026lt;-data.frame(tradphy=c(rep(\u0026#34;Traditional\u0026#34;,(groups*2)+2),rep(\u0026#34;Phylogenetic\u0026#34;,(groups*2)+2)),fixrand=rep(analysis,2),rbind.fill(phylometa.output.table(myoutput,2),phylometa.output.table(myoutput,4))) #Table of both trad and phylo limits2 \u0026lt;- aes(ymax = effsize + (CI_high-effsize), ymin = effsize - (effsize-CI_low)) dodge \u0026lt;- position_dodge(width=0.3) #The grid/wrap method, version 1 plot03 \u0026lt;- ggplot(bothanalyses,aes(y=effsize,x=tradphy,colour=Group)) + geom_point(size=3,position=dodge) + theme_bw() + opts(panel.grid.major = theme_blank(),panel.grid.minor=theme_blank()) + labs(x=\u0026#34;Group\u0026#34;,y=\u0026#34;Effect size\u0026#34;) + geom_errorbar(limits2, width=0.2, position=dodge) + geom_hline(yintercept=0,linetype=2) + facet_grid(.~fixrand) pdf(\u0026#34;plots_effsizes_wrap1.pdf\u0026#34;) plot03 dev.off() #The grid/wrap method, version 2 (excuse the sloppy x-axis labels) plot04 \u0026lt;- ggplot(bothanalyses,aes(y=effsize,x=Group,colour=tradphy)) + geom_point(size=3,position=dodge) + theme_bw() + opts(panel.grid.major = theme_blank(),panel.grid.minor=theme_blank()) + labs(x=\u0026#34;Group\u0026#34;,y=\u0026#34;Effect size\u0026#34;) + geom_errorbar(limits2, width=0.2, position=dodge) + geom_hline(yintercept=0,linetype=2) + facet_grid(.~fixrand) pdf(\u0026#34;plots_effsizes_wrap2.pdf\u0026#34;) plot04 dev.off() Below is an example output figure from the code. This example is from an analysis using 5 groups (i.e., 5 levels in the explanatory variable).\n","permalink":"http://localhost:1313/2010/12/phylogenetic-meta-analysis-in-r-using-phylometa/","summary":"\u003cp\u003eHere is some code to run Phylometa from R. Phylometa is a program that conducts phylogenetic meta-analyses. The great advantage of the approach below is that you can easily run Phylometa from R, and manipulate the output from Phylometa in R.\u003c/p\u003e\n\u003cp\u003ePhylometa was created by Marc Lajeunesse at University of South Florida, and is described in his 2009 AmNat \u003ca href=\"http://lajeunesse.myweb.usf.edu/publications.html\"\u003epaper\u003c/a\u003e. Phylometa can be downloaded free \u003ca href=\"http://lajeunesse.myweb.usf.edu/publications.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSave phylometa_fxn.R (get \u003ca href=\"https://gist.github.com/939970\"\u003ehere\u003c/a\u003e) to your working directory.  Then use the  block of code below to call the functions within phylometa_fxn.R.\u003c/p\u003e","title":"Phylogenetic meta-analysis in R using Phylometa"},{"content":"I am starting this blog not because I am a seasoned code writer, but because I am learning how to use R specifically for ecology and evolution, and figured many others might have the same questions I have. If I find cool solutions I will post them here for all to view, criticize, improve, etc.\n","permalink":"http://localhost:1313/2010/12/a-new-blog-about-using-r-for-ecology-and-evolution/","summary":"\u003cp\u003eI am starting this blog not because I am a seasoned code writer, but because I am learning how to use R specifically for ecology and evolution, and figured many others might have the same questions I have. If I find cool solutions I will post them here for all to view, criticize, improve, etc.\u003c/p\u003e","title":"A new blog about using R for ecology and evolution"},{"content":"Blog (mostly 1) by Scott Chamberlain.\nWebsite: https://scottchamberlain.info GitHub: https://github.com/sckott Mastodon: @sckottie@fosstodon.org Posts are mostly about code, though posts meander off that path sometimes.\nMost posts are by me. Some are by my friend Pascal :)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/about/","summary":"about","title":"About"}]
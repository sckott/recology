<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Sinatra on Recology</title>
    <link>http://localhost:1313/tags/sinatra/</link>
    <description>Recent content in Sinatra on Recology</description>
    <generator>Hugo -- 0.144.2</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Nov 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/sinatra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Web APIs with Sinatra, Mongo, Docker, and Caddy</title>
      <link>http://localhost:1313/2017/11/sinatra-mongo-docker-caddy/</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2017/11/sinatra-mongo-docker-caddy/</guid>
      <description>&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;
&lt;p&gt;The R community has a package distribution thing called &lt;a href=&#34;https://cran.rstudio.com/web/packages/&#34;&gt;CRAN&lt;/a&gt; just like Ruby has &lt;a href=&#34;https://rubygems.org/&#34;&gt;Rubygems&lt;/a&gt;, and Python has &lt;a href=&#34;https://pypi.python.org/pypi&#34;&gt;Pypi&lt;/a&gt;, etc. On all packages on CRAN, the CRAN maintainers run checks on each package on multiple versions of R and on many operating systems. They report those results on a page associated with the package, like &lt;a href=&#34;https://cran.rstudio.com/web/checks/check_results_crul.html&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You might be thinking: okay, but we have Travis-CI and friends, so who cares about that?  Well, it&amp;rsquo;s these checks that CRAN runs that will determine if your package on CRAN leads to emails to you asking for changes, and possibly the package being taken down if e.g., they email and you don&amp;rsquo;t respond for a period of time.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="the-problem">The problem</h2>
<p>The R community has a package distribution thing called <a href="https://cran.rstudio.com/web/packages/">CRAN</a> just like Ruby has <a href="https://rubygems.org/">Rubygems</a>, and Python has <a href="https://pypi.python.org/pypi">Pypi</a>, etc. On all packages on CRAN, the CRAN maintainers run checks on each package on multiple versions of R and on many operating systems. They report those results on a page associated with the package, like <a href="https://cran.rstudio.com/web/checks/check_results_crul.html">this one</a>.</p>
<p>You might be thinking: okay, but we have Travis-CI and friends, so who cares about that?  Well, it&rsquo;s these checks that CRAN runs that will determine if your package on CRAN leads to emails to you asking for changes, and possibly the package being taken down if e.g., they email and you don&rsquo;t respond for a period of time.</p>
<p>So CRAN provides these package checks. Now what?  Ideally, these would be available through an API so that the data is machine readable, which then makes many other things possible (see <a href="#whats-next">What&rsquo;s Next</a> below).</p>
<p>So how to build the API?</p>
<h2 id="building-the-cran-checks-api">Building the CRAN checks API</h2>
<p>On GitHub: <a href="https://github.com/ropensci/cchecksapi">https://github.com/ropensci/cchecksapi</a></p>
<p>My main goal learning goals with this API tech wise were two fold:</p>
<ul>
<li>learn how to dockerize the application</li>
<li>learn how to use MongoDB</li>
</ul>
<p>I hadn&rsquo;t Dockerized a web API myself before, so that was an important goal - and I had actually never used MongoDB, but wanted to give it a shot to get familiar with it.</p>
<p>The whole stack is:</p>
<ul>
<li>language: Ruby</li>
<li>web API framework: Sinatra</li>
<li>http Ruby gem: faraday</li>
<li>database: mongodb</li>
<li>server: caddy</li>
<li>container: all wrapped up in docker (docker-compose)</li>
<li>hosting: Amazon EC2</li>
<li>scheduling: crontab</li>
</ul>
<p>At a high level, the system is as so:</p>
<ul>
<li>Once a day a few Ruby scripts (<a href="https://github.com/ropensci/cchecksapi/blob/master/scrape.rb">for packages</a>, <a href="https://github.com/ropensci/cchecksapi/blob/master/scrape_maintainer.rb">for maintainers</a>):
<ul>
<li>collects the names of packages on CRAN from G치bor Cs치rdi&rsquo;s <a href="https://crandb.r-pkg.org">https://crandb.r-pkg.org</a> API and maintainer emails from CRAN itself, then</li>
<li>goes out to the CRAN website and collects check results for each package, then</li>
<li>insert data into a MongoDB database</li>
</ul>
</li>
<li>The API provides routes for getting data on specific packages by name, or all packages, and data on all packages for any given maintainers email adddress, or all maintainers
<ul>
<li>API calls make a query into the MongoDB database matching on the package name or maintainer email address</li>
<li>data is given back as JSON</li>
</ul>
</li>
</ul>
<p>The API doesn&rsquo;t currently use caching, but may add if it seems needed.</p>
<h2 id="ruby-and-sinatra">Ruby and Sinatra</h2>
<p>I really like Ruby. It&rsquo;s a language that is fun to use, the community is great, and there&rsquo;s tons of packages.  Ruby is great for making web stuff, including web APIs. When doing web stuff, for me that means web APIs. For web APIs in Ruby, Rails is too heavy for all the stuff I do - that&rsquo;s where <a href="https://www.sinatrarb.com/">Sinatra</a> comes in.</p>
<p>Sinatra is a lightweight framework for making web apps/APIs. I make all my web APIs with Sinatra, and have had few complaints. Some may say &ldquo;you should use X or Y because faster&rdquo;, or whatever, but Sinatra is plenty fast for my use cases. Not every use case is &ldquo;we&rsquo;re Facebook&rdquo;, or &ldquo;we&rsquo;re Google&rdquo;.</p>
<p>Until recently I&rsquo;ve been very much manually managing my Sinatra web APIs on servers - that is, installing/updating everything on the server itself, without using containers or any configuration management. This blog post is the blog post I would have wanted to read when I was figuring out how to dockerize my web APIs.</p>
<h2 id="the-api">The API</h2>
<p>The main meat of the API is definitions of routes. In addition, I&rsquo;ve included a number of rules about what HTTP verbs are allowed to be used, what headers to send in each response, how to respond to client and server failures, etc.</p>
<p>This is what one of the route definitions looks like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ruby" data-lang="ruby"><span style="display:flex;"><span>get <span style="color:#e6db74">&#39;/pkgs/?&#39;</span> <span style="color:#66d9ef">do</span>
</span></span><span style="display:flex;"><span>  headers_get
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">begin</span>
</span></span><span style="display:flex;"><span>    lim <span style="color:#f92672">=</span> (params<span style="color:#f92672">[</span><span style="color:#e6db74">:limit</span><span style="color:#f92672">]</span> <span style="color:#f92672">||</span> <span style="color:#ae81ff">10</span>)<span style="color:#f92672">.</span>to_i
</span></span><span style="display:flex;"><span>    off <span style="color:#f92672">=</span> (params<span style="color:#f92672">[</span><span style="color:#e6db74">:offset</span><span style="color:#f92672">]</span> <span style="color:#f92672">||</span> <span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to_i
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">raise</span> <span style="color:#66d9ef">Exception</span><span style="color:#f92672">.</span>new(<span style="color:#e6db74">&#39;limit too large (max 1000)&#39;</span>) <span style="color:#66d9ef">unless</span> lim <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>    d <span style="color:#f92672">=</span> $cks<span style="color:#f92672">.</span>find({}, {<span style="color:#e6db74">&#34;limit&#34;</span> <span style="color:#f92672">=&gt;</span> lim, <span style="color:#e6db74">&#34;skip&#34;</span> <span style="color:#f92672">=&gt;</span> off})
</span></span><span style="display:flex;"><span>    dat <span style="color:#f92672">=</span> d<span style="color:#f92672">.</span>to_a
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">raise</span> <span style="color:#66d9ef">Exception</span><span style="color:#f92672">.</span>new(<span style="color:#e6db74">&#39;no results found&#39;</span>) <span style="color:#66d9ef">if</span> d<span style="color:#f92672">.</span>nil?
</span></span><span style="display:flex;"><span>    { <span style="color:#e6db74">found</span>: d<span style="color:#f92672">.</span>count, <span style="color:#e6db74">count</span>: dat<span style="color:#f92672">.</span>length, <span style="color:#e6db74">offset</span>: <span style="color:#66d9ef">nil</span>, <span style="color:#e6db74">error</span>: <span style="color:#66d9ef">nil</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">data</span>: dat }<span style="color:#f92672">.</span>to_json
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">rescue</span> <span style="color:#66d9ef">Exception</span> <span style="color:#f92672">=&gt;</span> e
</span></span><span style="display:flex;"><span>    halt <span style="color:#ae81ff">400</span>, { <span style="color:#e6db74">count</span>: <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">error</span>: { <span style="color:#e6db74">message</span>: e<span style="color:#f92672">.</span>message }, <span style="color:#e6db74">data</span>: <span style="color:#66d9ef">nil</span> }<span style="color:#f92672">.</span>to_json
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><p>This code chunk is for the <code>/pkgs</code> route on the API (check it out at <a href="https://cranchecks.info/pkgs">https://cranchecks.info/pkgs</a>). The <code>headers_get</code> bit sends a pre-defined set of headers in the response. The <code>begin ... rescue ... end</code> bit is a &ldquo;try catch&rdquo; thing - leading to a JSON failure response in case there is a failure - and a JSON response on success.</p>
<h2 id="collecting-data-and-mongodb">Collecting data and MongoDB</h2>
<p>As stated above, data is updated once a day. The code for scraping data on the package level and maintainer level is pretty similar. For both, the steps are the following: a) collect all names (for <code>/pkgs</code> that&rsquo;s package names from <a href="https://crandb.r-pkg.org">https://crandb.r-pkg.org</a>; for <code>/maintainers</code> that&rsquo;s maintainer email addresses from <a href="https://cran.rstudio.com/web/checks/check_summary_by_maintainer.html">https://cran.rstudio.com/web/checks/check_summary_by_maintainer.html</a>), b) for each package name or maintainer email scrape CRAN check results, c) with all data collected drop data in MongoDB and then load all new data (maybe this could be an update step?). You can see the gory details on GitHub for <a href="https://github.com/ropensci/cchecksapi/blob/master/scrape.rb">packages</a> and <a href="https://github.com/ropensci/cchecksapi/blob/master/scrape_maintainer.rb">maintainers</a>.</p>
<p>Those steps above in code for packages is like the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ruby" data-lang="ruby"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">scrape_all</span>
</span></span><span style="display:flex;"><span>  pkgs <span style="color:#f92672">=</span> cran_packages; <span style="color:#75715e"># get all pkg names</span>
</span></span><span style="display:flex;"><span>  out <span style="color:#f92672">=</span> <span style="color:#f92672">[]</span> <span style="color:#75715e"># make an array</span>
</span></span><span style="display:flex;"><span>  pkgs<span style="color:#f92672">.</span>each <span style="color:#66d9ef">do</span> <span style="color:#f92672">|</span>x<span style="color:#f92672">|</span> <span style="color:#75715e"># for each pkg, scrape check results</span>
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">&lt;&lt;</span> scrape_pkg(x)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> $cks<span style="color:#f92672">.</span>count <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    $cks<span style="color:#f92672">.</span>drop <span style="color:#75715e"># drop data in Mongo</span>
</span></span><span style="display:flex;"><span>    $cks <span style="color:#f92672">=</span> $mongo<span style="color:#f92672">[</span><span style="color:#e6db74">:checks</span><span style="color:#f92672">]</span> <span style="color:#75715e"># recreate database in Mongo</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>  $cks<span style="color:#f92672">.</span>insert_many(out<span style="color:#f92672">.</span>map { <span style="color:#f92672">|</span>e<span style="color:#f92672">|</span> prep_mongo(e) }) <span style="color:#75715e"># load new data into Mongo</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><p><code>$cks</code> is the MongoDB database connection.</p>
<h2 id="docker">Docker</h2>
<p>For containerizing the API, I used Docker. A colleague had used <a href="https://docs.docker.com/compose/">Docker Compose</a>, and it was a really easy experience spinning up and taking down the application we were working on. So I wanted to learn how to do that myself. After trial and error, finally got to a solution for this API. Here is my <code>docker-compose.yml</code> file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Dockerfile" data-lang="Dockerfile"><span style="display:flex;"><span>mongo:<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>  image: mongo<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>  volumes:<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>    - $HOME/data/mongodb:/data/db <span style="color:#75715e"># persists data to disk outside container</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>  restart: always<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>  ports:<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>    - <span style="color:#e6db74">&#34;27017:27017&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>api:<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>  build: .<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>  ports:<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>    - <span style="color:#e6db74">&#34;8834:8834&#34;</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>  links:<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>    - mongo<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>This specifies the container for MongoDB and for the API, and specifies in the API container to link to the mongo container.</p>
<p>To build and run do</p>
<pre tabindex="0"><code>docker-compose build &amp;&amp; docker-compose up -d
</code></pre><p>The <code>-d</code> flag is for daemonize, i.e., run in the background. To kill them run</p>
<pre tabindex="0"><code>docker-compose stop &amp;&amp; docker-compose rm
</code></pre><h2 id="caddy-server">Caddy server</h2>
<p>Caddy is great server. I never really used Nginx, so I can&rsquo;t compare the two really - I just know that Caddy is super easy. To install, check out the installation page <a href="https://caddyserver.com/download">https://caddyserver.com/download</a>, and it&rsquo;s easy as something like <code>curl https://getcaddy.com | bash -s personal</code> (depends on configuration options on that page and license choice).</p>
<p>I know there&rsquo;s an option to run a separate container with Caddy, but I run Caddy outside containers.</p>
<p>My <code>Caddyfile</code> has something similar to the following:</p>
<pre tabindex="0"><code>cranchecks.info {
  gzip
  tls email@foobar.com

  log / logfile.log &#34;{remote} - [{when}] {method} {uri} {query} {proto} {status} {size} {&gt;User-Agent}&#34; {
     rotate_size 3
  }

  proxy / localhost:8834 {
    transparent
  }
}
</code></pre><ul>
<li><code>gzip</code> tells Caddy to serve gzipp&rsquo;ed content (see <a href="https://caddyserver.com/docs/gzip">https://caddyserver.com/docs/gzip</a>)</li>
<li><code>tls</code> says use the given email for registering with <a href="https://letsencrypt.org/">Letsencrypt</a> (see <a href="https://caddyserver.com/docs/tls">https://caddyserver.com/docs/tls</a>)</li>
<li><code>log</code> line specifies how to log requests (and <code>rotate_size</code> says start a new file when the current one reaches 3 MB) (see <a href="https://caddyserver.com/docs/log">https://caddyserver.com/docs/log</a>)</li>
<li><code>proxy</code> is for specifying reverse proxy (see <a href="https://caddyserver.com/docs/proxy">https://caddyserver.com/docs/proxy</a>)</li>
</ul>
<h2 id="whats-next">What&rsquo;s Next</h2>
<p>There&rsquo;s still more work to do.</p>
<ul>
<li>Better <code>/maintainers</code> results
<ul>
<li>right now, we have two arrays of results, one from the html table on the CRAN results page and the other from the text below it. This duplication isn&rsquo;t ideal.</li>
<li>it would be helpful to have a summary across all packages for any given maintainer</li>
</ul>
</li>
<li>Better <code>/pkgs</code> results
<ul>
<li>it would be helpful to have a summary across all R versions and platforms for any given package</li>
</ul>
</li>
<li>Include actual CRAN check results - CRAN check results can include output of the failures (whether they&rsquo;re in examples or the test suite, or an installation error). The API doesn&rsquo;t currently include that output, but thinking about how it could.</li>
<li>Possibly update data more often. Right now we update once per day. Seems like results do roll in at different times though, perhaps as builds are done for each pkg?</li>
<li>Notification service:  package maintainers can opt-in to notifications when their checks are failing so they can be on top of fixes quickly.  This could be managed through the API itself, with no GUI, but to make it palatable to all types may want to make a super simple web page to sign up.</li>
</ul>
<p>Check out the <a href="https://github.com/ropensci/cchecksapi/issues">issue tracker</a> to follow progress or file a feature request or bug.</p>
<!-- raw HTML omitted -->
<hr>
<h2 id="thanks">Thanks</h2>
<p>Thanks to <a href="https://github.com/gaborcsardi">G치bor Cs치rdi</a> for the idea to make a <code>/maintainers</code> route.</p>
<h2 id="further-reading">Further reading</h2>
<p>In a <a href="https://blog.cloud66.com/deploying-rest-apis-to-docker-using-ruby-and-sinatra/">similar post</a> Cloud66 folks talked about deploying an API with the same stack essentially: Sinatra, MongoDB, and Docker.</p>
<h2 id="ps">p.s.</h2>
<p>I mostly write about R software, so some readers may use R: if you want to make a web API but only know R, try learning Ruby!  It can&rsquo;t hurt to learn Ruby, and you&rsquo;ll be happy you did.</p>
]]></content:encoded>
    </item>
    <item>
      <title>gbids - GenBank IDs API is back up!</title>
      <link>http://localhost:1313/2016/09/gbids-is-back/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2016/09/gbids-is-back/</guid>
      <description>&lt;h2 id=&#34;gbids-api-is-back&#34;&gt;GBIDS API is back&lt;/h2&gt;
&lt;p&gt;Back in March this year I wrote &lt;a href=&#34;https://recology.info/2016/03/genbank-ids/&#34;&gt;a post about a new API for working with GenBank IDs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I had to take the API down because it was too expensive to keep up. Expensive because the dump of data is very large (3.8 GB compressed), and I need disk space on the server to uncompress that to I think about 18 GB, then load into MySQL, which is another maybe 30 GB or so. Anyway, it&amp;rsquo;s not expensive because of high traffic - although I wish that was the case - but because of needing lots of disk space.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="gbids-api-is-back">GBIDS API is back</h2>
<p>Back in March this year I wrote <a href="https://recology.info/2016/03/genbank-ids/">a post about a new API for working with GenBank IDs</a>.</p>
<p>I had to take the API down because it was too expensive to keep up. Expensive because the dump of data is very large (3.8 GB compressed), and I need disk space on the server to uncompress that to I think about 18 GB, then load into MySQL, which is another maybe 30 GB or so. Anyway, it&rsquo;s not expensive because of high traffic - although I wish that was the case - but because of needing lots of disk space.</p>
<p>I was fortuntate to recently receive some <a href="https://aws.amazon.com/research-credits/">Amazon Cloud Credits for Research</a>. The credits expire in 1 year. With these credits, I&rsquo;ve put the GBIDS API back up. In the next year I&rsquo;m hoping to gain user traction suggesting that&rsquo;s is useful to enough people to keep maintaining - in which case I&rsquo;ll seek ways to fund it.</p>
<p>But that means I need people to use it!  So please to give it a try. Let me know what could be better; what could be faster; what API routes/features/etc. you&rsquo;d like to see.</p>
<h2 id="plans">Plans</h2>
<p>Plans for the future of the GBIDS API:</p>
<ul>
<li>Auto-update the Genbank data. This is quite complicated since the dump is so large. I can either keep an EC2 attached disc large enough to do the dump download/expansion/load/etc, or spin up a new instance each Sunday when they do their data release, do the SQL load, make a dump, then shuttle the SQL dump to the instance running, then load in the new data from the dump. I haven&rsquo;t got this bit running yet, so data is from Aug 7. 2016.</li>
<li>Add taxonomic IDs. Genbank also dumps their taxonomic IDs. I think it should be possible to get taxonomic IDs into the API, so that users can do accession number to taxon IDs and vice versa.</li>
<li>Performance: as anyone would want, I want to continually improve performance. I&rsquo;ll watch out for things I can do, but also let me know what seems too slow.</li>
</ul>
<h2 id="links">Links</h2>
<ul>
<li>API base url: <a href="https://gbids.xyz">https://gbids.xyz</a></li>
<li>API docs: <a href="https://recology.info/gbidsdocs">https://recology.info/gbidsdocs</a> - Let me know if these could be improved</li>
<li>API status page: <a href="https://recology.info/gbidsstatus">https://recology.info/gbidsstatus</a> - I update this page whenever there&rsquo;s some down time, then when it&rsquo;s back up, etc.</li>
<li>API source code: <a href="https://github.com/sckott/gbids">https://github.com/sckott/gbids</a> - You can file issues here about the API</li>
</ul>
<h2 id="try-it">Try it</h2>
<p>Get 5 accession numbers</p>
<pre tabindex="0"><code>curl &#39;https://gbids.xyz/acc?limit=5&#39; | jq .
#&gt; {
#&gt;   &#34;matched&#34;: 692006925,
#&gt;   &#34;returned&#34;: 5,
#&gt;   &#34;data&#34;: [
#&gt;     &#34;A00002&#34;,
#&gt;     &#34;A00003&#34;,
#&gt;     &#34;X17276&#34;,
#&gt;     &#34;X60065&#34;,
#&gt;     &#34;CAA42669&#34;
#&gt;   ],
#&gt;   &#34;error&#34;: null
#&gt; }
</code></pre><p>Request to match accession identifiers, some exist, while some do not</p>
<pre tabindex="0"><code>curl &#39;https://gbids.xyz/acc/AACY024124486,AACY024124483,asdfd,asdf,AACY024124476&#39; | jq .
#&gt; {
#&gt;   &#34;matched&#34;: 3,
#&gt;   &#34;returned&#34;: 5,
#&gt;   &#34;data&#34;: {
#&gt;     &#34;AACY024124486&#34;: true,
#&gt;     &#34;AACY024124483&#34;: true,
#&gt;     &#34;asdfd&#34;: false,
#&gt;     &#34;asdf&#34;: false,
#&gt;     &#34;AACY024124476&#34;: true
#&gt;   },
#&gt;   &#34;error&#34;: null
#&gt; }
</code></pre><p>There&rsquo;s many more examples in the <a href="https://recology.info/gbidsdocs">API docs</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>GenBank IDs API - get, match, swap id types</title>
      <link>http://localhost:1313/2016/03/genbank-ids/</link>
      <pubDate>Tue, 29 Mar 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2016/03/genbank-ids/</guid>
      <description>&lt;p&gt;GenBank IDs, accession numbers and GI identifiers, are the two types of identifiers for entries in GenBank. (see &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html&#34;&gt;this page&lt;/a&gt; for why there are two types of identifiers). Actually, &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/news/03-02-2016-phase-out-of-GI-numbers/&#34;&gt;recent news&lt;/a&gt; from NCBI is that GI identifiers will be phased out by September this year, which affects what I&amp;rsquo;ll talk about below.&lt;/p&gt;
&lt;p&gt;There are a lot of sequences in GenBank. Sometimes you have identifiers and you want to check if they exist in GenBank, or want to get one type from another (accession from GI, or vice versa; although GI phase out will make this use case no longer needed), or just get a bunch of identifiers for software testing purposes perhaps.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>GenBank IDs, accession numbers and GI identifiers, are the two types of identifiers for entries in GenBank. (see <a href="https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html">this page</a> for why there are two types of identifiers). Actually, <a href="https://www.ncbi.nlm.nih.gov/news/03-02-2016-phase-out-of-GI-numbers/">recent news</a> from NCBI is that GI identifiers will be phased out by September this year, which affects what I&rsquo;ll talk about below.</p>
<p>There are a lot of sequences in GenBank. Sometimes you have identifiers and you want to check if they exist in GenBank, or want to get one type from another (accession from GI, or vice versa; although GI phase out will make this use case no longer needed), or just get a bunch of identifiers for software testing purposes perhaps.</p>
<p>Currently, the ENTREZ web services aren&rsquo;t super performant or easy to use for the above use cases. Thus, I&rsquo;ve built out a RESTful API for these use cases, called <a href="https://github.com/sckott/gbids">gbids</a>.</p>
<p><code>gbids</code> on GitHub: <a href="https://github.com/sckott/gbids">sckott/gbids</a></p>
<p>Here&rsquo;s the tech stack:</p>
<ul>
<li>API: Ruby/Sinatra</li>
<li>Storage: MySQL</li>
<li>Caching: Redis
<ul>
<li>each key cached for 3 hours</li>
</ul>
</li>
<li>Server: Caddy
<ul>
<li>https</li>
</ul>
</li>
<li>Authentication: none</li>
</ul>
<p>Will soon have a cron job update when new dump is available every Sunday, but for now we&rsquo;re about a month behind the current dump of identifiers. If usage of the API picks up, I&rsquo;ll know it&rsquo;s worth maintaining and make sure data is up to date.</p>
<p>The base url is <a href="https://gbids.xyz">https://gbids.xyz</a></p>
<p>Here&rsquo;s a rundown of the API routes:</p>
<ul>
<li><code>/</code> re-routes to <code>/heartbeat</code></li>
<li><code>/heartbeat</code> - list routes</li>
<li><code>/acc</code> - <code>GET</code> - list accession ids</li>
<li><code>/acc/:id,:id,...</code> - <code>GET</code> - submit many accession numbers, get back boolean (match or no match)</li>
<li><code>/acc</code> - <code>POST</code></li>
<li><code>/gi</code> - <code>GET</code> - list gi numbers</li>
<li><code>/gi/:id,:id,...</code> - <code>GET</code> - submit many gi numbers, get back boolean (match or no match)</li>
<li><code>/gi</code> - <code>POST</code></li>
<li><code>/acc2gi/:id,:id,...</code> - <code>GET</code> - get gi numbers from accession numbers</li>
<li><code>/acc2gi</code> - <code>POST</code></li>
<li><code>/gi2acc/:id,:id,...</code> - <code>GET</code> - get accession numbers from gi numbers</li>
<li><code>/gi2acc</code> - <code>POST</code></li>
</ul>
<p>Of course after GI identifiers are phased out, all <code>gi</code> routes will be removed.</p>
<p>The API docs are at <a href="https://recology.info/gbidsdocs">recology.info/gbidsdocs</a> - let me know if you have any feedback on those.</p>
<p>I also have a status page up at <a href="https://recology.info/gbidsstatus/">recology.info/gbidsstatus</a> - it&rsquo;s not automated, I have to update the status manually, but I do update that page whenever I&rsquo;m doing maintenance and the API will be down, or if it goes down due to any other reason.</p>
<h2 id="examples">examples</h2>
<p>Request to list accession identifiers, limit to 5</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl <span style="color:#e6db74">&#39;https://gbids.xyz/acc?limit=5&#39;</span> | jq .
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;matched&#34;</span>: 692006925,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;returned&#34;</span>: 5,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;data&#34;</span>: <span style="color:#f92672">[</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;A00002&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;A00003&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;X17276&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;X60065&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;CAA42669&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">]</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;error&#34;</span>: null
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>Request to match accession identifiers, some exist, while some do not</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl <span style="color:#e6db74">&#39;https://gbids.xyz/acc/AACY024124486,AACY024124483,asdfd,asdf,AACY024124476&#39;</span> | jq .
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;matched&#34;</span>: 3,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;returned&#34;</span>: 5,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;data&#34;</span>: <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;AACY024124486&#34;</span>: true,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;AACY024124483&#34;</span>: true,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;asdfd&#34;</span>: false,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;asdf&#34;</span>: false,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;AACY024124476&#34;</span>: true
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">}</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;error&#34;</span>: null
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><h2 id="to-do">to do</h2>
<p>I think it&rsquo;d probably be worth adding routes for getting accession from taxonomy id and vice versa since that&rsquo;s something that is currently not very easy. We could use the dumped accession2taxid data from <a href="ftp://ftp.ncbi.nih.gov/pub/taxonomy/accession2taxid/">ftp://ftp.ncbi.nih.gov/pub/taxonomy/accession2taxid/</a></p>
<h2 id="feedback">feedback!</h2>
<p>If you think this could be potentially useful, do try it out and send any feedback. I watch logs from the API, so I&rsquo;m hoping for an increase in usage so I know people find it useful.</p>
<p>Since servers aren&rsquo;t free, costs add up, and if I don&rsquo;t see usage pick up I&rsquo;ll discontinue the service at some point. So do use it!</p>
<p>And if anyone can offer free servers, I&rsquo;d gladly take advantage of that. I&rsquo;ve applied for AWS research grant, but won&rsquo;t hear back for a few months.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>

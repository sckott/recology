<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Database on Recology</title>
    <link>http://localhost:1313/tags/database/</link>
    <description>Recent content in Database on Recology</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/database/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>stories behind archived packages</title>
      <link>http://localhost:1313/2020/09/archived-pkgs/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2020/09/archived-pkgs/</guid>
      <description>Update on 2021-02-09: I&amp;rsquo;ve archived 8 more packages. Post below updated
Code is often arranged in packages for any given language. Packages are often cataloged in a package registry of some kind: NPM for node, crates.io for Rust, etc. For R, that registry is either CRAN or Bioconductor (for the most part).
CRAN has the concept of an archived package. That is, the namespace for a package (foo) is still in the registry (and can not be used again), but the package is archived - no longer gets updated and checks I think are no longer performed.</description>
    </item>
    <item>
      <title>taxizedb: an update</title>
      <link>http://localhost:1313/2020/08/taxizedb-update/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2020/08/taxizedb-update/</guid>
      <description>taxizedb arose from pain in using taxize when dealing with large amounts of data in a single request or doing a lot of requests of any data size. taxize works with remote data sources on the web, so there&amp;rsquo;s a number of issues that can slow the response down: internet speed, server response speed (was a response already cached or not; or do they even use caching), etc.
The idea with taxizedb was to allow users to do the same things as taxize allows, but much faster by accessing the entire database for a data source on their own computer.</description>
    </item>
    <item>
      <title>Faster solr with csv</title>
      <link>http://localhost:1313/2015/03/faster-solr/</link>
      <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2015/03/faster-solr/</guid>
      <description>With the help of user input, I&amp;rsquo;ve tweaked solr just a bit to make things faster using default setings. I imagine the main interface for people using the solr R client is via solr_search(), which used to have wt=json by default. Changing this to wt=csv gives better performance. And it sorta makes sense to use csv, as the point of using an R client is probably do get data eventually into a data.</description>
    </item>
    <item>
      <title>Intro to alpha ckanr - R client for CKAN RESTful API</title>
      <link>http://localhost:1313/2014/11/ckanr-intro/</link>
      <pubDate>Wed, 26 Nov 2014 11:42:36 +0000</pubDate>
      <guid>http://localhost:1313/2014/11/ckanr-intro/</guid>
      <description>Recently I had need to create a client for scraping museum metadata to help out some folks that use that kind of data. It&amp;rsquo;s called musemeta. One of the data sources in that package uses the open source data portal software CKAN, and so we can interact with the CKAN API to get data. Since many groups can use CKAN API/etc infrastucture because it&amp;rsquo;s open source, I thought why not have a general purpose R client for this, since there are other clients for Python, PHP, Ruby, etc.</description>
    </item>
    <item>
      <title>sofa - reboot</title>
      <link>http://localhost:1313/2014/11/sofa/</link>
      <pubDate>Tue, 18 Nov 2014 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2014/11/sofa/</guid>
      <description>I&amp;rsquo;ve reworked sofa recently after someone reported a bug in the package. Since the last post on this package on 2013-06-21, there&amp;rsquo;s a bunch of changes:
Removed the sofa_ prefix from all functions as it wasn&amp;rsquo;t really necessary. Replaced rjson/RJSONIO with jsonlite for JSON I/O. New functions: revisions() - to get the revision numbers for a document. uuids() - get any number of UUIDs - e.g., if you want to set document IDs with UUIDs Most functions that deal with documents are prefixed with doc_ Functions that deal with databases are prefixed with db_ Simplified all code, reducing duplication All functions take cushion as the first parameter, for consistency sake.</description>
    </item>
    <item>
      <title>Stashing and playing with raw data locally from the web</title>
      <link>http://localhost:1313/2013/06/couch/</link>
      <pubDate>Mon, 17 Jun 2013 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2013/06/couch/</guid>
      <description>It is getting easier to get data directly into R from the web. Often R packages that retrieve data from the web return useful R data structures to users like a data.frame. This is a good thing of course to make things user friendly.
However, what if you want to drill down into the data that&amp;rsquo;s returned from a query to a database in R? What if you want to get that nice data.</description>
    </item>
  </channel>
</rss>

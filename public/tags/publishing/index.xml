<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Publishing on Recology</title>
    <link>http://localhost:1313/tags/publishing/</link>
    <description>Recent content in Publishing on Recology</description>
    <generator>Hugo -- 0.139.3</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 25 Feb 2012 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/publishing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Journal Articles Need Interactive Graphics</title>
      <link>http://localhost:1313/2012/02/science-publications-need-interactive-graphics/</link>
      <pubDate>Sat, 25 Feb 2012 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2012/02/science-publications-need-interactive-graphics/</guid>
      <description>&lt;p&gt;I should have thought of it earlier: In a day and age when we are increasingly reading scientific literature on computer screens, why is it that we limit our peer-reviewed data representation to static, unchanging graphs and plots? Why do we not try to create dynamic visualizations of our rich and varied data sets? Would we not derive benefits in the quality and clarity of scientific discourse from publishing these visualizations?&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>I should have thought of it earlier: In a day and age when we are increasingly reading scientific literature on computer screens, why is it that we limit our peer-reviewed data representation to static, unchanging graphs and plots? Why do we not try to create dynamic visualizations of our rich and varied data sets? Would we not derive benefits in the quality and clarity of scientific discourse from publishing these visualizations?</p>
<p>An article in the very good (and under-appreciated, in my opinion) <em><a href="http://www.americanscientist.org/">American Scientist</a></em> magazine written by Brian Hayes started me thinking about these questions.  <a href="http://www.americanscientist.org/issues/pub/pixels-or-perish">&ldquo;Pixels or Perish&rdquo;</a> begins by recapping the evolution of graphics in scientific publications and notes that before people were good at making plots digitally, they were good at making figures from using photographic techniques; and before that, from elaborate engravings.  Clearly, the state-of-the-art in scientific publishing is a moving target.</p>
<p>Hayes points out that one of the primary advantages of static images is that everyone knows how to use them and that almost no one lacks the tools to view them.  That is, printed images in a magazine or static digital images in the portable document format (pdf) are easily viewed on paper or on a screen and can be readily interpreted by a wide audience.  While I agree that this feature is very important, why have we not, as scientists, moved to the next level?  We do not lack the ability to interpret data&ndash;it is our job to do so&ndash;not to mention that we are some of the heaviest generators of data in the first place.</p>
<p>The obstacles to progress towards interactive data are two-fold.  First, generating dynamic data visualizations is not as easy as generating static plots.  The data visualization tools simply are not as well developed and they do not show up as frequently in the programming environments in which scientists work.  One example Hayes cites is that the ideas from programs such as <a href="http://vis.stanford.edu/files/2011-D3-InfoVis.pdf">D^3</a> have not yet made an appearance in software, like <a href="http://www.r-project.org/">R</a> and <a href="http://www.mathworks.com/products/matlab/">Matlab</a>, that more scientists use. This is one reason why I am so excited by the work that our very own <a href="http://sckott.github.io/recologyabout.html">Scott</a> has been doing with this <a href="http://sckott.github.io/">Recology</a> blog, in trying to promote awareness of tools in <a href="http://www.r-project.org/">R</a>.</p>
<p>The second is that neither of our currently dominant publishing formats (physical paper and digital pdf files) support dynamic graphics. Hayes says it better than I could: &ldquo;…the Web is not where scientists publish…[publications are]…available <em>through</em> the Web, not <em>on</em> the Web.&rdquo;  So, not many current publications really take advantage of the new capabilities that the Web has offered us to showcase dynamic data sets.  In fact, while <a href="http://www.sciencemag.org/">Science</a> and <a href="http://wwww.nature.com">Nature</a>&ndash;just to name two prominent examples of scientific journals&ndash;make available HTML versions of their articles, it seems like most of the interactivity is limited to looking at larger versions of figures in the articles*.  I myself usually just download the pdf version of articles rather than viewing the HTML version.  This obstacle, however, is not a fundamental one; it is only the current situation.</p>
<p>The more serious obstacle that Hayes foresees in transitioning to dynamic graphics is one of archiving. Figures in journal articles printed in 1900 are still readable today, but there is no guarantee that a particular file format will survive in usable form to 2100, or even 2020.  I do not know the answer to this conundrum.  A balance might need to be struck between generating static and dynamic data.  At least in the medium term, papers should probably also contain static versions of figures representing dynamic data sets. It is inelegant, but it could avoid the situation where we lose access to information that was once there.</p>
<p>That said, if the <a href="http://www.nytimes.com">New York Times</a> can do it, so can we.  We should not wait to make our data presentation more dynamic and interactive.  At first, it will be difficult to incorporate these kinds of figures into the articles themselves, and they will likely be relegated to the &ldquo;supplemental material&rdquo; dead zone that is infrequently viewed.  But the more dynamic material that journals receive from authors, the more incentive they will have to expand upon their current offerings.  Ultimately, doing so will greatly improve the quality of scientific discourse.</p>

<small>* Whether the lack of dynamic data visualization on these journals' websites is due to the authors not submitting such material or due to restrictions from the journals themselves, I do not know. I suspect the burden falls more on the authors' shoulders at this point than the journals'.</small>

]]></content:encoded>
    </item>
    <item>
      <title>Taking a Closer Look at Peer Review</title>
      <link>http://localhost:1313/2012/01/reviewing-peer-review-process/</link>
      <pubDate>Mon, 16 Jan 2012 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2012/01/reviewing-peer-review-process/</guid>
      <description>&lt;p&gt;This post is only tangentially about open science.  It is more directly about the process of peer review and how it might be improved.  I am working on a follow-up post about how these points can be addressed in an open publishing environment.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;http://arxiv.org/abs/1110.0791&#34;&gt;recent paper on the arXiv&lt;/a&gt; got me thinking about the sticking points in the publishing pipeline.  As it stands, most scientists have a pretty good understanding of how peer reviewed publishing is supposed to work.  Once an author—or more likely, a group of authors—decides that a manuscript is ready for action, the following series of events will occur:&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>This post is only tangentially about open science.  It is more directly about the process of peer review and how it might be improved.  I am working on a follow-up post about how these points can be addressed in an open publishing environment.</p>
<p>A <a href="http://arxiv.org/abs/1110.0791">recent paper on the arXiv</a> got me thinking about the sticking points in the publishing pipeline.  As it stands, most scientists have a pretty good understanding of how peer reviewed publishing is supposed to work.  Once an author—or more likely, a group of authors—decides that a manuscript is ready for action, the following series of events will occur:</p>
<ol>
<li>the authors submit the manuscript to the journal of choice;</li>
<li>the journal&rsquo;s editor makes a general decision about whether the article is appropriate for the journal;</li>
<li>in the affirmative case, the editor selects referees for the manuscript and sends them the text for review;</li>
<li>the referees return reviews of the manuscript (the referees are not typically identified to the authors);</li>
<li>the editor makes the decision to reject the manuscript, accept it with minor revisions, or accept it with major revisions.  Rejected manuscripts usually start over the process in another journal.  Minor revisions to accepted manuscripts are usually made quickly and publication proceeds.  In the case of major revisions, the suggested changes are made, if possible, and the manuscript is returned to the editor.  At this point, the referees may get a second crack at the material (but not necessarily), before the editor makes a final accept/reject decision based on the feedback from the referees.</li>
</ol>
<p>Peer review of manuscripts exists for several reasons.  For one, self-regulation determines the suitability of the material for publication if it was not already obvious to the editor of the journal.  Having peer reviewers also improves the material and its presentation.  Furthermore, having expert reviewers lends credibility to the work and insures that misleading, wrong, or crackpot material does not receive the stamp of credibility.  Finally, finding appropriately skilled referees spreads the workload beyond the editors, who may not have the resources to evaluate every paper arriving at their desk.</p>
<p>Though peer review has a storied history, it also has its drawbacks.  First, and perhaps foremost, the process is often a slow one, with many months elapsing during even one round of communications between the authors, the editor, and the referees.  Peer review is not always an objective process either: referees have the power to delay, or outright reject, work that their competitors have completed, and thus they may lose their impartiality in the process.  Additionally, the publishing process does not reveal the feedback process that occurs between authors and referees, which can be a scientifically and pedagogically valuable exchange.</p>
<p><a href="http://arxiv.org/abs/1110.0791">One proposal</a> to address the shortcomings of the peer review process (alluded to in the first paragraph) was posted by Sergey Bozhevolnyi on the <a href="http://arxiv.org/">arXiv</a>, a pre-publication website for many physics-related manuscripts.  Bozhevolnyi calls his model of publishing Rapid, Impartial, and Comprehensive (RIC) publishing.  To him, &ldquo;rapid&rdquo; means that editors should approve or reject manuscripts before the manuscripts are sent to the referees for review.  Then, &ldquo;impartial&rdquo; means that referees, who might otherwise have an interest in rejecting a perfectly fine paper, lose the power to dictate whether or not a manuscript is published.  Instead, the referees critique the paper without assessing whether it is publication-worthy.  Lastly, &ldquo;comprehensive&rdquo; involves publishing everything having to do with the manuscript.  That is, all positive and negative reviews are published in conjunction with the all versions of a manuscript.</p>
<p>The primary benefit of RIC, according to Bozhevolnyi, is that it saves the energies of authors, editors, and referees, thus allowing them all to do more research and less wrangling.  Since most papers are ultimately accepted somewhere, then we should not cause additional delays in publishing by first rejecting them in multiple places.  Instead, collate the manuscript and the reviews and publish them all together, along with any revisions to the manuscript.  Having the reviews be publicly viewable will encourage referees to be more careful about writing their critiques and supporting their assertions, and the process as a whole will be more transparent than it currently is.</p>
<p>Before I critique the RIC publishing proposal, I should point out that some aspects of the proposal are very appealing.  I particularly like the idea of publishing all reviews in addition to the manuscript.  That said, I find it difficult to believe that the incentives for authors and referees change for the better under this proposal.  For example, what happens if authors receive feedback, do not wish to invest the time to address the critique, and subsequently allow the original manuscript and the reviews to stand as they are?  This situation seems like a moral hazard for authors that does a disservice to the quality of scientific literature.  On the part of the referees, does removing decision-making authority make reviewing less appealing?  Disempowering the referees by potentially ignoring their critique and only counting it as a minor part of the publishing process will not motivate them to write better reviews.  In the case of editors, what makes us believe that an editor, or an editorial board, has the background to properly evaluate the suitability of work for acceptance into a journal?  The reason we have referees in the current peer review system is because they have the very expertise and familiarity needed for this task.</p>
<p>Does the fact that Bozhevolnyi&rsquo;s RIC proposal does not make sense mean that peer review is fine as it is?  I do not think so.  Instead, it is worth asking what parts of peer review we like and what parts we would like to improve.  I posit that rejection, or the threat of rejection, is the greatest motivator for authors to make necessary changes to their manuscript.  As such, rejection by peers is still the best way to require and receive revisions.  Though I think that referees should retain their rejecting power (and their anonymity!), I feel strongly that the entire peer review process would benefit from the increased transparency and accountability that publishing unsigned reviews would add.  As far as editors, they play a role in shaping the kind of journal they run by selecting appropriate material on a general level, but they should not play too large a role in determining the &ldquo;important&rdquo; research in any field.  The model used by the journal [Public Library of Science One][] is a promising one in this regard, with the only acceptance criterion being whether the science is sound.</p>
<p>The amount of time that it takes to publish is one of the most frustrating aspects of peer review, however.  Journals could voluntarily publish time-to-publication figures, a number which could then be used by authors—along with impact factors and acceptance rates—to decide which journals to submit to.  For instance, an editor of the Journal of Orthodontics writes about just this fact in <a href="http://jorthod.maneyjournals.org/content/29/3/171.full">an editorial</a>.  A Google search for &ldquo;journal time to publication&rdquo; reveals that people have been thinking about this problem for a while (e.g. <a href="http://www.hutter1.net/journals.htm">computer science comparisons</a>), but no general standard exists across journals.  In fact, I suspect these are numbers most journals are afraid will hurt them more than help them.  Nevertheless, journals acknowledge the demand for rapid publication when they offer services like [Springer&rsquo;s Fast Track publishing<a href="http://www.springer.com/societies+%26+publishing+partners/society+%26+partner+zone?SGWID=0-173202-12-772912-0">fasttrackpub</a> or <a href="http://pra.aps.org/highlighting-rapids">Physical Review&rsquo;s Rapid Communications</a>.</p>
<p>Ultimately, it may not matter what journals do because authors are routing around this problem via pre-publication archives such as the <a href="http://arxiv.org/">arXiv</a> for physics-related subject matter.  Though not without complications, especially in the health sciences (see, for example, <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0010782">&ldquo;The Promise and Perils of Pre-Publication Review&rdquo;</a>), pre-publication allows authors to communicate results and establish priority without stressing about getting through the peer review process as fast as possible.  Instead, the process takes its normal, slower course while authors move along their on-going research.</p>
<p>I will conclude by leaving an open question that I may address in a future post:  how do you encourage peer reviewers to do the best possible job, in a timely manner, without only relying on their altruism to doing good science and being good members of the community?  It is this question about peer review, I feel, that is the most fraught with complication and subject to the law of unintended consequences if the incentives are changed.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
